# Seguimiento 2: An√°lisis de Grafos de Art√≠culos Cient√≠ficos

## Objetivo

Construir y analizar un **grafo dirigido ponderado** de art√≠culos cient√≠ficos donde los nodos representan art√≠culos y las aristas representan relaciones de similitud basadas en autores compartidos, keywords comunes y similitud sem√°ntica de t√≠tulos usando SBERT.

## Descripci√≥n General

Este m√≥dulo implementa un sistema completo de an√°lisis de grafos aplicado a literatura cient√≠fica, incluyendo:
- Construcci√≥n de grafo basado en m√∫ltiples criterios de similitud
- Algoritmo de Dijkstra para caminos m√≠nimos
- Algoritmo de Kosaraju para componentes fuertemente conexos
- Visualizaci√≥n de subgrafos

---

# Documentaci√≥n T√©cnica

## 1. Lectura y parsing del archivo `.bib`

* Se cargaron los art√≠culos desde un archivo BibTeX (`consolidado.bib`).
* Se extrajeron los siguientes campos por art√≠culo:
  * `title` ‚Üí para identificar el art√≠culo y calcular similitud de t√≠tulos.
  * `author` ‚Üí para comparar coincidencias de autores.
  * `keywords` ‚Üí para medir similitud tem√°tica.
  * `year` ‚Üí opcional, √∫til para an√°lisis temporal.
* Los art√≠culos se almacenaron como  **lista de diccionarios** .

**Limitante de prueba:**

* Inicialmente se tom√≥ un **subconjunto de los primeros 100 art√≠culos** para pruebas de desarrollo y optimizaci√≥n de algoritmos.

## 2. Nodos del grafo

* Cada art√≠culo corresponde a un  **nodo** .
* Identificador del nodo:
  * Se usa un nombre gen√©rico: `"Articulo_0"`, `"Articulo_1"`, etc.
  * Si existiera DOI √∫nico, podr√≠a usarse como ID.
* Informaci√≥n almacenada en el nodo:
  * `title` (t√≠tulo original)
  * `author_list` (lista de autores separados)
  * `keywords_list` (lista de palabras clave)
  * `year` (a√±o de publicaci√≥n)

## 3. Aristas del grafo

* Como el `.bib`  **no tiene citaciones expl√≠citas** , las relaciones se infieren.
* **Reglas para definir aristas (peso)** :

1. **Coincidencia de autores** : +1 por cada autor en com√∫n.
2. **Coincidencia de palabras clave** : +1 por cada palabra clave compartida.
3. **Similitud de t√≠tulos** :
   * Se calcul√≥ usando  **SBERT (`all-MiniLM-L6-v2`)** .
   * Similitud de coseno ‚â• 0.5 ‚Üí +1 al peso (puede sumarse proporcionalmente si se desea).

* **Direcci√≥n de la arista:** arbitraria, siempre de `i ‚Üí j` seg√∫n el orden de los nodos.
* **Arista solo se crea si el peso total > 0** .
* **Limitante de prueba:**
  * Para evitar que SBERT demorara demasiado, se tom√≥ un **subconjunto de los primeros 500 art√≠culos** para calcular embeddings y similitudes.

## 4. Pesos de aristas

* El peso refleja la ‚Äúfuerza‚Äù de la relaci√≥n:
  * +1 por autor en com√∫n
  * +1 por cada palabra clave compartida
  * +1 por similitud alta de t√≠tulo
* Esto permite diferenciar relaciones  **fuertes vs d√©biles** .

## 5. Construcci√≥n del grafo

* Se cre√≥ un diccionario `grafo_dict`:

<pre class="overflow-visible!" data-start="2340" data-end="2416"><div class="contain-inline-size rounded-2xl relative bg-token-sidebar-surface-primary"><div class="sticky top-9"><div class="absolute end-0 bottom-0 flex h-9 items-center pe-2"><div class="bg-token-bg-elevated-secondary text-token-text-secondary flex items-center gap-4 rounded-sm px-2 font-sans text-xs"></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="whitespace-pre! language-python"><span><span>grafo_dict = {
    nodo_id: {vecino_id: peso, ...}, 
    ...
}
</span></span></code></div></div></pre>

* Conserva  **direcci√≥n y peso de aristas** .
* Permite aplicar algoritmos cl√°sicos de grafos como:
  * Caminos m√≠nimos
  * Detecci√≥n de componentes conexas

**Tama√±o real en datos completos:**

* Nodos: 10,226
* Aristas: 49,662

## 6. An√°lisis de la red

### a) Caminos m√≠nimos

* Se implement√≥ **Dijkstra manual** para calcular rutas de menor peso entre dos art√≠culos.
* Esto permite ver c√≥mo se conecta un art√≠culo con otro a trav√©s de relaciones tem√°ticas o de autor√≠a.

### b) Componentes fuertemente conexos

* Algoritmo  **Kosaraju** :
  * Cada componente incluye nodos donde todos se pueden alcanzar entre s√≠ siguiendo la direcci√≥n de aristas


## Dependencias principales

* bibtexparser==1.4.3
* sentence-transformers==3.3.1
* torch==2.6.0
* networkx==3.5
* matplotlib==3.10.5
* numpy==2.2.6
* transformers==4.48.3
* tqdm==4.67.1

### Para instalar todas las dependencias:

```bash
cd /home/yep/Documentos/proyectoAnalisisAlgoritmos
pip install -r requirements.txt
```

---

# Gu√≠a de Uso

## Ejecuci√≥n del Notebook

```bash
jupyter notebook sr1.ipynb
```

## Configuraci√≥n de Par√°metros

### 1. Tama√±o de Muestra

```python
# N√∫mero de art√≠culos a procesar (para pruebas)
max_nodos = 500  # Reducir para pruebas r√°pidas

# Para procesar todos los art√≠culos
max_nodos = len(nodos)  # ~10,000 art√≠culos
```

**‚ö†Ô∏è Importante**: 
- Con 500 art√≠culos: ~10 segundos para embeddings
- Con 10,000 art√≠culos: ~10-15 minutos para embeddings

### 2. Umbrales de Similitud

```python
# Ajustar seg√∫n necesidad de conectividad
umbral_sbert = 0.5           # Similitud de t√≠tulos (0.0-1.0)
min_shared_keywords = 2      # M√≠nimo de keywords compartidas
min_shared_authors = 1       # M√≠nimo de autores compartidos
```

**Recomendaciones**:
- **Grafo denso** (muchas conexiones): Bajar umbrales
- **Grafo disperso** (pocas conexiones): Subir umbrales
- **Balance**: `umbral_sbert=0.5, keywords=2, authors=1`

### 3. Modelo SBERT

```python
# Modelo por defecto (recomendado)
model = SentenceTransformer('all-MiniLM-L6-v2')

# Alternativas:
# model = SentenceTransformer('all-mpnet-base-v2')  # M√°s preciso, m√°s lento
# model = SentenceTransformer('paraphrase-MiniLM-L3-v2')  # M√°s r√°pido, menos preciso
```

## Estructura del C√≥digo

### Flujo de Ejecuci√≥n

```
1. Cargar art√≠culos desde .bib
   ‚Üì
2. Crear nodos del grafo
   ‚Üì
3. Preprocesar campos (autores, keywords, t√≠tulos)
   ‚Üì
4. Generar embeddings con SBERT
   ‚Üì
5. Calcular matriz de similitud
   ‚Üì
6. Crear aristas seg√∫n reglas
   ‚Üì
7. Construir grafo con NetworkX
   ‚Üì
8. Filtrar componente m√°s grande
   ‚Üì
9. Visualizar subgrafo
   ‚Üì
10. Ejecutar Dijkstra
   ‚Üì
11. Ejecutar Kosaraju
```

## An√°lisis de Resultados

### Interpretaci√≥n del Grafo

```python
print(f"Nodos: {G.number_of_nodes()}")
print(f"Aristas: {G.number_of_edges()}")
print(f"Densidad: {nx.density(G):.4f}")
```

**M√©tricas clave**:
- **Densidad**: Proporci√≥n de aristas existentes vs posibles
  - `< 0.1`: Grafo disperso
  - `0.1-0.3`: Densidad media
  - `> 0.3`: Grafo denso

### Componentes Conexos

```python
# N√∫mero de componentes d√©biles
weak_components = list(nx.weakly_connected_components(G))
print(f"Componentes d√©biles: {len(weak_components)}")

# Tama√±o del componente m√°s grande
largest = max(weak_components, key=len)
print(f"Componente m√°s grande: {len(largest)} nodos")
```

**Interpretaci√≥n**:
- **Muchos componentes peque√±os**: Art√≠culos muy especializados
- **Un componente grande**: Tema bien conectado
- **Varios componentes medianos**: Subtemas diferenciados

### Caminos M√≠nimos (Dijkstra)

```python
# Ejemplo de uso
inicio = 'Articulo_0'
destino = 'Articulo_100'

distancias, previos = dijkstra_manual(grafo_dict, inicio)
camino = reconstruir_camino(previos, inicio, destino)

print(f"Camino: {' ‚Üí '.join(camino)}")
print(f"Costo total: {distancias[destino]:.2f}")
```

**Interpretaci√≥n del costo**:
- **Costo bajo (< 5)**: Art√≠culos muy relacionados
- **Costo medio (5-10)**: Relacionados indirectamente
- **Costo alto (> 10)**: Poca relaci√≥n tem√°tica

### Componentes Fuertemente Conexos (Kosaraju)

```python
scc = componentes_fuertemente_conexos(grafo_dict)
print(f"Componentes fuertemente conexas: {len(scc)}")

# Componentes grandes (> 1 art√≠culo)
grandes = [c for c in scc if len(c) > 1]
print(f"Clusters tem√°ticos: {len(grandes)}")
```

**Interpretaci√≥n**:
- **Muchas componentes individuales**: Art√≠culos √∫nicos
- **Pocas componentes grandes**: Temas bien definidos
- **Componente gigante**: Core del tema de investigaci√≥n

## Optimizaci√≥n y Rendimiento

### Para Datasets Grandes (> 5,000 art√≠culos)

#### 1. Procesar en Lotes

```python
BATCH_SIZE = 1000

for i in range(0, len(titulos), BATCH_SIZE):
    batch = titulos[i:i+BATCH_SIZE]
    batch_embeddings = model.encode(batch, batch_size=64)
    # Procesar batch
```

#### 2. Usar GPU (si disponible)

```python
import torch

# Verificar GPU
if torch.cuda.is_available():
    print(f"GPU disponible: {torch.cuda.get_device_name(0)}")
    model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')
else:
    print("Usando CPU")
    model = SentenceTransformer('all-MiniLM-L6-v2')
```

#### 3. Reducir Dimensionalidad

```python
# Usar modelo m√°s peque√±o
model = SentenceTransformer('paraphrase-MiniLM-L3-v2')  # 384 ‚Üí 128 dims
```

#### 4. Filtrar Art√≠culos

```python
# Solo art√≠culos recientes
articulos_filtrados = [a for a in articulos if int(a.get('year', 0)) >= 2020]
```

## Problemas Comunes y Soluciones

### 1. Error: "CUDA out of memory"

**Causa**: GPU sin suficiente memoria

**Soluci√≥n**:
```python
# Reducir batch size
embeddings = model.encode(titulos, batch_size=16)  # Default: 32

# O usar CPU
model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')
```

### 2. Warning: "Grafo vac√≠o o sin aristas"

**Causa**: Umbrales muy altos

**Soluci√≥n**:
```python
# Bajar umbrales
umbral_sbert = 0.3  # En lugar de 0.5
min_shared_keywords = 1  # En lugar de 2
```

### 3. Error: "No hay camino entre nodos"

**Causa**: Nodos en componentes diferentes

**Soluci√≥n**:
```python
# Verificar conectividad
if nx.has_path(G, inicio, destino):
    camino = nx.shortest_path(G, inicio, destino)
else:
    print("Nodos no conectados")
```

### 4. Visualizaci√≥n muy densa

**Causa**: Demasiados nodos/aristas

**Soluci√≥n**:
```python
# Reducir n√∫mero de nodos a visualizar
subgrafo_nodos = list(G.nodes())[:10]  # Solo 10 nodos
G_sub = G.subgraph(subgrafo_nodos)
```

## Tiempo de Ejecuci√≥n

| Operaci√≥n | 500 art√≠culos | 5,000 art√≠culos | 10,000 art√≠culos |
|-----------|---------------|-----------------|------------------|
| Carga de datos | 1s | 5s | 10s |
| Preprocesamiento | 2s | 15s | 30s |
| Embeddings SBERT (CPU) | 10s | 2min | 5min |
| Embeddings SBERT (GPU) | 3s | 20s | 45s |
| C√°lculo de similitud | 5s | 3min | 15min |
| Creaci√≥n de aristas | 10s | 5min | 20min |
| Construcci√≥n grafo | 1s | 5s | 10s |
| Dijkstra | <1s | 1s | 2s |
| Kosaraju | <1s | 2s | 5s |
| **Total (CPU)** | **30s** | **11min** | **41min** |
| **Total (GPU)** | **23s** | **9min** | **36min** |

## Archivos Generados

### Datos
- `primeros_500.bib` - Subconjunto de art√≠culos (generado por `scrip.py`)
- `grafo_dict.pkl` - Grafo serializado (opcional)
- `embeddings.npy` - Embeddings guardados (opcional)

### Visualizaciones
- Gr√°ficos de NetworkX en el notebook
- Subgrafos visualizados

### Logs
- Salida de Dijkstra con caminos
- Estad√≠sticas de componentes conexos

## Casos de Uso

### 1. Encontrar Art√≠culos Relacionados

```python
# Dado un art√≠culo, encontrar los m√°s similares
articulo_origen = 'Articulo_0'

# Calcular distancias desde el origen
distancias, _ = dijkstra_manual(grafo_dict, articulo_origen)

# Ordenar por distancia
relacionados = sorted(distancias.items(), key=lambda x: x[1])[:10]

print("Art√≠culos m√°s relacionados:")
for art, dist in relacionados[1:]:  # Excluir el mismo
    print(f"{art}: {dist:.2f}")
```

### 2. Identificar Clusters Tem√°ticos

```python
# Usar componentes fuertemente conexas
scc = componentes_fuertemente_conexos(grafo_dict)

# Analizar cada cluster
for i, cluster in enumerate(scc):
    if len(cluster) > 5:  # Solo clusters grandes
        print(f"\nCluster {i}: {len(cluster)} art√≠culos")
        
        # Extraer t√≠tulos del cluster
        titulos_cluster = [nodos[art]['title'] for art in cluster]
        print(titulos_cluster[:3])  # Primeros 3
```

### 3. An√°lisis de Centralidad

```python
# Art√≠culos m√°s "centrales" o importantes
import networkx as nx

# Centralidad de grado
degree_centrality = nx.degree_centrality(G)
top_degree = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]

print("Art√≠culos m√°s conectados:")
for art, score in top_degree:
    print(f"{art}: {score:.3f}")

# Centralidad de intermediaci√≥n
betweenness = nx.betweenness_centrality(G)
top_between = sorted(betweenness.items(), key=lambda x: x[1], reverse=True)[:10]

print("\nArt√≠culos m√°s 'puente':")
for art, score in top_between:
    print(f"{art}: {score:.3f}")
```

## Extensiones Posibles

### 1. An√°lisis Temporal

```python
# Evoluci√≥n del grafo por a√±o
for year in range(2020, 2025):
    articulos_a√±o = [a for a in articulos if a.get('year') == str(year)]
    # Construir grafo para ese a√±o
    # Analizar m√©tricas
```

### 2. Detecci√≥n de Comunidades

```python
from networkx.algorithms import community

# Louvain algorithm
communities = community.louvain_communities(G.to_undirected())

print(f"Comunidades detectadas: {len(communities)}")
for i, comm in enumerate(communities):
    print(f"Comunidad {i}: {len(comm)} art√≠culos")
```

### 3. An√°lisis de Influencia

```python
# PageRank para identificar art√≠culos influyentes
pagerank = nx.pagerank(G)
top_influential = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)[:10]

print("Art√≠culos m√°s influyentes:")
for art, score in top_influential:
    print(f"{art}: {score:.4f}")
```

## Notas Importantes

### ‚ö†Ô∏è Limitaciones

1. **Memoria**: Matriz de similitud requiere O(n¬≤) espacio
2. **Tiempo**: SBERT es el cuello de botella principal
3. **Calidad**: Depende de la calidad de los metadatos (autores, keywords)
4. **Direccionalidad**: El grafo es dirigido pero las relaciones son sim√©tricas

### ‚úÖ Buenas Pr√°cticas

1. **Empezar con muestra peque√±a** (500 art√≠culos) para ajustar par√°metros
2. **Guardar embeddings** para evitar recalcular
3. **Usar cache** para resultados intermedios
4. **Validar manualmente** algunos caminos y clusters
5. **Documentar umbrales** usados para reproducibilidad

### üîß Mantenimiento

- **Actualizar modelo SBERT** peri√≥dicamente
- **Revisar umbrales** si cambia el dataset
- **Limpiar datos** antes de procesar (duplicados, campos vac√≠os)

## Referencias

- **NetworkX**: https://networkx.org/
- **Sentence-BERT**: https://www.sbert.net/
- **Algoritmo de Dijkstra**: https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm
- **Algoritmo de Kosaraju**: https://en.wikipedia.org/wiki/Kosaraju%27s_algorithm

## Aplicaciones

‚úÖ **Sistemas de recomendaci√≥n**: Sugerir art√≠culos relacionados
‚úÖ **An√°lisis bibliom√©trico**: Identificar temas y tendencias
‚úÖ **Detecci√≥n de plagio**: Encontrar art√≠culos muy similares
‚úÖ **Mapeo de conocimiento**: Visualizar estructura de un campo
‚úÖ **Identificaci√≥n de gaps**: √Åreas poco conectadas
prueba.