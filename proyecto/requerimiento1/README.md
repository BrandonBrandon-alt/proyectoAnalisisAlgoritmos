# Requerimiento 1: ConsolidaciÃ³n y Limpieza de Datos BibTeX

## Objetivo

Consolidar mÃºltiples archivos BibTeX descargados de diferentes fuentes (IEEE, ScienceDirect, Taylor & Francis) en un Ãºnico archivo limpio, eliminando duplicados y normalizando los campos.

## DescripciÃ³n del Proceso

### 1. Lectura de Archivos BibTeX

El sistema lee todos los archivos `.bib` de las carpetas:
- `descargas/ieee/`
- `descargas/science/`
- `descargas/taylor/`

**LibrerÃ­a utilizada**: `pybtex`
- MÃ¡s robusta que `bibtexparser` para archivos grandes
- Mejor manejo de caracteres especiales
- Soporte para mÃºltiples formatos de entrada

### 2. DetecciÃ³n de Duplicados

Se implementan **tres mÃ©todos** de detecciÃ³n:

#### MÃ©todo 1: Por DOI
```python
# ComparaciÃ³n exacta de DOI
if doi1 == doi2:
    # Es duplicado
```
- **Ventaja**: 100% preciso
- **LimitaciÃ³n**: No todos los artÃ­culos tienen DOI

#### MÃ©todo 2: Por TÃ­tulo (Hash MD5)
```python
# NormalizaciÃ³n y hash del tÃ­tulo
titulo_normalizado = titulo.lower().strip()
hash_titulo = hashlib.md5(titulo_normalizado.encode()).hexdigest()
```
- **Ventaja**: RÃ¡pido y eficiente
- **LimitaciÃ³n**: Sensible a pequeÃ±as variaciones

#### MÃ©todo 3: Por Similitud de TÃ­tulo
```python
# Usando natsort para ordenamiento natural
# ComparaciÃ³n de tÃ­tulos similares
```
- **Ventaja**: Detecta variaciones menores
- **LimitaciÃ³n**: MÃ¡s lento

### 3. NormalizaciÃ³n de Campos

Se estandarizan los siguientes campos:

| Campo Original | Campo Normalizado | AcciÃ³n |
|---------------|-------------------|--------|
| `author` | `author` | Formato: "Apellido, Nombre" |
| `title` | `title` | Eliminar caracteres especiales |
| `year` | `year` | Validar rango 1900-2025 |
| `doi` | `doi` | Formato estÃ¡ndar |
| `abstract` | `abstract` | Limpiar espacios |
| `keywords` | `keywords` | Separar por punto y coma |

### 4. GeneraciÃ³n de Salida

**Archivo generado**: `consolidado.bib`

CaracterÃ­sticas:
- Formato BibTeX estÃ¡ndar
- Ordenado alfabÃ©ticamente por tÃ­tulo
- Sin duplicados
- Campos normalizados

## Estructura del CÃ³digo

```python
# 1. Importar librerÃ­as
import pybtex
from natsort import natsorted
import hashlib

# 2. Leer archivos BibTeX
def process_bibtex_file(file_path):
    # Leer y parsear archivo
    
# 3. Detectar duplicados
def detect_duplicates(entries):
    # Aplicar mÃ©todos de detecciÃ³n
    
# 4. Normalizar campos
def normalize_entry(entry):
    # Estandarizar formato
    
# 5. Escribir archivo consolidado
def write_consolidated_bib(entries, output_path):
    # Generar archivo final
```

## Uso

### EjecuciÃ³n del Notebook

```bash
jupyter notebook Requerimiento1.ipynb
```

### ParÃ¡metros Configurables

```python
# Rutas de entrada
INPUT_FOLDERS = [
    "../descargas/ieee/",
    "../descargas/science/",
    "../descargas/taylor/"
]

# Ruta de salida
OUTPUT_FILE = "../consolidado.bib"

# Umbral de similitud para tÃ­tulos
SIMILARITY_THRESHOLD = 0.9
```

## Resultados Esperados

### EstadÃ­sticas TÃ­picas

```
ğŸ“Š EstadÃ­sticas de ConsolidaciÃ³n:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Archivos procesados:        45
Entradas totales leÃ­das:    12,500
Duplicados detectados:      2,274
Entradas Ãºnicas:            10,226
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Duplicados por mÃ©todo:
  - Por DOI:                1,850
  - Por hash de tÃ­tulo:     324
  - Por similitud:          100
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

### Validaciones

El sistema verifica:
- âœ… Todos los artÃ­culos tienen tÃ­tulo
- âœ… Todos los artÃ­culos tienen autor
- âœ… Los aÃ±os estÃ¡n en rango vÃ¡lido
- âœ… No hay duplicados en el archivo final
- âœ… Formato BibTeX vÃ¡lido

## Dependencias

```python
pybtex==0.24.0
natsort==8.4.0
```

## Problemas Comunes y Soluciones

### Error: "Unable to parse BibTeX file"

**Causa**: Archivo BibTeX corrupto o con formato invÃ¡lido

**SoluciÃ³n**:
```python
# Usar modo de recuperaciÃ³n
parser = bibtex_input.Parser()
parser.parse_file(file_path, bib_format='bibtex')
```

### Warning: "Duplicate entry detected"

**Causa**: Normal, es parte del proceso de limpieza

**AcciÃ³n**: Ninguna, el sistema maneja automÃ¡ticamente

### Error: "Memory error"

**Causa**: Demasiados archivos para procesar simultÃ¡neamente

**SoluciÃ³n**:
```python
# Procesar en lotes
BATCH_SIZE = 10
for batch in chunks(files, BATCH_SIZE):
    process_batch(batch)
```

## Salidas Generadas

### Archivo Principal
- `consolidado.bib` - Base de datos consolidada

### Archivos de Log (opcionales)
- `duplicates_log.txt` - Lista de duplicados detectados
- `normalization_log.txt` - Cambios aplicados
- `errors_log.txt` - Errores encontrados

## MÃ©tricas de Calidad

El sistema reporta:
- **Tasa de duplicaciÃ³n**: % de entradas duplicadas
- **Cobertura de campos**: % de artÃ­culos con cada campo
- **Integridad de datos**: % de campos completos

## Tiempo de EjecuciÃ³n

| Cantidad de Archivos | Tiempo Estimado |
|---------------------|-----------------|
| 10 archivos         | ~30 segundos    |
| 50 archivos         | ~2 minutos      |
| 100 archivos        | ~5 minutos      |

## Notas Importantes

âš ï¸ **Antes de ejecutar**:
- Verificar que existan las carpetas de entrada
- Asegurar espacio en disco (mÃ­nimo 100 MB)
- Cerrar el archivo de salida si estÃ¡ abierto

âœ… **DespuÃ©s de ejecutar**:
- Verificar el archivo `consolidado.bib`
- Revisar las estadÃ­sticas de consolidaciÃ³n
- Validar que no haya errores en el log

## PrÃ³ximos Pasos

Una vez generado `consolidado.bib`, se puede proceder con:
- **Requerimiento 2**: AnÃ¡lisis de similitud
- **Requerimiento 3**: AnÃ¡lisis de frecuencias
- **Requerimiento 4**: Clustering
- **Requerimiento 5**: AnÃ¡lisis geogrÃ¡fico
