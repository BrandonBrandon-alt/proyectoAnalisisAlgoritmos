{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "271a3161",
   "metadata": {},
   "source": [
    "## Requerimiento 1: Consolidación y Limpieza de Datos BibTeX\n",
    "\n",
    "Este notebook implementa un sistema robusto de consolidación de archivos BibTeX de múltiples fuentes, con detección de duplicados y manejo de errores.\n",
    "\n",
    "### Objetivos:\n",
    "1. Buscar recursivamente archivos `.bib` en directorios\n",
    "2. Verificar unicidad de archivos por contenido (hash MD5)\n",
    "3. Detectar duplicados por título normalizado\n",
    "4. Consolidar entradas únicas en un solo archivo\n",
    "5. Exportar duplicados para auditoría\n",
    "6. Manejo robusto de errores en parsing\n",
    "\n",
    "### Flujo del Proceso:\n",
    "```\n",
    "Búsqueda Recursiva → Verificación de Hash → Ordenamiento Natural → \n",
    "Parsing con Entorno Limpio → Normalización de Títulos → \n",
    "Detección de Duplicados → Consolidación → Exportación\n",
    "```\n",
    "\n",
    "### Características Principales:\n",
    "\n",
    "#### Detección de Duplicados:\n",
    "- **Por contenido**: Hash MD5 de archivos\n",
    "- **Por título**: Normalización inteligente (sin puntuación, minúsculas)\n",
    "- **Registro detallado**: Origen de cada duplicado\n",
    "\n",
    "#### Manejo de Errores:\n",
    "- **Entorno limpio**: Directorio temporal por archivo\n",
    "- **Try-catch**: Captura errores de parsing\n",
    "- **Continuación**: Procesa archivos restantes si uno falla\n",
    "\n",
    "#### Estadísticas:\n",
    "- Archivos procesados\n",
    "- Entradas únicas consolidadas\n",
    "- Duplicados detectados\n",
    "- Errores encontrados\n",
    "\n",
    "### Tecnologías Utilizadas:\n",
    "- **pybtex**: Parsing y escritura de BibTeX\n",
    "- **hashlib**: Cálculo de hash MD5\n",
    "- **natsort**: Ordenamiento natural de archivos\n",
    "- **tempfile**: Directorios temporales\n",
    "- **re**: Normalización con regex\n",
    "\n",
    "### Estructura de Datos:\n",
    "\n",
    "#### Entrada:\n",
    "```\n",
    "descargas/\n",
    "├── ieee/\n",
    "│   ├── ieee_generative_ai_page_1.bib\n",
    "│   ├── ieee_generative_ai_page_2.bib\n",
    "│   └── ...\n",
    "├── sciencedirect/\n",
    "│   ├── sciencedirect_page_1.bib\n",
    "│   └── ...\n",
    "└── springer/\n",
    "    └── springer_page_1.bib\n",
    "```\n",
    "\n",
    "#### Salida:\n",
    "```\n",
    "proyecto/salidas/consolidado.bib    # Archivo consolidado\n",
    "duplicados/duplicados.bib            # Duplicados detectados\n",
    "```\n",
    "\n",
    "### Variables de Entorno:\n",
    "```python\n",
    "DOWNLOAD_PATH   # Ruta de archivos .bib a procesar\n",
    "SALIDA_PATH     # Ruta del archivo consolidado\n",
    "DUPLICATE_PATH  # Ruta del archivo de duplicados\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425e0d9d",
   "metadata": {},
   "source": [
    "### Implementación del Sistema de Consolidación\n",
    "\n",
    "Este script implementa 4 funciones principales para consolidar archivos BibTeX de forma robusta.\n",
    "\n",
    "## Función 1: `process_bibtex_file_with_clean_environment(file_path)`\n",
    "\n",
    "### Propósito:\n",
    "Procesa un archivo BibTeX en un entorno aislado para evitar problemas de caché del parser.\n",
    "\n",
    "### Problema que Resuelve:\n",
    "```python\n",
    "# Sin entorno limpio:\n",
    "parser = bibtex_input.Parser()\n",
    "bib1 = parser.parse_file(\"file1.bib\")  # OK\n",
    "bib2 = parser.parse_file(\"file2.bib\")  # Puede usar caché de file1\n",
    "```\n",
    "\n",
    "### Solución:\n",
    "```python\n",
    "# Con entorno limpio:\n",
    "temp_dir = tempfile.mkdtemp()           # Directorio temporal único\n",
    "temp_file = os.path.join(temp_dir, \"temp_file.bib\")\n",
    "shutil.copy2(file_path, temp_file)      # Copia a temp\n",
    "parser = bibtex_input.Parser()          # Parser nuevo\n",
    "bib_data = parser.parse_file(temp_file) # Parsing limpio\n",
    "shutil.rmtree(temp_dir)                 # Limpieza\n",
    "```\n",
    "\n",
    "### Ventajas:\n",
    "- Evita conflictos de caché\n",
    "- Cada archivo se procesa independientemente\n",
    "- Limpieza automática con `finally`\n",
    "\n",
    "## Función 2: `normalize_title(title)`\n",
    "\n",
    "### Propósito:\n",
    "Normaliza títulos para comparación robusta de duplicados.\n",
    "\n",
    "### Transformaciones:\n",
    "\n",
    "#### 1. Eliminación de Caracteres Especiales:\n",
    "```python\n",
    "re.sub(r'[^\\w\\s]', '', title.lower())\n",
    "```\n",
    "**Ejemplo**:\n",
    "```\n",
    "\"Machine Learning: A Survey (2024)\" \n",
    "→ \"machine learning a survey 2024\"\n",
    "```\n",
    "\n",
    "#### 2. Eliminación de Espacios Múltiples:\n",
    "```python\n",
    "re.sub(r'\\s+', ' ', normalized).strip()\n",
    "```\n",
    "\n",
    "#### 3. Conversión a Minúsculas:\n",
    "```python\n",
    ".lower()\n",
    "```\n",
    "\n",
    "### Casos de Uso:\n",
    "\n",
    "| Título Original 1 | Título Original 2 | Normalizado | Duplicado |\n",
    "|-------------------|-------------------|-------------|-----------|\n",
    "| \"AI: A Survey\" | \"AI - A Survey\" | \"ai a survey\" | Sí |\n",
    "| \"Machine Learning\" | \"Machine  Learning\" | \"machine learning\" | Sí |\n",
    "| \"Deep Learning\" | \"Deep Learning.\" | \"deep learning\" | Sí |\n",
    "| \"AI Survey\" | \"ML Survey\" | diferentes | No |\n",
    "\n",
    "### Limitaciones:\n",
    "- No detecta sinónimos (\"car\" vs \"automobile\")\n",
    "- No detecta variaciones (\"Part I\" vs \"Part II\")\n",
    "- Sensible a palabras adicionales\n",
    "\n",
    "## Función 3: `merge_bibtex_files(file_paths, output_path, duplicates_path)`\n",
    "\n",
    "### Propósito:\n",
    "Función principal que consolida múltiples archivos BibTeX.\n",
    "\n",
    "### Algoritmo:\n",
    "\n",
    "#### Paso 1: Inicialización\n",
    "```python\n",
    "merged_db = BibliographyData()      # Base de datos consolidada\n",
    "processed_titles = {}                # Diccionario de títulos vistos\n",
    "duplicate_entries = set()            # Set de IDs duplicados\n",
    "```\n",
    "\n",
    "#### Paso 2: Procesamiento por Archivo\n",
    "```python\n",
    "for file_path in file_paths:\n",
    "    bib_data = process_bibtex_file_with_clean_environment(file_path)\n",
    "```\n",
    "\n",
    "#### Paso 3: Procesamiento por Entrada\n",
    "```python\n",
    "for entry_id, entry in bib_data.entries.items():\n",
    "    # 1. Verificar si tiene título\n",
    "    if 'title' not in entry.fields:\n",
    "        merged_db.add_entry(entry_id, entry)\n",
    "        continue\n",
    "    \n",
    "    # 2. Normalizar título\n",
    "    title = entry.fields['title']\n",
    "    normalized_title = normalize_title(title)\n",
    "    \n",
    "    # 3. Verificar duplicados\n",
    "    if normalized_title in processed_titles:\n",
    "        # Es duplicado → guardar en lista\n",
    "        duplicate_entries.add(entry_id)\n",
    "        duplicates_list.append(entry_text)\n",
    "    else:\n",
    "        # Es único → agregar a consolidado\n",
    "        merged_db.add_entry(entry_id, entry)\n",
    "        processed_titles[normalized_title] = (entry_id, file_path)\n",
    "```\n",
    "\n",
    "#### Paso 4: Exportación\n",
    "```python\n",
    "writer = bibtex_output.Writer()\n",
    "writer.write_file(merged_db, output_path)           # Consolidado\n",
    "with open(duplicates_path, \"w\") as f:\n",
    "    f.writelines(duplicates_list)                    # Duplicados\n",
    "```\n",
    "\n",
    "### Manejo de Errores:\n",
    "```python\n",
    "try:\n",
    "    bib_data = process_bibtex_file_with_clean_environment(file_path)\n",
    "    # ... procesamiento ...\n",
    "except Exception as e:\n",
    "    print(f\"Error al procesar {file_path}: {e}\")\n",
    "    # Continúa con el siguiente archivo\n",
    "```\n",
    "\n",
    "**Errores comunes**:\n",
    "- `repeated bibliography entry`: Entrada duplicada dentro del mismo archivo\n",
    "- `syntax error`: Formato BibTeX inválido\n",
    "- `FileNotFoundError`: Archivo no existe\n",
    "\n",
    "## Función 4: `main()`\n",
    "\n",
    "### Propósito:\n",
    "Función principal que orquesta todo el proceso.\n",
    "\n",
    "### Flujo Detallado:\n",
    "\n",
    "#### 1. Búsqueda Recursiva de Archivos:\n",
    "```python\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.bib'):\n",
    "            bibtex_files.append(os.path.join(root, file))\n",
    "```\n",
    "\n",
    "#### 2. Ordenamiento Natural:\n",
    "```python\n",
    "bibtex_files = natsorted(bibtex_files)\n",
    "```\n",
    "\n",
    "**Diferencia**:\n",
    "```\n",
    "# Ordenamiento estándar:\n",
    "['page_1.bib', 'page_10.bib', 'page_2.bib']\n",
    "\n",
    "# Ordenamiento natural (natsorted):\n",
    "['page_1.bib', 'page_2.bib', 'page_10.bib']\n",
    "```\n",
    "\n",
    "#### 3. Verificación de Unicidad por Hash:\n",
    "```python\n",
    "file_hash = hashlib.md5()\n",
    "with open(file_path, 'rb') as f:\n",
    "    for chunk in iter(lambda: f.read(4096), b''):\n",
    "        file_hash.update(chunk)\n",
    "digest = file_hash.hexdigest()\n",
    "```\n",
    "\n",
    "**Ventajas del hash MD5**:\n",
    "- Detecta archivos idénticos con nombres diferentes\n",
    "- Rápido (lectura por chunks de 4KB)\n",
    "- Único por contenido\n",
    "\n",
    "#### 4. Consolidación:\n",
    "```python\n",
    "unique_count, duplicate_count = merge_bibtex_files(\n",
    "    unique_files, \n",
    "    output_path, \n",
    "    duplicates_path\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3fb6d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orden de procesamiento de archivos:\n",
      "1. /home/nop/Documentos/proyectoAnalisisAlgoritmos/descargas/sciencedirect/sciencedirect_page_4.bib\n",
      "2. /home/nop/Documentos/proyectoAnalisisAlgoritmos/descargas/sciencedirect/sciencedirect_page_5.bib\n",
      "3. /home/nop/Documentos/proyectoAnalisisAlgoritmos/descargas/springer/springer_page_1.bib\n",
      "\n",
      "Verificando unicidad de archivos...\n",
      "Total de archivos encontrados: 3\n",
      "Archivos únicos por contenido: 3\n",
      "\n",
      "Procesando: /home/nop/Documentos/proyectoAnalisisAlgoritmos/descargas/sciencedirect/sciencedirect_page_4.bib\n",
      "\n",
      "Procesando: /home/nop/Documentos/proyectoAnalisisAlgoritmos/descargas/sciencedirect/sciencedirect_page_5.bib\n",
      "\n",
      "Procesando: /home/nop/Documentos/proyectoAnalisisAlgoritmos/descargas/springer/springer_page_1.bib\n",
      "Error al procesar /home/nop/Documentos/proyectoAnalisisAlgoritmos/descargas/springer/springer_page_1.bib: syntax error in line 140: '=' expected\n",
      "Archivo consolidado guardado en: /home/nop/Documentos/proyectoAnalisisAlgoritmos/proyecto/salidas/consolidado.bib\n",
      "No se encontraron duplicados.\n",
      "\n",
      "Resumen:\n",
      "  Archivos procesados: 3\n",
      "  Entradas únicas: 50\n",
      "  Entradas duplicadas: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import tempfile\n",
    "import shutil\n",
    "import re\n",
    "from pybtex.database.input import bibtex as bibtex_input\n",
    "from pybtex.database.output import bibtex as bibtex_output\n",
    "from pybtex.database import BibliographyData\n",
    "from natsort import natsorted\n",
    "\n",
    "def process_bibtex_file_with_clean_environment(file_path):\n",
    "    \"\"\"Procesa un archivo BibTeX con un entorno limpio para evitar problemas de caché.\"\"\"\n",
    "    # Crear directorio temporal\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    try:\n",
    "        # Copiar el archivo a un directorio temporal con un nombre único\n",
    "        temp_file = os.path.join(temp_dir, f\"temp_{os.path.basename(file_path)}\")\n",
    "        shutil.copy2(file_path, temp_file)\n",
    "        \n",
    "        # Usar un nuevo parser para cada archivo\n",
    "        parser = bibtex_input.Parser()\n",
    "        bib_data = parser.parse_file(temp_file)\n",
    "        \n",
    "        return bib_data\n",
    "    finally:\n",
    "        # Limpiar el directorio temporal\n",
    "        shutil.rmtree(temp_dir)\n",
    "\n",
    "def normalize_title(title):\n",
    "    \"\"\"Normaliza un título para facilitar la comparación.\n",
    "    Elimina espacios extra, signos de puntuación y convierte a minúsculas.\"\"\"\n",
    "    if not title:\n",
    "        return \"\"\n",
    "    # Eliminar caracteres especiales y convertir a minúsculas\n",
    "    normalized = re.sub(r'[^\\w\\s]', '', title.lower())\n",
    "    # Eliminar espacios múltiples y convertir a minúsculas\n",
    "    normalized = re.sub(r'\\s+', ' ', normalized).strip().lower()\n",
    "    return normalized\n",
    "\n",
    "def merge_bibtex_files(file_paths, output_path, duplicates_path):\n",
    "    merged_db = BibliographyData()\n",
    "    duplicates_list = []\n",
    "    processed_titles = {}  # Cambiado de processed_ids a processed_titles\n",
    "    duplicate_entries = set()  # Para contar entradas duplicadas únicas\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            print(f\"\\nProcesando: {file_path}\")\n",
    "            bib_data = process_bibtex_file_with_clean_environment(file_path)\n",
    "\n",
    "            for entry_id, entry in bib_data.entries.items():\n",
    "                # Verificar si el entry tiene un campo de título\n",
    "                if 'title' not in entry.fields:\n",
    "                    print(f\"  Advertencia: Entrada {entry_id} sin título en {file_path}, se agregará como única\")\n",
    "                    merged_db.add_entry(entry_id, entry)\n",
    "                    continue\n",
    "                \n",
    "                # Normalizar el título para comparación\n",
    "                title = entry.fields['title']\n",
    "                normalized_title = normalize_title(title)\n",
    "                \n",
    "                if normalized_title in processed_titles:\n",
    "                    # Encontramos un título duplicado\n",
    "                    original_entry_id, original_file = processed_titles[normalized_title]\n",
    "                    print(f\"  Duplicado encontrado por título: {title}\")\n",
    "                    print(f\"  Original ID: {original_entry_id} en: {original_file}\")\n",
    "                    print(f\"  Duplicado ID: {entry_id} en: {file_path}\")\n",
    "                    \n",
    "                    # Agregar el ID a la lista de duplicados únicos\n",
    "                    duplicate_entries.add(entry_id)\n",
    "\n",
    "                    # Guardar duplicado como texto en la lista\n",
    "                    duplicates_list.append(f\"@{entry.type}{{{entry_id},\\n\")\n",
    "                    duplicates_list.append(f\"  title = {{{title}}},\\n\")\n",
    "                    for field, value in entry.fields.items():\n",
    "                        if field != 'title':  # Ya agregamos el título\n",
    "                            duplicates_list.append(f\"  {field} = {{{value}}},\\n\")\n",
    "                    duplicates_list.append(\"}\\n\\n\")\n",
    "                    \n",
    "                else:\n",
    "                    # Es un título nuevo, lo agregamos\n",
    "                    merged_db.add_entry(entry_id, entry)\n",
    "                    processed_titles[normalized_title] = (entry_id, file_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error al procesar {file_path}: {e}\")\n",
    "\n",
    "    # Crear directorios de salida si no existen\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(duplicates_path), exist_ok=True)\n",
    "    \n",
    "    # Guardar el archivo consolidado\n",
    "    writer = bibtex_output.Writer()\n",
    "    writer.write_file(merged_db, output_path)\n",
    "    print(f\"Archivo consolidado guardado en: {output_path}\")\n",
    "\n",
    "    # Guardar el archivo de duplicados manualmente\n",
    "    if duplicates_list:\n",
    "        with open(duplicates_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.writelines(duplicates_list)\n",
    "        print(f\"Duplicados guardados en: {duplicates_path}\")\n",
    "    else:\n",
    "        print(\"No se encontraron duplicados.\")\n",
    "    \n",
    "    return len(merged_db.entries), len(duplicate_entries)\n",
    "\n",
    "def main():\n",
    "    folder_path = os.getenv(\"DOWNLOAD_PATH\") \n",
    "\n",
    "    # Buscar archivos .bib recursivamente en todas las subcarpetas\n",
    "    bibtex_files = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.bib'):\n",
    "                full_path = os.path.join(root, file)\n",
    "                bibtex_files.append(full_path)\n",
    "    \n",
    "    # Ordenamos usando natsorted para lograr un \"orden natural\"\n",
    "    bibtex_files = natsorted(bibtex_files)\n",
    "\n",
    "    # Verificar en que orden se procesaran los archivos\n",
    "    print(\"Orden de procesamiento de archivos:\")\n",
    "    for i, f in enumerate(bibtex_files):\n",
    "        print(f\"{i+1}. {f}\")\n",
    "\n",
    "    # Verificar que los archivos sean únicos\n",
    "    print(\"\\nVerificando unicidad de archivos...\")\n",
    "    file_hashes = {}\n",
    "    unique_files = []\n",
    "    \n",
    "    for file_path in bibtex_files:\n",
    "        # Calcular hash del archivo\n",
    "        file_hash = hashlib.md5()\n",
    "        with open(file_path, 'rb') as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b''):\n",
    "                file_hash.update(chunk)\n",
    "        \n",
    "        # Verificar si ya hemos visto este hash\n",
    "        digest = file_hash.hexdigest()\n",
    "        if digest in file_hashes:\n",
    "            print(f\"¡ADVERTENCIA! Archivo duplicado detectado:\")\n",
    "            print(f\"  - {file_path}\")\n",
    "            print(f\"  - {file_hashes[digest]}\")\n",
    "            print(f\"  Ambos tienen el mismo hash: {digest}\")\n",
    "        else:\n",
    "            file_hashes[digest] = file_path\n",
    "            unique_files.append(file_path)\n",
    "    \n",
    "    print(f\"Total de archivos encontrados: {len(bibtex_files)}\")\n",
    "    print(f\"Archivos únicos por contenido: {len(unique_files)}\")\n",
    "    \n",
    "    # Proceder solo con archivos únicos\n",
    "    bibtex_files = unique_files\n",
    "\n",
    "    # Rutas para archivos de salida\n",
    "    output_path = os.path.join( os.getenv(\"SALIDA_PATH\") , \"consolidado.bib\")\n",
    "    duplicates_path = os.path.join( os.getenv(\"DUPLICATE_PATH\"), \"duplicados.bib\")\n",
    "    \n",
    "    unique_count, duplicate_count = merge_bibtex_files(bibtex_files, output_path, duplicates_path)\n",
    "    \n",
    "    print(f\"\\nResumen:\")\n",
    "    print(f\"  Archivos procesados: {len(bibtex_files)}\")\n",
    "    print(f\"  Entradas únicas: {unique_count}\")\n",
    "    print(f\"  Entradas duplicadas: {duplicate_count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
