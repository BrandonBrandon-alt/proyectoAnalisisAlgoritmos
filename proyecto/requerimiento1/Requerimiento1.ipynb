{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "271a3161",
   "metadata": {},
   "source": [
    "## Limpieza de archivos BibTeX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3fb6d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yep/Documentos/proyectoAnalisisAlgoritmos/venv/lib64/python3.13/site-packages/pybtex/plugin/__init__.py:26: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orden de procesamiento de archivos:\n",
      "1. /home/yep/Documentos/proyectoAnalisisAlgoritmos/descargas/sciencedirect/sciencedirect_page_1.bib\n",
      "2. /home/yep/Documentos/proyectoAnalisisAlgoritmos/descargas/sciencedirect/sciencedirect_page_2.bib\n",
      "3. /home/yep/Documentos/proyectoAnalisisAlgoritmos/descargas/springer/springer_page_1.bib\n",
      "4. /home/yep/Documentos/proyectoAnalisisAlgoritmos/descargas/springer/springer_page_2.bib\n",
      "5. /home/yep/Documentos/proyectoAnalisisAlgoritmos/descargas/springer/springer_page_3.bib\n",
      "\n",
      "Verificando unicidad de archivos...\n",
      "Total de archivos encontrados: 5\n",
      "Archivos únicos por contenido: 5\n",
      "\n",
      "Procesando: /home/yep/Documentos/proyectoAnalisisAlgoritmos/descargas/sciencedirect/sciencedirect_page_1.bib\n",
      "\n",
      "Procesando: /home/yep/Documentos/proyectoAnalisisAlgoritmos/descargas/sciencedirect/sciencedirect_page_2.bib\n",
      "\n",
      "Procesando: /home/yep/Documentos/proyectoAnalisisAlgoritmos/descargas/springer/springer_page_1.bib\n",
      "Error al procesar /home/yep/Documentos/proyectoAnalisisAlgoritmos/descargas/springer/springer_page_1.bib: syntax error in line 140: '=' expected\n",
      "\n",
      "Procesando: /home/yep/Documentos/proyectoAnalisisAlgoritmos/descargas/springer/springer_page_2.bib\n",
      "\n",
      "Procesando: /home/yep/Documentos/proyectoAnalisisAlgoritmos/descargas/springer/springer_page_3.bib\n",
      "Archivo consolidado guardado en: /home/yep/Documentos/proyectoAnalisisAlgoritmos/proyecto/salidas/consolidado.bib\n",
      "No se encontraron duplicados.\n",
      "\n",
      "Resumen:\n",
      "  Archivos procesados: 5\n",
      "  Entradas únicas: 220\n",
      "  Entradas duplicadas: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import tempfile\n",
    "import shutil\n",
    "import re\n",
    "from pybtex.database.input import bibtex as bibtex_input\n",
    "from pybtex.database.output import bibtex as bibtex_output\n",
    "from pybtex.database import BibliographyData\n",
    "from natsort import natsorted\n",
    "\n",
    "def process_bibtex_file_with_clean_environment(file_path):\n",
    "    \"\"\"Procesa un archivo BibTeX con un entorno limpio para evitar problemas de caché.\"\"\"\n",
    "    # Crear directorio temporal\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    try:\n",
    "        # Copiar el archivo a un directorio temporal con un nombre único\n",
    "        temp_file = os.path.join(temp_dir, f\"temp_{os.path.basename(file_path)}\")\n",
    "        shutil.copy2(file_path, temp_file)\n",
    "        \n",
    "        # Usar un nuevo parser para cada archivo\n",
    "        parser = bibtex_input.Parser()\n",
    "        bib_data = parser.parse_file(temp_file)\n",
    "        \n",
    "        return bib_data\n",
    "    finally:\n",
    "        # Limpiar el directorio temporal\n",
    "        shutil.rmtree(temp_dir)\n",
    "\n",
    "def normalize_title(title):\n",
    "    \"\"\"Normaliza un título para facilitar la comparación.\n",
    "    Elimina espacios extra, signos de puntuación y convierte a minúsculas.\"\"\"\n",
    "    if not title:\n",
    "        return \"\"\n",
    "    # Eliminar caracteres especiales y convertir a minúsculas\n",
    "    normalized = re.sub(r'[^\\w\\s]', '', title.lower())\n",
    "    # Eliminar espacios múltiples y convertir a minúsculas\n",
    "    normalized = re.sub(r'\\s+', ' ', normalized).strip().lower()\n",
    "    return normalized\n",
    "\n",
    "def merge_bibtex_files(file_paths, output_path, duplicates_path):\n",
    "    merged_db = BibliographyData()\n",
    "    duplicates_list = []\n",
    "    processed_titles = {}  # Cambiado de processed_ids a processed_titles\n",
    "    duplicate_entries = set()  # Para contar entradas duplicadas únicas\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            print(f\"\\nProcesando: {file_path}\")\n",
    "            bib_data = process_bibtex_file_with_clean_environment(file_path)\n",
    "\n",
    "            for entry_id, entry in bib_data.entries.items():\n",
    "                # Verificar si el entry tiene un campo de título\n",
    "                if 'title' not in entry.fields:\n",
    "                    print(f\"  Advertencia: Entrada {entry_id} sin título en {file_path}, se agregará como única\")\n",
    "                    merged_db.add_entry(entry_id, entry)\n",
    "                    continue\n",
    "                \n",
    "                # Normalizar el título para comparación\n",
    "                title = entry.fields['title']\n",
    "                normalized_title = normalize_title(title)\n",
    "                \n",
    "                if normalized_title in processed_titles:\n",
    "                    # Encontramos un título duplicado\n",
    "                    original_entry_id, original_file = processed_titles[normalized_title]\n",
    "                    print(f\"  Duplicado encontrado por título: {title}\")\n",
    "                    print(f\"  Original ID: {original_entry_id} en: {original_file}\")\n",
    "                    print(f\"  Duplicado ID: {entry_id} en: {file_path}\")\n",
    "                    \n",
    "                    # Agregar el ID a la lista de duplicados únicos\n",
    "                    duplicate_entries.add(entry_id)\n",
    "\n",
    "                    # Guardar duplicado como texto en la lista\n",
    "                    duplicates_list.append(f\"@{entry.type}{{{entry_id},\\n\")\n",
    "                    duplicates_list.append(f\"  title = {{{title}}},\\n\")\n",
    "                    for field, value in entry.fields.items():\n",
    "                        if field != 'title':  # Ya agregamos el título\n",
    "                            duplicates_list.append(f\"  {field} = {{{value}}},\\n\")\n",
    "                    duplicates_list.append(\"}\\n\\n\")\n",
    "                    \n",
    "                else:\n",
    "                    # Es un título nuevo, lo agregamos\n",
    "                    merged_db.add_entry(entry_id, entry)\n",
    "                    processed_titles[normalized_title] = (entry_id, file_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error al procesar {file_path}: {e}\")\n",
    "\n",
    "    # Crear directorios de salida si no existen\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(duplicates_path), exist_ok=True)\n",
    "    \n",
    "    # Guardar el archivo consolidado\n",
    "    writer = bibtex_output.Writer()\n",
    "    writer.write_file(merged_db, output_path)\n",
    "    print(f\"Archivo consolidado guardado en: {output_path}\")\n",
    "\n",
    "    # Guardar el archivo de duplicados manualmente\n",
    "    if duplicates_list:\n",
    "        with open(duplicates_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.writelines(duplicates_list)\n",
    "        print(f\"Duplicados guardados en: {duplicates_path}\")\n",
    "    else:\n",
    "        print(\"No se encontraron duplicados.\")\n",
    "    \n",
    "    return len(merged_db.entries), len(duplicate_entries)\n",
    "\n",
    "def main():\n",
    "    folder_path = os.getenv(\"DOWNLOAD_PATH\") \n",
    "\n",
    "    # Buscar archivos .bib recursivamente en todas las subcarpetas\n",
    "    bibtex_files = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.bib'):\n",
    "                full_path = os.path.join(root, file)\n",
    "                bibtex_files.append(full_path)\n",
    "    \n",
    "    # Ordenamos usando natsorted para lograr un \"orden natural\"\n",
    "    bibtex_files = natsorted(bibtex_files)\n",
    "\n",
    "    # Verificar en que orden se procesaran los archivos\n",
    "    print(\"Orden de procesamiento de archivos:\")\n",
    "    for i, f in enumerate(bibtex_files):\n",
    "        print(f\"{i+1}. {f}\")\n",
    "\n",
    "    # Verificar que los archivos sean únicos\n",
    "    print(\"\\nVerificando unicidad de archivos...\")\n",
    "    file_hashes = {}\n",
    "    unique_files = []\n",
    "    \n",
    "    for file_path in bibtex_files:\n",
    "        # Calcular hash del archivo\n",
    "        file_hash = hashlib.md5()\n",
    "        with open(file_path, 'rb') as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b''):\n",
    "                file_hash.update(chunk)\n",
    "        \n",
    "        # Verificar si ya hemos visto este hash\n",
    "        digest = file_hash.hexdigest()\n",
    "        if digest in file_hashes:\n",
    "            print(f\"¡ADVERTENCIA! Archivo duplicado detectado:\")\n",
    "            print(f\"  - {file_path}\")\n",
    "            print(f\"  - {file_hashes[digest]}\")\n",
    "            print(f\"  Ambos tienen el mismo hash: {digest}\")\n",
    "        else:\n",
    "            file_hashes[digest] = file_path\n",
    "            unique_files.append(file_path)\n",
    "    \n",
    "    print(f\"Total de archivos encontrados: {len(bibtex_files)}\")\n",
    "    print(f\"Archivos únicos por contenido: {len(unique_files)}\")\n",
    "    \n",
    "    # Proceder solo con archivos únicos\n",
    "    bibtex_files = unique_files\n",
    "\n",
    "    # Rutas para archivos de salida\n",
    "    output_path = os.path.join( os.getenv(\"SALIDA_PATH\") , \"consolidado.bib\")\n",
    "    duplicates_path = os.path.join( os.getenv(\"DUPLICATE_PATH\"), \"duplicados.bib\")\n",
    "    \n",
    "    unique_count, duplicate_count = merge_bibtex_files(bibtex_files, output_path, duplicates_path)\n",
    "    \n",
    "    print(f\"\\nResumen:\")\n",
    "    print(f\"  Archivos procesados: {len(bibtex_files)}\")\n",
    "    print(f\"  Entradas únicas: {unique_count}\")\n",
    "    print(f\"  Entradas duplicadas: {duplicate_count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
