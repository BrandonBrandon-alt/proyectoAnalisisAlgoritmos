id,title,authors,first_author,doi,year,venue,abstract,keywords,raw_entry
10315744,Do Robots Dream of Passing a Programming Course?,"Torres, Nicolás",Torres,10.1109/SCCC59417.2023.10315744,2023,2023 42nd IEEE International Conference of the Chilean Computer Science Society (SCCC),"Programming typically involves humans formulating instructions for a computer to execute computations. If we adhere to this definition, a machine would seemingly lack the capability to autonomously design algorithms. However, recent generative Artificial Intelligence models, such as GPT, have demonstrated an impressive ability to perform complex human tasks with remarkable precision. In this paper, we initially showcase how an AI model can successfully complete an entire college-level programming course, akin to one of the top-performing students in the class. We then put forward strategies for crafting programming exercises that enable educators to effectively integrate these innovative technologies into their teaching methods. Lastly, we illustrate how these models can transition from being perceived as a potential threat to educators to becoming a valuable opportunity when employed judiciously.",Training;Computational modeling;Instruments;Natural languages;Learning (artificial intelligence);Syntactics;Task analysis;Artificial Intelligence;Neural Networks;Programming;Learning Analysis;Technology-Enhanced Learning,"{'month': 'Oct', 'issn': '2691-0632', 'doi': '10.1109/SCCC59417.2023.10315744', 'keywords': 'Training;Computational modeling;Instruments;Natural languages;Learning (artificial intelligence);Syntactics;Task analysis;Artificial Intelligence;Neural Networks;Programming;Learning Analysis;Technology-Enhanced Learning', 'abstract': 'Programming typically involves humans formulating instructions for a computer to execute computations. If we adhere to this definition, a machine would seemingly lack the capability to autonomously design algorithms. However, recent generative Artificial Intelligence models, such as GPT, have demonstrated an impressive ability to perform complex human tasks with remarkable precision. In this paper, we initially showcase how an AI model can successfully complete an entire college-level programming course, akin to one of the top-performing students in the class. We then put forward strategies for crafting programming exercises that enable educators to effectively integrate these innovative technologies into their teaching methods. Lastly, we illustrate how these models can transition from being perceived as a potential threat to educators to becoming a valuable opportunity when employed judiciously.', 'pages': '1-8', 'number': '', 'volume': '', 'year': '2023', 'title': 'Do Robots Dream of Passing a Programming Course?', 'booktitle': '2023 42nd IEEE International Conference of the Chilean Computer Science Society (SCCC)', 'author': 'Torres, Nicolás', 'ENTRYTYPE': 'inproceedings', 'ID': '10315744'}"
10405724,WeAIR: Wearable Swarm Sensors for Air Quality Monitoring to Foster Citizens' Awareness of Climate Change,"Dimitri, Giovanna Maria and Parri, Lorenzo and Pozzebon, Alessandro and Vitanza, Eleonora and Fort, Ada and Mocenni, Chiara",Dimitri,10.1109/MetroXRAINE58569.2023.10405724,2023,"2023 IEEE International Conference on Metrology for eXtended Reality, Artificial Intelligence and Neural Engineering (MetroXRAINE)","The present study proposes the implementation of an air quality measurement tool through the use of a swarm of wearable devices, named WeAIR, consisting of wearable sensors for measuring NOx, CO2, CO, temperature, humidity, and barometric pressure. Data will be stored and processed in a secure cloud environment. Artificial Intelligence algorithms will be used to visualize and make available real-time predictions of future states of air quality, by reconstructing spatio-temporal dynamical maps collected through the habitual movement of citizens in space and time. Then a pre-processing phase will allow us to appropriately homogenize the collected data, by means of the application of innovative artificial intelligence methods. All the data collected will be embedded into an integrated system with multiple functions. Among them, an important goal will also be to raise citizens' awareness of climate change.",Temperature measurement;Climate change;Cloud computing;Pollution measurement;Reliability;Artificial intelligence;Wearable sensors;climate change;air quality;monitoring devices;health;citizens' science,"{'month': 'Oct', 'issn': '', 'doi': '10.1109/MetroXRAINE58569.2023.10405724', 'keywords': ""Temperature measurement;Climate change;Cloud computing;Pollution measurement;Reliability;Artificial intelligence;Wearable sensors;climate change;air quality;monitoring devices;health;citizens' science"", 'abstract': ""The present study proposes the implementation of an air quality measurement tool through the use of a swarm of wearable devices, named WeAIR, consisting of wearable sensors for measuring NOx, CO2, CO, temperature, humidity, and barometric pressure. Data will be stored and processed in a secure cloud environment. Artificial Intelligence algorithms will be used to visualize and make available real-time predictions of future states of air quality, by reconstructing spatio-temporal dynamical maps collected through the habitual movement of citizens in space and time. Then a pre-processing phase will allow us to appropriately homogenize the collected data, by means of the application of innovative artificial intelligence methods. All the data collected will be embedded into an integrated system with multiple functions. Among them, an important goal will also be to raise citizens' awareness of climate change."", 'pages': '98-103', 'number': '', 'volume': '', 'year': '2023', 'title': ""WeAIR: Wearable Swarm Sensors for Air Quality Monitoring to Foster Citizens' Awareness of Climate Change"", 'booktitle': '2023 IEEE International Conference on Metrology for eXtended Reality, Artificial Intelligence and Neural Engineering (MetroXRAINE)', 'author': 'Dimitri, Giovanna Maria and Parri, Lorenzo and Pozzebon, Alessandro and Vitanza, Eleonora and Fort, Ada and Mocenni, Chiara', 'ENTRYTYPE': 'inproceedings', 'ID': '10405724'}"
11037909,Discriminative-Generative Representation Learning for One-Class Anomaly Detection,"Li, Duanjiao and Chen, Yun and Zhang, Ying and Sun, Wenxing and He, Xing and Tong, Haoran and Ding, Ning and Xia, Xuan",Li,10.1109/CEII65291.2024.00052,2024,2024 7th Asia Conference on Cognitive Engineering and Intelligent lnteraction (CEII),"Generative Adversarial Networks (GANs), as a form of generative self-supervised learning, have garnered significant attention in anomaly detection. However, the generator's capacity for representation learning is constrained due to its excessive focus on pixel-level details, which hinders its ability to effectively learn abstract semantic representations from label prediction pretext tasks compared to the discriminator. To enhance the generator's representation learning capabilities, we introduce a self-supervised learning framework that integrates generative and discriminative approaches. Our proposed discriminative-generative representation learning method not only rivals the performance of discriminative methods but also offers a significant speed advantage. When applied to one-class anomaly detection tasks, our method surpasses several state-of-the-art models on various benchmark datasets, improving upon the top-performing GAN-based baseline by 6\% on CIFAR-10 and 2\% on MVTAD. Furthermore, ablation studies reveal that absolute positional information negatively impacts the representational learning ability of generative methods in geometric transformation tasks, offering a valuable guideline for the utilization of positional information.",Representation learning;Semantics;Asia;Self-supervised learning;Benchmark testing;Generative adversarial networks;Generators;Anomaly detection;Guidelines;self-supervised learning;anomaly detection;discriminative;generative;one-class,"{'month': 'Dec', 'issn': '', 'doi': '10.1109/CEII65291.2024.00052', 'keywords': 'Representation learning;Semantics;Asia;Self-supervised learning;Benchmark testing;Generative adversarial networks;Generators;Anomaly detection;Guidelines;self-supervised learning;anomaly detection;discriminative;generative;one-class', 'abstract': ""Generative Adversarial Networks (GANs), as a form of generative self-supervised learning, have garnered significant attention in anomaly detection. However, the generator's capacity for representation learning is constrained due to its excessive focus on pixel-level details, which hinders its ability to effectively learn abstract semantic representations from label prediction pretext tasks compared to the discriminator. To enhance the generator's representation learning capabilities, we introduce a self-supervised learning framework that integrates generative and discriminative approaches. Our proposed discriminative-generative representation learning method not only rivals the performance of discriminative methods but also offers a significant speed advantage. When applied to one-class anomaly detection tasks, our method surpasses several state-of-the-art models on various benchmark datasets, improving upon the top-performing GAN-based baseline by 6\\% on CIFAR-10 and 2\\% on MVTAD. Furthermore, ablation studies reveal that absolute positional information negatively impacts the representational learning ability of generative methods in geometric transformation tasks, offering a valuable guideline for the utilization of positional information."", 'pages': '227-232', 'number': '', 'volume': '', 'year': '2024', 'title': 'Discriminative-Generative Representation Learning for One-Class Anomaly Detection', 'booktitle': '2024 7th Asia Conference on Cognitive Engineering and Intelligent lnteraction (CEII)', 'author': 'Li, Duanjiao and Chen, Yun and Zhang, Ying and Sun, Wenxing and He, Xing and Tong, Haoran and Ding, Ning and Xia, Xuan', 'ENTRYTYPE': 'inproceedings', 'ID': '11037909'}"
11164582,3 Generative AI Models and LLM: Training Techniques and Evaluation Metrics,"Arun, C. and Karthick, S. and Selvakumara Samy, S. and Hariharan, B. and Lee, Po-Ming",Arun,,2024,Generative AI and LLMs: Natural Language Processing and Generative Adversarial Networks,"Generative artificial intelligence (AI) has been a prominent technique across data-driven applications, which uses deep learning architecture to learn the underlying characteristic of the sample to build the knowledge base in generating synthetic samples that mimic the real distribution. Generative AI models are ideal solutions where models suffer due to scarcity of data sample that hinders the training process be it text, video, audio, and image. Training the model plays a pivotal role, where it discovers the hidden pattern and understands the intrinsic behavior of samples that aid in generating realistic samples. The volume of data that is available for training and the computing power required pose threat on the performance of the intelligent systems, where large language models (LLM) has been an ideal solution. LLMs are generative AI systems that understand human language and provide intelligent, creative solutions to questions. Complex architecture of LLM allows them to capture the intricacies of language more precise, enabling to generate coherent and contextually relevant outputs. This chapter delves into comprehensive analysis on the well-known generative AI models such as generative adversarial networks, transformers, and LangChain. Generative AI employs different training techniques such as reinforcement learning, adversarial training, variational inference, transfer learning, and progressive training on diverse application domains. Furthermore, the study examines the crucial aspect of evaluating the effectiveness of generative models, using a variety of metrics ranging from BLUE, inception score, perplexity, Frechet inception distance, precision, ROUGE, recall, METEOR, BERT, MoverScore, and many more. A comparative analysis of these metrics offers insights into their respective advantages and disadvantages, aiding practitioners and researchers in selecting benchmarks that align with their specific use cases.",,"{'url': 'https://ieeexplore-ieee-org.crai.referencistas.com/document/11164582', 'isbn': '9783111425511', 'publisher': 'De Gruyter', 'issn': '', 'doi': '', 'keywords': '', 'abstract': 'Generative artificial intelligence (AI) has been a prominent technique across data-driven applications, which uses deep learning architecture to learn the underlying characteristic of the sample to build the knowledge base in generating synthetic samples that mimic the real distribution. Generative AI models are ideal solutions where models suffer due to scarcity of data sample that hinders the training process be it text, video, audio, and image. Training the model plays a pivotal role, where it discovers the hidden pattern and understands the intrinsic behavior of samples that aid in generating realistic samples. The volume of data that is available for training and the computing power required pose threat on the performance of the intelligent systems, where large language models (LLM) has been an ideal solution. LLMs are generative AI systems that understand human language and provide intelligent, creative solutions to questions. Complex architecture of LLM allows them to capture the intricacies of language more precise, enabling to generate coherent and contextually relevant outputs. This chapter delves into comprehensive analysis on the well-known generative AI models such as generative adversarial networks, transformers, and LangChain. Generative AI employs different training techniques such as reinforcement learning, adversarial training, variational inference, transfer learning, and progressive training on diverse application domains. Furthermore, the study examines the crucial aspect of evaluating the effectiveness of generative models, using a variety of metrics ranging from BLUE, inception score, perplexity, Frechet inception distance, precision, ROUGE, recall, METEOR, BERT, MoverScore, and many more. A comparative analysis of these metrics offers insights into their respective advantages and disadvantages, aiding practitioners and researchers in selecting benchmarks that align with their specific use cases.', 'pages': '43-68', 'number': '', 'volume': '', 'year': '2024', 'title': '3 Generative AI Models and LLM: Training Techniques and Evaluation Metrics', 'booktitle': 'Generative AI and LLMs: Natural Language Processing and Generative Adversarial Networks', 'author': 'Arun, C. and Karthick, S. and Selvakumara Samy, S. and Hariharan, B. and Lee, Po-Ming', 'ENTRYTYPE': 'inbook', 'ID': '11164582'}"
10305172,Virtual Human: A Comprehensive Survey on Academic and Applications,"Cui, Lipeng and Liu, Jiarui",Cui,10.1109/ACCESS.2023.3329573,2023,IEEE Access,"As a creative method for virtual human individuals based on multiple fusion technologies such as artificial intelligence, computer graphics, and speech synthesis, virtual human technology has developed rapidly since its birth, and continuous discussions and studies have been conducted in both academia and industry. Starting from the film and television industries, the cross-disciplinary application of virtual human has been continuously recognized and applied in fields such as media, games, and finance. Although virtual human has achieved sufficient development and innovation, it faces many challenges such as emotion recognition, privacy, and security, as well as the uncanny valley effect. This article starts with the development history of virtual human and analyzes the current academic research status and application scenarios in combination with the characteristics, technical architecture, and application of virtual human technology. At the same time, this article sorts out seven mainstream application scenarios of virtual human and analyzes their main advantages and possible future challenges. This article provides a valuable reference for subsequent related research by exploring development trends, application fields, and future research trends in virtual human.",Digital humans;Motion capture;Face recognition;Three-dimensional displays;Speech recognition;Solid modeling;Rendering (computer graphics);Artificial intelligence;Deep learning;Machine learning;Human computer interaction;Metaverse;Artificial intelligence;deep learning;machine learning;human-computer interaction;virtual reality;virtual human;metaverse,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2023.3329573', 'keywords': 'Digital humans;Motion capture;Face recognition;Three-dimensional displays;Speech recognition;Solid modeling;Rendering (computer graphics);Artificial intelligence;Deep learning;Machine learning;Human computer interaction;Metaverse;Artificial intelligence;deep learning;machine learning;human-computer interaction;virtual reality;virtual human;metaverse', 'abstract': 'As a creative method for virtual human individuals based on multiple fusion technologies such as artificial intelligence, computer graphics, and speech synthesis, virtual human technology has developed rapidly since its birth, and continuous discussions and studies have been conducted in both academia and industry. Starting from the film and television industries, the cross-disciplinary application of virtual human has been continuously recognized and applied in fields such as media, games, and finance. Although virtual human has achieved sufficient development and innovation, it faces many challenges such as emotion recognition, privacy, and security, as well as the uncanny valley effect. This article starts with the development history of virtual human and analyzes the current academic research status and application scenarios in combination with the characteristics, technical architecture, and application of virtual human technology. At the same time, this article sorts out seven mainstream application scenarios of virtual human and analyzes their main advantages and possible future challenges. This article provides a valuable reference for subsequent related research by exploring development trends, application fields, and future research trends in virtual human.', 'pages': '123830-123845', 'number': '', 'volume': '11', 'year': '2023', 'title': 'Virtual Human: A Comprehensive Survey on Academic and Applications', 'journal': 'IEEE Access', 'author': 'Cui, Lipeng and Liu, Jiarui', 'ENTRYTYPE': 'article', 'ID': '10305172'}"
9339015,Research of Low-light Image Enhancement Method for Enclosed Tank,"Zhang, Xu and Wang, Haipeng and Li, Yunqi and Cui, Qihui and Lyn, Juntao",Zhang,10.1109/ITAIC49862.2020.9339015,2020,2020 IEEE 9th Joint International Information Technology and Artificial Intelligence Conference (ITAIC),"Due to closed environment of gas insulated switchgear (GIS) chamber, low-light images are easily produced when taking pictures under weak lighting conditions. The lost details and low contrast not only cause unpleasant subjective feelings, but also hurt the performance of many computer vision systems which are designed for normal-light images. Thus, a low-light image enhancement algorithm based on improved conditional generative adversarial network (GAN) is proposed here to solve this poor visual perception problem. Decom-Net referenced to RetinexNet used here decompose image into reflection map and illumination map. This study proposed an encode-decode convolutional neural network model as the generative model and a lightweight convolutional neural network(CNN) as the discriminative model. After joint training of generative model and discriminative model, the network outputs the final enhanced images. The experimental results demonstrate that the proposed method is more effective than existing methods in perception.",Training;Lighting;Generative adversarial networks;Reflection;Gas insulation;Convolutional neural networks;Image enhancement;low-light image;convolutional neural network;image enhancement;generative adversarial network,"{'month': 'Dec', 'issn': '2693-2865', 'doi': '10.1109/ITAIC49862.2020.9339015', 'keywords': 'Training;Lighting;Generative adversarial networks;Reflection;Gas insulation;Convolutional neural networks;Image enhancement;low-light image;convolutional neural network;image enhancement;generative adversarial network', 'abstract': 'Due to closed environment of gas insulated switchgear (GIS) chamber, low-light images are easily produced when taking pictures under weak lighting conditions. The lost details and low contrast not only cause unpleasant subjective feelings, but also hurt the performance of many computer vision systems which are designed for normal-light images. Thus, a low-light image enhancement algorithm based on improved conditional generative adversarial network (GAN) is proposed here to solve this poor visual perception problem. Decom-Net referenced to RetinexNet used here decompose image into reflection map and illumination map. This study proposed an encode-decode convolutional neural network model as the generative model and a lightweight convolutional neural network(CNN) as the discriminative model. After joint training of generative model and discriminative model, the network outputs the final enhanced images. The experimental results demonstrate that the proposed method is more effective than existing methods in perception.', 'pages': '991-994', 'number': '', 'volume': '9', 'year': '2020', 'title': 'Research of Low-light Image Enhancement Method for Enclosed Tank', 'booktitle': '2020 IEEE 9th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)', 'author': 'Zhang, Xu and Wang, Haipeng and Li, Yunqi and Cui, Qihui and Lyn, Juntao', 'ENTRYTYPE': 'inproceedings', 'ID': '9339015'}"
9414429,Continuous Face Aging Generative Adversarial Networks,"Jeon, Seogkyu and Lee, Pilhyeon and Hong, Kibeom and Byun, Hyeran",Jeon,10.1109/ICASSP39728.2021.9414429,2021,"ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","Face aging is the task aiming to translate the faces in input images to designated ages. To simplify the problem, previous methods have limited themselves only able to produce discrete age groups, each of which consists of ten years. Consequently, the exact ages of the translated results are unknown and it is unable to obtain the faces of different ages within groups. To this end, we propose the continuous face aging generative adversarial networks (CFA-GAN). Specifically, to make the continuous aging feasible, we propose to decompose image features into two orthogonal features: the identity and the age basis features. Moreover, we introduce the novel loss function for identity preservation which maximizes the cosine similarity between the original and the generated identity basis features. With the qualitative and quantitative evaluations on MORPH, we demonstrate the realistic and continuous aging ability of our model, validating its superiority against existing models. To the best of our knowledge, this work is the first attempt to handle continuous target ages.",Conferences;Aging;Signal processing;Generative adversarial networks;Acoustics;Task analysis;Speech processing;Face aging;Image-to-Image translation;Unsupervised Learning;Generative adversarial networks,"{'month': 'June', 'issn': '2379-190X', 'doi': '10.1109/ICASSP39728.2021.9414429', 'keywords': 'Conferences;Aging;Signal processing;Generative adversarial networks;Acoustics;Task analysis;Speech processing;Face aging;Image-to-Image translation;Unsupervised Learning;Generative adversarial networks', 'abstract': 'Face aging is the task aiming to translate the faces in input images to designated ages. To simplify the problem, previous methods have limited themselves only able to produce discrete age groups, each of which consists of ten years. Consequently, the exact ages of the translated results are unknown and it is unable to obtain the faces of different ages within groups. To this end, we propose the continuous face aging generative adversarial networks (CFA-GAN). Specifically, to make the continuous aging feasible, we propose to decompose image features into two orthogonal features: the identity and the age basis features. Moreover, we introduce the novel loss function for identity preservation which maximizes the cosine similarity between the original and the generated identity basis features. With the qualitative and quantitative evaluations on MORPH, we demonstrate the realistic and continuous aging ability of our model, validating its superiority against existing models. To the best of our knowledge, this work is the first attempt to handle continuous target ages.', 'pages': '1995-1999', 'number': '', 'volume': '', 'year': '2021', 'title': 'Continuous Face Aging Generative Adversarial Networks', 'booktitle': 'ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)', 'author': 'Jeon, Seogkyu and Lee, Pilhyeon and Hong, Kibeom and Byun, Hyeran', 'ENTRYTYPE': 'inproceedings', 'ID': '9414429'}"
9377424,BCGAN: Facial Expression Synthesis by Bottleneck-Layered Conditional Generative Adversarial Networks,"Shin, Yeji and Bum, Junghyun and Son, Chang-Hwan and Choo, Hyunseung",Shin,10.1109/IMCOM51814.2021.9377424,2021,2021 15th International Conference on Ubiquitous Information Management and Communication (IMCOM),"Facial expression synthesis is widely applied to emotion prediction and face recognition for human-computer interaction. This task is challenging because it is difficult to reconstruct realistic and accurate facial expressions. Early deep learning methods focus only on pixel-level manipulation and are not suitable for generating realistic facial expressions. In this paper, we propose a bottleneck-layered conditional generative adversarial networks (BCGAN) for more realistic and accurate facial expression synthesis. BCGAN adopts a bottleneck layer that uses channel-wise concatenation in the generator to train with meaningful features only. In addition, a dense connection that links all bottleneck layers is added to generate an image which preserves the facial details of the original image. Both quantitative and qualitative evaluations were performed using the Radboud Faces Database (RaFD). Experimental results showed that BCGAN had 2\% higher classification accuracy (98.7\%) on the generated images as well as faster training speed compared to state-of-the-art approach.",Training;Human computer interaction;Face recognition;Generative adversarial networks;Information management;Task analysis;Image reconstruction;Facial expression synthesis;generative adversarial networks;densely connected convolutional networks,"{'month': 'Jan', 'issn': '', 'doi': '10.1109/IMCOM51814.2021.9377424', 'keywords': 'Training;Human computer interaction;Face recognition;Generative adversarial networks;Information management;Task analysis;Image reconstruction;Facial expression synthesis;generative adversarial networks;densely connected convolutional networks', 'abstract': 'Facial expression synthesis is widely applied to emotion prediction and face recognition for human-computer interaction. This task is challenging because it is difficult to reconstruct realistic and accurate facial expressions. Early deep learning methods focus only on pixel-level manipulation and are not suitable for generating realistic facial expressions. In this paper, we propose a bottleneck-layered conditional generative adversarial networks (BCGAN) for more realistic and accurate facial expression synthesis. BCGAN adopts a bottleneck layer that uses channel-wise concatenation in the generator to train with meaningful features only. In addition, a dense connection that links all bottleneck layers is added to generate an image which preserves the facial details of the original image. Both quantitative and qualitative evaluations were performed using the Radboud Faces Database (RaFD). Experimental results showed that BCGAN had 2\\% higher classification accuracy (98.7\\%) on the generated images as well as faster training speed compared to state-of-the-art approach.', 'pages': '1-4', 'number': '', 'volume': '', 'year': '2021', 'title': 'BCGAN: Facial Expression Synthesis by Bottleneck-Layered Conditional Generative Adversarial Networks', 'booktitle': '2021 15th International Conference on Ubiquitous Information Management and Communication (IMCOM)', 'author': 'Shin, Yeji and Bum, Junghyun and Son, Chang-Hwan and Choo, Hyunseung', 'ENTRYTYPE': 'inproceedings', 'ID': '9377424'}"
10435654,Identification of Fingerprint Orientation Using Improved Generative Adversarial Network with Support Vector Machine,"Hameed Abdul Hussein, Abbas and Ravindran, Gobinath and Latef naser, Zamen and Almusawi, Muntather and K, Santhiya",Hameed Abdul Hussein,10.1109/ICMNWC60182.2023.10435654,2023,2023 3rd International Conference on Mobile Networks and Wireless Communications (ICMNWC),"Fingerprint authentication is one of the methods used to prevent the loss of personal data, but the minutiae and angle variance of finger patterns make the process difficult. To overcome the issue, an authentication system for fingerprint orientation using an improved Generative Adversarial Network (GAN) with Support Vector Machine (SVM) is being developed. The process begins with image enhancement using Contrastive Limited Adaptive Histogram Equalization (CLAHE) techniques to increase image contrast. Next, GAN with SVM is employed to generate synthetic data through data augmentation, and decision-making is carried out using SVM. Both real and synthetic samples are binarized using the global thresholding technique, and edge-based thinning methods are applied to enhance the fingerprint patterns. Finally, features such as minutiae points of fingerprints are extracted using the Scale Invariant Feature Transform (SIFT) algorithm, serving as input for the SVM classifier. The implemented GAN-SVM model demonstrates superior performance, achieving a False Acceptance Rate (FAR) of 0.12\%, a False Rejection Rate (FRR) of 1.3\%, an Equal Error Rate (EER) of 0.29\%, and an accuracy of 95.87\%. When compared to previous models like Multi-layer Perceptron Neural Network (MLP), Fuzzy Commitment (FC), and Genetic Encryption Algorithm (GEA).",Support vector machines;Image edge detection;Authentication;Fingerprint recognition;Feature extraction;Generative adversarial networks;Synthetic data;Contrastive Limited Adaptive Histogram Equalization;Generative Adversarial Network;Global Thresholding;Scale Invariant Feature Transform;Support Vector Machine,"{'month': 'Dec', 'issn': '', 'doi': '10.1109/ICMNWC60182.2023.10435654', 'keywords': 'Support vector machines;Image edge detection;Authentication;Fingerprint recognition;Feature extraction;Generative adversarial networks;Synthetic data;Contrastive Limited Adaptive Histogram Equalization;Generative Adversarial Network;Global Thresholding;Scale Invariant Feature Transform;Support Vector Machine', 'abstract': 'Fingerprint authentication is one of the methods used to prevent the loss of personal data, but the minutiae and angle variance of finger patterns make the process difficult. To overcome the issue, an authentication system for fingerprint orientation using an improved Generative Adversarial Network (GAN) with Support Vector Machine (SVM) is being developed. The process begins with image enhancement using Contrastive Limited Adaptive Histogram Equalization (CLAHE) techniques to increase image contrast. Next, GAN with SVM is employed to generate synthetic data through data augmentation, and decision-making is carried out using SVM. Both real and synthetic samples are binarized using the global thresholding technique, and edge-based thinning methods are applied to enhance the fingerprint patterns. Finally, features such as minutiae points of fingerprints are extracted using the Scale Invariant Feature Transform (SIFT) algorithm, serving as input for the SVM classifier. The implemented GAN-SVM model demonstrates superior performance, achieving a False Acceptance Rate (FAR) of 0.12\\%, a False Rejection Rate (FRR) of 1.3\\%, an Equal Error Rate (EER) of 0.29\\%, and an accuracy of 95.87\\%. When compared to previous models like Multi-layer Perceptron Neural Network (MLP), Fuzzy Commitment (FC), and Genetic Encryption Algorithm (GEA).', 'pages': '1-5', 'number': '', 'volume': '', 'year': '2023', 'title': 'Identification of Fingerprint Orientation Using Improved Generative Adversarial Network with Support Vector Machine', 'booktitle': '2023 3rd International Conference on Mobile Networks and Wireless Communications (ICMNWC)', 'author': 'Hameed Abdul Hussein, Abbas and Ravindran, Gobinath and Latef naser, Zamen and Almusawi, Muntather and K, Santhiya', 'ENTRYTYPE': 'inproceedings', 'ID': '10435654'}"
11065002,Generative Adversarial Network based Data Augmentation Method for Bearing Fault diagnosis,"Li, Zhichao and Shen, Mingxue and Qin, Lei and Tian, Li",Li,10.1109/DDCLS66240.2025.11065002,2025,2025 IEEE 14th Data Driven Control and Learning Systems (DDCLS),"As a critical component of rotating machinery, the fault diagnosis of bearings holds significant importance. The deep learning-based approach to bearing fault diagnosis necessitates a substantial amount of fault data, which is often challenging to obtain in practical engineering scenarios. This data imbalance issue severely impacts the accuracy of fault diagnosis. To address this challenge, we propose a data augmentation method based on Multi-Source Generative Adversarial Network with Quality Assessment and Screening Module (MGAN-QASM). Firstly, the generator and discriminator are restructured using three loss functions, enhancing the stability of training and diversifying the generated data. Secondly, QASM screens for both similar and diverse samples to balance the training dataset. Finally, Deep Convolutional Neural Networks with Wide first-layer kernels (WDCNN) are employed for fault diagnosis. The results show that the proposed method has higher fault diagnosis accuracy than other methods under different imbalance ratio conditions.",Fault diagnosis;Training;Learning systems;Accuracy;Process control;Generative adversarial networks;Data augmentation;Quality assessment;Machinery;Kernel;Generative adversarial networks;Data augmentation;Imbalance;Quality assessment and screening;Fault diagnosis,"{'month': 'May', 'issn': '2767-9861', 'doi': '10.1109/DDCLS66240.2025.11065002', 'keywords': 'Fault diagnosis;Training;Learning systems;Accuracy;Process control;Generative adversarial networks;Data augmentation;Quality assessment;Machinery;Kernel;Generative adversarial networks;Data augmentation;Imbalance;Quality assessment and screening;Fault diagnosis', 'abstract': 'As a critical component of rotating machinery, the fault diagnosis of bearings holds significant importance. The deep learning-based approach to bearing fault diagnosis necessitates a substantial amount of fault data, which is often challenging to obtain in practical engineering scenarios. This data imbalance issue severely impacts the accuracy of fault diagnosis. To address this challenge, we propose a data augmentation method based on Multi-Source Generative Adversarial Network with Quality Assessment and Screening Module (MGAN-QASM). Firstly, the generator and discriminator are restructured using three loss functions, enhancing the stability of training and diversifying the generated data. Secondly, QASM screens for both similar and diverse samples to balance the training dataset. Finally, Deep Convolutional Neural Networks with Wide first-layer kernels (WDCNN) are employed for fault diagnosis. The results show that the proposed method has higher fault diagnosis accuracy than other methods under different imbalance ratio conditions.', 'pages': '1095-1101', 'number': '', 'volume': '', 'year': '2025', 'title': 'Generative Adversarial Network based Data Augmentation Method for Bearing Fault diagnosis', 'booktitle': '2025 IEEE 14th Data Driven Control and Learning Systems (DDCLS)', 'author': 'Li, Zhichao and Shen, Mingxue and Qin, Lei and Tian, Li', 'ENTRYTYPE': 'inproceedings', 'ID': '11065002'}"
10097009,A Few Shot Learning of Singing Technique Conversion Based on Cycle Consistency Generative Adversarial Networks,"Chen, Po-Wei and Soo, Von-Wun",Chen,10.1109/ICASSP49357.2023.10097009,2023,"ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","We adopt the recent cycle consistent generative adversarial network (MaskCycleGAN-VC) that allows converting a specific singing technique using only a few articulations of singing voice as examples. Since it is often prone to fail to preserve the content information of the singing voice due to distortion and noise during the conversion, a self-supervised learning module is proposed as the basic framework to enforce content consistency without additional annotations. We evaluate the proposed methods on three datasets that were commonly used in pop songs which involve singing techniques in terms of breathy voice, vibrato, and vocal fry. Experiments showed that our proposed methods outperform the baseline in terms of audio quality and content preservation, including melody and singer’s timbral identity, without affecting the perception of singing techniques. 1",Acoustic distortion;Annotations;Self-supervised learning;Generative adversarial networks;Acoustics;Timbre;Task analysis;singing technique conversion;generative adversarial networks;few shot learning;cycle consistency;triplet learning,"{'month': 'June', 'issn': '2379-190X', 'doi': '10.1109/ICASSP49357.2023.10097009', 'keywords': 'Acoustic distortion;Annotations;Self-supervised learning;Generative adversarial networks;Acoustics;Timbre;Task analysis;singing technique conversion;generative adversarial networks;few shot learning;cycle consistency;triplet learning', 'abstract': 'We adopt the recent cycle consistent generative adversarial network (MaskCycleGAN-VC) that allows converting a specific singing technique using only a few articulations of singing voice as examples. Since it is often prone to fail to preserve the content information of the singing voice due to distortion and noise during the conversion, a self-supervised learning module is proposed as the basic framework to enforce content consistency without additional annotations. We evaluate the proposed methods on three datasets that were commonly used in pop songs which involve singing techniques in terms of breathy voice, vibrato, and vocal fry. Experiments showed that our proposed methods outperform the baseline in terms of audio quality and content preservation, including melody and singer’s timbral identity, without affecting the perception of singing techniques. 1', 'pages': '1-5', 'number': '', 'volume': '', 'year': '2023', 'title': 'A Few Shot Learning of Singing Technique Conversion Based on Cycle Consistency Generative Adversarial Networks', 'booktitle': 'ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)', 'author': 'Chen, Po-Wei and Soo, Von-Wun', 'ENTRYTYPE': 'inproceedings', 'ID': '10097009'}"
10928135,How Could Generative AI Support Compliance with the EU AI Act? A Review for Safe Automated Driving Perception,"Keser, Mert and Shoeb, Youssef and Knoll, Alois",Keser,10.1109/ICVES61986.2024.10928135,2024,2024 IEEE International Conference on Vehicular Electronics and Safety (ICVES),"Deep Neural Networks (DNNs) have become central for the perception functions of autonomous vehicles, substantially enhancing their ability to understand and interpret the environment. However, these systems exhibit inherent limitations such as brittleness, opacity, and unpredictable behavior in out-of-distribution scenarios. The European Union (EU) Artificial Intelligence (AI) Act, as a pioneering legislative framework, aims to address these challenges by establishing stringent norms and standards for AI systems, including those used in autonomous driving (AD), which are categorized as high-risk AI. In this work, we explore how the newly available generative AI models can potentially support addressing upcoming regulatory requirements in AD perception, particularly with respect to safety. This short review paper summarizes the requirements arising from the EU AI Act regarding DNN-based perception systems and systematically categorizes existing generative AI applications in AD. While generative AI models show promise in addressing some of the EU AI Act's requirements, such as transparency and robustness, this review examines their potential benefits and discusses how developers could leverage these methods to enhance compliance with the Act. The paper also highlights areas where further research is needed to ensure reliable and safe integration of these technologies.",Surveys;Vehicular and wireless technologies;Generative AI;Reviews;Europe;Robustness;Safety;Standards;Autonomous vehicles;Monitoring,"{'month': 'Dec', 'issn': '', 'doi': '10.1109/ICVES61986.2024.10928135', 'keywords': 'Surveys;Vehicular and wireless technologies;Generative AI;Reviews;Europe;Robustness;Safety;Standards;Autonomous vehicles;Monitoring', 'abstract': ""Deep Neural Networks (DNNs) have become central for the perception functions of autonomous vehicles, substantially enhancing their ability to understand and interpret the environment. However, these systems exhibit inherent limitations such as brittleness, opacity, and unpredictable behavior in out-of-distribution scenarios. The European Union (EU) Artificial Intelligence (AI) Act, as a pioneering legislative framework, aims to address these challenges by establishing stringent norms and standards for AI systems, including those used in autonomous driving (AD), which are categorized as high-risk AI. In this work, we explore how the newly available generative AI models can potentially support addressing upcoming regulatory requirements in AD perception, particularly with respect to safety. This short review paper summarizes the requirements arising from the EU AI Act regarding DNN-based perception systems and systematically categorizes existing generative AI applications in AD. While generative AI models show promise in addressing some of the EU AI Act's requirements, such as transparency and robustness, this review examines their potential benefits and discusses how developers could leverage these methods to enhance compliance with the Act. The paper also highlights areas where further research is needed to ensure reliable and safe integration of these technologies."", 'pages': '1-6', 'number': '', 'volume': '', 'year': '2024', 'title': 'How Could Generative AI Support Compliance with the EU AI Act? A Review for Safe Automated Driving Perception', 'booktitle': '2024 IEEE International Conference on Vehicular Electronics and Safety (ICVES)', 'author': 'Keser, Mert and Shoeb, Youssef and Knoll, Alois', 'ENTRYTYPE': 'inproceedings', 'ID': '10928135'}"
10988785,Designing Future Energy Systems with Generative AI,"Glaws, Andrew and King, Ryan N. and Emami, Patrick and Buster, Grant and Benton, Brandon N. and Zhang, Xiangyu and Zamzam, Ahmed and Venkataramanan, Venkatesh and Macwan, Richard",Glaws,10.1109/MCSE.2025.3567208,2025,Computing in Science \& Engineering,"Energy systems are experiencing various changes that impact the distribution, use, and reliability of energy. Local utilities and municipalities must respond and adapt to these changes, moving towards a future energy system with modernized infrastructure and other targeted investments and policy decisions. However, planning for and enacting these advancements requires significant effort from experts and engineers to develop strategies that ensure a reliable and secure energy future. This includes characterizing the current energy infrastructure, identifying areas for development, and engaging with local community members. Emerging generative artificial intelligence techniques can alleviate pain points and help support the development of the next generation of energy systems. In this article, we highlight on-going generative AI work in the areas of atmospheric modeling, building energy management, and distribution network design, and we propose a vision for the role of generative AI that considers opportunities and identifies challenges inherent to this technology.",Generative AI;Data models;Reliability;Training;Next generation networking;Costs;Computational modeling;Urban areas;Superresolution;Investment,"{'month': '', 'issn': '1558-366X', 'doi': '10.1109/MCSE.2025.3567208', 'keywords': 'Generative AI;Data models;Reliability;Training;Next generation networking;Costs;Computational modeling;Urban areas;Superresolution;Investment', 'abstract': 'Energy systems are experiencing various changes that impact the distribution, use, and reliability of energy. Local utilities and municipalities must respond and adapt to these changes, moving towards a future energy system with modernized infrastructure and other targeted investments and policy decisions. However, planning for and enacting these advancements requires significant effort from experts and engineers to develop strategies that ensure a reliable and secure energy future. This includes characterizing the current energy infrastructure, identifying areas for development, and engaging with local community members. Emerging generative artificial intelligence techniques can alleviate pain points and help support the development of the next generation of energy systems. In this article, we highlight on-going generative AI work in the areas of atmospheric modeling, building energy management, and distribution network design, and we propose a vision for the role of generative AI that considers opportunities and identifies challenges inherent to this technology.', 'pages': '1-11', 'number': '', 'volume': '', 'year': '2025', 'title': 'Designing Future Energy Systems with Generative AI', 'journal': 'Computing in Science \\& Engineering', 'author': 'Glaws, Andrew and King, Ryan N. and Emami, Patrick and Buster, Grant and Benton, Brandon N. and Zhang, Xiangyu and Zamzam, Ahmed and Venkataramanan, Venkatesh and Macwan, Richard', 'ENTRYTYPE': 'article', 'ID': '10988785'}"
11082753,"The use of Generative AI, Optimization Algorithms, and IoT Technologies in Enhancing Pilgrims’ Journey","Abudawood, T and Ahmed, Hamzah H. and Alrasheed, Mohammed and Alzeer, Abdullah",Abudawood,10.1109/AIIT63112.2025.11082753,2025,2025 International Conference on Innovation in Artificial Intelligence and Internet of Things (AIIT),"This paper presents an innovative solution to enhance the transportation experience of pilgrims during Hajj, focusing on the journey from Jeddah to the holy cities of Makkah and Madinah. Managing the transportation of millions of pilgrims within a constrained timeframe poses significant logistical challenges, often addressed using inefficient manual methods. Our approach integrates Generative AI for data extraction and retrieval, Mixed-Integer Linear Programming (MILP) for algorithmic optimization, and Internet of Things (IoT) technologies for real-time tracking. The system automates the allocation of pilgrims to buses, optimizing travel distances, reducing the number of buses required, and maximizing capacity utilization. Furthermore, a voice-enabled chat interface powered by Retrieval-Augmented Generation (RAG) provides real-time information access for both staff and pilgrims. Comparative evaluations reveal that our approach outperforms traditional methods, achieving over 40 times faster bus allocations and a 13\% reduction in the number of buses required, leading to enhanced efficiency, resource utilization, and cost savings of an approximately 16 million Saudi Riyals in two months. This system ultimately delivers a seamless, efficient, and comfortable transportation experience for pilgrims.",Technological innovation;Generative AI;Urban areas;Retrieval augmented generation;Transportation;Real-time systems;Mixed integer linear programming;Internet of Things;Resource management;Optimization;Artificial Intelligence;Generative AI;Optimization;Internet of Things;Hajj Pilgrims;Logistics,"{'month': 'May', 'issn': '', 'doi': '10.1109/AIIT63112.2025.11082753', 'keywords': 'Technological innovation;Generative AI;Urban areas;Retrieval augmented generation;Transportation;Real-time systems;Mixed integer linear programming;Internet of Things;Resource management;Optimization;Artificial Intelligence;Generative AI;Optimization;Internet of Things;Hajj Pilgrims;Logistics', 'abstract': 'This paper presents an innovative solution to enhance the transportation experience of pilgrims during Hajj, focusing on the journey from Jeddah to the holy cities of Makkah and Madinah. Managing the transportation of millions of pilgrims within a constrained timeframe poses significant logistical challenges, often addressed using inefficient manual methods. Our approach integrates Generative AI for data extraction and retrieval, Mixed-Integer Linear Programming (MILP) for algorithmic optimization, and Internet of Things (IoT) technologies for real-time tracking. The system automates the allocation of pilgrims to buses, optimizing travel distances, reducing the number of buses required, and maximizing capacity utilization. Furthermore, a voice-enabled chat interface powered by Retrieval-Augmented Generation (RAG) provides real-time information access for both staff and pilgrims. Comparative evaluations reveal that our approach outperforms traditional methods, achieving over 40 times faster bus allocations and a 13\\% reduction in the number of buses required, leading to enhanced efficiency, resource utilization, and cost savings of an approximately 16 million Saudi Riyals in two months. This system ultimately delivers a seamless, efficient, and comfortable transportation experience for pilgrims.', 'pages': '1-7', 'number': '', 'volume': '', 'year': '2025', 'title': 'The use of Generative AI, Optimization Algorithms, and IoT Technologies in Enhancing Pilgrims’ Journey', 'booktitle': '2025 International Conference on Innovation in Artificial Intelligence and Internet of Things (AIIT)', 'author': 'Abudawood, T and Ahmed, Hamzah H. and Alrasheed, Mohammed and Alzeer, Abdullah', 'ENTRYTYPE': 'inproceedings', 'ID': '11082753'}"
9475883,Artificial Intelligence and Data Fusion at the Edge,"Munir, Arslan and Blasch, Erik and Kwon, Jisu and Kong, Joonho and Aved, Alexander",Munir,10.1109/MAES.2020.3043072,2021,IEEE Aerospace and Electronic Systems Magazine,"Artificial intelligence (AI), owing to recent breakthroughs in deep learning, has revolutionized applications and services in almost all technology domains including aerospace. AI and deep learning rely on huge amounts of training data that are mostly generated at the network edge by Internet of Things (IoT) devices and sensors. Bringing the sensed data from the edge of a distributed network to a centralized cloud is often infeasible because of the massive data volume, limited network bandwidth, and real-time application constraints. Consequently, there is a desire to push AI frontiers to the network edge toward utilizing the enormous amount of data generated by IoT devices near the data source. The merger of edge computing and AI has engendered a new discipline, that is, AI at the edge or edge intelligence. To help AI make sense of gigantic data at the network edge, data fusion is of paramount significance and goes hand in hand with AI. This article focuses on data fusion and AI at the edge. In this article, we propose a framework for data fusion and AI processing at the edge. We then provide a comparative discussion of different data fusion and AI models and architectures. We discuss multiple levels of fusion and different types of AI, and how different types of AI align with different levels of fusion. We then highlight the benefits of combining data fusion with AI at the edge. The methods of AI and data fusion at the edge detailed in this article are applicable to many application domains including aerospace systems. We evaluate the effectiveness of combined data fusion and AI at the edge using convolutional neural network models and multiple hardware platforms suitable for edge computing. Experimental results reveal that combining AI with data fusion can impart a speedup of 9.8× while reducing energy consumption up to 88.5\% over AI without data fusion. Furthermore, results demonstrate that data fusion either maintains or improves the accuracy of AI in most cases. For our experiments, data fusion imparts a maximum improvement of 15.8\% in accuracy to AI.",Deep learning;Data integration;Distributed databases;Training data;Computer architecture;Data models,"{'month': 'July', 'issn': '1557-959X', 'doi': '10.1109/MAES.2020.3043072', 'keywords': 'Deep learning;Data integration;Distributed databases;Training data;Computer architecture;Data models', 'abstract': 'Artificial intelligence (AI), owing to recent breakthroughs in deep learning, has revolutionized applications and services in almost all technology domains including aerospace. AI and deep learning rely on huge amounts of training data that are mostly generated at the network edge by Internet of Things (IoT) devices and sensors. Bringing the sensed data from the edge of a distributed network to a centralized cloud is often infeasible because of the massive data volume, limited network bandwidth, and real-time application constraints. Consequently, there is a desire to push AI frontiers to the network edge toward utilizing the enormous amount of data generated by IoT devices near the data source. The merger of edge computing and AI has engendered a new discipline, that is, AI at the edge or edge intelligence. To help AI make sense of gigantic data at the network edge, data fusion is of paramount significance and goes hand in hand with AI. This article focuses on data fusion and AI at the edge. In this article, we propose a framework for data fusion and AI processing at the edge. We then provide a comparative discussion of different data fusion and AI models and architectures. We discuss multiple levels of fusion and different types of AI, and how different types of AI align with different levels of fusion. We then highlight the benefits of combining data fusion with AI at the edge. The methods of AI and data fusion at the edge detailed in this article are applicable to many application domains including aerospace systems. We evaluate the effectiveness of combined data fusion and AI at the edge using convolutional neural network models and multiple hardware platforms suitable for edge computing. Experimental results reveal that combining AI with data fusion can impart a speedup of 9.8× while reducing energy consumption up to 88.5\\% over AI without data fusion. Furthermore, results demonstrate that data fusion either maintains or improves the accuracy of AI in most cases. For our experiments, data fusion imparts a maximum improvement of 15.8\\% in accuracy to AI.', 'pages': '62-78', 'number': '7', 'volume': '36', 'year': '2021', 'title': 'Artificial Intelligence and Data Fusion at the Edge', 'journal': 'IEEE Aerospace and Electronic Systems Magazine', 'author': 'Munir, Arslan and Blasch, Erik and Kwon, Jisu and Kong, Joonho and Aved, Alexander', 'ENTRYTYPE': 'article', 'ID': '9475883'}"
10489036,A Novel Approach for Detecting Deepfake Face Using Machine Learning Algorithms,"Kumar, Manoj and Rai, Praveen Kumar and Kumar, Pankaj",Kumar,10.1109/ICDT61202.2024.10489036,2024,2024 2nd International Conference on Disruptive Technologies (ICDT),"In today's digital age, the ability to identify, differentiate, and authenticate manipulated online content is essential. Being ability to discriminate between the real and the fake is crucial. Recent advances in technologies such as artificial intelligence, machine learning, and deep learning are playing a major role in the generation of deepfake media (images and videos). Very realistic deep fake images and videos can be produced by utilizing sophisticated deep learning models such as generative adversarial neural networks (GAN s) and autoencoders, in conjunction with a sizable image collection pertaining to the subject matter. Deepfakes (DF) refer to artificially synthesized images or videos created using features such as face swapping and facial expression recombination. These face manipulation techniques have become extremely sophisticated. Deepfakes can be used to create child pornography, pornographic images of celebrities, revenge porn, fake news and harassment, spreading disinformation on social media platforms, financial fraud, election manipulation, and more. Therefore, there is a need to design and develop a robust framework to identify these deepfake images and videos. The purpose of this paper is to identify deepfakes from visual deepfake datasets and perform a comparative analysis of deep fake detection through machine learning algorithms.",Deep learning;Deepfakes;Visualization;Machine learning algorithms;Social networking (online);Face recognition;Voting;Deepfake;Artificial Intelligence (AI);Machine Learning (ML);Deep Learning;Generative Adversarial Neural Networks (GANs);Face Swapping,"{'month': 'March', 'issn': '', 'doi': '10.1109/ICDT61202.2024.10489036', 'keywords': 'Deep learning;Deepfakes;Visualization;Machine learning algorithms;Social networking (online);Face recognition;Voting;Deepfake;Artificial Intelligence (AI);Machine Learning (ML);Deep Learning;Generative Adversarial Neural Networks (GANs);Face Swapping', 'abstract': ""In today's digital age, the ability to identify, differentiate, and authenticate manipulated online content is essential. Being ability to discriminate between the real and the fake is crucial. Recent advances in technologies such as artificial intelligence, machine learning, and deep learning are playing a major role in the generation of deepfake media (images and videos). Very realistic deep fake images and videos can be produced by utilizing sophisticated deep learning models such as generative adversarial neural networks (GAN s) and autoencoders, in conjunction with a sizable image collection pertaining to the subject matter. Deepfakes (DF) refer to artificially synthesized images or videos created using features such as face swapping and facial expression recombination. These face manipulation techniques have become extremely sophisticated. Deepfakes can be used to create child pornography, pornographic images of celebrities, revenge porn, fake news and harassment, spreading disinformation on social media platforms, financial fraud, election manipulation, and more. Therefore, there is a need to design and develop a robust framework to identify these deepfake images and videos. The purpose of this paper is to identify deepfakes from visual deepfake datasets and perform a comparative analysis of deep fake detection through machine learning algorithms."", 'pages': '1588-1592', 'number': '', 'volume': '', 'year': '2024', 'title': 'A Novel Approach for Detecting Deepfake Face Using Machine Learning Algorithms', 'booktitle': '2024 2nd International Conference on Disruptive Technologies (ICDT)', 'author': 'Kumar, Manoj and Rai, Praveen Kumar and Kumar, Pankaj', 'ENTRYTYPE': 'inproceedings', 'ID': '10489036'}"
9335504,Generalized Zero-Shot Learning With Multiple Graph Adaptive Generative Networks,"Xie, Guo-Sen and Zhang, Zheng and Liu, Guoshuai and Zhu, Fan and Liu, Li and Shao, Ling and Li, Xuelong",Xie,10.1109/TNNLS.2020.3046924,2022,IEEE Transactions on Neural Networks and Learning Systems,"Generative adversarial networks (GANs) for (generalized) zero-shot learning (ZSL) aim to generate unseen image features when conditioned on unseen class embeddings, each of which corresponds to one unique category. Most existing works on GANs for ZSL generate features by merely feeding the seen image feature/class embedding (combined with random Gaussian noise) pairs into the generator/discriminator for a two-player minimax game. However, the structure consistency of the distributions among the real/fake image features, which may shift the generated features away from their real distribution to some extent, is seldom considered. In this paper, to align the weights of the generator for better structure consistency between real/fake features, we propose a novel multigraph adaptive GAN (MGA-GAN). Specifically, a Wasserstein GAN equipped with a classification loss is trained to generate discriminative features with structure consistency. MGA-GAN leverages the multigraph similarity structures between sliced seen real/fake feature samples to assist in updating the generator weights in the local feature manifold. Moreover, correlation graphs for the whole real/fake features are adopted to guarantee structure correlation in the global feature manifold. Extensive evaluations on four benchmarks demonstrate well the superiority of MGA-GAN over its state-of-the-art counterparts.",Semantics;Generative adversarial networks;Training;Gallium nitride;Generators;Correlation;Task analysis;Feature generation;graph constraint;Wasserstein GAN;zero-shot learning (ZSL),"{'month': 'July', 'issn': '2162-2388', 'doi': '10.1109/TNNLS.2020.3046924', 'keywords': 'Semantics;Generative adversarial networks;Training;Gallium nitride;Generators;Correlation;Task analysis;Feature generation;graph constraint;Wasserstein GAN;zero-shot learning (ZSL)', 'abstract': 'Generative adversarial networks (GANs) for (generalized) zero-shot learning (ZSL) aim to generate unseen image features when conditioned on unseen class embeddings, each of which corresponds to one unique category. Most existing works on GANs for ZSL generate features by merely feeding the seen image feature/class embedding (combined with random Gaussian noise) pairs into the generator/discriminator for a two-player minimax game. However, the structure consistency of the distributions among the real/fake image features, which may shift the generated features away from their real distribution to some extent, is seldom considered. In this paper, to align the weights of the generator for better structure consistency between real/fake features, we propose a novel multigraph adaptive GAN (MGA-GAN). Specifically, a Wasserstein GAN equipped with a classification loss is trained to generate discriminative features with structure consistency. MGA-GAN leverages the multigraph similarity structures between sliced seen real/fake feature samples to assist in updating the generator weights in the local feature manifold. Moreover, correlation graphs for the whole real/fake features are adopted to guarantee structure correlation in the global feature manifold. Extensive evaluations on four benchmarks demonstrate well the superiority of MGA-GAN over its state-of-the-art counterparts.', 'pages': '2903-2915', 'number': '7', 'volume': '33', 'year': '2022', 'title': 'Generalized Zero-Shot Learning With Multiple Graph Adaptive Generative Networks', 'journal': 'IEEE Transactions on Neural Networks and Learning Systems', 'author': 'Xie, Guo-Sen and Zhang, Zheng and Liu, Guoshuai and Zhu, Fan and Liu, Li and Shao, Ling and Li, Xuelong', 'ENTRYTYPE': 'article', 'ID': '9335504'}"
10400892,Latent Vector Optimization-Based Generative Image Steganography for Consumer Electronic Applications,"Zhou, Zhili and Bao, Zhipeng and Jiang, Weiwei and Huang, Yuan and Peng, Yun and Shankar, Achyut and Maple, Carsten and Selvarajan, Shitharth",Zhou,10.1109/TCE.2024.3354824,2024,IEEE Transactions on Consumer Electronics,"In consumer electronic applications, to transmit secret images securely, it is required to explore the advanced covert communication technology, i.e., Generative Image Steganography (GIS). However, the existing GIS schemes suffer from the issues of poor stego-image quality and limited hiding capacity. Consequently, these GIS schemes cannot meet the requirements of consumer electronic applications, in which massive secret information needs to be transmitted securely. To address the above issues, we propose a Latent Vector Optimization (LVO)-based GIS scheme, in which the information hiding is implemented by the flow-based generative model during the image generation. Specifically, the LVO algorithm is introduced to compute the hiding probability of each element of latent vector according to its impact on the quality of the stego-image generated from the latent vector. Then, it hides more information in elements with higher hiding probability. The extensive experiments demonstrate that, compared to current GIS schemes, the proposed LVO-based GIS scheme generates higher-quality images, while maintaining hiding capacity (up to $5.0 \, bpp$ ) and accurate information extraction (almost 100\% accuracy rate).",Steganography;Consumer electronics;Generative adversarial networks;Servers;Pareto optimization;Data models;Receivers;Generative model;generative steganography;AI-generated content;consumer electronics,"{'month': 'Feb', 'issn': '1558-4127', 'doi': '10.1109/TCE.2024.3354824', 'keywords': 'Steganography;Consumer electronics;Generative adversarial networks;Servers;Pareto optimization;Data models;Receivers;Generative model;generative steganography;AI-generated content;consumer electronics', 'abstract': 'In consumer electronic applications, to transmit secret images securely, it is required to explore the advanced covert communication technology, i.e., Generative Image Steganography (GIS). However, the existing GIS schemes suffer from the issues of poor stego-image quality and limited hiding capacity. Consequently, these GIS schemes cannot meet the requirements of consumer electronic applications, in which massive secret information needs to be transmitted securely. To address the above issues, we propose a Latent Vector Optimization (LVO)-based GIS scheme, in which the information hiding is implemented by the flow-based generative model during the image generation. Specifically, the LVO algorithm is introduced to compute the hiding probability of each element of latent vector according to its impact on the quality of the stego-image generated from the latent vector. Then, it hides more information in elements with higher hiding probability. The extensive experiments demonstrate that, compared to current GIS schemes, the proposed LVO-based GIS scheme generates higher-quality images, while maintaining hiding capacity (up to $5.0 \\, bpp$ ) and accurate information extraction (almost 100\\% accuracy rate).', 'pages': '4357-4366', 'number': '1', 'volume': '70', 'year': '2024', 'title': 'Latent Vector Optimization-Based Generative Image Steganography for Consumer Electronic Applications', 'journal': 'IEEE Transactions on Consumer Electronics', 'author': 'Zhou, Zhili and Bao, Zhipeng and Jiang, Weiwei and Huang, Yuan and Peng, Yun and Shankar, Achyut and Maple, Carsten and Selvarajan, Shitharth', 'ENTRYTYPE': 'article', 'ID': '10400892'}"
9261734,Modified Generative Adversarial Network for Super-Resolution of Terahertz Image,"Zhang, Zhen and Zhang, Liuyang and Chen, Xuefeng and Xu, Yafei",Zhang,10.1109/ICSMD50554.2020.9261734,2020,"2020 International Conference on Sensing, Measurement \& Data Analytics in the era of Artificial Intelligence (ICSMD)","Terahertz (THz) images have low spatial resolution, blurring contour features and high background noise owing to the limitation of terahertz (THz) wavelengths and the THz imaging systems. We have proposed a modified Generative Adversarial Network (GAN) for super-resolution (SR) purpose. To fit the THz images, we design a kind of image degradation model to generate low-resolution images with Gaussian blur and white Gaussian noise. We establish a dataset of damage images in the field of non-destructive testing (NDT) for training and testing. The experimental results on THz images demonstrate that the improved GAN model can improve the quality of THz images effectively. Our method can be beneficial to improve the accuracy of THz NDT with low resolution.",Training;Image resolution;Imaging;Gallium nitride;Testing;Generators;Generative adversarial networks;THz image;super-resolution;degradation model;deep learning,"{'month': 'Oct', 'issn': '', 'doi': '10.1109/ICSMD50554.2020.9261734', 'keywords': 'Training;Image resolution;Imaging;Gallium nitride;Testing;Generators;Generative adversarial networks;THz image;super-resolution;degradation model;deep learning', 'abstract': 'Terahertz (THz) images have low spatial resolution, blurring contour features and high background noise owing to the limitation of terahertz (THz) wavelengths and the THz imaging systems. We have proposed a modified Generative Adversarial Network (GAN) for super-resolution (SR) purpose. To fit the THz images, we design a kind of image degradation model to generate low-resolution images with Gaussian blur and white Gaussian noise. We establish a dataset of damage images in the field of non-destructive testing (NDT) for training and testing. The experimental results on THz images demonstrate that the improved GAN model can improve the quality of THz images effectively. Our method can be beneficial to improve the accuracy of THz NDT with low resolution.', 'pages': '602-605', 'number': '', 'volume': '', 'year': '2020', 'title': 'Modified Generative Adversarial Network for Super-Resolution of Terahertz Image', 'booktitle': '2020 International Conference on Sensing, Measurement \\& Data Analytics in the era of Artificial Intelligence (ICSMD)', 'author': 'Zhang, Zhen and Zhang, Liuyang and Chen, Xuefeng and Xu, Yafei', 'ENTRYTYPE': 'inproceedings', 'ID': '9261734'}"
10070223,Intrusion Detection Method Based On Improved Conditional Generative Adversarial Network,"Ding, Yuhang and Jiang, Wenrong",Ding,10.1109/AIIPCC57291.2022.00016,2022,"2022 International Conference on Artificial Intelligence, Information Processing and Cloud Computing (AIIPCC)","At present, the intrusion detection model mainly uses anomalous behavior to establish a library of intrusion behavior patterns, and determines whether the intrusion behavior conforms to the intrusion behavior specification by comparing the library of intrusion behavior patterns. Once there is a change in intrusion behavior or a new type of network attack, the existing intrusion detection model cannot make corresponding changes according to the actual changes. Therefore, making intrusion detection models have the ability to learn autonomously and be able to adapt to changes in the network environment to detect new types of unknown attacks has received increasing attention from many security researchers. In this paper, we propose an intrusion detection model (CGAN-RF) based on conditional generative adversarial network (CGAN) and random forest (RF). The CGAN-RF model improves the class imbalance problem of the dataset by generating samples to enhance the detection efficiency of minority and unknown classes.",Radio frequency;Adaptation models;Intrusion detection;Information processing;Forestry;Generative adversarial networks;Libraries;component;intrusion detection;convolutional neural networks;conditional generation adversarial networks;class balancing technology;random forest,"{'month': 'Aug', 'issn': '', 'doi': '10.1109/AIIPCC57291.2022.00016', 'keywords': 'Radio frequency;Adaptation models;Intrusion detection;Information processing;Forestry;Generative adversarial networks;Libraries;component;intrusion detection;convolutional neural networks;conditional generation adversarial networks;class balancing technology;random forest', 'abstract': 'At present, the intrusion detection model mainly uses anomalous behavior to establish a library of intrusion behavior patterns, and determines whether the intrusion behavior conforms to the intrusion behavior specification by comparing the library of intrusion behavior patterns. Once there is a change in intrusion behavior or a new type of network attack, the existing intrusion detection model cannot make corresponding changes according to the actual changes. Therefore, making intrusion detection models have the ability to learn autonomously and be able to adapt to changes in the network environment to detect new types of unknown attacks has received increasing attention from many security researchers. In this paper, we propose an intrusion detection model (CGAN-RF) based on conditional generative adversarial network (CGAN) and random forest (RF). The CGAN-RF model improves the class imbalance problem of the dataset by generating samples to enhance the detection efficiency of minority and unknown classes.', 'pages': '37-40', 'number': '', 'volume': '', 'year': '2022', 'title': 'Intrusion Detection Method Based On Improved Conditional Generative Adversarial Network', 'booktitle': '2022 International Conference on Artificial Intelligence, Information Processing and Cloud Computing (AIIPCC)', 'author': 'Ding, Yuhang and Jiang, Wenrong', 'ENTRYTYPE': 'inproceedings', 'ID': '10070223'}"
9065225,Chinese Story Generation Using Conditional Generative Adversarial Network,"Lin, Jhe-Wei and Tseng, Jo-Han and Chang, Rong-Guey",Lin,10.1109/ICAIIC48513.2020.9065225,2020,2020 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),"Natural language processing has become popular considerably in a wide range of applications, but it still has been rarely applied to automatic text generation. In this paper, we focus on allowing the user to determine which content the machine is to narrate and the user can just give a summary to generate a complete paragraph. Based on the attention mechanism, we propose a Syntax-Guided Machine Reading Comprehension (SG-Net) to accept Chinese word vectors, learn from Chinese data sets, and semi-supervised self-growing generative adversarial network (SG-GAN) generate sequences with more realistic sequences. In order to reasonably evaluate the quality of the text produced by the machine, we also design a set of experiments by manipulating the content of the input sequence semantic information. It can be seen from the experimental results that both SG-Net and SG-GAN can understand the basic semantics and grammar and write a readable article. In summary, SG-Net may only recite the statements that have been read and SG-GAN can understand more semantic and grammar than SG-Net.",Vocabulary;Mathematical model;Generative adversarial networks;Gallium nitride;Semantics;Neural networks;Computational modeling;Deep Learning;Neural Chinese Story Generation;Nature language processing;Attention mechanisms,"{'month': 'Feb', 'issn': '', 'doi': '10.1109/ICAIIC48513.2020.9065225', 'keywords': 'Vocabulary;Mathematical model;Generative adversarial networks;Gallium nitride;Semantics;Neural networks;Computational modeling;Deep Learning;Neural Chinese Story Generation;Nature language processing;Attention mechanisms', 'abstract': 'Natural language processing has become popular considerably in a wide range of applications, but it still has been rarely applied to automatic text generation. In this paper, we focus on allowing the user to determine which content the machine is to narrate and the user can just give a summary to generate a complete paragraph. Based on the attention mechanism, we propose a Syntax-Guided Machine Reading Comprehension (SG-Net) to accept Chinese word vectors, learn from Chinese data sets, and semi-supervised self-growing generative adversarial network (SG-GAN) generate sequences with more realistic sequences. In order to reasonably evaluate the quality of the text produced by the machine, we also design a set of experiments by manipulating the content of the input sequence semantic information. It can be seen from the experimental results that both SG-Net and SG-GAN can understand the basic semantics and grammar and write a readable article. In summary, SG-Net may only recite the statements that have been read and SG-GAN can understand more semantic and grammar than SG-Net.', 'pages': '457-462', 'number': '', 'volume': '', 'year': '2020', 'title': 'Chinese Story Generation Using Conditional Generative Adversarial Network', 'booktitle': '2020 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)', 'author': 'Lin, Jhe-Wei and Tseng, Jo-Han and Chang, Rong-Guey', 'ENTRYTYPE': 'inproceedings', 'ID': '9065225'}"
10206191,Look AI – An Intelligent System for Socialization of Visually Impaired,"Mendis, G. L. M. M. and Deshan, W. M. Y and Bandara, H. M. G. M. and Gunethilake, K. C. and Wijendra, Dinuka and Krishara, Jenny",Mendis,10.1109/ICAIBD57115.2023.10206191,2023,2023 6th International Conference on Artificial Intelligence and Big Data (ICAIBD),"It has found that 30\% of the people among the visually impaired people are having a significant number of depressive symptoms whereas, the prevalence of depression in blind people was reported to be 33\%, or nearly double the rate of the general population. Therefore, social activities among visually impaired people should increase. This research paper provides a unique system for assisting visually impaired individuals in navigating indoor environments heavily relies on the use of artificial intelligence, particularly deep learning and computer vision techniques. By leveraging these advanced technologies, the system can analyze and process large amounts of visual data in real-time, accurately identifying and locating various objects in the environment, including faces, doors, stairs, cross walks, traffic lights, potholes and other obstacles. The system then provides real-time feedback to the user via an audio interface, enabling them to navigate indoor and outdoor environments safely and independently. The use of artificial intelligence in this system is particularly significant, as it enables the system to adapt and improve over time based on user feedback and additional data. That means, the system can continuously improve its performance and accuracy, ultimately resulting in a more effective and reliable tool for visually impaired individuals. The authors have also evaluated the system in a real-world setting, demonstrating its effectiveness in assisting visually impaired individuals with indoor navigation. Overall, the proposed system has the potential to significantly enhance the quality of life for visually impaired individuals, showcasing the transformative power of artificial intelligence in addressing real-world challenges.",Visualization;Machine learning algorithms;Local government;Smart homes;Organizations;Stairs;User experience;artificial intelligence;machine learning;convolutional neural networks;recurrent neural network,"{'month': 'May', 'issn': '2769-3554', 'doi': '10.1109/ICAIBD57115.2023.10206191', 'keywords': 'Visualization;Machine learning algorithms;Local government;Smart homes;Organizations;Stairs;User experience;artificial intelligence;machine learning;convolutional neural networks;recurrent neural network', 'abstract': 'It has found that 30\\% of the people among the visually impaired people are having a significant number of depressive symptoms whereas, the prevalence of depression in blind people was reported to be 33\\%, or nearly double the rate of the general population. Therefore, social activities among visually impaired people should increase. This research paper provides a unique system for assisting visually impaired individuals in navigating indoor environments heavily relies on the use of artificial intelligence, particularly deep learning and computer vision techniques. By leveraging these advanced technologies, the system can analyze and process large amounts of visual data in real-time, accurately identifying and locating various objects in the environment, including faces, doors, stairs, cross walks, traffic lights, potholes and other obstacles. The system then provides real-time feedback to the user via an audio interface, enabling them to navigate indoor and outdoor environments safely and independently. The use of artificial intelligence in this system is particularly significant, as it enables the system to adapt and improve over time based on user feedback and additional data. That means, the system can continuously improve its performance and accuracy, ultimately resulting in a more effective and reliable tool for visually impaired individuals. The authors have also evaluated the system in a real-world setting, demonstrating its effectiveness in assisting visually impaired individuals with indoor navigation. Overall, the proposed system has the potential to significantly enhance the quality of life for visually impaired individuals, showcasing the transformative power of artificial intelligence in addressing real-world challenges.', 'pages': '351-356', 'number': '', 'volume': '', 'year': '2023', 'title': 'Look AI – An Intelligent System for Socialization of Visually Impaired', 'booktitle': '2023 6th International Conference on Artificial Intelligence and Big Data (ICAIBD)', 'author': 'Mendis, G. L. M. M. and Deshan, W. M. Y and Bandara, H. M. G. M. and Gunethilake, K. C. and Wijendra, Dinuka and Krishara, Jenny', 'ENTRYTYPE': 'inproceedings', 'ID': '10206191'}"
10489393,The Utilization of AI Extends Beyond Payment Systems to E-Commerce Store Development,"Mimani, Sushant and Ramakrishnan, Rakesh and Rohella, Piyush and Jiwani, Nasmin and Logeshwaran, J.",Mimani,10.1109/ICDT61202.2024.10489393,2024,2024 2nd International Conference on Disruptive Technologies (ICDT),"In the age of advancing technology, Artificial Intelligence (AI) has become a fundamental force in almost every industry, including electronic commerce (e-commerce). AI can be used to power e-commerce store development in multiple ways, such as enhancing customer experience, improving search, optimizing product recommendations and inventory management, and improving payment systems. Through algorithms and machine learning, AI allows e-commerce storeowners to create and deploy intelligent bots, with predetermined parameters, to chat with customers, answer queries and suggest products or services. AI also enables customers to access automated search functions, product recommendations based on their previous purchases, and automated customer experiences. Improved payment systems can also be facilitated by AI to streamline transactions, reduce security risks, and enhance user experience. All of these features allow e-commerce stores to remain competitive in today's market. Ultimately, AI empowers e-commerce stores to stand out and drive sales.",Industries;Machine learning algorithms;Force;Machine learning;Inventory management;User experience;Electronic commerce;Artificial Intelligence;Payment Systems;Commerce;Store Development;Utilization,"{'month': 'March', 'issn': '', 'doi': '10.1109/ICDT61202.2024.10489393', 'keywords': 'Industries;Machine learning algorithms;Force;Machine learning;Inventory management;User experience;Electronic commerce;Artificial Intelligence;Payment Systems;Commerce;Store Development;Utilization', 'abstract': ""In the age of advancing technology, Artificial Intelligence (AI) has become a fundamental force in almost every industry, including electronic commerce (e-commerce). AI can be used to power e-commerce store development in multiple ways, such as enhancing customer experience, improving search, optimizing product recommendations and inventory management, and improving payment systems. Through algorithms and machine learning, AI allows e-commerce storeowners to create and deploy intelligent bots, with predetermined parameters, to chat with customers, answer queries and suggest products or services. AI also enables customers to access automated search functions, product recommendations based on their previous purchases, and automated customer experiences. Improved payment systems can also be facilitated by AI to streamline transactions, reduce security risks, and enhance user experience. All of these features allow e-commerce stores to remain competitive in today's market. Ultimately, AI empowers e-commerce stores to stand out and drive sales."", 'pages': '555-560', 'number': '', 'volume': '', 'year': '2024', 'title': 'The Utilization of AI Extends Beyond Payment Systems to E-Commerce Store Development', 'booktitle': '2024 2nd International Conference on Disruptive Technologies (ICDT)', 'author': 'Mimani, Sushant and Ramakrishnan, Rakesh and Rohella, Piyush and Jiwani, Nasmin and Logeshwaran, J.', 'ENTRYTYPE': 'inproceedings', 'ID': '10489393'}"
10533091,Clinical Applications of AI in Post-Cancer Rehabilitation,"Al-Remawi, Mayyas and Aburub, Faisal",Al-Remawi,10.1109/ICCR61006.2024.10533091,2024,2024 2nd International Conference on Cyber Resilience (ICCR),"The article examines the potential of Artificial Intelligence (AI) and machine learning in oncology rehabilitation. Traditional rehabilitation models have limitations in delivering personalized care in real-time. AI technologies close these gaps by utilizing advanced predictive capabilities and optimizing treatment strategies. Convolutional Neural Networks (CNNs) in radiomics provide a proactive approach to managing conditions such as lymphedema. In the field of physical rehabilitation, the integration of robotic systems with AI algorithms allows for real-time adaptive control mechanisms. This integration results in optimized muscle fiber recruitment and improves functional outcomes. Moreover, AI-powered platforms provide individualized psychological and nutritional assistance, enhancing the comprehensive care of individuals who have survived cancer. Despite the promising advancements, ethical considerations, including data privacy and algorithmic bias, necessitate a multidisciplinary approach for responsible implementation. Computational limitations, such as the requirement for extensive labeled datasets, present additional challenges. The analysis highlights the necessity of additional research to validate these emerging technologies, overcome their limitations, and establish ethical frameworks for their responsible clinical implementation.",Ethics;Psychology;Optical fiber networks;Muscles;Oncology;Prediction algorithms;Real-time systems;Post-Cancer Rehabilitation;Artificial Intelligence in Healthcare;Ethical Considerations in AI Implementation;Convolutional Neural Networks;Predictive Modeling in Oncology,"{'month': 'Feb', 'issn': '', 'doi': '10.1109/ICCR61006.2024.10533091', 'keywords': 'Ethics;Psychology;Optical fiber networks;Muscles;Oncology;Prediction algorithms;Real-time systems;Post-Cancer Rehabilitation;Artificial Intelligence in Healthcare;Ethical Considerations in AI Implementation;Convolutional Neural Networks;Predictive Modeling in Oncology', 'abstract': 'The article examines the potential of Artificial Intelligence (AI) and machine learning in oncology rehabilitation. Traditional rehabilitation models have limitations in delivering personalized care in real-time. AI technologies close these gaps by utilizing advanced predictive capabilities and optimizing treatment strategies. Convolutional Neural Networks (CNNs) in radiomics provide a proactive approach to managing conditions such as lymphedema. In the field of physical rehabilitation, the integration of robotic systems with AI algorithms allows for real-time adaptive control mechanisms. This integration results in optimized muscle fiber recruitment and improves functional outcomes. Moreover, AI-powered platforms provide individualized psychological and nutritional assistance, enhancing the comprehensive care of individuals who have survived cancer. Despite the promising advancements, ethical considerations, including data privacy and algorithmic bias, necessitate a multidisciplinary approach for responsible implementation. Computational limitations, such as the requirement for extensive labeled datasets, present additional challenges. The analysis highlights the necessity of additional research to validate these emerging technologies, overcome their limitations, and establish ethical frameworks for their responsible clinical implementation.', 'pages': '1-6', 'number': '', 'volume': '', 'year': '2024', 'title': 'Clinical Applications of AI in Post-Cancer Rehabilitation', 'booktitle': '2024 2nd International Conference on Cyber Resilience (ICCR)', 'author': 'Al-Remawi, Mayyas and Aburub, Faisal', 'ENTRYTYPE': 'inproceedings', 'ID': '10533091'}"
10550305,Navigating Emerging Technologies: A Comprehensive Review of 5G and the Evolving Landscape of 6G Communication,"Dhanasekar, R. and Vijayaraja, L. and Rithika, C and Avanthika, S and Thalapathiraj, S. and Premkumar, R",Dhanasekar,10.1109/IC3IoT60841.2024.10550305,2024,"2024 International Conference on Communication, Computing and Internet of Things (IC3IoT)","The purpose of this review is to investigate the application of artificial intelligence (AI), machine learning (ML), and the internet of things (IoT) within the context of 5G and 6G. The abstract provides an overview of this investigation. It sheds light on the growing demand for these technologies as well as the ways in which they could encourage innovation across a variety of business sectors. The most recent work in these disciplines will be examined, and insights into the potential and difficulties of implementing it in 5G and 6G networks will be provided by the study. The focus of the review is discussed briefly in the abstract; however, additional information on specific research and development fields would be valuable. It is also important to explore the potential repercussions that these technologies may have in the context of 5G and 6G, including the implications that they may have on businesses and on society.",6G mobile communication;Technological innovation;5G mobile communication;Reviews;Navigation;Machine learning;Telecommunication computing;Artificial Intelligence;Machine learning;Internet of Things,"{'month': 'April', 'issn': '', 'doi': '10.1109/IC3IoT60841.2024.10550305', 'keywords': '6G mobile communication;Technological innovation;5G mobile communication;Reviews;Navigation;Machine learning;Telecommunication computing;Artificial Intelligence;Machine learning;Internet of Things', 'abstract': 'The purpose of this review is to investigate the application of artificial intelligence (AI), machine learning (ML), and the internet of things (IoT) within the context of 5G and 6G. The abstract provides an overview of this investigation. It sheds light on the growing demand for these technologies as well as the ways in which they could encourage innovation across a variety of business sectors. The most recent work in these disciplines will be examined, and insights into the potential and difficulties of implementing it in 5G and 6G networks will be provided by the study. The focus of the review is discussed briefly in the abstract; however, additional information on specific research and development fields would be valuable. It is also important to explore the potential repercussions that these technologies may have in the context of 5G and 6G, including the implications that they may have on businesses and on society.', 'pages': '1-6', 'number': '', 'volume': '', 'year': '2024', 'title': 'Navigating Emerging Technologies: A Comprehensive Review of 5G and the Evolving Landscape of 6G Communication', 'booktitle': '2024 International Conference on Communication, Computing and Internet of Things (IC3IoT)', 'author': 'Dhanasekar, R. and Vijayaraja, L. and Rithika, C and Avanthika, S and Thalapathiraj, S. and Premkumar, R', 'ENTRYTYPE': 'inproceedings', 'ID': '10550305'}"
10709187,Exploring Meibomian Gland Dysfunction Grading: A Comparison of Machine Learning and XAI Approaches,"Liang, Congxiao and Wang, Mini Han and Liu, Haoyang and Chong, Kelvin KL",Liang,10.1109/ICIPCA61593.2024.10709187,2024,2024 IEEE 2nd International Conference on Image Processing and Computer Applications (ICIPCA),"This research investigates the effectiveness of employing a machine learning methodology and an Explainable Artificial Intelligence (XAI) tool for the assessment of Meibomian Gland Dysfunction (MGD). Utilizing ResNet34, a convolutional neural network (CNN), trained on a diverse dataset comprising images depicting varying degrees of MGD severity, the machine learning approach endeavors to classify MGD severity into four distinct grades (0, 1, 2, 3) by leveraging extracted characteristic features. Findings reveal comparable accuracy between ResNet34 and the XAI tool, with ResNet34 achieving a slightly lower accuracy of 99.1\% in contrast to the XAI tool's 99.4\%. However, ResNet34 demonstrates marginally higher precision and F1-score at 98.2\% and 98.45\%, respectively, suggesting a nuanced advantage in precision-recall equilibrium. Conversely, the XAI tool maintains a commendable precision of 97.2\% and an Fl-score of 97.94\%, alongside a recall of 98.7\%, underscoring its effectiveness in discerning positive instances. Noteworthy is that ResNet34 operates as a black-box model, providing limited interpretability, while the XAI tool emphasizes transparency by furnishing clinicians with understandable MGD grading elucidations derived from meibomian gland segmentation and atrophy analysis. This transparency enhances trust and comprehension, which are integral for clinical acceptance and decision-making. Furthermore, the utilization of XAI heatmap generation facilitates data quality enhancement, as evidenced by instances of erroneous artifact identification, emphasizing the imperative nature of artifact elimination for the refinement of MGD grading procedures.",Heating systems;Accuracy;Explainable AI;Data integrity;Decision making;Glands;Closed box;Computer applications;Feature extraction;Convolutional neural networks;Meibomian Gland Dysfunction;Machine Learning;Explainable Artificial Intelligence;ResNet34;Interpretability,"{'month': 'June', 'issn': '', 'doi': '10.1109/ICIPCA61593.2024.10709187', 'keywords': 'Heating systems;Accuracy;Explainable AI;Data integrity;Decision making;Glands;Closed box;Computer applications;Feature extraction;Convolutional neural networks;Meibomian Gland Dysfunction;Machine Learning;Explainable Artificial Intelligence;ResNet34;Interpretability', 'abstract': ""This research investigates the effectiveness of employing a machine learning methodology and an Explainable Artificial Intelligence (XAI) tool for the assessment of Meibomian Gland Dysfunction (MGD). Utilizing ResNet34, a convolutional neural network (CNN), trained on a diverse dataset comprising images depicting varying degrees of MGD severity, the machine learning approach endeavors to classify MGD severity into four distinct grades (0, 1, 2, 3) by leveraging extracted characteristic features. Findings reveal comparable accuracy between ResNet34 and the XAI tool, with ResNet34 achieving a slightly lower accuracy of 99.1\\% in contrast to the XAI tool's 99.4\\%. However, ResNet34 demonstrates marginally higher precision and F1-score at 98.2\\% and 98.45\\%, respectively, suggesting a nuanced advantage in precision-recall equilibrium. Conversely, the XAI tool maintains a commendable precision of 97.2\\% and an Fl-score of 97.94\\%, alongside a recall of 98.7\\%, underscoring its effectiveness in discerning positive instances. Noteworthy is that ResNet34 operates as a black-box model, providing limited interpretability, while the XAI tool emphasizes transparency by furnishing clinicians with understandable MGD grading elucidations derived from meibomian gland segmentation and atrophy analysis. This transparency enhances trust and comprehension, which are integral for clinical acceptance and decision-making. Furthermore, the utilization of XAI heatmap generation facilitates data quality enhancement, as evidenced by instances of erroneous artifact identification, emphasizing the imperative nature of artifact elimination for the refinement of MGD grading procedures."", 'pages': '2021-2024', 'number': '', 'volume': '', 'year': '2024', 'title': 'Exploring Meibomian Gland Dysfunction Grading: A Comparison of Machine Learning and XAI Approaches', 'booktitle': '2024 IEEE 2nd International Conference on Image Processing and Computer Applications (ICIPCA)', 'author': 'Liang, Congxiao and Wang, Mini Han and Liu, Haoyang and Chong, Kelvin KL', 'ENTRYTYPE': 'inproceedings', 'ID': '10709187'}"
11119538,Deepfakes in Visual Art: Differentiating AI-Generated Art From Human Art Using Convolutional Neural Networks (CNN),"Tinago, Ngonidzashe and Verkijika, Silas Formunyuy and Eva Mamabolo, Kelibone",Tinago,10.1109/ACCESS.2025.3596882,2025,IEEE Access,"As AI technology evolves, seeing is not believing. The boundary between human and machine creativity is increasingly blurred, presenting challenges for the art industry. This is more pronounced nowadays as advancements in AI technology make it increasingly easy to create highly realistic synthetic art. This study explores the use of Convolutional Neural Networks (CNNs) to differentiate AI-generated art from human-created art. By employing Error Level Analysis (ELA), an image forensic technique for detecting fake and real images, this study develops a robust CNN classifier. Using the AI-ArtBench dataset, the optimal model achieves a 99\% classification accuracy, even when tested on art from a different generative model. While AI-image detection remains a “cat and mouse” pursuit due to advancements in generative AI, the findings of this study highlight that there are clear, discriminable differences between AI-generated and human-created art. The implications of this research extend beyond academic inquiry. They offer support for artists, collectors, curators, and policymakers as they navigate the complexities of AI’s expanding availability and address the evolving role of AI in the art world. It sets the groundwork for application within fields faced with the same or similar challenges.",Art;Deepfakes;Convolutional neural networks;Artificial intelligence;Feature extraction;Accuracy;Generative adversarial networks;Data models;Standards;Diffusion models;Artificial intelligence (AI);convolutional neural network (CNN);deep learning;diffusion model;error level analysis (ELA);generative adversarial network (GAN),"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2025.3596882', 'keywords': 'Art;Deepfakes;Convolutional neural networks;Artificial intelligence;Feature extraction;Accuracy;Generative adversarial networks;Data models;Standards;Diffusion models;Artificial intelligence (AI);convolutional neural network (CNN);deep learning;diffusion model;error level analysis (ELA);generative adversarial network (GAN)', 'abstract': 'As AI technology evolves, seeing is not believing. The boundary between human and machine creativity is increasingly blurred, presenting challenges for the art industry. This is more pronounced nowadays as advancements in AI technology make it increasingly easy to create highly realistic synthetic art. This study explores the use of Convolutional Neural Networks (CNNs) to differentiate AI-generated art from human-created art. By employing Error Level Analysis (ELA), an image forensic technique for detecting fake and real images, this study develops a robust CNN classifier. Using the AI-ArtBench dataset, the optimal model achieves a 99\\% classification accuracy, even when tested on art from a different generative model. While AI-image detection remains a “cat and mouse” pursuit due to advancements in generative AI, the findings of this study highlight that there are clear, discriminable differences between AI-generated and human-created art. The implications of this research extend beyond academic inquiry. They offer support for artists, collectors, curators, and policymakers as they navigate the complexities of AI’s expanding availability and address the evolving role of AI in the art world. It sets the groundwork for application within fields faced with the same or similar challenges.', 'pages': '141484-141495', 'number': '', 'volume': '13', 'year': '2025', 'title': 'Deepfakes in Visual Art: Differentiating AI-Generated Art From Human Art Using Convolutional Neural Networks (CNN)', 'journal': 'IEEE Access', 'author': 'Tinago, Ngonidzashe and Verkijika, Silas Formunyuy and Eva Mamabolo, Kelibone', 'ENTRYTYPE': 'article', 'ID': '11119538'}"
10303076,SoMeMax - A Novel AI-driven Approach to Generate Artificial Social Media Content That Maximises User Engagement,"Stave, Daniel Årrestad and Korneliussen, Hanne and Hjellup, H. Nøkleby and Shrestha, Raju",Stave,10.1109/AIRC57904.2023.10303076,2023,"2023 4th International Conference on Artificial Intelligence, Robotics and Control (AIRC)","Today, many artificial or virtual influencers roam social media platforms to maximise followers and offer commercial options for companies. This work focuses on developing artificial influencers using state-of-the-art techniques within deep learning. Specifically, an autonomous theoretical framework for generating social media content that maximises user engagement is proposed. Deep learning models for generating realistic images and hashtags are trained on a dataset from a social media platform, and content is optimised for user engagement using an evolutionary algorithm. The generated images were evaluated by participants from existing social media users through two separate surveys. The complete framework is built, trained, and tested, and functionality is confirmed. The framework, which appears to be the first of its kind, produces content that matches the users’ preferences well.",Deep learning;Surveys;Social networking (online);Sociology;Evolutionary computation;Companies;Artificial intelligence;AI;Deep learning;CNN;GAN;EA;Social Media;Artificial Influencer;User engagement,"{'month': 'May', 'issn': '', 'doi': '10.1109/AIRC57904.2023.10303076', 'keywords': 'Deep learning;Surveys;Social networking (online);Sociology;Evolutionary computation;Companies;Artificial intelligence;AI;Deep learning;CNN;GAN;EA;Social Media;Artificial Influencer;User engagement', 'abstract': 'Today, many artificial or virtual influencers roam social media platforms to maximise followers and offer commercial options for companies. This work focuses on developing artificial influencers using state-of-the-art techniques within deep learning. Specifically, an autonomous theoretical framework for generating social media content that maximises user engagement is proposed. Deep learning models for generating realistic images and hashtags are trained on a dataset from a social media platform, and content is optimised for user engagement using an evolutionary algorithm. The generated images were evaluated by participants from existing social media users through two separate surveys. The complete framework is built, trained, and tested, and functionality is confirmed. The framework, which appears to be the first of its kind, produces content that matches the users’ preferences well.', 'pages': '94-100', 'number': '', 'volume': '', 'year': '2023', 'title': 'SoMeMax - A Novel AI-driven Approach to Generate Artificial Social Media Content That Maximises User Engagement', 'booktitle': '2023 4th International Conference on Artificial Intelligence, Robotics and Control (AIRC)', 'author': 'Stave, Daniel Årrestad and Korneliussen, Hanne and Hjellup, H. Nøkleby and Shrestha, Raju', 'ENTRYTYPE': 'inproceedings', 'ID': '10303076'}"
10823621,Exploring Potential Applications of Generative Artificial Intelligence in Future Healthcare: The Case of Sora,"Liu, Yonggang and Awang, Hapini and Mansor, Nur Suhaili",Liu,10.1109/NETAPPS63333.2024.10823621,2024,"2024 7th International Conference on Internet Applications, Protocols, and Services (NETAPPS)","In February 2024, OpenAI unveiled Sora, which built on the foundation of ChatGPT and was able to generate videos based on text, representing one of the most advanced Generative Artificial Intelligences (GAIs) in the current world. As a diffusion model, Sora has the ability to generate long and imaginative videos with multiple characters, genre-specific movements, and sophisticated scenarios based only on textual descriptions, as well as excellent scalability. The research objectives are formulated as follows: to explore Sora's potential applications in future healthcare; to identify Sora's potential influence on visualizations of the healthcare industry; and to investigate Sora's potential impact on diagnostic methods. This study adopts the documentary research method. This study finds that Sora has great potential applications in future healthcare in the following aspects: healthcare robots, virtual doctors, simulating surgical procedures, visualizations of medical academic achievements, visualizing medical records, private assistant-type and companion-type robotic doctors, humanmachine interaction in healthcare, reducing burnout among doctors and nurses and so on. This study is one of the earliest to research Sora's potential applications in future healthcare in above mentioned aspects. The current study will not only enrich theoretical research on the integration of GAI (especially Sora) and healthcare, but also contribute to healthcare practice.",Industries;Visualization;Technological innovation;Protocols;Service robots;Scalability;Medical services;Chatbots;Text to video;Medical diagnostic imaging;Sora;Generative Artificial Intelligence (GAI);Healthcare,"{'month': 'Nov', 'issn': '', 'doi': '10.1109/NETAPPS63333.2024.10823621', 'keywords': 'Industries;Visualization;Technological innovation;Protocols;Service robots;Scalability;Medical services;Chatbots;Text to video;Medical diagnostic imaging;Sora;Generative Artificial Intelligence (GAI);Healthcare', 'abstract': ""In February 2024, OpenAI unveiled Sora, which built on the foundation of ChatGPT and was able to generate videos based on text, representing one of the most advanced Generative Artificial Intelligences (GAIs) in the current world. As a diffusion model, Sora has the ability to generate long and imaginative videos with multiple characters, genre-specific movements, and sophisticated scenarios based only on textual descriptions, as well as excellent scalability. The research objectives are formulated as follows: to explore Sora's potential applications in future healthcare; to identify Sora's potential influence on visualizations of the healthcare industry; and to investigate Sora's potential impact on diagnostic methods. This study adopts the documentary research method. This study finds that Sora has great potential applications in future healthcare in the following aspects: healthcare robots, virtual doctors, simulating surgical procedures, visualizations of medical academic achievements, visualizing medical records, private assistant-type and companion-type robotic doctors, humanmachine interaction in healthcare, reducing burnout among doctors and nurses and so on. This study is one of the earliest to research Sora's potential applications in future healthcare in above mentioned aspects. The current study will not only enrich theoretical research on the integration of GAI (especially Sora) and healthcare, but also contribute to healthcare practice."", 'pages': '1-8', 'number': '', 'volume': '', 'year': '2024', 'title': 'Exploring Potential Applications of Generative Artificial Intelligence in Future Healthcare: The Case of Sora', 'booktitle': '2024 7th International Conference on Internet Applications, Protocols, and Services (NETAPPS)', 'author': 'Liu, Yonggang and Awang, Hapini and Mansor, Nur Suhaili', 'ENTRYTYPE': 'inproceedings', 'ID': '10823621'}"
10932698,Emergency Routing Protocol for Intelligent Transportation Systems Using IoT and Generative Artificial Intelligence,"Song, Shijun and Fan, Min",Song,10.1109/TITS.2025.3546340,2025,IEEE Transactions on Intelligent Transportation Systems,"Urban populations are always on the increase, increasing the number of vehicles, leading to severe congestion, longer travel times, and substantial emergency vehicle response time delays. This paper presents a routing protocol for Intelligent Transportation Systems (ITS), incorporating a Belief-Desire-Intention (BDI) model and generative Artificial Intelligence (AI) techniques. The proposed system utilizes BDI-based generative AI models, which rely on predefined logic and rules rather than machine learning to make real-time routing decisions. These decisions are based on data from vehicle-to-vehicle (V2V) communications, roadside units (RSUs) and IoT sensors, which integrate traffic density, congestion levels, collision avoidance and hazard detection. The system aims to optimize the accuracy of route selection, energy consumption, and communication latency. The simulation results, implemented using NS3 for real-world traffic scenarios, show improvements in route selection accuracy, collision avoidance, and energy efficiency compared to traditional routing methods. Specifically, the proposed system achieved a route selection accuracy of 95\%, a collision avoidance rate of 95\%, and reduced the communication latency to 105 ms, outperforming the other two methods. Furthermore, energy consumption was minimized and reduced to 85 J per route. These results highlight the potential of BDI-based routing with generative AI to improve ITS performance, particularly in real-time traffic management.",Real-time systems;Routing protocols;Routing;Generative AI;Sensors;Artificial intelligence;Decision making;Intelligent sensors;Transportation;Sensor systems;BDI-based decision making;real-time routing;intelligent transportation systems;generative AI;NS3 simulation,"{'month': '', 'issn': '1558-0016', 'doi': '10.1109/TITS.2025.3546340', 'keywords': 'Real-time systems;Routing protocols;Routing;Generative AI;Sensors;Artificial intelligence;Decision making;Intelligent sensors;Transportation;Sensor systems;BDI-based decision making;real-time routing;intelligent transportation systems;generative AI;NS3 simulation', 'abstract': 'Urban populations are always on the increase, increasing the number of vehicles, leading to severe congestion, longer travel times, and substantial emergency vehicle response time delays. This paper presents a routing protocol for Intelligent Transportation Systems (ITS), incorporating a Belief-Desire-Intention (BDI) model and generative Artificial Intelligence (AI) techniques. The proposed system utilizes BDI-based generative AI models, which rely on predefined logic and rules rather than machine learning to make real-time routing decisions. These decisions are based on data from vehicle-to-vehicle (V2V) communications, roadside units (RSUs) and IoT sensors, which integrate traffic density, congestion levels, collision avoidance and hazard detection. The system aims to optimize the accuracy of route selection, energy consumption, and communication latency. The simulation results, implemented using NS3 for real-world traffic scenarios, show improvements in route selection accuracy, collision avoidance, and energy efficiency compared to traditional routing methods. Specifically, the proposed system achieved a route selection accuracy of 95\\%, a collision avoidance rate of 95\\%, and reduced the communication latency to 105 ms, outperforming the other two methods. Furthermore, energy consumption was minimized and reduced to 85 J per route. These results highlight the potential of BDI-based routing with generative AI to improve ITS performance, particularly in real-time traffic management.', 'pages': '1-12', 'number': '', 'volume': '', 'year': '2025', 'title': 'Emergency Routing Protocol for Intelligent Transportation Systems Using IoT and Generative Artificial Intelligence', 'journal': 'IEEE Transactions on Intelligent Transportation Systems', 'author': 'Song, Shijun and Fan, Min', 'ENTRYTYPE': 'article', 'ID': '10932698'}"
10408888,Anomaly Detection Method Based on Gaussian Mixture Model and Orthogonal Generative Adversarial Network,"Fu, Cong and Tao, Hongzhu and Shang, Jingan and Huang, Yunhao and Wang, Jiaqi and Xu, Kai and Xin Ma, Xin and Sheng, XinXin",Fu,10.1109/ITAIC58329.2023.10408888,2023,2023 IEEE 11th Joint International Information Technology and Artificial Intelligence Conference (ITAIC),"In the analysis of power grid data, a critical task is to identify samples that significantly deviate from others, which constitutes anomaly detection. Presently, anomaly detection methods based on deep generative models face challenges related to complex network structures and difficult training processes. Addressing these issues, this paper proposes a lightweight anomaly detection method for power grid data based on Gaussian Mixture Model and Orthogonal Generative Adversarial Network. This approach combines traditional statistical techniques with advanced deep learning methods. In comparison to anomaly detection methods relying solely on deep generative models, this method features a more concise network structure, reducing both the model's complexity and training time, while retaining a certain level of expressive power.",Training;Deep learning;Feature extraction;Generative adversarial networks;Encoding;Anomaly detection;Gaussian mixture model;Anomaly Detection;Gaussian Mixture Model;Generative Adversarial Network,"{'month': 'Dec', 'issn': '2693-2865', 'doi': '10.1109/ITAIC58329.2023.10408888', 'keywords': 'Training;Deep learning;Feature extraction;Generative adversarial networks;Encoding;Anomaly detection;Gaussian mixture model;Anomaly Detection;Gaussian Mixture Model;Generative Adversarial Network', 'abstract': ""In the analysis of power grid data, a critical task is to identify samples that significantly deviate from others, which constitutes anomaly detection. Presently, anomaly detection methods based on deep generative models face challenges related to complex network structures and difficult training processes. Addressing these issues, this paper proposes a lightweight anomaly detection method for power grid data based on Gaussian Mixture Model and Orthogonal Generative Adversarial Network. This approach combines traditional statistical techniques with advanced deep learning methods. In comparison to anomaly detection methods relying solely on deep generative models, this method features a more concise network structure, reducing both the model's complexity and training time, while retaining a certain level of expressive power."", 'pages': '1909-1915', 'number': '', 'volume': '11', 'year': '2023', 'title': 'Anomaly Detection Method Based on Gaussian Mixture Model and Orthogonal Generative Adversarial Network', 'booktitle': '2023 IEEE 11th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)', 'author': 'Fu, Cong and Tao, Hongzhu and Shang, Jingan and Huang, Yunhao and Wang, Jiaqi and Xu, Kai and Xin Ma, Xin and Sheng, XinXin', 'ENTRYTYPE': 'inproceedings', 'ID': '10408888'}"
11158710,Prospects of Generative Video Technology in the Field of Chinese Classical Literature: Exploring Female Education Through the Reimagining of Chinese Poetic Imagery Using Runway ML Gen-3 Alpha,"Wang, Yunning",Wang,10.1109/ICAIE64856.2025.11158710,2025,2025 5th International Conference on Artificial Intelligence and Education (ICAIE),"With the rapid advancement of artificial intelligence, generative video technology offers new possibilities in the field of education, particularly in enhancing women's literary skills and driving educational innovation. Chinese classical poetry, as a cultural treasure, often poses challenges for students due to its abstract imagery and complex content. These challenges are especially pronounced in women's education, where traditional teaching methods often lack intuitiveness and interactivity. This study, based on the generative video capabilities of the Runway ML Gen-3 Alpha model, explores how video generation technology can enhance female students' interest in learning and improve their memory retention by enabling them to perceive the imagery of classical poetry more intuitively. The research analyzes the applicability of generative video technology in women's education from a theoretical perspective and proposes specific methods and pathways for integrating it into classical literature teaching. By emphasizing women's cognitive strengths in visual perception and emotional resonance, the study further investigates how technology-driven teaching models can optimize educational outcomes and achieve a deep integration of traditional literature with modern technology. The findings indicate that generative video technology not only enhances the vividness of learning but also promotes the innovation of educational methods, offering new directions and approaches for cultural heritage and educational reform in the future.",Surveys;Technological innovation;Visualization;Atmospheric modeling;Education;Resonance;Cultural differences;Artificial intelligence;Videos;Visual perception;Women's Education;Multimedia Teaching;Educational Innovation;Runway ML;Gen-3 Alpha,"{'month': 'May', 'issn': '', 'doi': '10.1109/ICAIE64856.2025.11158710', 'keywords': ""Surveys;Technological innovation;Visualization;Atmospheric modeling;Education;Resonance;Cultural differences;Artificial intelligence;Videos;Visual perception;Women's Education;Multimedia Teaching;Educational Innovation;Runway ML;Gen-3 Alpha"", 'abstract': ""With the rapid advancement of artificial intelligence, generative video technology offers new possibilities in the field of education, particularly in enhancing women's literary skills and driving educational innovation. Chinese classical poetry, as a cultural treasure, often poses challenges for students due to its abstract imagery and complex content. These challenges are especially pronounced in women's education, where traditional teaching methods often lack intuitiveness and interactivity. This study, based on the generative video capabilities of the Runway ML Gen-3 Alpha model, explores how video generation technology can enhance female students' interest in learning and improve their memory retention by enabling them to perceive the imagery of classical poetry more intuitively. The research analyzes the applicability of generative video technology in women's education from a theoretical perspective and proposes specific methods and pathways for integrating it into classical literature teaching. By emphasizing women's cognitive strengths in visual perception and emotional resonance, the study further investigates how technology-driven teaching models can optimize educational outcomes and achieve a deep integration of traditional literature with modern technology. The findings indicate that generative video technology not only enhances the vividness of learning but also promotes the innovation of educational methods, offering new directions and approaches for cultural heritage and educational reform in the future."", 'pages': '407-411', 'number': '', 'volume': '', 'year': '2025', 'title': 'Prospects of Generative Video Technology in the Field of Chinese Classical Literature: Exploring Female Education Through the Reimagining of Chinese Poetic Imagery Using Runway ML Gen-3 Alpha', 'booktitle': '2025 5th International Conference on Artificial Intelligence and Education (ICAIE)', 'author': 'Wang, Yunning', 'ENTRYTYPE': 'inproceedings', 'ID': '11158710'}"
10593404,Generative AI in Business Analytics by Digital Transformation of Artificial Intelligence Techniques,"Pooja and Krishna, Somanchi Hari and Kumar, G.M. Prem and Reddy, Y Manohar and Ayarekar, Sachin and Lourens, Melanie",Pooja,10.1109/IC3SE62002.2024.10593404,2024,"2024 International Conference on Communication, Computer Sciences and Engineering (IC3SE)","The advent of Digital Transformation introduces numerous challenges, opportunities, and changes for both the economy and society. Companies are confronted with rapidly evolving dynamics and must enhance their agility to effectively leverage these advancements. However, a significant aspect of Digital Transformation is the increasing prominence of Artificial Intelligence (AI). This document proposes a pragmatic approach to change management for organizations, encompassing both top-down and bottom-up elements. The aim is to facilitate a structured, organization-wide transformation process for the integration of AI technology into daily business operations. This approach endeavours to address the organization comprehensively and seeks to seamlessly integrate AI within existing strategic frameworks. In today's swiftly evolving business landscape, ongoing employee learning stands as a crucial element for organizational success. Businesses must prioritize transforming their capabilities and prioritize learning as a strategic imperative. To ensure the sustainability of their operations, organizational leadership should concentrate on enhancing employees' skills and promoting mental well-being. This involves gradually integrating new techniques that facilitate transformative change and fostering an environment conducive to disrupting prevailing patterns and nurturing the emergence of new ones. An example of such transformation is the rapid acceleration of digitalization.",Technological innovation;Leadership;Generative AI;Digital transformation;Bibliographies;Key performance indicator;Companies;SMEs;Digital transformation;Business Analytics;Artificial Intelligence,"{'month': 'May', 'issn': '', 'doi': '10.1109/IC3SE62002.2024.10593404', 'keywords': 'Technological innovation;Leadership;Generative AI;Digital transformation;Bibliographies;Key performance indicator;Companies;SMEs;Digital transformation;Business Analytics;Artificial Intelligence', 'abstract': ""The advent of Digital Transformation introduces numerous challenges, opportunities, and changes for both the economy and society. Companies are confronted with rapidly evolving dynamics and must enhance their agility to effectively leverage these advancements. However, a significant aspect of Digital Transformation is the increasing prominence of Artificial Intelligence (AI). This document proposes a pragmatic approach to change management for organizations, encompassing both top-down and bottom-up elements. The aim is to facilitate a structured, organization-wide transformation process for the integration of AI technology into daily business operations. This approach endeavours to address the organization comprehensively and seeks to seamlessly integrate AI within existing strategic frameworks. In today's swiftly evolving business landscape, ongoing employee learning stands as a crucial element for organizational success. Businesses must prioritize transforming their capabilities and prioritize learning as a strategic imperative. To ensure the sustainability of their operations, organizational leadership should concentrate on enhancing employees' skills and promoting mental well-being. This involves gradually integrating new techniques that facilitate transformative change and fostering an environment conducive to disrupting prevailing patterns and nurturing the emergence of new ones. An example of such transformation is the rapid acceleration of digitalization."", 'pages': '1532-1536', 'number': '', 'volume': '', 'year': '2024', 'title': 'Generative AI in Business Analytics by Digital Transformation of Artificial Intelligence Techniques', 'booktitle': '2024 International Conference on Communication, Computer Sciences and Engineering (IC3SE)', 'author': 'Pooja and Krishna, Somanchi Hari and Kumar, G.M. Prem and Reddy, Y Manohar and Ayarekar, Sachin and Lourens, Melanie', 'ENTRYTYPE': 'inproceedings', 'ID': '10593404'}"
10392083,Improving Image Quality of Noisy Images Through Denoising and Style GAN Technique,"Ghadekar, Premanand and Gundawar, Ayush and Kamnapure, Somesh and Manjramkar, Devang and Gujarathi, Ishan and Deore, Dhananjay",Ghadekar,10.1109/ICCUBEA58933.2023.10392083,2023,"2023 7th International Conference On Computing, Communication, Control And Automation (ICCUBEA)","This research proposes an approach to enhance the denoising and upscaling performance of noisy images using Generative Adversarial Networks (GANs), particularly Style GAN architecture. Denoising and upscaling noisy images are crucial in many computer vision applications, and GANs have shown remarkable effectiveness in creating high-quality images. However, training Style GAN requires huge amount of data and is computationally expensive. To address this issue, this study proposes using various filters such as mean, median, and weighted median to pre-process the noisy images before feeding them to Style GAN. The proposed approach achieves superior denoising and up scaling compared with other system in terms of FID and inception score, and further exploration of hyperparameters and variations of the Style GAN architecture can lead to even better results.",Image quality;Training;Filtering;Noise reduction;Superresolution;Computer architecture;Generative adversarial networks;Diffusion Modelling;Style Gan;Deep learning;Super resolution;Image Processing;median filtering,"{'month': 'Aug', 'issn': '2771-1358', 'doi': '10.1109/ICCUBEA58933.2023.10392083', 'keywords': 'Image quality;Training;Filtering;Noise reduction;Superresolution;Computer architecture;Generative adversarial networks;Diffusion Modelling;Style Gan;Deep learning;Super resolution;Image Processing;median filtering', 'abstract': 'This research proposes an approach to enhance the denoising and upscaling performance of noisy images using Generative Adversarial Networks (GANs), particularly Style GAN architecture. Denoising and upscaling noisy images are crucial in many computer vision applications, and GANs have shown remarkable effectiveness in creating high-quality images. However, training Style GAN requires huge amount of data and is computationally expensive. To address this issue, this study proposes using various filters such as mean, median, and weighted median to pre-process the noisy images before feeding them to Style GAN. The proposed approach achieves superior denoising and up scaling compared with other system in terms of FID and inception score, and further exploration of hyperparameters and variations of the Style GAN architecture can lead to even better results.', 'pages': '1-6', 'number': '', 'volume': '', 'year': '2023', 'title': 'Improving Image Quality of Noisy Images Through Denoising and Style GAN Technique', 'booktitle': '2023 7th International Conference On Computing, Communication, Control And Automation (ICCUBEA)', 'author': 'Ghadekar, Premanand and Gundawar, Ayush and Kamnapure, Somesh and Manjramkar, Devang and Gujarathi, Ishan and Deore, Dhananjay', 'ENTRYTYPE': 'inproceedings', 'ID': '10392083'}"
10770591,Synthetic Data Generation via Generative Adversarial Networks in Healthcare: A Systematic Review of Image- and Signal-Based Studies,"Akpinar, Muhammed Halil and Sengur, Abdulkadir and Salvi, Massimo and Seoni, Silvia and Faust, Oliver and Mir, Hasan and Molinari, Filippo and Acharya, U. Rajendra",Akpinar,10.1109/OJEMB.2024.3508472,2025,IEEE Open Journal of Engineering in Medicine and Biology,"Generative Adversarial Networks (GANs) have emerged as a powerful tool in artificial intelligence, particularly for unsupervised learning. This systematic review analyzes GAN applications in healthcare, focusing on image and signal-based studies across various clinical domains. Following Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA) guidelines, we reviewed 72 relevant journal articles. Our findings reveal that magnetic resonance imaging (MRI) and electrocardiogram (ECG) signal acquisition techniques were most utilized, with brain studies (22\%), cardiology (18\%), cancer (15\%), ophthalmology (12\%), and lung studies (10\%) being the most researched areas. We discuss key GAN architectures, including cGAN (31\%) and CycleGAN (18\%), along with datasets, evaluation metrics, and performance outcomes. The review highlights promising data augmentation, anonymization, and multi-task learning results. We identify current limitations, such as the lack of standardized metrics and direct comparisons, and propose future directions, including the development of no-reference metrics, immersive simulation scenarios, and enhanced interpretability.",Generative adversarial networks;Biomedical imaging;Measurement;Magnetic resonance imaging;Medical diagnostic imaging;Imaging;Generators;Synthetic data;Computed tomography;Training;Generative adversarial networks (GANs);medical imaging;data generation;signal simulation;deep learning,"{'month': '', 'issn': '2644-1276', 'doi': '10.1109/OJEMB.2024.3508472', 'keywords': 'Generative adversarial networks;Biomedical imaging;Measurement;Magnetic resonance imaging;Medical diagnostic imaging;Imaging;Generators;Synthetic data;Computed tomography;Training;Generative adversarial networks (GANs);medical imaging;data generation;signal simulation;deep learning', 'abstract': 'Generative Adversarial Networks (GANs) have emerged as a powerful tool in artificial intelligence, particularly for unsupervised learning. This systematic review analyzes GAN applications in healthcare, focusing on image and signal-based studies across various clinical domains. Following Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA) guidelines, we reviewed 72 relevant journal articles. Our findings reveal that magnetic resonance imaging (MRI) and electrocardiogram (ECG) signal acquisition techniques were most utilized, with brain studies (22\\%), cardiology (18\\%), cancer (15\\%), ophthalmology (12\\%), and lung studies (10\\%) being the most researched areas. We discuss key GAN architectures, including cGAN (31\\%) and CycleGAN (18\\%), along with datasets, evaluation metrics, and performance outcomes. The review highlights promising data augmentation, anonymization, and multi-task learning results. We identify current limitations, such as the lack of standardized metrics and direct comparisons, and propose future directions, including the development of no-reference metrics, immersive simulation scenarios, and enhanced interpretability.', 'pages': '183-192', 'number': '', 'volume': '6', 'year': '2025', 'title': 'Synthetic Data Generation via Generative Adversarial Networks in Healthcare: A Systematic Review of Image- and Signal-Based Studies', 'journal': 'IEEE Open Journal of Engineering in Medicine and Biology', 'author': 'Akpinar, Muhammed Halil and Sengur, Abdulkadir and Salvi, Massimo and Seoni, Silvia and Faust, Oliver and Mir, Hasan and Molinari, Filippo and Acharya, U. Rajendra', 'ENTRYTYPE': 'article', 'ID': '10770591'}"
10143123,ChatGPT and Generative AI Guidelines for Addressing Academic Integrity and Augmenting Pre-Existing Chatbots,"De Silva, Daswin and Mills, Nishan and El-Ayoubi, Mona and Manic, Milos and Alahakoon, Damminda",De Silva,10.1109/ICIT58465.2023.10143123,2023,2023 IEEE International Conference on Industrial Technology (ICIT),"Chat Generative Pretrained Transformer (Chat-GPT) and related Generative AI models are leading a paradigm shift in the acceptance and application of Artificial Intelligence (AI) across all disciplines and industry sectors. Despite the criticisms of an ‘intelligence without knowledge or reasoning or the notions of truth’, ChatGPT is highly effective at human-like conversation with seemingly sophisticated and useful responses to questions, summarization, classification, extraction and generation tasks. Unlike similar large AI models in the modalities of image, audio and video, text-based conversation is straightforward and familiar to a large audience of regular users of the Internet and smartphone applications. This is further accentuated by the large-scale adoption of ‘standard’ chatbot technologies for trivial conversations in task-specific automation, across every industry sector. This rare combination of highly effective human-like conversation, familiarity of foundational technology and versatility of intelligent application, has led to several challenges and opportunities in leveraging generative AI. A primary challenge is its impact on the academic integrity of scholarly work, where AI-generated content can be useful and detrimental in both teaching and research. On the other hand, ChatGPT presents a unique opportunity in augmenting preexisting (‘standard’) chatbots with human-like conversation for advanced intelligent automation, across all application domains. Although diametrically opposed, the challenge of addressing academic integrity and the opportunity of augmenting pre-existing chatbots are grounded in the conversational AI capabilities of ChatGPT and similar generative AI models. In this paper, we investigate these formative capabilities and present guidelines for leveraging ChatGPT and similar generative AI models.",Industries;Intelligent automation;Education;Oral communication;Chatbots;Transformers;Internet;ChatGPT;Academic Integrity;Generative AI;Intelligent Automation;Chatbot;Artificial Intelligence;Pretrained Language Models;GPT3,"{'month': 'April', 'issn': '2643-2978', 'doi': '10.1109/ICIT58465.2023.10143123', 'keywords': 'Industries;Intelligent automation;Education;Oral communication;Chatbots;Transformers;Internet;ChatGPT;Academic Integrity;Generative AI;Intelligent Automation;Chatbot;Artificial Intelligence;Pretrained Language Models;GPT3', 'abstract': 'Chat Generative Pretrained Transformer (Chat-GPT) and related Generative AI models are leading a paradigm shift in the acceptance and application of Artificial Intelligence (AI) across all disciplines and industry sectors. Despite the criticisms of an ‘intelligence without knowledge or reasoning or the notions of truth’, ChatGPT is highly effective at human-like conversation with seemingly sophisticated and useful responses to questions, summarization, classification, extraction and generation tasks. Unlike similar large AI models in the modalities of image, audio and video, text-based conversation is straightforward and familiar to a large audience of regular users of the Internet and smartphone applications. This is further accentuated by the large-scale adoption of ‘standard’ chatbot technologies for trivial conversations in task-specific automation, across every industry sector. This rare combination of highly effective human-like conversation, familiarity of foundational technology and versatility of intelligent application, has led to several challenges and opportunities in leveraging generative AI. A primary challenge is its impact on the academic integrity of scholarly work, where AI-generated content can be useful and detrimental in both teaching and research. On the other hand, ChatGPT presents a unique opportunity in augmenting preexisting (‘standard’) chatbots with human-like conversation for advanced intelligent automation, across all application domains. Although diametrically opposed, the challenge of addressing academic integrity and the opportunity of augmenting pre-existing chatbots are grounded in the conversational AI capabilities of ChatGPT and similar generative AI models. In this paper, we investigate these formative capabilities and present guidelines for leveraging ChatGPT and similar generative AI models.', 'pages': '1-6', 'number': '', 'volume': '', 'year': '2023', 'title': 'ChatGPT and Generative AI Guidelines for Addressing Academic Integrity and Augmenting Pre-Existing Chatbots', 'booktitle': '2023 IEEE International Conference on Industrial Technology (ICIT)', 'author': 'De Silva, Daswin and Mills, Nishan and El-Ayoubi, Mona and Manic, Milos and Alahakoon, Damminda', 'ENTRYTYPE': 'inproceedings', 'ID': '10143123'}"
11163176,Wavelet-Based Dual-Flow Feature Collaborativing GAN for Image Inpainting,"Yan, Jie and Yang, Tao and Zhou, Zhinan and Yuan, Haijing and Liu, Shan",Yan,10.1109/ITAIC64559.2025.11163176,2025,2025 IEEE 12th Joint International Information Technology and Artificial Intelligence Conference (ITAIC),"Image inpainting is an important field in artificial intelligence, that aims to generate exquisite images from incomplete input images. At present, most methods utilize unilateral structural information to assist in the repair process, but when there is extensive distortion in the image, this method cannot generate beautiful results. Based on this, a wavelet-based dual-flow feature collaborativing GAN (WDFC-GAN) is proposed, which consists of two encoding and decoding subnetworks, namely the wavelet stream and Transformer stream. The wavelet stream is devised to adopt the wavelet transform for better capturing frequency domain features. The Transformer stream is presented to use stacked fast Fourier Transformers (FFTr) to expand the receptive fields effectively. Besides, a wavelet-based self-attention (WSA) is presented for constructing long-range correlated prominent structural information in multidimensional frequency information, thereby generating better results. A large number of qualitative and quantitative experiments have shown that the proposed method outperforms the state-of-the-art methods on Paris StreetView and CelebA-HQ.",Wavelet transforms;Wavelet domain;Frequency-domain analysis;Semantics;Streaming media;Maintenance engineering;Transformers;Generative adversarial networks;Feature extraction;Artificial intelligence;Image inpainting;fast Fourier Transformer;self-attention;generative adversarial network,"{'month': 'May', 'issn': '2693-2865', 'doi': '10.1109/ITAIC64559.2025.11163176', 'keywords': 'Wavelet transforms;Wavelet domain;Frequency-domain analysis;Semantics;Streaming media;Maintenance engineering;Transformers;Generative adversarial networks;Feature extraction;Artificial intelligence;Image inpainting;fast Fourier Transformer;self-attention;generative adversarial network', 'abstract': 'Image inpainting is an important field in artificial intelligence, that aims to generate exquisite images from incomplete input images. At present, most methods utilize unilateral structural information to assist in the repair process, but when there is extensive distortion in the image, this method cannot generate beautiful results. Based on this, a wavelet-based dual-flow feature collaborativing GAN (WDFC-GAN) is proposed, which consists of two encoding and decoding subnetworks, namely the wavelet stream and Transformer stream. The wavelet stream is devised to adopt the wavelet transform for better capturing frequency domain features. The Transformer stream is presented to use stacked fast Fourier Transformers (FFTr) to expand the receptive fields effectively. Besides, a wavelet-based self-attention (WSA) is presented for constructing long-range correlated prominent structural information in multidimensional frequency information, thereby generating better results. A large number of qualitative and quantitative experiments have shown that the proposed method outperforms the state-of-the-art methods on Paris StreetView and CelebA-HQ.', 'pages': '412-418', 'number': '', 'volume': '12', 'year': '2025', 'title': 'Wavelet-Based Dual-Flow Feature Collaborativing GAN for Image Inpainting', 'booktitle': '2025 IEEE 12th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)', 'author': 'Yan, Jie and Yang, Tao and Zhou, Zhinan and Yuan, Haijing and Liu, Shan', 'ENTRYTYPE': 'inproceedings', 'ID': '11163176'}"
10723573,Improving Directional Resolution of Ocean Sound Sources Based on Oceanic Boundary Generative Adversarial Network,"Sun, Wan and Wang, Tong and Liang, Guolong and Wang, Jinjin",Sun,10.1109/COA58979.2024.10723573,2024,2024 OES China Ocean Acoustics (COA),"The rapid development of artificial intelligence has brought new possibilities to signal processing. In the field of ocean moving target monitoring, traditional array signal processing faces significant challenges due to strong ocean background noise and observation equipment limitations. This paper presents a novel approach for enhancing the directional resolution of ocean sound sources using an oceanic boundary generative adversarial network (OB-GAN). Traditional methods used to analyze underwater sound signals often face challenges in accurately determining the direction of sound sources due to complex environmental conditions. The proposed OB-GAN framework leverages generative adversarial networks (GANs) to learn and enhance the spatial features of underwater sound signals, enabling more precise localization of sound sources. Experimental results demonstrate significant improvements in the directional resolution of ocean sound sources compared to existing methods, showcasing the potential of OB-GAN for advancing underwater acoustic signal processing.",Location awareness;Oceans;Parallel processing;Generative adversarial networks;Background noise;Spatial resolution;Underwater acoustics;Signal resolution;Faces;Monitoring;resolution enhance;sound source improvement;artificial intelligence;OB-GAN,"{'month': 'May', 'issn': '', 'doi': '10.1109/COA58979.2024.10723573', 'keywords': 'Location awareness;Oceans;Parallel processing;Generative adversarial networks;Background noise;Spatial resolution;Underwater acoustics;Signal resolution;Faces;Monitoring;resolution enhance;sound source improvement;artificial intelligence;OB-GAN', 'abstract': 'The rapid development of artificial intelligence has brought new possibilities to signal processing. In the field of ocean moving target monitoring, traditional array signal processing faces significant challenges due to strong ocean background noise and observation equipment limitations. This paper presents a novel approach for enhancing the directional resolution of ocean sound sources using an oceanic boundary generative adversarial network (OB-GAN). Traditional methods used to analyze underwater sound signals often face challenges in accurately determining the direction of sound sources due to complex environmental conditions. The proposed OB-GAN framework leverages generative adversarial networks (GANs) to learn and enhance the spatial features of underwater sound signals, enabling more precise localization of sound sources. Experimental results demonstrate significant improvements in the directional resolution of ocean sound sources compared to existing methods, showcasing the potential of OB-GAN for advancing underwater acoustic signal processing.', 'pages': '1-5', 'number': '', 'volume': '', 'year': '2024', 'title': 'Improving Directional Resolution of Ocean Sound Sources Based on Oceanic Boundary Generative Adversarial Network', 'booktitle': '2024 OES China Ocean Acoustics (COA)', 'author': 'Sun, Wan and Wang, Tong and Liang, Guolong and Wang, Jinjin', 'ENTRYTYPE': 'inproceedings', 'ID': '10723573'}"
8947758,Voice Conversion Using Conditional CycleGAN,"Yook, Dongsuk and Yoo, In-Chul and Yoo, Seungho",Yook,10.1109/CSCI46756.2018.00290,2018,2018 International Conference on Computational Science and Computational Intelligence (CSCI),"Voice conversion (VC) modifies characteristics of speech, such as gender and speaker identities. The VC can be applied to various tasks including speaking assistance and speaker anonymization. Generally, such VC techniques require parallel speech data for training, which is very expensive. Recently, voice conversion has been accomplished using CycleGAN, which does not require parallel speech data. In this paper, we further extend the idea of using CycleGAN to convert multiple speakers' voices by conditioning the CycleGAN using speaker identity information.",Generative adversarial networks;Generators;Gallium nitride;Task analysis;Logic gates;Training;Linguistics;voice conversion;generative adversarial networks (GAN);CycleGAN;Conditional CycleGAN (CC-GAN),"{'month': 'Dec', 'issn': '', 'doi': '10.1109/CSCI46756.2018.00290', 'keywords': 'Generative adversarial networks;Generators;Gallium nitride;Task analysis;Logic gates;Training;Linguistics;voice conversion;generative adversarial networks (GAN);CycleGAN;Conditional CycleGAN (CC-GAN)', 'abstract': ""Voice conversion (VC) modifies characteristics of speech, such as gender and speaker identities. The VC can be applied to various tasks including speaking assistance and speaker anonymization. Generally, such VC techniques require parallel speech data for training, which is very expensive. Recently, voice conversion has been accomplished using CycleGAN, which does not require parallel speech data. In this paper, we further extend the idea of using CycleGAN to convert multiple speakers' voices by conditioning the CycleGAN using speaker identity information."", 'pages': '1460-1461', 'number': '', 'volume': '', 'year': '2018', 'title': 'Voice Conversion Using Conditional CycleGAN', 'booktitle': '2018 International Conference on Computational Science and Computational Intelligence (CSCI)', 'author': 'Yook, Dongsuk and Yoo, In-Chul and Yoo, Seungho', 'ENTRYTYPE': 'inproceedings', 'ID': '8947758'}"
9792975,Image Resolution Enhancer using Deep Learning,"Mittal, Harsh and Rai, Vaibhav and Sonawane, Swaraj and Mhatre, Sneha",Mittal,10.1109/ICAAIC53929.2022.9792975,2022,2022 International Conference on Applied Artificial Intelligence and Computing (ICAAIC),"Image Super-Resolution is a technique that is used to obtain high-resolution, realistic images from low-resolution input images. Deep learning algorithms such as SRCNN, ESRGAN, RDN, etc. have shown significant results in this field. But these algorithms at times vary in results. To solve this problem, this research study has proposed an image super-resolution by using Patch Extraction on Deep Learning Algorithm, in which the LR image is first divided into patches and then the algorithms like RDN and ESRGAN are applied. Comparing each patch from each algorithm based on PSNR values, the patch with the highest PSNR value will be selected. After picking up all the patches for that image, it will be reconstructed and hence the super-resolution image will be obtained as the output.",Deep learning;Superresolution;Estimation;Computer architecture;Observers;Artificial intelligence;Image reconstruction;Computer Vision;Deep Learning;Convolutional Neural Networks;Image Super-Resolution;Residual Dense Networks;Generative Adversarial Networks;Patch Extraction,"{'month': 'May', 'issn': '', 'doi': '10.1109/ICAAIC53929.2022.9792975', 'keywords': 'Deep learning;Superresolution;Estimation;Computer architecture;Observers;Artificial intelligence;Image reconstruction;Computer Vision;Deep Learning;Convolutional Neural Networks;Image Super-Resolution;Residual Dense Networks;Generative Adversarial Networks;Patch Extraction', 'abstract': 'Image Super-Resolution is a technique that is used to obtain high-resolution, realistic images from low-resolution input images. Deep learning algorithms such as SRCNN, ESRGAN, RDN, etc. have shown significant results in this field. But these algorithms at times vary in results. To solve this problem, this research study has proposed an image super-resolution by using Patch Extraction on Deep Learning Algorithm, in which the LR image is first divided into patches and then the algorithms like RDN and ESRGAN are applied. Comparing each patch from each algorithm based on PSNR values, the patch with the highest PSNR value will be selected. After picking up all the patches for that image, it will be reconstructed and hence the super-resolution image will be obtained as the output.', 'pages': '578-586', 'number': '', 'volume': '', 'year': '2022', 'title': 'Image Resolution Enhancer using Deep Learning', 'booktitle': '2022 International Conference on Applied Artificial Intelligence and Computing (ICAAIC)', 'author': 'Mittal, Harsh and Rai, Vaibhav and Sonawane, Swaraj and Mhatre, Sneha', 'ENTRYTYPE': 'inproceedings', 'ID': '9792975'}"
9750760,GAN Based Image Inpainting Methods: A Taxonomy,"Dong, Xun and Hua, Ruijia",Dong,10.1109/IWECAI55315.2022.00037,2022,2022 3rd International Conference on Electronic Communication and Artificial Intelligence (IWECAI),"Image inpainting is the technique that's able to “repair” or “correct” the image by reconstructing missing regions of an image using AI algorithms. As the techniques of images inpainting become mature in recent years, the demand for image inpainting algorithms rises in the market. It can be used to fill out missing areas in a picture, denoise images, or even remove a specific object from an image. To this end, it is important to propose a survey for image inpainting method, which can provide a comprehensive introduction of this area. The reason for creating this paper is due to the lack of resources that conclude all the methods that are being used in image inpainting. As the field of image inpainting soars, researchers may need documents which explain all the methods that can be used to train their image inpainting models. In the paper, we gathered some most successful methods that are used by many researchers. We explained their model, methods they used, the results of the training, and the advantages of the approaches they used in their research.",Training;Iris;Image color analysis;Taxonomy;Artificial intelligence;Image reconstruction;Image inpainting;GAN;Image generation;generative model,"{'month': 'Jan', 'issn': '', 'doi': '10.1109/IWECAI55315.2022.00037', 'keywords': 'Training;Iris;Image color analysis;Taxonomy;Artificial intelligence;Image reconstruction;Image inpainting;GAN;Image generation;generative model', 'abstract': ""Image inpainting is the technique that's able to “repair” or “correct” the image by reconstructing missing regions of an image using AI algorithms. As the techniques of images inpainting become mature in recent years, the demand for image inpainting algorithms rises in the market. It can be used to fill out missing areas in a picture, denoise images, or even remove a specific object from an image. To this end, it is important to propose a survey for image inpainting method, which can provide a comprehensive introduction of this area. The reason for creating this paper is due to the lack of resources that conclude all the methods that are being used in image inpainting. As the field of image inpainting soars, researchers may need documents which explain all the methods that can be used to train their image inpainting models. In the paper, we gathered some most successful methods that are used by many researchers. We explained their model, methods they used, the results of the training, and the advantages of the approaches they used in their research."", 'pages': '145-150', 'number': '', 'volume': '', 'year': '2022', 'title': 'GAN Based Image Inpainting Methods: A Taxonomy', 'booktitle': '2022 3rd International Conference on Electronic Communication and Artificial Intelligence (IWECAI)', 'author': 'Dong, Xun and Hua, Ruijia', 'ENTRYTYPE': 'inproceedings', 'ID': '9750760'}"
10137971,A Virtual Try-on Model with Enhanced Feature Representation Capability,"Ma, Hui and Hu, Zhuhua and Zheng, Yan",Ma,10.1109/ACAIT56212.2022.10137971,2022,2022 6th Asian Conference on Artificial Intelligence Technology (ACAIT),"When consumers choose to buy clothing online, virtual try-on technology can provide them with a better shopping experience. The optimization of virtual try-on technology not only helps consumers to evaluate the selected clothing, but also can improve the profit for merchants. However, the traditional virtual try-on technology has problems such as high cost, image distortion, and deviation of clothing style. In order to solve the above problems, this paper proposes a virtual try-on model with enhanced feature representation capability. Through the improved residual block of Squeeze-and-Excitation Networks (SENet) and the style encoding module introduced by the Pyramid Squeeze Attention (PSA) module, our model enriches the content and style information, strengthens the representation ability of features, and the reconstructed image preserves the more details. Compared with related work, we improve the structural similarity measure by 1.1\% and the Inception Score by 10.1\%. It is demonstrated that our model can reconstruct more accurate and realistic images.",Image coding;Costs;Clothing;Distortion;Artificial intelligence;Image reconstruction;Optimization;Attention;Generative Adversarial Network (GAN);style transfer;image synthesis;Virtual try-on,"{'month': 'Dec', 'issn': '', 'doi': '10.1109/ACAIT56212.2022.10137971', 'keywords': 'Image coding;Costs;Clothing;Distortion;Artificial intelligence;Image reconstruction;Optimization;Attention;Generative Adversarial Network (GAN);style transfer;image synthesis;Virtual try-on', 'abstract': 'When consumers choose to buy clothing online, virtual try-on technology can provide them with a better shopping experience. The optimization of virtual try-on technology not only helps consumers to evaluate the selected clothing, but also can improve the profit for merchants. However, the traditional virtual try-on technology has problems such as high cost, image distortion, and deviation of clothing style. In order to solve the above problems, this paper proposes a virtual try-on model with enhanced feature representation capability. Through the improved residual block of Squeeze-and-Excitation Networks (SENet) and the style encoding module introduced by the Pyramid Squeeze Attention (PSA) module, our model enriches the content and style information, strengthens the representation ability of features, and the reconstructed image preserves the more details. Compared with related work, we improve the structural similarity measure by 1.1\\% and the Inception Score by 10.1\\%. It is demonstrated that our model can reconstruct more accurate and realistic images.', 'pages': '1-7', 'number': '', 'volume': '', 'year': '2022', 'title': 'A Virtual Try-on Model with Enhanced Feature Representation Capability', 'booktitle': '2022 6th Asian Conference on Artificial Intelligence Technology (ACAIT)', 'author': 'Ma, Hui and Hu, Zhuhua and Zheng, Yan', 'ENTRYTYPE': 'inproceedings', 'ID': '10137971'}"
10115412,Accelerating Innovation With Generative AI: AI-Augmented Digital Prototyping and Innovation Methods,"Bilgram, Volker and Laarmann, Felix",Bilgram,10.1109/EMR.2023.3272799,2023,IEEE Engineering Management Review,"Easy-to-use generative artificial intelligence (AI) is democratizing the use of AI in innovation management and may significantly change the way how we work and innovate. In this article, we show how large language models (LLMs), such as generative pretrained transformer (GPT), can augment the early phases of innovation, in particular, exploration, ideation, and digital prototyping. Drawing on six months of experimenting with LLMs in internal and client innovation projects, we share first-hand experiences and concrete examples of AI-assisted approaches. The article highlights a large variety of use cases for generative AI ranging from user journey mapping to idea generation and prototyping and foreshadows the promising role LLMs may play in future knowledge management systems. Moreover, we argue that generative AI may become a game changer in early prototyping as the delegation of tasks to an artificial agent can result in faster iterations and reduced costs. Our experiences also provide insights into how human innovation teams purposively and effectively interact with AIs and integrate them into their workflows.",Artificial intelligence;Technological innovation;Chatbots;Codes;Prototypes;Task analysis;Companies;AI-augmented innovation management;artificial intelligence (AI);digital prototyping;generative AI;idea generation;innovation;large language model (LLM);need identification;no-code prototyping;UX/UI,"{'month': 'Secondquarter', 'issn': '1937-4178', 'doi': '10.1109/EMR.2023.3272799', 'keywords': 'Artificial intelligence;Technological innovation;Chatbots;Codes;Prototypes;Task analysis;Companies;AI-augmented innovation management;artificial intelligence (AI);digital prototyping;generative AI;idea generation;innovation;large language model (LLM);need identification;no-code prototyping;UX/UI', 'abstract': 'Easy-to-use generative artificial intelligence (AI) is democratizing the use of AI in innovation management and may significantly change the way how we work and innovate. In this article, we show how large language models (LLMs), such as generative pretrained transformer (GPT), can augment the early phases of innovation, in particular, exploration, ideation, and digital prototyping. Drawing on six months of experimenting with LLMs in internal and client innovation projects, we share first-hand experiences and concrete examples of AI-assisted approaches. The article highlights a large variety of use cases for generative AI ranging from user journey mapping to idea generation and prototyping and foreshadows the promising role LLMs may play in future knowledge management systems. Moreover, we argue that generative AI may become a game changer in early prototyping as the delegation of tasks to an artificial agent can result in faster iterations and reduced costs. Our experiences also provide insights into how human innovation teams purposively and effectively interact with AIs and integrate them into their workflows.', 'pages': '18-25', 'number': '2', 'volume': '51', 'year': '2023', 'title': 'Accelerating Innovation With Generative AI: AI-Augmented Digital Prototyping and Innovation Methods', 'journal': 'IEEE Engineering Management Review', 'author': 'Bilgram, Volker and Laarmann, Felix', 'ENTRYTYPE': 'article', 'ID': '10115412'}"
10620174,The Detection of Abnormal Behavior by Artificial Intelligence Algorithms Under Network Security,"Cao, Hui",Cao,10.1109/ACCESS.2024.3436541,2024,IEEE Access,"With the continuous evolution of network attack methods, traditional rule-based and signature-based security strategies are becoming increasingly hard to deal with increasingly complex network threats. The research focuses on the problem of network traffic anomaly detection in network security, and proposes an improved Transformer and Generative Adversarial Networks network traffic anomaly detection model. The innovation lies in utilizing the Patch segmentation in the Transformer module to reduce information loss, while introducing random masked data blocks to enhance the anti-interference ability of Generative Adversarial Networks, and proposing a class balance model. Therefore, a Transformer Multi Receive Field Fusion (Trans-M) model for network traffic anomaly detection is constructed. The performance test results showed that after category balancing, the accuracy, recall, and F1-score of each model were been significantly improved. The accuracy of the Trans-M model on the balanced dataset arrived 98.12\%, an improvement of 8.59\% compared to before balancing. The recall rate of the Trans-M model was improved by 8.62\% to 97.86\%. On Balanced F Score (F1-score), the highest score of the Trans-M model was 98.46\%, which was 8.18\% higher than before balancing. The experiment outcomes demonstrate that the raised network traffic anomaly detection system is superior to common anomaly traffic detection models and can meet the actual network security protection needs.",Transformers;Security;Computational modeling;Telecommunication traffic;Feature extraction;Adaptation models;Network security;Artificial intelligence;Traffic control;Generative adversarial networks;AI;network security;traffic detection;GAN;Transformer,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2024.3436541', 'keywords': 'Transformers;Security;Computational modeling;Telecommunication traffic;Feature extraction;Adaptation models;Network security;Artificial intelligence;Traffic control;Generative adversarial networks;AI;network security;traffic detection;GAN;Transformer', 'abstract': 'With the continuous evolution of network attack methods, traditional rule-based and signature-based security strategies are becoming increasingly hard to deal with increasingly complex network threats. The research focuses on the problem of network traffic anomaly detection in network security, and proposes an improved Transformer and Generative Adversarial Networks network traffic anomaly detection model. The innovation lies in utilizing the Patch segmentation in the Transformer module to reduce information loss, while introducing random masked data blocks to enhance the anti-interference ability of Generative Adversarial Networks, and proposing a class balance model. Therefore, a Transformer Multi Receive Field Fusion (Trans-M) model for network traffic anomaly detection is constructed. The performance test results showed that after category balancing, the accuracy, recall, and F1-score of each model were been significantly improved. The accuracy of the Trans-M model on the balanced dataset arrived 98.12\\%, an improvement of 8.59\\% compared to before balancing. The recall rate of the Trans-M model was improved by 8.62\\% to 97.86\\%. On Balanced F Score (F1-score), the highest score of the Trans-M model was 98.46\\%, which was 8.18\\% higher than before balancing. The experiment outcomes demonstrate that the raised network traffic anomaly detection system is superior to common anomaly traffic detection models and can meet the actual network security protection needs.', 'pages': '118605-118617', 'number': '', 'volume': '12', 'year': '2024', 'title': 'The Detection of Abnormal Behavior by Artificial Intelligence Algorithms Under Network Security', 'journal': 'IEEE Access', 'author': 'Cao, Hui', 'ENTRYTYPE': 'article', 'ID': '10620174'}"
10953304,RISKS AND CHALLENGES TO MANAGE,"Marr, Bernard",Marr,,2024,Generative AI in Practice: 100+ Amazing Ways Generative Artificial Intelligence is Changing Business and Society,"Summary <p>Among the many ethical and societal concerns are the propagation of false information and the potential for malicious uses, as well as the risk of us becoming overly dependent on the technology (and thereby losing vital human skills). GenAI gives people and organizations the ability to produce masses of content, making it very easy to spread misinformation or disinformation. Another concern is that we'll become overly dependent on GenAI, which could lead to the withering of key human skills. Many of the big AI companies are of course working to mitigate the ethical challenges surrounding GenAI. Meta, for instance, says it is working with governments, AI experts, and privacy experts to establish \&\#x201c;responsible guardrails\&\#x201d; for its AI features. And there's the environmental impact of GenAI in terms of the massive energy usage, and the rare earth materials used in the production of AI hardware.</p>",Deepfakes;Generative AI;Web sites;Watermarking;Video on demand;Regulation;Medical services;Ethics;Degradation;Companies,"{'url': 'https://ieeexplore-ieee-org.crai.referencistas.com/document/10953304', 'isbn': '9781394254255', 'publisher': 'Wiley', 'issn': '', 'doi': '', 'keywords': 'Deepfakes;Generative AI;Web sites;Watermarking;Video on demand;Regulation;Medical services;Ethics;Degradation;Companies', 'abstract': ""Summary <p>Among the many ethical and societal concerns are the propagation of false information and the potential for malicious uses, as well as the risk of us becoming overly dependent on the technology (and thereby losing vital human skills). GenAI gives people and organizations the ability to produce masses of content, making it very easy to spread misinformation or disinformation. Another concern is that we'll become overly dependent on GenAI, which could lead to the withering of key human skills. Many of the big AI companies are of course working to mitigate the ethical challenges surrounding GenAI. Meta, for instance, says it is working with governments, AI experts, and privacy experts to establish \\&\\#x201c;responsible guardrails\\&\\#x201d; for its AI features. And there's the environmental impact of GenAI in terms of the massive energy usage, and the rare earth materials used in the production of AI hardware.</p>"", 'pages': '45-61', 'number': '', 'volume': '', 'year': '2024', 'title': 'RISKS AND CHALLENGES TO MANAGE', 'booktitle': 'Generative AI in Practice: 100+ Amazing Ways Generative Artificial Intelligence is Changing Business and Society', 'author': 'Marr, Bernard', 'ENTRYTYPE': 'inbook', 'ID': '10953304'}"
11048670,Keyword Searchable Provable Data Possession in Generative AI Enabled Intelligent Transportation Systems,"Cui, Yuxin and Wang, Chen and Shen, Jian and Vijayakumar, Pandi and Alfarraj, Osama and Tolba, Amr",Cui,10.1109/TITS.2025.3579407,2025,IEEE Transactions on Intelligent Transportation Systems,"In generative artificial intelligence (AI) enabled intelligent transportation systems (ITS), real-time interaction and analysis of massive multi-source data are the core drivers for system optimization. However, large scale outsourcing storage of data, while enhancing computational efficiency, also poses security threats such as data integrity compromise, sensitive information leakage, and malicious tampering. These issues severely impact the quality of generative AI model training and the reliability of system services. Although existing cloud provable data possession (PDP) schemes can verify data integrity, they incur high computational overhead in ITS scenarios and lack support for fine-grained data retrieval. To address the need for specific data auditing by data users in generative AI enabled ITS, we propose a keyword searchable provable data possession scheme, which integrates data integrity verification with efficient data retrieval. First, a lightweight indexing approach is integrated to optimize the keyword search process, allowing different data users to initiate keyword based targeted queries based on their specific business needs, with integrity verification performed only on matching data blocks. Second, an efficient integrity auditing scheme is designed to ensure the integrity of outsourced intelligent vehicle data. Finally, security analysis and experimental results demonstrate that the proposed scheme achieves acceptable efficiency in computation overhead compared to existing works, while improving the auditability and searchability of outsourced ITS data.",Generative AI;Data integrity;Cloud computing;Security;Data privacy;Real-time systems;Encryption;Training;Driver behavior;Data models;Intelligent transportation systems;provable data possession;generative artificial intelligence;keyword,"{'month': '', 'issn': '1558-0016', 'doi': '10.1109/TITS.2025.3579407', 'keywords': 'Generative AI;Data integrity;Cloud computing;Security;Data privacy;Real-time systems;Encryption;Training;Driver behavior;Data models;Intelligent transportation systems;provable data possession;generative artificial intelligence;keyword', 'abstract': 'In generative artificial intelligence (AI) enabled intelligent transportation systems (ITS), real-time interaction and analysis of massive multi-source data are the core drivers for system optimization. However, large scale outsourcing storage of data, while enhancing computational efficiency, also poses security threats such as data integrity compromise, sensitive information leakage, and malicious tampering. These issues severely impact the quality of generative AI model training and the reliability of system services. Although existing cloud provable data possession (PDP) schemes can verify data integrity, they incur high computational overhead in ITS scenarios and lack support for fine-grained data retrieval. To address the need for specific data auditing by data users in generative AI enabled ITS, we propose a keyword searchable provable data possession scheme, which integrates data integrity verification with efficient data retrieval. First, a lightweight indexing approach is integrated to optimize the keyword search process, allowing different data users to initiate keyword based targeted queries based on their specific business needs, with integrity verification performed only on matching data blocks. Second, an efficient integrity auditing scheme is designed to ensure the integrity of outsourced intelligent vehicle data. Finally, security analysis and experimental results demonstrate that the proposed scheme achieves acceptable efficiency in computation overhead compared to existing works, while improving the auditability and searchability of outsourced ITS data.', 'pages': '1-10', 'number': '', 'volume': '', 'year': '2025', 'title': 'Keyword Searchable Provable Data Possession in Generative AI Enabled Intelligent Transportation Systems', 'journal': 'IEEE Transactions on Intelligent Transportation Systems', 'author': 'Cui, Yuxin and Wang, Chen and Shen, Jian and Vijayakumar, Pandi and Alfarraj, Osama and Tolba, Amr', 'ENTRYTYPE': 'article', 'ID': '11048670'}"
10605237,Enhancing Human-Computer Interaction through AI: A Study on ChatGPT in Educational Environments,"Kothari, Dhruval Kenal and Noel Newton Fernando, Owen",Kothari,10.1109/CAI59869.2024.00100,2024,2024 IEEE Conference on Artificial Intelligence (CAI),"This research investigates the potential of Chat Generative Pre-Trained Transformer (ChatGPT) in Human-Computer Interaction (HCI) within educational contexts. Examining the intersection of Artificial Intelligence (AI) and HCI, the study emphasizes ChatGPT's ability to provide personalized and immediate responses, enhancing student engagement and understanding. A literature review reveals ChatGPT's applications in higher education, while highlighting challenges related to critical thinking in the field of HCI. This paper then outlines our objectives, focusing on answering student queries and generating Multiple-Choice Questions (MCQs) for revision. Experimental results demonstrate the superiority of Custom GPTs, emphasizing the importance of context-specific tailoring. The discussion addresses limitations, proposing future work on model fine-tuning, optimization, and human testing. In conclusion, this research contributes insights into leveraging ChatGPT in HCI education, highlighting its potential for personalized and effective learning experiences.",Bibliographies;Education;Focusing;Chatbots;Transformers;Artificial intelligence;Optimization;ChatGPT;human computer interaction (HCI);Custom GPT;education;artificial intelligence (AI),"{'month': 'June', 'issn': '', 'doi': '10.1109/CAI59869.2024.00100', 'keywords': 'Bibliographies;Education;Focusing;Chatbots;Transformers;Artificial intelligence;Optimization;ChatGPT;human computer interaction (HCI);Custom GPT;education;artificial intelligence (AI)', 'abstract': ""This research investigates the potential of Chat Generative Pre-Trained Transformer (ChatGPT) in Human-Computer Interaction (HCI) within educational contexts. Examining the intersection of Artificial Intelligence (AI) and HCI, the study emphasizes ChatGPT's ability to provide personalized and immediate responses, enhancing student engagement and understanding. A literature review reveals ChatGPT's applications in higher education, while highlighting challenges related to critical thinking in the field of HCI. This paper then outlines our objectives, focusing on answering student queries and generating Multiple-Choice Questions (MCQs) for revision. Experimental results demonstrate the superiority of Custom GPTs, emphasizing the importance of context-specific tailoring. The discussion addresses limitations, proposing future work on model fine-tuning, optimization, and human testing. In conclusion, this research contributes insights into leveraging ChatGPT in HCI education, highlighting its potential for personalized and effective learning experiences."", 'pages': '500-503', 'number': '', 'volume': '', 'year': '2024', 'title': 'Enhancing Human-Computer Interaction through AI: A Study on ChatGPT in Educational Environments', 'booktitle': '2024 IEEE Conference on Artificial Intelligence (CAI)', 'author': 'Kothari, Dhruval Kenal and Noel Newton Fernando, Owen', 'ENTRYTYPE': 'inproceedings', 'ID': '10605237'}"
11037821,Generative AI Enabled Secure Communication in Smart Grid: Challenges and Solutions,"Zeng, Mingfei and Xie, Ming and Meng, Liang and Zhang, Huacheng and Wu, Tong and Wang, Jianquan",Zeng,10.1109/MNET.2025.3580477,2025,IEEE Network,"Generative AI has recently emerged as a promising solution to address critical challenges in smart grid communication, including security, privacy, and efficiency. In this paper, we first introduce some concepts of smart grid and generative AI, outline their current state, and discuss the potential applications of integrating generative AI into smart grid. Then, we examine traditional and non-generative AI solutions, highlighting their limitations. Subsequently, we propose a promising solution by integrating generative AI into the smart grid to develop a generative AI-based agent, aiming to enhance the security and efficiency of smart grid communication while addressing privacy concerns. Additionally, we provide an experimental evaluation to demonstrate how the proposed solution can be applied to optimize smart grid holistically. Experimental results clearly show that the proposed generative AI agent approach can protect user privacy and significantly improve the overall efficiency and security of smart grid communication. Furthermore, we highlight future research directions for generative AI in smart grid applications, including the synergistic role of generative AI in smart grids, cross-domain applications and extensions, and quantum security of smart grids.",Smart grids;Generative AI;Security;Artificial intelligence;Privacy;Firewalls (computing);Data privacy;Data models;Synthetic data;Training;Generative AI;smart grid;security;privacy;generative AI agent,"{'month': 'Sep.', 'issn': '1558-156X', 'doi': '10.1109/MNET.2025.3580477', 'keywords': 'Smart grids;Generative AI;Security;Artificial intelligence;Privacy;Firewalls (computing);Data privacy;Data models;Synthetic data;Training;Generative AI;smart grid;security;privacy;generative AI agent', 'abstract': 'Generative AI has recently emerged as a promising solution to address critical challenges in smart grid communication, including security, privacy, and efficiency. In this paper, we first introduce some concepts of smart grid and generative AI, outline their current state, and discuss the potential applications of integrating generative AI into smart grid. Then, we examine traditional and non-generative AI solutions, highlighting their limitations. Subsequently, we propose a promising solution by integrating generative AI into the smart grid to develop a generative AI-based agent, aiming to enhance the security and efficiency of smart grid communication while addressing privacy concerns. Additionally, we provide an experimental evaluation to demonstrate how the proposed solution can be applied to optimize smart grid holistically. Experimental results clearly show that the proposed generative AI agent approach can protect user privacy and significantly improve the overall efficiency and security of smart grid communication. Furthermore, we highlight future research directions for generative AI in smart grid applications, including the synergistic role of generative AI in smart grids, cross-domain applications and extensions, and quantum security of smart grids.', 'pages': '81-87', 'number': '5', 'volume': '39', 'year': '2025', 'title': 'Generative AI Enabled Secure Communication in Smart Grid: Challenges and Solutions', 'journal': 'IEEE Network', 'author': 'Zeng, Mingfei and Xie, Ming and Meng, Liang and Zhang, Huacheng and Wu, Tong and Wang, Jianquan', 'ENTRYTYPE': 'article', 'ID': '11037821'}"
10917080,Systematic GAN Parameter Selection for Fault Data Generation using Particle Swarm Optimization,"Mingyue, Sun and Fan, Wu and Xin, Ji and Yanhong, Jian",Mingyue,10.1109/ICEI63732.2024.10917080,2024,2024 IEEE International Conference on Energy Internet (ICEI),"In real-world applications of artificial intelligence, fault data are often insufficient, making data training challenging. Generative adversarial networks (GAN) are widely recognized for addressing data generation issues. This study introduces swarm intelligence algorithms, specifically particle swarm optimization (PSO) and genetic algorithm (GA), to enhance GAN performance. The single-channel vibration signal data from a faulty motor serves as the case study. Fault vibration signals can rapidly, accurately, and comprehensively reflect the nature and extent of mechanical faults. The study compares and analyzes the results of PSO, GA, and random number (RN) optimization of GAN parameters. The findings demonstrate that PSO outperforms GA in terms of time efficiency and reducing generation errors. Swarm intelligence algorithms eliminate the need for manual experience or repetitive trials when selecting parameters. Compared to GA and RN, PSO improves performance by 80.92\% and 90.44\%, respectively, while also reducing optimization time by 46.55\% compared to GA.",Vibrations;Training;Accuracy;Data collection;Generative adversarial networks;Motors;Particle swarm optimization;Optimization;Genetic algorithms;Convergence;genetic algorithm (GA);generative adversarial networks (GAN);parameter determination;vibration signal;particle swarm optimization (PSO),"{'month': 'Nov', 'issn': '', 'doi': '10.1109/ICEI63732.2024.10917080', 'keywords': 'Vibrations;Training;Accuracy;Data collection;Generative adversarial networks;Motors;Particle swarm optimization;Optimization;Genetic algorithms;Convergence;genetic algorithm (GA);generative adversarial networks (GAN);parameter determination;vibration signal;particle swarm optimization (PSO)', 'abstract': 'In real-world applications of artificial intelligence, fault data are often insufficient, making data training challenging. Generative adversarial networks (GAN) are widely recognized for addressing data generation issues. This study introduces swarm intelligence algorithms, specifically particle swarm optimization (PSO) and genetic algorithm (GA), to enhance GAN performance. The single-channel vibration signal data from a faulty motor serves as the case study. Fault vibration signals can rapidly, accurately, and comprehensively reflect the nature and extent of mechanical faults. The study compares and analyzes the results of PSO, GA, and random number (RN) optimization of GAN parameters. The findings demonstrate that PSO outperforms GA in terms of time efficiency and reducing generation errors. Swarm intelligence algorithms eliminate the need for manual experience or repetitive trials when selecting parameters. Compared to GA and RN, PSO improves performance by 80.92\\% and 90.44\\%, respectively, while also reducing optimization time by 46.55\\% compared to GA.', 'pages': '580-585', 'number': '', 'volume': '', 'year': '2024', 'title': 'Systematic GAN Parameter Selection for Fault Data Generation using Particle Swarm Optimization', 'booktitle': '2024 IEEE International Conference on Energy Internet (ICEI)', 'author': 'Mingyue, Sun and Fan, Wu and Xin, Ji and Yanhong, Jian', 'ENTRYTYPE': 'inproceedings', 'ID': '10917080'}"
10772906,Explainable Artificial Intelligence and Causal Inference Based ATM Fraud Detection,"Vivek, Yelleti and Ravi, Vadlamani and Mane, Abhay and Naidu, Laveti Ramesh",Vivek,10.1109/CIFEr62890.2024.10772906,2024,2024 IEEE Symposium on Computational Intelligence for Financial Engineering and Economics (CIFEr),"Gaining the empathy and trust of customers is paramount in the financial domain. However, the recurring occurrence of fraudulent activities undermines both of these factors. ATM fraud is a prevalent issue faced in today's banking landscape. The critical challenges in fraud datasets are highly imbalanced datasets, evolving fraud patterns, and lack of explainability. In this study, we handled these techniques on an ATM transaction dataset collected from India. In binary classification, we investigated the effectiveness of various over-sampling techniques, such as the Synthetic Minority Oversampling Technique (SMOTE) and its variants, Generative Adversarial Networks (GAN), to achieve oversampling. Gradient Boosting Tree (GBT), outperformed the rest of the techniques by achieving an AUC of 0.963, and Decision Tree (DT) stands second with an AUC of 0.958. In terms of complexity and interpretability, DT is the winner. Among the oversampling approaches, SMOTE and its variants performed better. We incorporated explainable artificial intelligence (XAI) and Causal Inference (CI) in the fraud detection framework and studied them via various analyses. Further, we provided managerial impact.",Economics;Explainable AI;Banking;Generative adversarial networks;Boosting;Fraud;Complexity theory;Decision trees;Computational intelligence;Fraud detection;XAI;Causal Inference;SMOTE;GAN,"{'month': 'Oct', 'issn': '2640-7701', 'doi': '10.1109/CIFEr62890.2024.10772906', 'keywords': 'Economics;Explainable AI;Banking;Generative adversarial networks;Boosting;Fraud;Complexity theory;Decision trees;Computational intelligence;Fraud detection;XAI;Causal Inference;SMOTE;GAN', 'abstract': ""Gaining the empathy and trust of customers is paramount in the financial domain. However, the recurring occurrence of fraudulent activities undermines both of these factors. ATM fraud is a prevalent issue faced in today's banking landscape. The critical challenges in fraud datasets are highly imbalanced datasets, evolving fraud patterns, and lack of explainability. In this study, we handled these techniques on an ATM transaction dataset collected from India. In binary classification, we investigated the effectiveness of various over-sampling techniques, such as the Synthetic Minority Oversampling Technique (SMOTE) and its variants, Generative Adversarial Networks (GAN), to achieve oversampling. Gradient Boosting Tree (GBT), outperformed the rest of the techniques by achieving an AUC of 0.963, and Decision Tree (DT) stands second with an AUC of 0.958. In terms of complexity and interpretability, DT is the winner. Among the oversampling approaches, SMOTE and its variants performed better. We incorporated explainable artificial intelligence (XAI) and Causal Inference (CI) in the fraud detection framework and studied them via various analyses. Further, we provided managerial impact."", 'pages': '1-7', 'number': '', 'volume': '', 'year': '2024', 'title': 'Explainable Artificial Intelligence and Causal Inference Based ATM Fraud Detection', 'booktitle': '2024 IEEE Symposium on Computational Intelligence for Financial Engineering and Economics (CIFEr)', 'author': 'Vivek, Yelleti and Ravi, Vadlamani and Mane, Abhay and Naidu, Laveti Ramesh', 'ENTRYTYPE': 'inproceedings', 'ID': '10772906'}"
10596771,CTGAN in Augmentation of Radiomics Features Classification from Narrow Band Imaging for Laryngeal Cancer,"WANG, Haiyang and Mainardi, Luca",WANG,10.1109/MeMeA60663.2024.10596771,2024,2024 IEEE International Symposium on Medical Measurements and Applications (MeMeA),"Artificial intelligence (AI) holds immense promise in revolutionizing biomedical research, particularly in the field of early medical assistance analysis. This paper explores the application of AI in the context of laryngeal cancer, a disease where early screening, accurate diagnosis, effective management, and favorable prognosis are crucial for patient outcomes. The implementation of AI in biomedical field practice always faces challenges due to data scarcity. The limited availability of data leads to less satisfactory testing results. Even if the methods like geometric transformation and photometric transformation have been applied, it does not still enlarge the diversity in nature of data. Here, we investigated CTGAN on tabular radiomics features in a public laryngeal cancer dataset to check how various amount of data augmentation affects classifier's performance. The results were assessed by the synthetic data reports which captures the similarity with the columns shapes score (median value 71.23 \%) and the trend and correction across columns with a column pair score median value 90.30\%. The synthetic data respect the original data structure(100\%) and overall synthetic data validity is above 81 \%. It enhances the diversity and increase the amount of training data for laryngeal cancer detection. After assessing the synthetic report, a comparison of performances across different classifiers was followed. Result shows an increases in accuracy from 5 \% to 10\%. This proves the positive performance of the classifying improvement on an independent testing dataset (real data) and provides clues how much data should be synthesized. Our paper provides a positive and meaningful reference on tabular radiomics data augmentation for medical intelligent diagnosis design in the future.",Accuracy;Data augmentation;Market research;Data models;Artificial intelligence;Medical diagnostic imaging;Synthetic data;CTGAN;radiomics;data augmentation;laryngeal cancer;medical image,"{'month': 'June', 'issn': '2837-5882', 'doi': '10.1109/MeMeA60663.2024.10596771', 'keywords': 'Accuracy;Data augmentation;Market research;Data models;Artificial intelligence;Medical diagnostic imaging;Synthetic data;CTGAN;radiomics;data augmentation;laryngeal cancer;medical image', 'abstract': ""Artificial intelligence (AI) holds immense promise in revolutionizing biomedical research, particularly in the field of early medical assistance analysis. This paper explores the application of AI in the context of laryngeal cancer, a disease where early screening, accurate diagnosis, effective management, and favorable prognosis are crucial for patient outcomes. The implementation of AI in biomedical field practice always faces challenges due to data scarcity. The limited availability of data leads to less satisfactory testing results. Even if the methods like geometric transformation and photometric transformation have been applied, it does not still enlarge the diversity in nature of data. Here, we investigated CTGAN on tabular radiomics features in a public laryngeal cancer dataset to check how various amount of data augmentation affects classifier's performance. The results were assessed by the synthetic data reports which captures the similarity with the columns shapes score (median value 71.23 \\%) and the trend and correction across columns with a column pair score median value 90.30\\%. The synthetic data respect the original data structure(100\\%) and overall synthetic data validity is above 81 \\%. It enhances the diversity and increase the amount of training data for laryngeal cancer detection. After assessing the synthetic report, a comparison of performances across different classifiers was followed. Result shows an increases in accuracy from 5 \\% to 10\\%. This proves the positive performance of the classifying improvement on an independent testing dataset (real data) and provides clues how much data should be synthesized. Our paper provides a positive and meaningful reference on tabular radiomics data augmentation for medical intelligent diagnosis design in the future."", 'pages': '1-5', 'number': '', 'volume': '', 'year': '2024', 'title': 'CTGAN in Augmentation of Radiomics Features Classification from Narrow Band Imaging for Laryngeal Cancer', 'booktitle': '2024 IEEE International Symposium on Medical Measurements and Applications (MeMeA)', 'author': 'WANG, Haiyang and Mainardi, Luca', 'ENTRYTYPE': 'inproceedings', 'ID': '10596771'}"
9148327,Recent Research on AI in Games,"Xia, Boming and Ye, Xiaozhen and Abuassba, Adnan O.M",Xia,10.1109/IWCMC48107.2020.9148327,2020,2020 International Wireless Communications and Mobile Computing (IWCMC),"Games tend to have the properties of vast state space and high complexity, making them excellent benchmarks for evaluating various techniques, including AI ones. Techniques utilized in games capable of making them more attractive, immersive, smarter etc. can all be considered to be certain forms of game AI. Considering there are few reviews on the more recent work in the game AI field from the perspective of essential applications, in this paper, we make a systematic review of typical research from 2018 on three application fields of game AI: believable agents in non-player characters research, game level generation in procedural content generation, and player profiling in player modeling. We also provide a timeline of game AI history to give the readers a clearer picture of the game AI field. Moreover, general game AI and hybrid intelligence for games are discussed.",Games;Artificial intelligence;Solid modeling;Natural language processing;Benchmark testing;History;Augmented reality;Game;Artificial Intelligence (AI);Game AI,"{'month': 'June', 'issn': '2376-6506', 'doi': '10.1109/IWCMC48107.2020.9148327', 'keywords': 'Games;Artificial intelligence;Solid modeling;Natural language processing;Benchmark testing;History;Augmented reality;Game;Artificial Intelligence (AI);Game AI', 'abstract': 'Games tend to have the properties of vast state space and high complexity, making them excellent benchmarks for evaluating various techniques, including AI ones. Techniques utilized in games capable of making them more attractive, immersive, smarter etc. can all be considered to be certain forms of game AI. Considering there are few reviews on the more recent work in the game AI field from the perspective of essential applications, in this paper, we make a systematic review of typical research from 2018 on three application fields of game AI: believable agents in non-player characters research, game level generation in procedural content generation, and player profiling in player modeling. We also provide a timeline of game AI history to give the readers a clearer picture of the game AI field. Moreover, general game AI and hybrid intelligence for games are discussed.', 'pages': '505-510', 'number': '', 'volume': '', 'year': '2020', 'title': 'Recent Research on AI in Games', 'booktitle': '2020 International Wireless Communications and Mobile Computing (IWCMC)', 'author': 'Xia, Boming and Ye, Xiaozhen and Abuassba, Adnan O.M', 'ENTRYTYPE': 'inproceedings', 'ID': '9148327'}"
10739229,Machine Learning in Action: Supervised Learning Models for Classifying Credit Card Fraud,"Pendalwar, Rishab and Verma, Aditya and Patil, Ratna",Pendalwar,10.1109/ICEECT61758.2024.10739229,2024,2024 International Conference on Electrical Electronics and Computing Technologies (ICEECT),"Financial institutions face a relentless battle against fraudulent credit card transactions. Machine learning (ML) offers a promising approach for tackling this challenge. This paper explores six supervised machine learning models (Support Vector Machine (SVM), Random Forest, Naive Bayes, K-Nearest Neighbours (KNN), XGBoost, and LIGHT GBM) for detecting credit card fraud using a benchmark dataset. We explore the impact of dimensionality reduction with Principal Component Analysis (PCA) and data balancing with Generative Adversarial Networks (GANs) on imbalanced class distributions. The assessment relies on metrics such as accuracy, precision, recall, and F1-score. The results indicate that Random Forest demonstrates the best overall performance, especially when paired with GAN-based data balancing techniques. This study highlights the importance of addressing class imbalance and exploring dimensionality reduction techniques for enhancing credit card fraud detection capabilities.",Dimensionality reduction;Support vector machines;Accuracy;Nearest neighbor methods;Credit cards;Data models;Fraud;Bayes methods;Random forests;Principal component analysis;imbalanced learning;principal component analysis (pca);generative adversarial networks (gans);random forest;credit card fraud detection;light gbm,"{'month': 'Aug', 'issn': '', 'doi': '10.1109/ICEECT61758.2024.10739229', 'keywords': 'Dimensionality reduction;Support vector machines;Accuracy;Nearest neighbor methods;Credit cards;Data models;Fraud;Bayes methods;Random forests;Principal component analysis;imbalanced learning;principal component analysis (pca);generative adversarial networks (gans);random forest;credit card fraud detection;light gbm', 'abstract': 'Financial institutions face a relentless battle against fraudulent credit card transactions. Machine learning (ML) offers a promising approach for tackling this challenge. This paper explores six supervised machine learning models (Support Vector Machine (SVM), Random Forest, Naive Bayes, K-Nearest Neighbours (KNN), XGBoost, and LIGHT GBM) for detecting credit card fraud using a benchmark dataset. We explore the impact of dimensionality reduction with Principal Component Analysis (PCA) and data balancing with Generative Adversarial Networks (GANs) on imbalanced class distributions. The assessment relies on metrics such as accuracy, precision, recall, and F1-score. The results indicate that Random Forest demonstrates the best overall performance, especially when paired with GAN-based data balancing techniques. This study highlights the importance of addressing class imbalance and exploring dimensionality reduction techniques for enhancing credit card fraud detection capabilities.', 'pages': '1-5', 'number': '', 'volume': '1', 'year': '2024', 'title': 'Machine Learning in Action: Supervised Learning Models for Classifying Credit Card Fraud', 'booktitle': '2024 International Conference on Electrical Electronics and Computing Technologies (ICEECT)', 'author': 'Pendalwar, Rishab and Verma, Aditya and Patil, Ratna', 'ENTRYTYPE': 'inproceedings', 'ID': '10739229'}"
10265048,Brainstorming Generative Adversarial Network (BGAN): Toward Multiagent Generative Models With Distributed Data Sets,"Ferdowsi, Aidin and Saad, Walid",Ferdowsi,10.1109/JIOT.2023.3319630,2024,IEEE Internet of Things Journal,"To achieve a high-learning accuracy, generative adversarial networks (GANs) must be fed by large data sets that adequately represent the data space. However, in many scenarios, the available data sets may be limited and distributed across multiple agents, each of which is seeking to learn the distribution of the data on its own. In such scenarios, the agents often do not wish to share their local data as it can cause communication overhead for large data sets. In this article, to address this multiagent GAN problem, a novel brainstorming GAN (BGAN) architecture is proposed using which multiple agents can generate real-like data samples while operating in a fully distributed manner. BGAN allows the agents to gain information from other agents without sharing their real data sets but by “brainstorming” via the sharing of their generated data samples. In contrast to existing distributed GAN solutions, the proposed BGAN architecture is designed to be fully distributed, and it does not need any centralized controller. Moreover, BGANs are shown to be scalable and not dependent on the hyperparameters of the agents’ deep neural networks (DNNs) thus enabling the agents to have different DNN architectures. Theoretically, the interactions between BGAN agents are analyzed as a game whose unique Nash equilibrium is derived. Experimental results show that BGAN can generate real-like data samples with higher quality and lower Jensen-Shannon divergence (JSD) and Frèchet inception distance (FID) compared to other distributed GAN architectures.",Computer architecture;Generators;Distributed databases;Generative adversarial networks;Particle swarm optimization;Heuristic algorithms;Training;Communication efficiency;distributed learning;generative adversarial networks (GANs),"{'month': 'March', 'issn': '2327-4662', 'doi': '10.1109/JIOT.2023.3319630', 'keywords': 'Computer architecture;Generators;Distributed databases;Generative adversarial networks;Particle swarm optimization;Heuristic algorithms;Training;Communication efficiency;distributed learning;generative adversarial networks (GANs)', 'abstract': 'To achieve a high-learning accuracy, generative adversarial networks (GANs) must be fed by large data sets that adequately represent the data space. However, in many scenarios, the available data sets may be limited and distributed across multiple agents, each of which is seeking to learn the distribution of the data on its own. In such scenarios, the agents often do not wish to share their local data as it can cause communication overhead for large data sets. In this article, to address this multiagent GAN problem, a novel brainstorming GAN (BGAN) architecture is proposed using which multiple agents can generate real-like data samples while operating in a fully distributed manner. BGAN allows the agents to gain information from other agents without sharing their real data sets but by “brainstorming” via the sharing of their generated data samples. In contrast to existing distributed GAN solutions, the proposed BGAN architecture is designed to be fully distributed, and it does not need any centralized controller. Moreover, BGANs are shown to be scalable and not dependent on the hyperparameters of the agents’ deep neural networks (DNNs) thus enabling the agents to have different DNN architectures. Theoretically, the interactions between BGAN agents are analyzed as a game whose unique Nash equilibrium is derived. Experimental results show that BGAN can generate real-like data samples with higher quality and lower Jensen-Shannon divergence (JSD) and Frèchet inception distance (FID) compared to other distributed GAN architectures.', 'pages': '7828-7840', 'number': '5', 'volume': '11', 'year': '2024', 'title': 'Brainstorming Generative Adversarial Network (BGAN): Toward Multiagent Generative Models With Distributed Data Sets', 'journal': 'IEEE Internet of Things Journal', 'author': 'Ferdowsi, Aidin and Saad, Walid', 'ENTRYTYPE': 'article', 'ID': '10265048'}"
9617735,Compressed Sensing via Measurement-Conditional Generative Models,"Kim, Kyung-Su and Lee, Jung Hyun and Yang, Eunho",Kim,10.1109/ACCESS.2021.3128721,2021,IEEE Access,"Pre-trained generators have been frequently adopted in compressed sensing (CS) owing to their ability to effectively estimate signals with the prior of NNs. To further refine the NN-based prior, we propose a framework that allows the generator to utilize additional information from given measurements of training samples for prior learning, thereby yielding more accurate reconstruction for signals. As our framework has a simple form, it can be easily applied to existing CS methods using pre-trained generators. Through extensive experiments, we demonstrate that our framework consistently outperforms these works by a large margin and can reduce the reconstruction error up to an order of magnitude for the presented target applications. We also explain the experimental success theoretically by showing that our framework can slightly relax the stringent signal presence condition, which is required to guarantee the success of signal recovery.",Generators;Training;Image reconstruction;Generative adversarial networks;Magnetic resonance imaging;Phase measurement;Artificial neural networks;Compressed sensing;artificial neural networks;image reconstruction;image enhancement;signal reconstruction and prediction;measurement-conditional generative models;mitigation of signal presence condition;magnetic resonance imaging,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2021.3128721', 'keywords': 'Generators;Training;Image reconstruction;Generative adversarial networks;Magnetic resonance imaging;Phase measurement;Artificial neural networks;Compressed sensing;artificial neural networks;image reconstruction;image enhancement;signal reconstruction and prediction;measurement-conditional generative models;mitigation of signal presence condition;magnetic resonance imaging', 'abstract': 'Pre-trained generators have been frequently adopted in compressed sensing (CS) owing to their ability to effectively estimate signals with the prior of NNs. To further refine the NN-based prior, we propose a framework that allows the generator to utilize additional information from given measurements of training samples for prior learning, thereby yielding more accurate reconstruction for signals. As our framework has a simple form, it can be easily applied to existing CS methods using pre-trained generators. Through extensive experiments, we demonstrate that our framework consistently outperforms these works by a large margin and can reduce the reconstruction error up to an order of magnitude for the presented target applications. We also explain the experimental success theoretically by showing that our framework can slightly relax the stringent signal presence condition, which is required to guarantee the success of signal recovery.', 'pages': '155335-155352', 'number': '', 'volume': '9', 'year': '2021', 'title': 'Compressed Sensing via Measurement-Conditional Generative Models', 'journal': 'IEEE Access', 'author': 'Kim, Kyung-Su and Lee, Jung Hyun and Yang, Eunho', 'ENTRYTYPE': 'article', 'ID': '9617735'}"
9698877,Automatic cone-beam computed tomography segmentation with small samples based on generative adversarial networks and semantic segmentation,"Yang, Huifang and Li, Gang",Yang,10.1109/ICMIPE53131.2021.9698877,2021,2021 IEEE International Conference on Medical Imaging Physics and Engineering (ICMIPE),"This paper establishes a method to realize semiautomatic or automatic labeling of multidimensional data based on small samples and weak labeling. This method could effectively assist dentists in the segmentation of different tissues. Based on U-Net combined with the generative adversarial network method, segmentation can be realized on multidimensional data. It also includes three-dimensional mesh reconstruction of the segmented tissue, smoothing the boundary, and the resulting data can be used to aid clinical diagnosis and printing. The segmentation results can reflect the structural distribution of different tissues and effectively build a mechanical model based on cone-beam computed tomography system (CBCT) datasets.",Printing;Image segmentation;Smoothing methods;Computed tomography;Magnetic resonance imaging;Semantics;Magnetic resonance;segmentation;generative adversarial networks;CBCT;annotation,"{'month': 'Nov', 'issn': '', 'doi': '10.1109/ICMIPE53131.2021.9698877', 'keywords': 'Printing;Image segmentation;Smoothing methods;Computed tomography;Magnetic resonance imaging;Semantics;Magnetic resonance;segmentation;generative adversarial networks;CBCT;annotation', 'abstract': 'This paper establishes a method to realize semiautomatic or automatic labeling of multidimensional data based on small samples and weak labeling. This method could effectively assist dentists in the segmentation of different tissues. Based on U-Net combined with the generative adversarial network method, segmentation can be realized on multidimensional data. It also includes three-dimensional mesh reconstruction of the segmented tissue, smoothing the boundary, and the resulting data can be used to aid clinical diagnosis and printing. The segmentation results can reflect the structural distribution of different tissues and effectively build a mechanical model based on cone-beam computed tomography system (CBCT) datasets.', 'pages': '1-4', 'number': '', 'volume': '', 'year': '2021', 'title': 'Automatic cone-beam computed tomography segmentation with small samples based on generative adversarial networks and semantic segmentation', 'booktitle': '2021 IEEE International Conference on Medical Imaging Physics and Engineering (ICMIPE)', 'author': 'Yang, Huifang and Li, Gang', 'ENTRYTYPE': 'inproceedings', 'ID': '9698877'}"
11129384,Unethical Academic Practices through Ethical Tools,"Tirumala, Sreenivas Sremath and Khwakhali, Ushik Shrestha",Tirumala,10.1109/iSTEM-Ed65612.2025.11129384,2025,2025 10th International STEM Education Conference (iSTEM-Ed),"Generative Artificial Intelligence (Gen-AI) tools have changed the course of academic teaching and learning practices, particularly, the way of writing assessment and scholarly reports. Although the use of Artificial Intelligence based tools or similar systems is not new to academia, advanced Gen-AI tools such as Chat-GPT has opened a new path for academic misconduct. The use of Gen-AI tools forced academic institutions to adapt new policies around the use of AI generated content in assessments and research. Reputed plagiarism checking tools like Turnitin has provided AI detection mechanisms with little success, due to inconsistency in the results attributed to the quality of detection algorithms. Tools like ZeroGPT, Originality.ai, GPTZero, etc., are exclusively developed for detecting AI generated content. However, at present, some Gen-AI based companies like StealthAI, AIHumanizer etc., are offering services that can be used to evade detection tools for plagiarism. These services use advanced algorithms and techniques to identify patterns and rewrite the content to mimic human styled text. This posed serious challenges in identifying academic misconduct. This research explores various tools that learners use or can be used in assessments and research that can evade AI and plagiarism detection. A list of 6 reputed tools like StealthAI, AIHumanizer, ByPassGPT are reviewed to identify how these tools evade AI detection. The problem of academic misconduct is critically evaluated using literature, and the practical implications of misconduct are presented. As this ongoing research is closely aligned with learner abilities, the impact of Gen-AI is presented through the lenses of learners.",Ethics;Generative AI;Plagiarism;Education;Learning (artificial intelligence);Companies;Writing;Detection algorithms;Lenses;education;academic misconduct;ethical practices;Gen-AI tools;AI writing,"{'month': 'July', 'issn': '', 'doi': '10.1109/iSTEM-Ed65612.2025.11129384', 'keywords': 'Ethics;Generative AI;Plagiarism;Education;Learning (artificial intelligence);Companies;Writing;Detection algorithms;Lenses;education;academic misconduct;ethical practices;Gen-AI tools;AI writing', 'abstract': 'Generative Artificial Intelligence (Gen-AI) tools have changed the course of academic teaching and learning practices, particularly, the way of writing assessment and scholarly reports. Although the use of Artificial Intelligence based tools or similar systems is not new to academia, advanced Gen-AI tools such as Chat-GPT has opened a new path for academic misconduct. The use of Gen-AI tools forced academic institutions to adapt new policies around the use of AI generated content in assessments and research. Reputed plagiarism checking tools like Turnitin has provided AI detection mechanisms with little success, due to inconsistency in the results attributed to the quality of detection algorithms. Tools like ZeroGPT, Originality.ai, GPTZero, etc., are exclusively developed for detecting AI generated content. However, at present, some Gen-AI based companies like StealthAI, AIHumanizer etc., are offering services that can be used to evade detection tools for plagiarism. These services use advanced algorithms and techniques to identify patterns and rewrite the content to mimic human styled text. This posed serious challenges in identifying academic misconduct. This research explores various tools that learners use or can be used in assessments and research that can evade AI and plagiarism detection. A list of 6 reputed tools like StealthAI, AIHumanizer, ByPassGPT are reviewed to identify how these tools evade AI detection. The problem of academic misconduct is critically evaluated using literature, and the practical implications of misconduct are presented. As this ongoing research is closely aligned with learner abilities, the impact of Gen-AI is presented through the lenses of learners.', 'pages': '1-6', 'number': '', 'volume': '', 'year': '2025', 'title': 'Unethical Academic Practices through Ethical Tools', 'booktitle': '2025 10th International STEM Education Conference (iSTEM-Ed)', 'author': 'Tirumala, Sreenivas Sremath and Khwakhali, Ushik Shrestha', 'ENTRYTYPE': 'inproceedings', 'ID': '11129384'}"
10745288,,"Bahree, Amit",Bahree,,2024,Generative AI in Action,"Generative AI can transform your business by streamlining the process of creating text, images, and code. This book will show you how to get in on the action! Generative AI in Action is the comprehensive and concrete guide to generative AI you’ve been searching for. It introduces both AI’s fundamental principles and its practical applications in an enterprise context—from generating text and images for product catalogs and marketing campaigns, to technical reporting, and even writing software. Inside, author Amit Bahree shares his experience leading Generative AI projects at Microsoft for nearly a decade, starting well before the current GPT revolution. Inside Generative AI in Action you will find: A practical overview of of generative AI applications Architectural patterns, integration guidance, and best practices for generative AI The latest techniques like RAG, prompt engineering, and multi-modality The challenges and risks of generative AI like hallucinations and jailbreaks How to integrate generative AI into your business and IT strategy Generative AI in Action is full of real-world use cases for generative AI, showing you where and how to start integrating this powerful technology into your products and workflows. You’ll benefit from tried-and-tested implementation advice, as well as application architectures to deploy GenAI in production at enterprise scale.",prompt engineering;model fine tuning;enterprise;safety;ethics;integration;RAG;multi-modality;LLMs;hallucinations;jailbreaks;architectural patterns;ChatGPT;Bard;Copilot,"{'url': 'https://ieeexplore-ieee-org.crai.referencistas.com/document/10745288', 'isbn': '9781633436947', 'publisher': 'Manning', 'issn': '', 'doi': '', 'keywords': 'prompt engineering;model fine tuning;enterprise;safety;ethics;integration;RAG;multi-modality;LLMs;hallucinations;jailbreaks;architectural patterns;ChatGPT;Bard;Copilot', 'abstract': 'Generative AI can transform your business by streamlining the process of creating text, images, and code. This book will show you how to get in on the action! Generative AI in Action is the comprehensive and concrete guide to generative AI you’ve been searching for. It introduces both AI’s fundamental principles and its practical applications in an enterprise context—from generating text and images for product catalogs and marketing campaigns, to technical reporting, and even writing software. Inside, author Amit Bahree shares his experience leading Generative AI projects at Microsoft for nearly a decade, starting well before the current GPT revolution. Inside Generative AI in Action you will find: A practical overview of of generative AI applications Architectural patterns, integration guidance, and best practices for generative AI The latest techniques like RAG, prompt engineering, and multi-modality The challenges and risks of generative AI like hallucinations and jailbreaks How to integrate generative AI into your business and IT strategy Generative AI in Action is full of real-world use cases for generative AI, showing you where and how to start integrating this powerful technology into your products and workflows. You’ll benefit from tried-and-tested implementation advice, as well as application architectures to deploy GenAI in production at enterprise scale.', 'pages': '', 'number': '', 'volume': '', 'year': '2024', 'booktitle': 'Generative AI in Action', 'author': 'Bahree, Amit', 'ENTRYTYPE': 'book', 'ID': '10745288'}"
10955601,Role of AI and Digital Twin in Smart Manufacturing,"Anand, M. and Sheeba, T. M. and Fancy, C.",Anand,10.1002/9781394303601.ch11,2024,Artificial Intelligence-Enabled Digital Twin for Smart Manufacturing,"Summary <p>Recent years we have seen the emergence of several technologies that are critical to the development of the Industrial IoT and smart manufacturing. These include next\&\#x2010;generation material science, advanced robotics, cyber\&\#x2010;physical systems, big data, advanced analytics, artificial intelligence (AI) and machine learning (ML), operational intelligence and generative design for additive manufacturing. Manufacturers are thinking about new business models based on real\&\#x2010;world implementations, in which they offer services instead of products and use the digital twin to optimize the product's performance and availability. In addition to full maintenance and operational optimization established on the digital twin's predictive and prescriptive capabilities, customers are provided the usage of the product or equipment. As a more profitable and manageable business model, the manufacturer retains ownership of the equipment while offering the maintenance service established on a digital twin.</p>",Digital twins;Smart manufacturing;Optimization;Business;Real-time systems;Maintenance;Robot sensing systems;Machine learning;Assembly;Service robots,"{'url': 'https://ieeexplore-ieee-org.crai.referencistas.com/document/10955601', 'isbn': '9781394303595', 'publisher': 'Wiley', 'issn': '', 'doi': '10.1002/9781394303601.ch11', 'keywords': 'Digital twins;Smart manufacturing;Optimization;Business;Real-time systems;Maintenance;Robot sensing systems;Machine learning;Assembly;Service robots', 'abstract': ""Summary <p>Recent years we have seen the emergence of several technologies that are critical to the development of the Industrial IoT and smart manufacturing. These include next\\&\\#x2010;generation material science, advanced robotics, cyber\\&\\#x2010;physical systems, big data, advanced analytics, artificial intelligence (AI) and machine learning (ML), operational intelligence and generative design for additive manufacturing. Manufacturers are thinking about new business models based on real\\&\\#x2010;world implementations, in which they offer services instead of products and use the digital twin to optimize the product's performance and availability. In addition to full maintenance and operational optimization established on the digital twin's predictive and prescriptive capabilities, customers are provided the usage of the product or equipment. As a more profitable and manageable business model, the manufacturer retains ownership of the equipment while offering the maintenance service established on a digital twin.</p>"", 'pages': '233-248', 'number': '', 'volume': '', 'year': '2024', 'title': 'Role of AI and Digital Twin in Smart Manufacturing', 'booktitle': 'Artificial Intelligence-Enabled Digital Twin for Smart Manufacturing', 'author': 'Anand, M. and Sheeba, T. M. and Fancy, C.', 'ENTRYTYPE': 'inbook', 'ID': '10955601'}"
10776431,Reservoir Facies Modeling Based on Generative Adversarial Network,"Lin, Shaolin and Yin, Senlin and Zhang, Yaowei and Liu, Juanxia and Tao, Chao",Lin,10.1109/NTCI64025.2024.10776431,2024,2024 International Conference on New Trends in Computational Intelligence (NTCI),"Three-dimensional geological modeling of reservoirs is of great significance for developing oil and gas resources, groundwater resources, and carbon dioxide geological storage. Geological facies models are the basis for accurately predicting underground oil reservoirs, geological carbon dioxide storage potential, and groundwater resources. Traditional geostatistical modeling methods can be consistent with geological models to some extent, but there are obvious shortcomings when the characteristics of geological models become complex. Therefore, this paper takes the braided river deltaic diversion channel and estuarine dam phase of the Hanjiang Formation in the Epping Depression of the Pearl River Estuary Basin as the research objective. Based on the geological background of the workings, 3D seismic data and seven logging wells, a conditionally bounded phase simulation based on an improved GAN model is proposed. The research results show that the phase model generated by modeling well facies data matches the input well facies data. The generated phase model has good diversity, and the trained generator can generate high-quality phase models with 100\%. accuracy in reproducing well facies.",Geology;Computational modeling;Stochastic processes;Predictive models;Generative adversarial networks;Reservoirs;Data models;Generators;Vectors;Synthetic data;conditional phase modeling;generate adversarial networks;deep learning;geological model;reservoir prediction,"{'month': 'Oct', 'issn': '', 'doi': '10.1109/NTCI64025.2024.10776431', 'keywords': 'Geology;Computational modeling;Stochastic processes;Predictive models;Generative adversarial networks;Reservoirs;Data models;Generators;Vectors;Synthetic data;conditional phase modeling;generate adversarial networks;deep learning;geological model;reservoir prediction', 'abstract': 'Three-dimensional geological modeling of reservoirs is of great significance for developing oil and gas resources, groundwater resources, and carbon dioxide geological storage. Geological facies models are the basis for accurately predicting underground oil reservoirs, geological carbon dioxide storage potential, and groundwater resources. Traditional geostatistical modeling methods can be consistent with geological models to some extent, but there are obvious shortcomings when the characteristics of geological models become complex. Therefore, this paper takes the braided river deltaic diversion channel and estuarine dam phase of the Hanjiang Formation in the Epping Depression of the Pearl River Estuary Basin as the research objective. Based on the geological background of the workings, 3D seismic data and seven logging wells, a conditionally bounded phase simulation based on an improved GAN model is proposed. The research results show that the phase model generated by modeling well facies data matches the input well facies data. The generated phase model has good diversity, and the trained generator can generate high-quality phase models with 100\\%. accuracy in reproducing well facies.', 'pages': '483-487', 'number': '', 'volume': '', 'year': '2024', 'title': 'Reservoir Facies Modeling Based on Generative Adversarial Network', 'booktitle': '2024 International Conference on New Trends in Computational Intelligence (NTCI)', 'author': 'Lin, Shaolin and Yin, Senlin and Zhang, Yaowei and Liu, Juanxia and Tao, Chao', 'ENTRYTYPE': 'inproceedings', 'ID': '10776431'}"
10948951,Artificial Intelligence in Action: Real-World Applications and Innovations,"Banafa, Ahmed",Banafa,,2025,Artificial Intelligence in Action: Real-World Applications and Innovations,"This comprehensive book dives deep into the current landscape of AI, exploring its fundamental principles, development challenges, potential risks, and the cutting-edge breakthroughs that are propelling it forward. Artificial intelligence (AI) is rapidly transforming industries and societies worldwide through groundbreaking innovations and real-world applications. Starting with the core concepts, the book examines the various types of AI systems, generative AI models, and the complexities of machine learning. It delves into the programming languages driving AI development, data pipelines, model creation and deployment processes, while shedding light on issues like AI hallucinations and the intricate path of machine unlearning. The book then showcases the remarkable real-world applications of AI across diverse domains. From preventing job displacement and promoting environmental sustainability, to enhancing disaster response, drone technology, and even nuclear energy innovation, it highlights how AI is tackling complex challenges and driving positive change. The book also explores the double-edged nature of AI, recognizing its tremendous potential while cautioning about the risks of misuse, unintended consequences, and the urgent need for responsible development practices. It examines the intersection of AI and fields like operating system design, warfare, and semiconductor technology, underscoring the wide-ranging implications of this transformative force. As the quest for artificial general intelligence (AGI) and superintelligent AI systems intensifies, the book delves into cutting-edge research, emerging trends, and the pursuit of multimodal, explainable, and causally aware AI systems. It explores the symbiotic relationship between AI and human creativity, the rise of user-friendly ""casual AI,"" and the potential of AI to tackle open-ended tasks. This is an essential guide for understanding the profound impact of AI on our world today and its potential to shape our future. From the frontiers of innovation to the challenges of responsible development, this book offers a comprehensive and insightful exploration of the remarkable real-world applications and innovations driving the AI revolution.",,"{'url': 'https://ieeexplore-ieee-org.crai.referencistas.com/document/10948951', 'isbn': '9788770046190', 'publisher': 'River Publishers', 'issn': '', 'doi': '', 'keywords': '', 'abstract': 'This comprehensive book dives deep into the current landscape of AI, exploring its fundamental principles, development challenges, potential risks, and the cutting-edge breakthroughs that are propelling it forward. Artificial intelligence (AI) is rapidly transforming industries and societies worldwide through groundbreaking innovations and real-world applications. Starting with the core concepts, the book examines the various types of AI systems, generative AI models, and the complexities of machine learning. It delves into the programming languages driving AI development, data pipelines, model creation and deployment processes, while shedding light on issues like AI hallucinations and the intricate path of machine unlearning. The book then showcases the remarkable real-world applications of AI across diverse domains. From preventing job displacement and promoting environmental sustainability, to enhancing disaster response, drone technology, and even nuclear energy innovation, it highlights how AI is tackling complex challenges and driving positive change. The book also explores the double-edged nature of AI, recognizing its tremendous potential while cautioning about the risks of misuse, unintended consequences, and the urgent need for responsible development practices. It examines the intersection of AI and fields like operating system design, warfare, and semiconductor technology, underscoring the wide-ranging implications of this transformative force. As the quest for artificial general intelligence (AGI) and superintelligent AI systems intensifies, the book delves into cutting-edge research, emerging trends, and the pursuit of multimodal, explainable, and causally aware AI systems. It explores the symbiotic relationship between AI and human creativity, the rise of user-friendly ""casual AI,"" and the potential of AI to tackle open-ended tasks. This is an essential guide for understanding the profound impact of AI on our world today and its potential to shape our future. From the frontiers of innovation to the challenges of responsible development, this book offers a comprehensive and insightful exploration of the remarkable real-world applications and innovations driving the AI revolution.', 'pages': 'i-xxx', 'number': '', 'volume': '', 'year': '2025', 'title': 'Artificial Intelligence in Action: Real-World Applications and Innovations', 'booktitle': 'Artificial Intelligence in Action: Real-World Applications and Innovations', 'author': 'Banafa, Ahmed', 'ENTRYTYPE': 'inbook', 'ID': '10948951'}"
11046234,Software Application Experiences: Cloud Technology and Collaborative Work in a University Course,"Rafael Morano-Okuno, Hector and Enrique Chong-Quero, J. and Esqueda-Merino, Donovan M. and Sandoval-Benitez, Guillermo and Jaramillo-Godinez, Ricardo and Alfredo Murano-Labastida, Daishi",Rafael Morano-Okuno,10.1109/ICIET66371.2025.11046234,2025,2025 13th International Conference on Information and Education Technology (ICIET),"Nowadays, many software applications based on artificial intelligence (AI) algorithms create generative designs, which can be used as didactic tools in university courses. Thanks to AI, almost all of them are user-friendly and allow collaborative work in cloud environments. However, it is necessary to explore them to understand their aims. This work was focused on studying a software application that allows collaborative work and cloud technology interaction; it also permits the creation of Generative Designs of products. It was employed in a course on Cyber-physical systems for engineering majors. Different activities were developed to identify their objectives. The results show that the cloud technology used by the software application allows students to work in multidisciplinary teams, from a distance, and in an adequate collaborative way. Finally, the experiences gained from working with this computational tool are shared, and some recommendations on creating generative designs are also defined.",Technological innovation;Federated learning;Education;Software algorithms;Collaboration;Cyber-physical systems;Software;Object recognition;Artificial intelligence;cloud technology;collaborative work;generative design;Fusion 360;educational innovation;higher education;professional education;tec21 model,"{'month': 'April', 'issn': '', 'doi': '10.1109/ICIET66371.2025.11046234', 'keywords': 'Technological innovation;Federated learning;Education;Software algorithms;Collaboration;Cyber-physical systems;Software;Object recognition;Artificial intelligence;cloud technology;collaborative work;generative design;Fusion 360;educational innovation;higher education;professional education;tec21 model', 'abstract': 'Nowadays, many software applications based on artificial intelligence (AI) algorithms create generative designs, which can be used as didactic tools in university courses. Thanks to AI, almost all of them are user-friendly and allow collaborative work in cloud environments. However, it is necessary to explore them to understand their aims. This work was focused on studying a software application that allows collaborative work and cloud technology interaction; it also permits the creation of Generative Designs of products. It was employed in a course on Cyber-physical systems for engineering majors. Different activities were developed to identify their objectives. The results show that the cloud technology used by the software application allows students to work in multidisciplinary teams, from a distance, and in an adequate collaborative way. Finally, the experiences gained from working with this computational tool are shared, and some recommendations on creating generative designs are also defined.', 'pages': '257-261', 'number': '', 'volume': '', 'year': '2025', 'title': 'Software Application Experiences: Cloud Technology and Collaborative Work in a University Course', 'booktitle': '2025 13th International Conference on Information and Education Technology (ICIET)', 'author': 'Rafael Morano-Okuno, Hector and Enrique Chong-Quero, J. and Esqueda-Merino, Donovan M. and Sandoval-Benitez, Guillermo and Jaramillo-Godinez, Ricardo and Alfredo Murano-Labastida, Daishi', 'ENTRYTYPE': 'inproceedings', 'ID': '11046234'}"
9562726,Front Matter,"Reznik, Leon",Reznik,10.1002/9781119771579.fmatter,2022,"Intelligent Security Systems: How Artificial Intelligence, Machine Learning and Data Science Work For and Against Computer Security",The prelims comprise: Half‐Title Page Series Page Title Page Copyright Page Dedication Page Table of Contents Acknowledgments Introduction,,"{'url': 'https://ieeexplore-ieee-org.crai.referencistas.com/document/9562726', 'isbn': '9781119771555', 'publisher': 'IEEE', 'issn': '', 'doi': '10.1002/9781119771579.fmatter', 'keywords': '', 'abstract': 'The prelims comprise: Half‐Title Page Series Page Title Page Copyright Page Dedication Page Table of Contents Acknowledgments Introduction', 'pages': 'i-xxvi', 'number': '', 'volume': '', 'year': '2022', 'title': 'Front Matter', 'booktitle': 'Intelligent Security Systems: How Artificial Intelligence, Machine Learning and Data Science Work For and Against Computer Security', 'author': 'Reznik, Leon', 'ENTRYTYPE': 'inbook', 'ID': '9562726'}"
10424147,Identifying the impact of Human-AI co-creation on students' creativity development: a conceptual framework,"Zhong, Jinping and Zheng, Yunxiang",Zhong,10.1109/ICET59358.2023.10424147,2023,2023 3rd International Conference on Educational Technology (ICET),"The amazing progress in conversational AI such as ChatGPT seems to be driving people into an era where human-AI co-creation is prevalent. Efforts need to focus on the new context of creativity development, as AI is more to creativity than simplifying creative tasks. The current study aims to explore the possible impact of AI on creativity and how students co-create with it in a way that optimizes its benefits. Whereas AI may challenge the position of humans in certain creative tasks, we argue that it provides multiple supports to the creative process. Accordingly, we proposed a conceptual framework of the impact of human-AI co-creation on creativity based on creativity theories and the SAMR model. Then we concluded with four principles of student-AI co-creation.",Ethics;Cognitive processes;Educational technology;Chatbots;Artificial intelligence;Task analysis;Creativity;creativity;artificial intelligence;human-AI co-creation;impact,"{'month': 'Sep.', 'issn': '', 'doi': '10.1109/ICET59358.2023.10424147', 'keywords': 'Ethics;Cognitive processes;Educational technology;Chatbots;Artificial intelligence;Task analysis;Creativity;creativity;artificial intelligence;human-AI co-creation;impact', 'abstract': 'The amazing progress in conversational AI such as ChatGPT seems to be driving people into an era where human-AI co-creation is prevalent. Efforts need to focus on the new context of creativity development, as AI is more to creativity than simplifying creative tasks. The current study aims to explore the possible impact of AI on creativity and how students co-create with it in a way that optimizes its benefits. Whereas AI may challenge the position of humans in certain creative tasks, we argue that it provides multiple supports to the creative process. Accordingly, we proposed a conceptual framework of the impact of human-AI co-creation on creativity based on creativity theories and the SAMR model. Then we concluded with four principles of student-AI co-creation.', 'pages': '66-70', 'number': '', 'volume': '', 'year': '2023', 'title': ""Identifying the impact of Human-AI co-creation on students' creativity development: a conceptual framework"", 'booktitle': '2023 3rd International Conference on Educational Technology (ICET)', 'author': 'Zhong, Jinping and Zheng, Yunxiang', 'ENTRYTYPE': 'inproceedings', 'ID': '10424147'}"
10382492,Semisupervised Transfer Boosting (SS-TrBoosting),"Deng, Lingfei and Zhao, Changming and Du, Zhenbang and Xia, Kun and Wu, Dongrui",Deng,10.1109/TAI.2024.3350543,2024,IEEE Transactions on Artificial Intelligence,"Semisupervised domain adaptation (SSDA) aims at training a high-performance model for a target domain using few labeled target data, many unlabeled target data, and plenty of auxiliary data from a source domain. Previous works in SSDA mainly focused on learning transferable representations across domains. However, it is difficult to find a feature space where the source and target domains share the same conditional probability distribution. Additionally, there is no flexible and effective strategy extending existing unsupervised domain adaptation (UDA) approaches to SSDA settings. In order to solve the above two challenges, we propose a novel fine-tuning framework, semisupervised transfer boosting (SS-TrBoosting). Given a well-trained deep learning-based UDA or SSDA model, we use it as the initial model, generate additional base learners by boosting, and then use all of them as an ensemble. More specifically, half of the base learners are generated by supervised domain adaptation, and half by semisupervised learning. Furthermore, for more efficient data transmission and better data privacy protection, we propose a source data generation approach to extend SS-TrBoosting to semisupervised source-free domain adaptation (SS-SFDA). Extensive experiments showed that SS-TrBoosting can be applied to a variety of existing UDA, SSDA, and SFDA approaches to further improve their performance.",Feature extraction;Boosting;Adaptation models;Data models;Prototypes;Artificial intelligence;Ensemble learning;Boosting;ensemble learning;fine-tuning;semisupervised domain adaptation (SSDA);source-free domain adaptation (SFDA),"{'month': 'July', 'issn': '2691-4581', 'doi': '10.1109/TAI.2024.3350543', 'keywords': 'Feature extraction;Boosting;Adaptation models;Data models;Prototypes;Artificial intelligence;Ensemble learning;Boosting;ensemble learning;fine-tuning;semisupervised domain adaptation (SSDA);source-free domain adaptation (SFDA)', 'abstract': 'Semisupervised domain adaptation (SSDA) aims at training a high-performance model for a target domain using few labeled target data, many unlabeled target data, and plenty of auxiliary data from a source domain. Previous works in SSDA mainly focused on learning transferable representations across domains. However, it is difficult to find a feature space where the source and target domains share the same conditional probability distribution. Additionally, there is no flexible and effective strategy extending existing unsupervised domain adaptation (UDA) approaches to SSDA settings. In order to solve the above two challenges, we propose a novel fine-tuning framework, semisupervised transfer boosting (SS-TrBoosting). Given a well-trained deep learning-based UDA or SSDA model, we use it as the initial model, generate additional base learners by boosting, and then use all of them as an ensemble. More specifically, half of the base learners are generated by supervised domain adaptation, and half by semisupervised learning. Furthermore, for more efficient data transmission and better data privacy protection, we propose a source data generation approach to extend SS-TrBoosting to semisupervised source-free domain adaptation (SS-SFDA). Extensive experiments showed that SS-TrBoosting can be applied to a variety of existing UDA, SSDA, and SFDA approaches to further improve their performance.', 'pages': '3431-3444', 'number': '7', 'volume': '5', 'year': '2024', 'title': 'Semisupervised Transfer Boosting (SS-TrBoosting)', 'journal': 'IEEE Transactions on Artificial Intelligence', 'author': 'Deng, Lingfei and Zhao, Changming and Du, Zhenbang and Xia, Kun and Wu, Dongrui', 'ENTRYTYPE': 'article', 'ID': '10382492'}"
10539708,The Application of Neural Networks in the Field of Architecture in Machine Learning,"Wen, Hao and Sun, Zhongya",Wen,10.1109/EDPEE61724.2024.00079,2024,"2024 International Conference on Electrical Drives, Power Electronics \& Engineering (EDPEE)","The field of architectural design is known for its unique traditions and complexity. It is a multidisciplinary field that involves various disciplines, but machine learning has never been involved in this field before. However, with the emergence of Big Data and the 5G era, this barrier has been broken. Currently, the machine learning and artificial intelligence industries are gradually penetrating various aspects of the construction industry. This article explores the application of deep learning in architectural design through machine learning research and examines whether artificial intelligence has its own creativity today. The article first introduces the concepts of artificial intelligence, machine learning, and deep learning, and discusses their differences and connections to help readers understand the content. It then delves into deep learning, focusing on the use of neural networks in architectural design. Finally, it summarizes how neural networks are applied to architectural design in machine learning.",Training;Deep learning;Recurrent neural networks;Neurons;Focusing;Iterative methods;Object recognition;Artificial Intelligence;Deep Learning;Neural Networks;Architectural Applications,"{'month': 'Feb', 'issn': '', 'doi': '10.1109/EDPEE61724.2024.00079', 'keywords': 'Training;Deep learning;Recurrent neural networks;Neurons;Focusing;Iterative methods;Object recognition;Artificial Intelligence;Deep Learning;Neural Networks;Architectural Applications', 'abstract': 'The field of architectural design is known for its unique traditions and complexity. It is a multidisciplinary field that involves various disciplines, but machine learning has never been involved in this field before. However, with the emergence of Big Data and the 5G era, this barrier has been broken. Currently, the machine learning and artificial intelligence industries are gradually penetrating various aspects of the construction industry. This article explores the application of deep learning in architectural design through machine learning research and examines whether artificial intelligence has its own creativity today. The article first introduces the concepts of artificial intelligence, machine learning, and deep learning, and discusses their differences and connections to help readers understand the content. It then delves into deep learning, focusing on the use of neural networks in architectural design. Finally, it summarizes how neural networks are applied to architectural design in machine learning.', 'pages': '386-390', 'number': '', 'volume': '', 'year': '2024', 'title': 'The Application of Neural Networks in the Field of Architecture in Machine Learning', 'booktitle': '2024 International Conference on Electrical Drives, Power Electronics \\& Engineering (EDPEE)', 'author': 'Wen, Hao and Sun, Zhongya', 'ENTRYTYPE': 'inproceedings', 'ID': '10539708'}"
10971683,Transforming Software Architecture Design With Intelligent Assistants-A Comparative Analysis,"Ramachandran, Renjith",Ramachandran,10.1109/SoutheastCon56624.2025.10971683,2025,SoutheastCon 2025,"As artificial intelligence tools such as GitHub Copilot, Chat Generative Pre-Trained Transformer (ChatGPT), or Black Box Artificial Intelligence (BlackBoxAI) become more embedded in software development workflows, they are reshaping how architects approach modern software design. While enterprises are increasingly leveraging Generative Artificial Intelligence (GenAI) tools to enhance developer productivity, limited research has been conducted on their impact on software architects. Traditionally, architects utilize various tools to create architectural blueprints, including Unified Modelling Language (UML) diagrams, class diagrams, sequence diagrams, use case diagrams, and state diagrams. Many of these diagrams follow repetitive structures that can be automated with well-defined contexts. This paper examines the role of three AI tools in assisting architects during the design phase, assessing how closely the generated outputs adhere to established architectural principles. By analyzing deviations, the study explores how refined prompts and additional context can improve accuracy. Research also includes cross tool comparison. Ultimately, this research aims to evaluate the potential of AI assistant tools to enhance productivity, ensure design consistency, and support architectural decision-making, while providing best practices for integrating AI into modern software architecture.",Productivity;Software design;Software architecture;Generative AI;Unified modeling language;Chatbots;Transformers;Security;Artificial intelligence;Software development management;BlackboxAI;ChatGPT;Gen AI;Github Copilot;Software Architecture,"{'month': 'March', 'issn': '1558-058X', 'doi': '10.1109/SoutheastCon56624.2025.10971683', 'keywords': 'Productivity;Software design;Software architecture;Generative AI;Unified modeling language;Chatbots;Transformers;Security;Artificial intelligence;Software development management;BlackboxAI;ChatGPT;Gen AI;Github Copilot;Software Architecture', 'abstract': 'As artificial intelligence tools such as GitHub Copilot, Chat Generative Pre-Trained Transformer (ChatGPT), or Black Box Artificial Intelligence (BlackBoxAI) become more embedded in software development workflows, they are reshaping how architects approach modern software design. While enterprises are increasingly leveraging Generative Artificial Intelligence (GenAI) tools to enhance developer productivity, limited research has been conducted on their impact on software architects. Traditionally, architects utilize various tools to create architectural blueprints, including Unified Modelling Language (UML) diagrams, class diagrams, sequence diagrams, use case diagrams, and state diagrams. Many of these diagrams follow repetitive structures that can be automated with well-defined contexts. This paper examines the role of three AI tools in assisting architects during the design phase, assessing how closely the generated outputs adhere to established architectural principles. By analyzing deviations, the study explores how refined prompts and additional context can improve accuracy. Research also includes cross tool comparison. Ultimately, this research aims to evaluate the potential of AI assistant tools to enhance productivity, ensure design consistency, and support architectural decision-making, while providing best practices for integrating AI into modern software architecture.', 'pages': '1446-1454', 'number': '', 'volume': '', 'year': '2025', 'title': 'Transforming Software Architecture Design With Intelligent Assistants-A Comparative Analysis', 'booktitle': 'SoutheastCon 2025', 'author': 'Ramachandran, Renjith', 'ENTRYTYPE': 'inproceedings', 'ID': '10971683'}"
10258299,3G-AN: Triple-Generative Adversarial Network Under Corse-Medium-Fine Generator Architecture,"Avilés-Cruz, Carlos and Celis-Escudero, Gabriel J.",Avilés-Cruz,10.1109/ACCESS.2023.3317897,2023,IEEE Access,"In recent years, Generative Adversarial Networks (GANs) have gained worldwide interest and have marked a breakthrough in deep learning, encouraging detailed studies in generating artificial images. A new Generative Adversarial Networks (GAN) is proposed to unveil how Human visual perception takes place, focusing on how human beings perceive images, firstly, coarse structures and then their details. The network called 3G-AN consists of three generation stages and a single Discriminator. In this paper, a novel three-branch generator is proposed, which takes into account Coarse, Medium, and Fine structure of a given image. Coarse RGB decomposition image provides the general structure, while Medium RGB stage provides general-fine structure. Finally, Fine RGB decomposition provides fine details of the image. The proposal is tested on MNIST, CIFAR10, and Celebrity faces databases, generating realistic images with almost no anomalies. The RGB decomposition into coarse, medium, and fine, allows to understand the composition of an image from a structural point of view. The qualitative analysis carried out in this research paper outperforms the six most competitive models existing in the literature.",Generators;Generative adversarial networks;Task analysis;Faces;Training;Proposals;Deep learning;Artificial intelligence;Fake news;Image analysis;GAN;artificial intelligence;deep learning;fake images,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2023.3317897', 'keywords': 'Generators;Generative adversarial networks;Task analysis;Faces;Training;Proposals;Deep learning;Artificial intelligence;Fake news;Image analysis;GAN;artificial intelligence;deep learning;fake images', 'abstract': 'In recent years, Generative Adversarial Networks (GANs) have gained worldwide interest and have marked a breakthrough in deep learning, encouraging detailed studies in generating artificial images. A new Generative Adversarial Networks (GAN) is proposed to unveil how Human visual perception takes place, focusing on how human beings perceive images, firstly, coarse structures and then their details. The network called 3G-AN consists of three generation stages and a single Discriminator. In this paper, a novel three-branch generator is proposed, which takes into account Coarse, Medium, and Fine structure of a given image. Coarse RGB decomposition image provides the general structure, while Medium RGB stage provides general-fine structure. Finally, Fine RGB decomposition provides fine details of the image. The proposal is tested on MNIST, CIFAR10, and Celebrity faces databases, generating realistic images with almost no anomalies. The RGB decomposition into coarse, medium, and fine, allows to understand the composition of an image from a structural point of view. The qualitative analysis carried out in this research paper outperforms the six most competitive models existing in the literature.', 'pages': '105344-105354', 'number': '', 'volume': '11', 'year': '2023', 'title': '3G-AN: Triple-Generative Adversarial Network Under Corse-Medium-Fine Generator Architecture', 'journal': 'IEEE Access', 'author': 'Avilés-Cruz, Carlos and Celis-Escudero, Gabriel J.', 'ENTRYTYPE': 'article', 'ID': '10258299'}"
10654499,ISRnet: Compressed Image Inpainting Based on Generative Adversarial Network,"Huang, Junjian and Zheng, Mao and Li, Zhizhang and He, Xing and Wen, Shiping",Huang,10.1109/TETCI.2024.3446690,2025,IEEE Transactions on Emerging Topics in Computational Intelligence,"In recent years, significant advancements have been made in the domain of image restoration, particularly in the context of repairing damaged images and super-resolution reconstruction, primarily owing to the emergence of deep learning techniques. However, during the course of transmission across various media devices, the original image quality may deteriorate due to factors such as the network environment, hardware constraints, and related conditions. Moreover, the restoration process becomes increasingly challenging when the original image quality is low or compromised. Presently, prevailing methods involve repairing the image prior to performing super-resolution reconstruction. However, this methodology typically relies on the utilization of multiple autonomous models, where the efficacy and time efficiency are not optimal. In light of this, we propose a novel neural network model based on GAN, termed ISRnet, designed to repair damaged compressed images. Our method is the first to leverage GAN networks specifically for compressed image restoration. ISRnet integrates the principles of image inpainting and super-resolution reconstruction, enabling the transformation of low-resolution images into high-resolution counterparts during the restoration process, thereby achieving superior restoration outcomes. Despite the partial increase in bias, this approach counteracts the variability inherent in a singularly trained neural network model. Compared to a single neural network model trained on the same dataset, our model demonstrates reduced variance and diminished sensitivity to data, thereby achieving optimal restoration quality and expedited repair speeds for damaged compressed images. Consequently, our proposed methodology presents a promising avenue for the utilization of neural networks in repairing damaged compressed images.",Image restoration;Superresolution;Image resolution;Generative adversarial networks;Image reconstruction;Generators;Training;Generative adversarial networks;super resolution;image inpainting,"{'month': 'Aug', 'issn': '2471-285X', 'doi': '10.1109/TETCI.2024.3446690', 'keywords': 'Image restoration;Superresolution;Image resolution;Generative adversarial networks;Image reconstruction;Generators;Training;Generative adversarial networks;super resolution;image inpainting', 'abstract': 'In recent years, significant advancements have been made in the domain of image restoration, particularly in the context of repairing damaged images and super-resolution reconstruction, primarily owing to the emergence of deep learning techniques. However, during the course of transmission across various media devices, the original image quality may deteriorate due to factors such as the network environment, hardware constraints, and related conditions. Moreover, the restoration process becomes increasingly challenging when the original image quality is low or compromised. Presently, prevailing methods involve repairing the image prior to performing super-resolution reconstruction. However, this methodology typically relies on the utilization of multiple autonomous models, where the efficacy and time efficiency are not optimal. In light of this, we propose a novel neural network model based on GAN, termed ISRnet, designed to repair damaged compressed images. Our method is the first to leverage GAN networks specifically for compressed image restoration. ISRnet integrates the principles of image inpainting and super-resolution reconstruction, enabling the transformation of low-resolution images into high-resolution counterparts during the restoration process, thereby achieving superior restoration outcomes. Despite the partial increase in bias, this approach counteracts the variability inherent in a singularly trained neural network model. Compared to a single neural network model trained on the same dataset, our model demonstrates reduced variance and diminished sensitivity to data, thereby achieving optimal restoration quality and expedited repair speeds for damaged compressed images. Consequently, our proposed methodology presents a promising avenue for the utilization of neural networks in repairing damaged compressed images.', 'pages': '2743-2753', 'number': '4', 'volume': '9', 'year': '2025', 'title': 'ISRnet: Compressed Image Inpainting Based on Generative Adversarial Network', 'journal': 'IEEE Transactions on Emerging Topics in Computational Intelligence', 'author': 'Huang, Junjian and Zheng, Mao and Li, Zhizhang and He, Xing and Wen, Shiping', 'ENTRYTYPE': 'article', 'ID': '10654499'}"
10940581,Exploring the Impact of Conditional Deep Convolutional Generative Adversarial Networks in Brain Tumor Image Classification: A Novel Approach,"Upadhyay, Deepak and Upadhyay, Abhay and Sharma, Kuj Bihari and Dhondiyal, Shiv Ashish and Venu, Nookala",Upadhyay,10.1109/ICPCT64145.2025.10940581,2025,2025 International Conference on Pervasive Computational Technologies (ICPCT),"In this paper, we explore the impact of conditional Deep Convolutional Generative Adversarial Networks (cDCGANs) with brain tumor image classification and introduce a new way to improve diagnosis accuracy. Shortage of Data/Data Imbalance: In Medical Imaging datasets, it helps in surging the data scarcity and class imbalance with mimicking generation images using cDCGAN. The detailed investigation demonstrates that synthetic data effectively improves classifier performance by acting as a form of augmentation, providing additional training examples for classifiers and serving as ground truth. From the computational efficiency analysis, we see that there is also scalability in training time with respect to dataset size. Consequently, this study presents cDCGANs as an important asset with which to improve diagnostic accuracy through brain tumor categorization in clinical contexts.",Training;Accuracy;Scalability;Brain tumors;Medical services;Sensitivity and specificity;Generative adversarial networks;Robustness;Image classification;Synthetic data;cDCGANs;CNNs;ANNs;AI;ML,"{'month': 'Feb', 'issn': '', 'doi': '10.1109/ICPCT64145.2025.10940581', 'keywords': 'Training;Accuracy;Scalability;Brain tumors;Medical services;Sensitivity and specificity;Generative adversarial networks;Robustness;Image classification;Synthetic data;cDCGANs;CNNs;ANNs;AI;ML', 'abstract': 'In this paper, we explore the impact of conditional Deep Convolutional Generative Adversarial Networks (cDCGANs) with brain tumor image classification and introduce a new way to improve diagnosis accuracy. Shortage of Data/Data Imbalance: In Medical Imaging datasets, it helps in surging the data scarcity and class imbalance with mimicking generation images using cDCGAN. The detailed investigation demonstrates that synthetic data effectively improves classifier performance by acting as a form of augmentation, providing additional training examples for classifiers and serving as ground truth. From the computational efficiency analysis, we see that there is also scalability in training time with respect to dataset size. Consequently, this study presents cDCGANs as an important asset with which to improve diagnostic accuracy through brain tumor categorization in clinical contexts.', 'pages': '12-16', 'number': '', 'volume': '', 'year': '2025', 'title': 'Exploring the Impact of Conditional Deep Convolutional Generative Adversarial Networks in Brain Tumor Image Classification: A Novel Approach', 'booktitle': '2025 International Conference on Pervasive Computational Technologies (ICPCT)', 'author': 'Upadhyay, Deepak and Upadhyay, Abhay and Sharma, Kuj Bihari and Dhondiyal, Shiv Ashish and Venu, Nookala', 'ENTRYTYPE': 'inproceedings', 'ID': '10940581'}"
10823604,A Study on CartoonGAN Using High-Resolution Generative Networks Through Feature Emphasis,"Choi, Min Sung and Lee, Hae Won and Bae, Seong Geon",Choi,10.1109/ICECCE63537.2024.10823604,2024,"2024 International Conference on Electrical, Communication and Computer Engineering (ICECCE)","GAN is a model that generates completely new data and can create various media contents. CartoonGAN is a GAN model that converts real images into cartoon style, and can apply various cartoon styles. In this study, we propose ESRGAN (Enhanced Super-Resolution Generative Adversarial Networks) technology as a preprocessing method to improve the performance of CartoonGAN. ESRGAN is a model that improves resolution using GAN. This method showed better performance than other SR techniques in the evaluation indices MSE, PSNR, and SSIM results. This suggests that ESRGAN is effective in high-resolution restoration ability and visual quality improvement in cartoon style image conversion. The method proposed in this study can be used as a useful tool for high-resolution cartoon image generation. Since the learning data of the model is limited to a specific domain, there may be limitations in generalizability. Due to the characteristics of ESRGAN, which is computationally complex and consumes a lot of resources, there may be limitations in applying it to real-time processing or large datasets. In future studies, it is necessary to build an extended dataset for various images to overcome these limitations. It is important to reduce computational costs by making models lightweight and optimizing them, and to develop algorithms suitable for real-time processing. It is expected that these additional studies and improvements will be utilized to produce high-quality images.",Visualization;Image synthesis;Computational modeling;Noise;Media;Generative adversarial networks;Real-time systems;Data models;Image restoration;Computational efficiency;GAN;CartoonGAN;Image Processing;ESRGAN;Computer Vision,"{'month': 'Oct', 'issn': '', 'doi': '10.1109/ICECCE63537.2024.10823604', 'keywords': 'Visualization;Image synthesis;Computational modeling;Noise;Media;Generative adversarial networks;Real-time systems;Data models;Image restoration;Computational efficiency;GAN;CartoonGAN;Image Processing;ESRGAN;Computer Vision', 'abstract': 'GAN is a model that generates completely new data and can create various media contents. CartoonGAN is a GAN model that converts real images into cartoon style, and can apply various cartoon styles. In this study, we propose ESRGAN (Enhanced Super-Resolution Generative Adversarial Networks) technology as a preprocessing method to improve the performance of CartoonGAN. ESRGAN is a model that improves resolution using GAN. This method showed better performance than other SR techniques in the evaluation indices MSE, PSNR, and SSIM results. This suggests that ESRGAN is effective in high-resolution restoration ability and visual quality improvement in cartoon style image conversion. The method proposed in this study can be used as a useful tool for high-resolution cartoon image generation. Since the learning data of the model is limited to a specific domain, there may be limitations in generalizability. Due to the characteristics of ESRGAN, which is computationally complex and consumes a lot of resources, there may be limitations in applying it to real-time processing or large datasets. In future studies, it is necessary to build an extended dataset for various images to overcome these limitations. It is important to reduce computational costs by making models lightweight and optimizing them, and to develop algorithms suitable for real-time processing. It is expected that these additional studies and improvements will be utilized to produce high-quality images.', 'pages': '1-5', 'number': '', 'volume': '', 'year': '2024', 'title': 'A Study on CartoonGAN Using High-Resolution Generative Networks Through Feature Emphasis', 'booktitle': '2024 International Conference on Electrical, Communication and Computer Engineering (ICECCE)', 'author': 'Choi, Min Sung and Lee, Hae Won and Bae, Seong Geon', 'ENTRYTYPE': 'inproceedings', 'ID': '10823604'}"
10645866,Adopting Generative AI with Precaution in Dentistry: A Review and Reflection,"Xu, Mingming and Ye, Chen and Zeng, Zheng and Chang, Chenyang and Qi, Shijie and Wu, Yujia and Yang, Huifang and Chen, Yifan and Huang, Haifeng and Liu, Lin and Cao, Zhanqiang and Deng, Xuliang",Xu,10.1109/ICDH62654.2024.00047,2024,2024 IEEE International Conference on Digital Health (ICDH),"The progress in large language models (LLMs) brings much excitement and efforts in medical artificial intelligence, which could transform patient-doctor conversation while making joint medical decisions. LLMs, exemplified by ChatGPT, are proficient in grasping and generating text, and can perform tasks such as question answering, document summarising, and paraphrasing with a level of proficiency comparable to that of a human. Their potential applications span across various tasks in medicine, notably improving clinical patient care experience, advancing scientific medical research, and revolutionizing medical education. This survey critically examines the evolving landscape of medical large language models (Med LLMs), with a special focus on their application in stomatology. While Med LLMs are inevitably becoming an integral part to medical text processing and image processing, their use in enhancing clinical care requires extra precaution and assurance due to the stringent requirements on ethics and patient safety. The design, deployment and use of LLMs and services requires thorough risks analysis of technology misuse and potential harms. This survey looks into the current status, different prospects and challenges in LLMs development in medical use cases and ways to control and mitigates risks of generative artificial intelligence.",Surveys;Ethics;Uncertainty;Generative AI;Large language models;Data security;Transforms;large language model;medicine;stomatology;ethics;risk analysis,"{'month': 'July', 'issn': '', 'doi': '10.1109/ICDH62654.2024.00047', 'keywords': 'Surveys;Ethics;Uncertainty;Generative AI;Large language models;Data security;Transforms;large language model;medicine;stomatology;ethics;risk analysis', 'abstract': 'The progress in large language models (LLMs) brings much excitement and efforts in medical artificial intelligence, which could transform patient-doctor conversation while making joint medical decisions. LLMs, exemplified by ChatGPT, are proficient in grasping and generating text, and can perform tasks such as question answering, document summarising, and paraphrasing with a level of proficiency comparable to that of a human. Their potential applications span across various tasks in medicine, notably improving clinical patient care experience, advancing scientific medical research, and revolutionizing medical education. This survey critically examines the evolving landscape of medical large language models (Med LLMs), with a special focus on their application in stomatology. While Med LLMs are inevitably becoming an integral part to medical text processing and image processing, their use in enhancing clinical care requires extra precaution and assurance due to the stringent requirements on ethics and patient safety. The design, deployment and use of LLMs and services requires thorough risks analysis of technology misuse and potential harms. This survey looks into the current status, different prospects and challenges in LLMs development in medical use cases and ways to control and mitigates risks of generative artificial intelligence.', 'pages': '244-256', 'number': '', 'volume': '', 'year': '2024', 'title': 'Adopting Generative AI with Precaution in Dentistry: A Review and Reflection', 'booktitle': '2024 IEEE International Conference on Digital Health (ICDH)', 'author': 'Xu, Mingming and Ye, Chen and Zeng, Zheng and Chang, Chenyang and Qi, Shijie and Wu, Yujia and Yang, Huifang and Chen, Yifan and Huang, Haifeng and Liu, Lin and Cao, Zhanqiang and Deng, Xuliang', 'ENTRYTYPE': 'inproceedings', 'ID': '10645866'}"
10536541,AIsop: Exploring Immersive VR Storytelling Leveraging Generative AI,"Gatti, Elia and Giunchi, Daniele and Numan, Nels and Steed, Anthony",Gatti,10.1109/VRW62533.2024.00229,2024,2024 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW),"We introduce Alsop, a system that autonomously generates VR sto-rytelling experiences using generative artificial intelligence (AI). Alsop crafts unique stories by leveraging state-of-the-art Large Lan-guage Models (LLMs) and employs Text-To-Speech (TTS) technology for narration. Further enriching the experience, a visual representation of the narrative is produced through a pipeline that pairs LLM-generated prompts with diffusion models, rendering visuals for clusters of sentences in the story. Our evaluation encompasses two distinct use cases: the narration of pre-existing content and the generation of entirely new narratives. Alsop highlights the myriad research prospects spanning its technical architecture and user engagement.",Visualization;Solid modeling;Three-dimensional displays;Generative AI;Conferences;Pipelines;Virtual reality;large language model;VR;storytelling;generative AI;Human-centered computing—Virtual reality Human—centered computing-Naturallanguage interfaces,"{'month': 'March', 'issn': '', 'doi': '10.1109/VRW62533.2024.00229', 'keywords': 'Visualization;Solid modeling;Three-dimensional displays;Generative AI;Conferences;Pipelines;Virtual reality;large language model;VR;storytelling;generative AI;Human-centered computing—Virtual reality Human—centered computing-Naturallanguage interfaces', 'abstract': 'We introduce Alsop, a system that autonomously generates VR sto-rytelling experiences using generative artificial intelligence (AI). Alsop crafts unique stories by leveraging state-of-the-art Large Lan-guage Models (LLMs) and employs Text-To-Speech (TTS) technology for narration. Further enriching the experience, a visual representation of the narrative is produced through a pipeline that pairs LLM-generated prompts with diffusion models, rendering visuals for clusters of sentences in the story. Our evaluation encompasses two distinct use cases: the narration of pre-existing content and the generation of entirely new narratives. Alsop highlights the myriad research prospects spanning its technical architecture and user engagement.', 'pages': '865-866', 'number': '', 'volume': '', 'year': '2024', 'title': 'AIsop: Exploring Immersive VR Storytelling Leveraging Generative AI', 'booktitle': '2024 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)', 'author': 'Gatti, Elia and Giunchi, Daniele and Numan, Nels and Steed, Anthony', 'ENTRYTYPE': 'inproceedings', 'ID': '10536541'}"
10707920,"An Educational Simulation Game Integrated with Generative AI as Conversational Contextual Scaffolding for Business English: A Preliminary Analysis of Learning Achievement, Self-Efficacy and Cognitive Load","Li, Cheng-Tai and Lee, Liang-Hsuan and Hou, Huei-Tse",Li,10.1109/IIAI-AAI63651.2024.00155,2024,2024 16th IIAI International Congress on Advanced Applied Informatics (IIAI-AAI),"The rise of generative artificial intelligence (GAI) has had a significant impact on the English as foreign language (EFL) teaching field. How EFL teachers can make good use of GAI to enhance their teaching and students' learning is an important issue. The research developed an educational simulation game integrated with GAI to provide conversational contextual scaffolding and immersive language context to help EFL students learn tourism English. Evaluation of the proposed game-based learning approach was conducted through action research. The results indicated that this approach significantly promoted learning achievement and self-efficacy, and did not add too much to the workload of students' learning.",Generative AI;Education;Games;Cognitive load;Informatics;Business;Generative AI;EFL;conversational contextual scaffolding;immersive context;tourism English,"{'month': 'July', 'issn': '2472-0070', 'doi': '10.1109/IIAI-AAI63651.2024.00155', 'keywords': 'Generative AI;Education;Games;Cognitive load;Informatics;Business;Generative AI;EFL;conversational contextual scaffolding;immersive context;tourism English', 'abstract': ""The rise of generative artificial intelligence (GAI) has had a significant impact on the English as foreign language (EFL) teaching field. How EFL teachers can make good use of GAI to enhance their teaching and students' learning is an important issue. The research developed an educational simulation game integrated with GAI to provide conversational contextual scaffolding and immersive language context to help EFL students learn tourism English. Evaluation of the proposed game-based learning approach was conducted through action research. The results indicated that this approach significantly promoted learning achievement and self-efficacy, and did not add too much to the workload of students' learning."", 'pages': '726-727', 'number': '', 'volume': '', 'year': '2024', 'title': 'An Educational Simulation Game Integrated with Generative AI as Conversational Contextual Scaffolding for Business English: A Preliminary Analysis of Learning Achievement, Self-Efficacy and Cognitive Load', 'booktitle': '2024 16th IIAI International Congress on Advanced Applied Informatics (IIAI-AAI)', 'author': 'Li, Cheng-Tai and Lee, Liang-Hsuan and Hou, Huei-Tse', 'ENTRYTYPE': 'inproceedings', 'ID': '10707920'}"
10972906,Generative AI for Personalized Multisensory Immersive Experiences: Challenges and Opportunities for Stress Reduction,"Lopes, Marilia K. S. and Falk, Tiago H.",Lopes,10.1109/VRW66409.2025.00036,2025,2025 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW),"Stress management and relaxation are critical areas of interest in mental health and well-being. Forest bathing is a practice that has been shown to have a positive effect on reducing stress by stimulating all the senses in an immersive nature experience. Since access to nature is not universally available to everyone, virtual reality has emerged as a promising tool to simulate this type of experience. Furthermore, generative artificial intelligence (GenAI) tools offer new opportunities to create highly personalized and immersive experiences that can enhance relaxation and reduce stress. This study explores the potential of personalized multisensory VR environments, designed using GenAI tools, to optimize relaxation and stress relief via two experiments that are currently underway. The first evaluates the effectiveness of non-personalized versus personalized VR scenes generated using AI tools to promote increased relaxation. The second explores the potential benefits of providing the user with additional personalization tools, from adding new virtual elements to the AI-generated scene, to adding AI-generated sounds and scent/haptics customization. Ultimately, this research aims to identify which customizable elements may lead to improved therapeutic benefits for multisensory VR experiences.",Three-dimensional displays;Generative AI;Conferences;Virtual reality;Forestry;Mental health;User interfaces;Generative AI;environment personalization;multi-sensory virtual reality;forest bathing;relaxation,"{'month': 'March', 'issn': '', 'doi': '10.1109/VRW66409.2025.00036', 'keywords': 'Three-dimensional displays;Generative AI;Conferences;Virtual reality;Forestry;Mental health;User interfaces;Generative AI;environment personalization;multi-sensory virtual reality;forest bathing;relaxation', 'abstract': 'Stress management and relaxation are critical areas of interest in mental health and well-being. Forest bathing is a practice that has been shown to have a positive effect on reducing stress by stimulating all the senses in an immersive nature experience. Since access to nature is not universally available to everyone, virtual reality has emerged as a promising tool to simulate this type of experience. Furthermore, generative artificial intelligence (GenAI) tools offer new opportunities to create highly personalized and immersive experiences that can enhance relaxation and reduce stress. This study explores the potential of personalized multisensory VR environments, designed using GenAI tools, to optimize relaxation and stress relief via two experiments that are currently underway. The first evaluates the effectiveness of non-personalized versus personalized VR scenes generated using AI tools to promote increased relaxation. The second explores the potential benefits of providing the user with additional personalization tools, from adding new virtual elements to the AI-generated scene, to adding AI-generated sounds and scent/haptics customization. Ultimately, this research aims to identify which customizable elements may lead to improved therapeutic benefits for multisensory VR experiences.', 'pages': '143-146', 'number': '', 'volume': '', 'year': '2025', 'title': 'Generative AI for Personalized Multisensory Immersive Experiences: Challenges and Opportunities for Stress Reduction', 'booktitle': '2025 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)', 'author': 'Lopes, Marilia K. S. and Falk, Tiago H.', 'ENTRYTYPE': 'inproceedings', 'ID': '10972906'}"
11092216,Research on the Application and Impact of Generative AI (AIGC) on College Students' Self-directed Learning,"Zhang, Yongfang and Gong, Xin and Wang, Xuan and Wang, Ziming and Zhou, Jiayu and Wang, Yuewen",Zhang,10.1109/CSTE64638.2025.11092216,2025,2025 7th International Conference on Computer Science and Technologies in Education (CSTE),"With the rapid development of artificial intelligence technology, the application of Generative AI (AIGC) in the field of education has become increasingly widespread. This study, through interviews, delves into the current status, attitudes, impacts, and existing problems of college students' use of AIGC tools in their self-directed learning. The findings indicate that AIGC tools have played a positive role in improving learning efficiency, stimulating learning inspiration, and assisting academic writing. However, there are also issues such as insufficient accuracy of information, limited content innovation, and the potential for increased dependency. This paper proposes corresponding solutions and offers prospects for the future application of AIGC in the field of education.",Computer science;Technological innovation;Data privacy;Accuracy;Generative AI;Data security;Education;Interviews;Protection;Creativity;Generative AI(AIGC);college students;self-directed learning;information accuracy,"{'month': 'April', 'issn': '', 'doi': '10.1109/CSTE64638.2025.11092216', 'keywords': 'Computer science;Technological innovation;Data privacy;Accuracy;Generative AI;Data security;Education;Interviews;Protection;Creativity;Generative AI(AIGC);college students;self-directed learning;information accuracy', 'abstract': ""With the rapid development of artificial intelligence technology, the application of Generative AI (AIGC) in the field of education has become increasingly widespread. This study, through interviews, delves into the current status, attitudes, impacts, and existing problems of college students' use of AIGC tools in their self-directed learning. The findings indicate that AIGC tools have played a positive role in improving learning efficiency, stimulating learning inspiration, and assisting academic writing. However, there are also issues such as insufficient accuracy of information, limited content innovation, and the potential for increased dependency. This paper proposes corresponding solutions and offers prospects for the future application of AIGC in the field of education."", 'pages': '557-560', 'number': '', 'volume': '', 'year': '2025', 'title': ""Research on the Application and Impact of Generative AI (AIGC) on College Students' Self-directed Learning"", 'booktitle': '2025 7th International Conference on Computer Science and Technologies in Education (CSTE)', 'author': 'Zhang, Yongfang and Gong, Xin and Wang, Xuan and Wang, Ziming and Zhou, Jiayu and Wang, Yuewen', 'ENTRYTYPE': 'inproceedings', 'ID': '11092216'}"
10811956,Agent-as-a-Service: An AI-Native Edge Computing Framework for 6G Networks,"Li, Borui and Liu, Tianen and Wang, Weilong and Zhao, Chengqing and Wang, Shuai",Li,10.1109/MNET.2024.3520987,2025,IEEE Network,"It has become a consensus that the integration of computing, sensing, and communication with ubiquitous intelligence will be the cornerstone of the sixth-generation (6G) network. The concept of edge computing and intelligence, which push the frontier of computation closer to the data source, is a suitable paradigm that aligns with these visions of 6G. In this article, we introduce Agent-as-a-Service (AaaS), an AI-native edge computing framework that leverages AI agents for both the control-plane operations and user-plane services of the 6G network. In the AaaS framework, agents can perform computing, sensing, and communicating tasks automatically by harnessing of the pervasive intelligence offered by 6G infrastructures. By AI-native, we refer to redesigning the whole lifecycle of an edge computing task to align with the prominent reasoning and planning ability of the generative AI. The lifecycle includes plan generation, execution orchestration, resource management, and long-term evolvement. The AaaS framework is built upon emerging techniques such as deviceless computing and WebAssembly to cope with the heterogeneous and geo-distributed 6G edge deployments. Based on the AaaS framework, we conduct a case study on autonomous driving in 6G edge computing to showcase the benefits in terms of overall latency reduction. Finally, we outline potential research directions to form a more efficient and integrated 6G edge computing with artificial intelligence.",6G mobile communication;Artificial intelligence;Edge computing;Planning;Robot sensing systems;Cognition;Autonomous vehicles;Generative AI;Computational modeling;Smart cities;Integrated sensing and communication;6G network;edge intelligence;AI agents;large language model,"{'month': 'March', 'issn': '1558-156X', 'doi': '10.1109/MNET.2024.3520987', 'keywords': '6G mobile communication;Artificial intelligence;Edge computing;Planning;Robot sensing systems;Cognition;Autonomous vehicles;Generative AI;Computational modeling;Smart cities;Integrated sensing and communication;6G network;edge intelligence;AI agents;large language model', 'abstract': 'It has become a consensus that the integration of computing, sensing, and communication with ubiquitous intelligence will be the cornerstone of the sixth-generation (6G) network. The concept of edge computing and intelligence, which push the frontier of computation closer to the data source, is a suitable paradigm that aligns with these visions of 6G. In this article, we introduce Agent-as-a-Service (AaaS), an AI-native edge computing framework that leverages AI agents for both the control-plane operations and user-plane services of the 6G network. In the AaaS framework, agents can perform computing, sensing, and communicating tasks automatically by harnessing of the pervasive intelligence offered by 6G infrastructures. By AI-native, we refer to redesigning the whole lifecycle of an edge computing task to align with the prominent reasoning and planning ability of the generative AI. The lifecycle includes plan generation, execution orchestration, resource management, and long-term evolvement. The AaaS framework is built upon emerging techniques such as deviceless computing and WebAssembly to cope with the heterogeneous and geo-distributed 6G edge deployments. Based on the AaaS framework, we conduct a case study on autonomous driving in 6G edge computing to showcase the benefits in terms of overall latency reduction. Finally, we outline potential research directions to form a more efficient and integrated 6G edge computing with artificial intelligence.', 'pages': '44-51', 'number': '2', 'volume': '39', 'year': '2025', 'title': 'Agent-as-a-Service: An AI-Native Edge Computing Framework for 6G Networks', 'journal': 'IEEE Network', 'author': 'Li, Borui and Liu, Tianen and Wang, Weilong and Zhao, Chengqing and Wang, Shuai', 'ENTRYTYPE': 'article', 'ID': '10811956'}"
9261711,Incremental Learning of Bearing Fault Diagnosis Via Style-Based Generative Adversarial Network,"Wang, Yinjun and Zeng, Liling and Ding, Xiaoxi and Wang, Liming and Shao, Yimin",Wang,10.1109/ICSMD50554.2020.9261711,2020,"2020 International Conference on Sensing, Measurement \& Data Analytics in the era of Artificial Intelligence (ICSMD)","At present, transfer learning of machine fault is a relatively popular research, its main problem is the imbalance of training data caused by the lack of actual fault data. The existing incremental learning model cannot solve the entanglement problem of sample features, and the ability to obtain new samples by combining features is limited. In this paper, Style-based Generative Adversarial Networks (StyleGAN) is used to map the data features to intermediate latent space, and then generate data by recombining features. StyleGAN realizes the complete separation of signal features. Therefore, StyleGAN can be used as a tool of data incremental learning to enrich the original data, solve the problem of imbalance between training data and test data, and achieve the goal of improving the accuracy of fault classification in the later stage. In the process of training, the category label is used as the auxiliary information to help the training model. The data of training set is enhanced, and the accuracy of fault diagnosis and classification is improved, the accuracy of fault classification network model is increased from 81.4\% to more than 90\%, so the validity of this method is proved.",Training;Wavelet transforms;Generators;Wavelet coefficients;Gray-scale;Data models;Convolution;Data imbalance;incremental learning;StyleGAN;transfer learning;bearing fault diagnosis,"{'month': 'Oct', 'issn': '', 'doi': '10.1109/ICSMD50554.2020.9261711', 'keywords': 'Training;Wavelet transforms;Generators;Wavelet coefficients;Gray-scale;Data models;Convolution;Data imbalance;incremental learning;StyleGAN;transfer learning;bearing fault diagnosis', 'abstract': 'At present, transfer learning of machine fault is a relatively popular research, its main problem is the imbalance of training data caused by the lack of actual fault data. The existing incremental learning model cannot solve the entanglement problem of sample features, and the ability to obtain new samples by combining features is limited. In this paper, Style-based Generative Adversarial Networks (StyleGAN) is used to map the data features to intermediate latent space, and then generate data by recombining features. StyleGAN realizes the complete separation of signal features. Therefore, StyleGAN can be used as a tool of data incremental learning to enrich the original data, solve the problem of imbalance between training data and test data, and achieve the goal of improving the accuracy of fault classification in the later stage. In the process of training, the category label is used as the auxiliary information to help the training model. The data of training set is enhanced, and the accuracy of fault diagnosis and classification is improved, the accuracy of fault classification network model is increased from 81.4\\% to more than 90\\%, so the validity of this method is proved.', 'pages': '512-517', 'number': '', 'volume': '', 'year': '2020', 'title': 'Incremental Learning of Bearing Fault Diagnosis Via Style-Based Generative Adversarial Network', 'booktitle': '2020 International Conference on Sensing, Measurement \\& Data Analytics in the era of Artificial Intelligence (ICSMD)', 'author': 'Wang, Yinjun and Zeng, Liling and Ding, Xiaoxi and Wang, Liming and Shao, Yimin', 'ENTRYTYPE': 'inproceedings', 'ID': '9261711'}"
10762540,Art in the Era of Algorithms: Is Generative AI a Friend or Foe for 2D Artists?,"Hartato, Frandi and Yahya, Jovan and Enrico Christiano Hartono, Liem and Sudiana",Hartato,10.1109/iSemantic63362.2024.10762540,2024,2024 International Seminar on Application for Technology of Information and Communication (iSemantic),"This research investigates the transformative impact of AI technologies such as DALL-E, ChatGPT, and other generative AI tools on the arts, particularly focusing on their implications for 2D artwork. These advancements have democratized the creation of 2D art, making it more accessible, but they have also raised concerns about the value of original talent and the potential impact on careers in the arts. The study employs the Technology Acceptance Model (TAM) to evaluate the acceptance and impact of generative AI among individuals in creative fields, including students, lecturers, and professionals. Data collected from 127 respondents through structured surveys indicate high levels of engagement with generative AI, reflecting significant curiosity and usage rates. Analysis using SmartPLS 4.0 Tools validated the initial research model, demonstrating that generative AI significantly influences perceived usefulness, perceived ease of use, and behavioral intention in 2D art creation. These findings underscore the critical need to consider AI's evolving role in the arts, its impact on perceptions of creativity and talent, and its broader implications for the creative industry. The study tested seven hypotheses, with six accepted and one rejected. This indicates that while factors like user computer self-efficacy and quality information significantly influence the perceived usefulness and ease of use of generative AI, ease of use alone does not necessarily lead to a positive attitude towards its use. However, perceived usefulness significantly contributes to a positive attitude towards using generative AI. Additionally, a positive attitude towards generative AI influences the behavioral intention to use it, thereby enhancing skills and providing inspiration for creating artwork.",Surveys;Seminars;Productivity;Industries;Java;Art;Technology acceptance model;Generative AI;Focusing;Chatbots;Artificial Intelligence;Generative AI;2D Artist;TAM Model;SEM-PLS,"{'month': 'Sep.', 'issn': '', 'doi': '10.1109/iSemantic63362.2024.10762540', 'keywords': 'Surveys;Seminars;Productivity;Industries;Java;Art;Technology acceptance model;Generative AI;Focusing;Chatbots;Artificial Intelligence;Generative AI;2D Artist;TAM Model;SEM-PLS', 'abstract': ""This research investigates the transformative impact of AI technologies such as DALL-E, ChatGPT, and other generative AI tools on the arts, particularly focusing on their implications for 2D artwork. These advancements have democratized the creation of 2D art, making it more accessible, but they have also raised concerns about the value of original talent and the potential impact on careers in the arts. The study employs the Technology Acceptance Model (TAM) to evaluate the acceptance and impact of generative AI among individuals in creative fields, including students, lecturers, and professionals. Data collected from 127 respondents through structured surveys indicate high levels of engagement with generative AI, reflecting significant curiosity and usage rates. Analysis using SmartPLS 4.0 Tools validated the initial research model, demonstrating that generative AI significantly influences perceived usefulness, perceived ease of use, and behavioral intention in 2D art creation. These findings underscore the critical need to consider AI's evolving role in the arts, its impact on perceptions of creativity and talent, and its broader implications for the creative industry. The study tested seven hypotheses, with six accepted and one rejected. This indicates that while factors like user computer self-efficacy and quality information significantly influence the perceived usefulness and ease of use of generative AI, ease of use alone does not necessarily lead to a positive attitude towards its use. However, perceived usefulness significantly contributes to a positive attitude towards using generative AI. Additionally, a positive attitude towards generative AI influences the behavioral intention to use it, thereby enhancing skills and providing inspiration for creating artwork."", 'pages': '190-195', 'number': '', 'volume': '', 'year': '2024', 'title': 'Art in the Era of Algorithms: Is Generative AI a Friend or Foe for 2D Artists?', 'booktitle': '2024 International Seminar on Application for Technology of Information and Communication (iSemantic)', 'author': 'Hartato, Frandi and Yahya, Jovan and Enrico Christiano Hartono, Liem and Sudiana', 'ENTRYTYPE': 'inproceedings', 'ID': '10762540'}"
10761527,Blockchain-based Edge Intelligence Enabled by AI Large Models for Future Internet of Things,"Zhang, Dajun and Shi, Wei",Zhang,10.1109/ICICN62625.2024.10761527,2024,"2024 IEEE 12th International Conference on Information, Communication and Networks (ICICN)","In recent years, the integration of Internet of Things (IoT) and Artificial Intelligence (AI) technologies has become a key factor in the development of modern communication systems. Through the deep integration of AI and IoT, the intelligence level of the network has been greatly improved, achieving higher data transmission speed, lower latency, and higher reliability to meet the growing communication needs. Especially in the context of intelligence networking and edge computing, large-scale language models (LLM) such as the Generative Pretrained Transformer (GPT) series have extended the capabilities to handle complex tasks and predict user intentions, programming, and planning. These capabilities provide new possibilities for optimizing communication systems, reducing semantic communication costs, and customizing services according to user preferences. This article explores the application of blockchain and AI large models in the edge intelligence of the Internet of Things, and proposes a distributed, non-tamperable knowledge and learning achievement recording system based on blockchain technology and AI large models. The system is designed to automatically generate code to train new models in a privacy-preserving manner. Experimental results show that the system can accurately understand user needs, efficiently execute, and create high-performance artificial intelligence models at minimal cost on edge servers.",Costs;Computational modeling;Data models;Blockchains;Recording;Internet of Things;Servers;Security;Artificial intelligence;Edge computing;Internet of Things;edge intelligence;AI large models;blockchain,"{'month': 'Aug', 'issn': '', 'doi': '10.1109/ICICN62625.2024.10761527', 'keywords': 'Costs;Computational modeling;Data models;Blockchains;Recording;Internet of Things;Servers;Security;Artificial intelligence;Edge computing;Internet of Things;edge intelligence;AI large models;blockchain', 'abstract': 'In recent years, the integration of Internet of Things (IoT) and Artificial Intelligence (AI) technologies has become a key factor in the development of modern communication systems. Through the deep integration of AI and IoT, the intelligence level of the network has been greatly improved, achieving higher data transmission speed, lower latency, and higher reliability to meet the growing communication needs. Especially in the context of intelligence networking and edge computing, large-scale language models (LLM) such as the Generative Pretrained Transformer (GPT) series have extended the capabilities to handle complex tasks and predict user intentions, programming, and planning. These capabilities provide new possibilities for optimizing communication systems, reducing semantic communication costs, and customizing services according to user preferences. This article explores the application of blockchain and AI large models in the edge intelligence of the Internet of Things, and proposes a distributed, non-tamperable knowledge and learning achievement recording system based on blockchain technology and AI large models. The system is designed to automatically generate code to train new models in a privacy-preserving manner. Experimental results show that the system can accurately understand user needs, efficiently execute, and create high-performance artificial intelligence models at minimal cost on edge servers.', 'pages': '368-374', 'number': '', 'volume': '', 'year': '2024', 'title': 'Blockchain-based Edge Intelligence Enabled by AI Large Models for Future Internet of Things', 'booktitle': '2024 IEEE 12th International Conference on Information, Communication and Networks (ICICN)', 'author': 'Zhang, Dajun and Shi, Wei', 'ENTRYTYPE': 'inproceedings', 'ID': '10761527'}"
9018815,Emerging Trends of ML-based Intelligent Services for Industrial Internet of Things (IIoT),"Chen, Baotong and Wan, Jiafu",Chen,10.1109/ComComAp46287.2019.9018815,2019,"2019 Computing, Communications and IoT Applications (ComComAp)","Intelligent information technology is a notable feature in the context of industry 4.0. A key factor in obtaining intelligent industrial Internet of things (IIoT) services is to integrate machine learning (ML) into IIoT. With the increasing scale of deployed terminals, IIoT becomes heterogeneous, diverse, and dynamically changeable. Traditional optimization methods are difficult to deal with the emerging network problems. This paper first proposes a ML-based IIoT architecture for intelligent IIoT services and expounds two ML methods for IIoT analysis, namely, deep learning (DL) and reinforcement learning (RL). Secondly, advanced applications and development trends of ML in industrial field are summarized. Opportunities and challenges of ML for IIoT analysis are discussed finally. The purpose of this paper is to point out the role of artificial intelligence (AI) technology in IIoT from the macroscopic view.",Machine learning;Internet of Things;Learning (artificial intelligence);Routing;Adaptation models;Computer architecture;Industry 4.0;Industrial Internet of things;Machine Learning;Deep Learning;Reinforcement Learning,"{'month': 'Oct', 'issn': '', 'doi': '10.1109/ComComAp46287.2019.9018815', 'keywords': 'Machine learning;Internet of Things;Learning (artificial intelligence);Routing;Adaptation models;Computer architecture;Industry 4.0;Industrial Internet of things;Machine Learning;Deep Learning;Reinforcement Learning', 'abstract': 'Intelligent information technology is a notable feature in the context of industry 4.0. A key factor in obtaining intelligent industrial Internet of things (IIoT) services is to integrate machine learning (ML) into IIoT. With the increasing scale of deployed terminals, IIoT becomes heterogeneous, diverse, and dynamically changeable. Traditional optimization methods are difficult to deal with the emerging network problems. This paper first proposes a ML-based IIoT architecture for intelligent IIoT services and expounds two ML methods for IIoT analysis, namely, deep learning (DL) and reinforcement learning (RL). Secondly, advanced applications and development trends of ML in industrial field are summarized. Opportunities and challenges of ML for IIoT analysis are discussed finally. The purpose of this paper is to point out the role of artificial intelligence (AI) technology in IIoT from the macroscopic view.', 'pages': '135-139', 'number': '', 'volume': '', 'year': '2019', 'title': 'Emerging Trends of ML-based Intelligent Services for Industrial Internet of Things (IIoT)', 'booktitle': '2019 Computing, Communications and IoT Applications (ComComAp)', 'author': 'Chen, Baotong and Wan, Jiafu', 'ENTRYTYPE': 'inproceedings', 'ID': '9018815'}"
10820388,Deep Learning and Machine Learning for Materials Design,"Mudabbirudin, Mohammed and Takacs, Judit and Mosavi, Amir and Imre, Felde and Nabipour, Narjes",Mudabbirudin,10.1109/LINDI63813.2024.10820388,2024,2024 IEEE 6th International Symposium on Logistics and Industrial Informatics (LINDI),"The current materials design methodologies depend on AI-driven approaches to accelerate the development of materials. Deep learning (DL) and Machine Learning (ML) play essential roles in advancing materials design by automating analysis tasks and uncovering complex relationships within large datasets. These computational methods offer essential tools for materials innovation. This article investigates the impact of DL and ML on materials design innovation through proposing two taxonomies, i.e., one which focuses on the methods and the other on applications. The method-based taxonomy categorizes the different approaches used in DL and ML for materials design, while the application-based taxonomy presents the real-world scenarios.",Deep learning;Materials science and technology;Technological innovation;Reviews;Taxonomy;Learning (artificial intelligence);Mathematical models;Reliability;Informatics;Logistics;material science;materials design;artificial intelligence;ML;DL;applied mathematics;applied informatics;XAI;big data;data science;soft computing;computer science;applied ML;review;survey;generative AI;mathematics;mechatronics;model;information systems;data mining;generative artificial intelligence;sustainable development;SDGs;information fusion;data fusion;deep learning;machine learning,"{'month': 'Oct', 'issn': '2156-8804', 'doi': '10.1109/LINDI63813.2024.10820388', 'keywords': 'Deep learning;Materials science and technology;Technological innovation;Reviews;Taxonomy;Learning (artificial intelligence);Mathematical models;Reliability;Informatics;Logistics;material science;materials design;artificial intelligence;ML;DL;applied mathematics;applied informatics;XAI;big data;data science;soft computing;computer science;applied ML;review;survey;generative AI;mathematics;mechatronics;model;information systems;data mining;generative artificial intelligence;sustainable development;SDGs;information fusion;data fusion;deep learning;machine learning', 'abstract': 'The current materials design methodologies depend on AI-driven approaches to accelerate the development of materials. Deep learning (DL) and Machine Learning (ML) play essential roles in advancing materials design by automating analysis tasks and uncovering complex relationships within large datasets. These computational methods offer essential tools for materials innovation. This article investigates the impact of DL and ML on materials design innovation through proposing two taxonomies, i.e., one which focuses on the methods and the other on applications. The method-based taxonomy categorizes the different approaches used in DL and ML for materials design, while the application-based taxonomy presents the real-world scenarios.', 'pages': '73-82', 'number': '', 'volume': '', 'year': '2024', 'title': 'Deep Learning and Machine Learning for Materials Design', 'booktitle': '2024 IEEE 6th International Symposium on Logistics and Industrial Informatics (LINDI)', 'author': 'Mudabbirudin, Mohammed and Takacs, Judit and Mosavi, Amir and Imre, Felde and Nabipour, Narjes', 'ENTRYTYPE': 'inproceedings', 'ID': '10820388'}"
10664756,"Impact of Generative AI Adoption in Academia and How it Influences Ethics, Cognitive Thinking and AI Singularity","Marimekala, Daniel and Lamb, John",Marimekala,10.1109/ISEC61299.2024.10664756,2024,2024 IEEE Integrated STEM Education Conference (ISEC),"Our Gen Alpha, born between year 2010 and 2024 see and experience more of AI and they adapt quickly to AI and will have less impact on advancements in Technology when compared to Generation X (19651980); Generation Y (Millennials) born from 1980 to 1994; Generation Z from 1995 to 2009. The reason is that Gen Alpha constantly uses electronic gadgets either in gaming, learning or for social media. They are susceptible quickly to Generative AI when compared to Gen X, Gen Y or Gen Z. Especially in Academia, where Gen Alpha is more encouraged to Generative AI such as ChatGPT in homework, assignments, and in research. Well, this is a good approach for those who are struggling to complete their work or for those who do not have a clue on how to complete their homework, assignment, or research. On other hand, the Generative AI tools such as ChatGPT are slowly becoming a part of the eco system that students lean on the Generative AI tools instead of doing research or thinking critically. As a result, there will be some behavioral changes that will be developed over a period. These behavioral changes are, impatient for answers to the problems, express panic while solving a problem, shows anxiety, low self-esteem in solving a problem, and low confidence factor. There is always a positive side of Generative AI, if we look at it from a different angle. Some of them are, it helps an individual and guides them with possible answers, all individuals can ask questions using prompt engineering and get responses. But the fact of the matter is how authentic the response from Generative AI is a question? Can the author quote the responses he/she received from ChatGPT and How can we avoid plagiarism? How can we reduce bias? How can we avoid AI singularity?",Technological innovation;Generative AI;Social networking (online);Shape;Plagiarism;Writing;Chatbots;Generative AI;ChatGPT;Guardrails;Singularity;AI Models;Large Language Models;ML,"{'month': 'March', 'issn': '2473-7623', 'doi': '10.1109/ISEC61299.2024.10664756', 'keywords': 'Technological innovation;Generative AI;Social networking (online);Shape;Plagiarism;Writing;Chatbots;Generative AI;ChatGPT;Guardrails;Singularity;AI Models;Large Language Models;ML', 'abstract': 'Our Gen Alpha, born between year 2010 and 2024 see and experience more of AI and they adapt quickly to AI and will have less impact on advancements in Technology when compared to Generation X (19651980); Generation Y (Millennials) born from 1980 to 1994; Generation Z from 1995 to 2009. The reason is that Gen Alpha constantly uses electronic gadgets either in gaming, learning or for social media. They are susceptible quickly to Generative AI when compared to Gen X, Gen Y or Gen Z. Especially in Academia, where Gen Alpha is more encouraged to Generative AI such as ChatGPT in homework, assignments, and in research. Well, this is a good approach for those who are struggling to complete their work or for those who do not have a clue on how to complete their homework, assignment, or research. On other hand, the Generative AI tools such as ChatGPT are slowly becoming a part of the eco system that students lean on the Generative AI tools instead of doing research or thinking critically. As a result, there will be some behavioral changes that will be developed over a period. These behavioral changes are, impatient for answers to the problems, express panic while solving a problem, shows anxiety, low self-esteem in solving a problem, and low confidence factor. There is always a positive side of Generative AI, if we look at it from a different angle. Some of them are, it helps an individual and guides them with possible answers, all individuals can ask questions using prompt engineering and get responses. But the fact of the matter is how authentic the response from Generative AI is a question? Can the author quote the responses he/she received from ChatGPT and How can we avoid plagiarism? How can we reduce bias? How can we avoid AI singularity?', 'pages': '1-4', 'number': '', 'volume': '', 'year': '2024', 'title': 'Impact of Generative AI Adoption in Academia and How it Influences Ethics, Cognitive Thinking and AI Singularity', 'booktitle': '2024 IEEE Integrated STEM Education Conference (ISEC)', 'author': 'Marimekala, Daniel and Lamb, John', 'ENTRYTYPE': 'inproceedings', 'ID': '10664756'}"
10880522,Tackling the Complexities of Federated Learning,"Thakur, Raj and Patel, Shreyansh and Singh, Neelesh and Barde, Aaryan and Barde, Snehlata",Thakur,10.1002/9781394280735.ch17,2025,Generative Artificial Intelligence for Biomedical and Smart Health Informatics,"Summary <p>Federated learning is an innovative machine learning approach enabling multiple devices to collaboratively train a model without sharing sensitive data, overseen by a central server. This chapter explores the strengths and vulnerabilities of federated learning, emphasizing its secure nature while acknowledging potential risks. It discusses key challenges like communication overhead and data heterogeneity, along with techniques to address them. The chapter also provides an overview of current federated learning methods, highlighting their efficacy and areas for improvement. Future directions are outlined, including enhancing robustness against attacks and scaling for larger systems, making this chapter a valuable resource for researchers and practitioners in the field of privacy\&\#x2010;preserving machine learning.</p>",Training;Data models;Servers;Brain modeling;Atmospheric modeling;Security;Predictive models;Monitoring;Inspection;Distributed databases,"{'url': 'https://ieeexplore-ieee-org.crai.referencistas.com/document/10880522', 'isbn': '9781394280728', 'publisher': 'IEEE', 'issn': '', 'doi': '10.1002/9781394280735.ch17', 'keywords': 'Training;Data models;Servers;Brain modeling;Atmospheric modeling;Security;Predictive models;Monitoring;Inspection;Distributed databases', 'abstract': 'Summary <p>Federated learning is an innovative machine learning approach enabling multiple devices to collaboratively train a model without sharing sensitive data, overseen by a central server. This chapter explores the strengths and vulnerabilities of federated learning, emphasizing its secure nature while acknowledging potential risks. It discusses key challenges like communication overhead and data heterogeneity, along with techniques to address them. The chapter also provides an overview of current federated learning methods, highlighting their efficacy and areas for improvement. Future directions are outlined, including enhancing robustness against attacks and scaling for larger systems, making this chapter a valuable resource for researchers and practitioners in the field of privacy\\&\\#x2010;preserving machine learning.</p>', 'pages': '343-353', 'number': '', 'volume': '', 'year': '2025', 'title': 'Tackling the Complexities of Federated Learning', 'booktitle': 'Generative Artificial Intelligence for Biomedical and Smart Health Informatics', 'author': 'Thakur, Raj and Patel, Shreyansh and Singh, Neelesh and Barde, Aaryan and Barde, Snehlata', 'ENTRYTYPE': 'inbook', 'ID': '10880522'}"
11159054,Impact Analysis of Generative AI on the Accuracy and Scalability of Machine Learning Models,"Boljam, Ajay Mysore and Ul Hassan Mohammed, Hameed and Alluri, Venkat Rama Raju and Saini, Vipin and Bojja, Sai Ganesh Reddy",Boljam,10.1109/ICCTDC64446.2025.11159054,2025,2025 International Conference on Computing Technologies \& Data Communication (ICCTDC),"Generative AI has become a fundamental tool in machine learning which address the issues of data scarcity, model generalization, providing additional model efficacy and allowing them to scale horizontally in nature. In this paper, presents the effect of generative AI on the performance of machine learning models with respect to improved accuracy and enhanced scalability. Advanced generative AI methods including GANs and VAEs are examined for data augmentation, model training, and other robustness improvements. The research shows that integrating generative AI with machine learning pipelines can dramatically boost the performance of machine learning models, especially in data-scarce settings. This enhances the scalability of the models, allowing them to perform better on larger and more complex datasets. These results indicate the potential significance of generative AI in paving the way for better application of machine learning, enabling a streamlined model while being generalizable to a wider scope of real-world systems.",Training;Analytical models;Accuracy;Generative AI;Scalability;Pipelines;Machine learning;Data augmentation;Data models;Robustness;Generative AI;Machine Learning Models;Accuracy;Scalability;Neural Networks;Model Efficiency;Data Augmentation;Deep Learning;Model Robustness,"{'month': 'July', 'issn': '', 'doi': '10.1109/ICCTDC64446.2025.11159054', 'keywords': 'Training;Analytical models;Accuracy;Generative AI;Scalability;Pipelines;Machine learning;Data augmentation;Data models;Robustness;Generative AI;Machine Learning Models;Accuracy;Scalability;Neural Networks;Model Efficiency;Data Augmentation;Deep Learning;Model Robustness', 'abstract': 'Generative AI has become a fundamental tool in machine learning which address the issues of data scarcity, model generalization, providing additional model efficacy and allowing them to scale horizontally in nature. In this paper, presents the effect of generative AI on the performance of machine learning models with respect to improved accuracy and enhanced scalability. Advanced generative AI methods including GANs and VAEs are examined for data augmentation, model training, and other robustness improvements. The research shows that integrating generative AI with machine learning pipelines can dramatically boost the performance of machine learning models, especially in data-scarce settings. This enhances the scalability of the models, allowing them to perform better on larger and more complex datasets. These results indicate the potential significance of generative AI in paving the way for better application of machine learning, enabling a streamlined model while being generalizable to a wider scope of real-world systems.', 'pages': '1-7', 'number': '', 'volume': '', 'year': '2025', 'title': 'Impact Analysis of Generative AI on the Accuracy and Scalability of Machine Learning Models', 'booktitle': '2025 International Conference on Computing Technologies \\& Data Communication (ICCTDC)', 'author': 'Boljam, Ajay Mysore and Ul Hassan Mohammed, Hameed and Alluri, Venkat Rama Raju and Saini, Vipin and Bojja, Sai Ganesh Reddy', 'ENTRYTYPE': 'inproceedings', 'ID': '11159054'}"
10500778,Improving the Flexural Behaviour of Small Clear 3D-Printed PLA Specimens Through Generative Design,"Ramful, Raviduth and Shoaib Casseem, Mohammad",Ramful,10.1109/i-COSTE60462.2023.10500778,2023,2023 International Conference on Sustainable Technology and Engineering (i-COSTE),"The interest to look into alternative engineered materials, which would maximize the efficiency and performance attributes in numerous modern engineering application as well as to push the boundary imposed by present-day material science in the field engineering, is ever so high. One commonly searched feature in engineered materials is their high strength-to-weight ratio. The new AI (Artificial Intelligence) -driven technique of generative design has revolutionized the method of design refinement in Computer-Aided Design (CAD) by enabling advanced optimization techniques which was formerly-unavailable. This study seeks to apply the technique of generative design to improve the flexural behaviour of small clear Polylactic acid (PLA) specimens produced through the additive manufacturing process. The results obtained have shown that despite the optimized material geometry and weight, the refined model was still able to bear significantly high bending loads. Further analysis via the Finite Element Method (FEM) was conducted to substantiate the experimental investigation. Findings of this study illustrates the benefits of generative design which can seamlessly provide advanced design refinements all while preserving or enhancing mechanical performance of the material.",Geometry;Solid modeling;Design automation;Programmable logic arrays;Bending;Three-dimensional printing;Finite element analysis;Generative design;3D-Printing;flexural behaviour;additive manufacturing;design optimization;Finite Element Method (FEM),"{'month': 'Dec', 'issn': '', 'doi': '10.1109/i-COSTE60462.2023.10500778', 'keywords': 'Geometry;Solid modeling;Design automation;Programmable logic arrays;Bending;Three-dimensional printing;Finite element analysis;Generative design;3D-Printing;flexural behaviour;additive manufacturing;design optimization;Finite Element Method (FEM)', 'abstract': 'The interest to look into alternative engineered materials, which would maximize the efficiency and performance attributes in numerous modern engineering application as well as to push the boundary imposed by present-day material science in the field engineering, is ever so high. One commonly searched feature in engineered materials is their high strength-to-weight ratio. The new AI (Artificial Intelligence) -driven technique of generative design has revolutionized the method of design refinement in Computer-Aided Design (CAD) by enabling advanced optimization techniques which was formerly-unavailable. This study seeks to apply the technique of generative design to improve the flexural behaviour of small clear Polylactic acid (PLA) specimens produced through the additive manufacturing process. The results obtained have shown that despite the optimized material geometry and weight, the refined model was still able to bear significantly high bending loads. Further analysis via the Finite Element Method (FEM) was conducted to substantiate the experimental investigation. Findings of this study illustrates the benefits of generative design which can seamlessly provide advanced design refinements all while preserving or enhancing mechanical performance of the material.', 'pages': '1-5', 'number': '', 'volume': '', 'year': '2023', 'title': 'Improving the Flexural Behaviour of Small Clear 3D-Printed PLA Specimens Through Generative Design', 'booktitle': '2023 International Conference on Sustainable Technology and Engineering (i-COSTE)', 'author': 'Ramful, Raviduth and Shoaib Casseem, Mohammad', 'ENTRYTYPE': 'inproceedings', 'ID': '10500778'}"
10973296,"Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study","Liu, Yinqiu and Du, Hongyang and Niyato, Dusit and Kang, Jiawen and Xiong, Zehui and Wen, Yonggang and Kim, Dong In",Liu,10.1109/MNET.2025.3563262,2025,IEEE Network,"Generative AI (GenAI), exemplified by Large Language Models (LLMs), such as OpenAI’s ChatGPT, is revolutionizing various fields. Central to this transformation is Data Center Networking (DCN), which not only provides the infrastructure support for GenAI operation, but also provisions GenAI services to users. Hence, this article explores the interplay between GenAI and DCNs, analyzing their symbiotic relationship and mutual advances. We begin by reviewing the current challenges of DCNs and GenAI-based solutions, such as data augmentation, process automation, and domain transfer. We then discuss the distinctive characteristics of GenAI workloads on DCNs, gaining insights that catalyze the evolution of DCNs to more effectively support GenAI. Moreover, to illustrate the seamless integration of GenAI with DCNs, we present a case study on GenAI-empowered DCN digital twins. Specifically, we employ an LLM equipped with retrieval augmented generation to formulate optimization problems for DCNs (e.g., resource allocation and routing) and adopt diffusion-deep reinforcement learning to solve optimization. The experimental results on a representative DCN optimization problem, i.e., knowledge placement, demonstrate the validity and efficiency of our proposals. We anticipate that this article can promote further research to enhance the virtuous interaction between GenAI and DCNs.",Artificial intelligence;Training;Servers;Data centers;Optimization;Reviews;Load management;Data models;Topology;Routing;Generative artificial intelligence;large language model;data center networking;sustainability,"{'month': '', 'issn': '1558-156X', 'doi': '10.1109/MNET.2025.3563262', 'keywords': 'Artificial intelligence;Training;Servers;Data centers;Optimization;Reviews;Load management;Data models;Topology;Routing;Generative artificial intelligence;large language model;data center networking;sustainability', 'abstract': 'Generative AI (GenAI), exemplified by Large Language Models (LLMs), such as OpenAI’s ChatGPT, is revolutionizing various fields. Central to this transformation is Data Center Networking (DCN), which not only provides the infrastructure support for GenAI operation, but also provisions GenAI services to users. Hence, this article explores the interplay between GenAI and DCNs, analyzing their symbiotic relationship and mutual advances. We begin by reviewing the current challenges of DCNs and GenAI-based solutions, such as data augmentation, process automation, and domain transfer. We then discuss the distinctive characteristics of GenAI workloads on DCNs, gaining insights that catalyze the evolution of DCNs to more effectively support GenAI. Moreover, to illustrate the seamless integration of GenAI with DCNs, we present a case study on GenAI-empowered DCN digital twins. Specifically, we employ an LLM equipped with retrieval augmented generation to formulate optimization problems for DCNs (e.g., resource allocation and routing) and adopt diffusion-deep reinforcement learning to solve optimization. The experimental results on a representative DCN optimization problem, i.e., knowledge placement, demonstrate the validity and efficiency of our proposals. We anticipate that this article can promote further research to enhance the virtuous interaction between GenAI and DCNs.', 'pages': '1-1', 'number': '', 'volume': '', 'year': '2025', 'title': 'Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study', 'journal': 'IEEE Network', 'author': 'Liu, Yinqiu and Du, Hongyang and Niyato, Dusit and Kang, Jiawen and Xiong, Zehui and Wen, Yonggang and Kim, Dong In', 'ENTRYTYPE': 'article', 'ID': '10973296'}"
10915950,The application of generative AI in the creation of picture books for young children in early childhood education undergraduate course,"Hsu, Chia-fang and Kuo, Liang-Yin",Hsu,10.1049/icp.2025.0213,2024,"International Conference on Innovation, Communication and Engineering 2024 (ICICE 2024)","Artificial Intelligence (AI) technologies are emerging as a critical technological capability of the 21st century and have been spreading their influence in various fields of education, including early childhood education (ECE) in recent years. When properly used and thoughtfully integrated into teaching, AI can be a powerful and effective tool in the classroom, not only to provide inspiration, generate ideas and encourage creativity for both children and teachers, but most importantly to prepare them for a future where technology plays an increasingly important role in schooling. For early childhood educators, AI can also be a tremendous assistant as an administrative helper, idea generator or data organizer, helping teachers to think outside the box and prepare for teaching more creatively and effectively. It appears that there is a growing need for early childhood educators to have not only pedagogical expertise but also a strong foundation in AI literacy, and that pre-service teachers need to be equipped with the ability to use AI appropriately in their formative or undergraduate stages. Therefore, this study aims to explore the ability of early childhood education students to use AI in the creation of picture books for young children, and seeks to provide insights into the potential strengths, weaknesses and ethical issues of integrating AI into early childhood education undergraduate course.",,"{'month': 'Nov', 'issn': '', 'doi': '10.1049/icp.2025.0213', 'keywords': '', 'abstract': 'Artificial Intelligence (AI) technologies are emerging as a critical technological capability of the 21st century and have been spreading their influence in various fields of education, including early childhood education (ECE) in recent years. When properly used and thoughtfully integrated into teaching, AI can be a powerful and effective tool in the classroom, not only to provide inspiration, generate ideas and encourage creativity for both children and teachers, but most importantly to prepare them for a future where technology plays an increasingly important role in schooling. For early childhood educators, AI can also be a tremendous assistant as an administrative helper, idea generator or data organizer, helping teachers to think outside the box and prepare for teaching more creatively and effectively. It appears that there is a growing need for early childhood educators to have not only pedagogical expertise but also a strong foundation in AI literacy, and that pre-service teachers need to be equipped with the ability to use AI appropriately in their formative or undergraduate stages. Therefore, this study aims to explore the ability of early childhood education students to use AI in the creation of picture books for young children, and seeks to provide insights into the potential strengths, weaknesses and ethical issues of integrating AI into early childhood education undergraduate course.', 'pages': '128-131', 'number': '', 'volume': '2024', 'year': '2024', 'title': 'The application of generative AI in the creation of picture books for young children in early childhood education undergraduate course', 'booktitle': 'International Conference on Innovation, Communication and Engineering 2024 (ICICE 2024)', 'author': 'Hsu, Chia-fang and Kuo, Liang-Yin', 'ENTRYTYPE': 'inproceedings', 'ID': '10915950'}"
10575055,Implementation of Cycle Consistent GANs for Map to Image Translation using Deep Learning Techniques,"Tejaswi, Majeti and Tapan, Kandikonda and Sathvik, Kunapareddi and Valiveti, Hima Bindu",Tejaswi,10.1109/ICAAIC60222.2024.10575055,2024,2024 3rd International Conference on Applied Artificial Intelligence and Computing (ICAAIC),"With the rapid urbanization and population growth, locating suitable geographical areas to accommodate these drastic changes is paramount. This study explores the use of Generative Adversarial Networks (GANs), specifically Cycle- Consistent Generative Adversarial Networks (Cycle GAN) and Conditional GAN(C-GAN), to generate a map image from a satellite image, that is easy to read and understand. By conditioning the generator and discriminator networks on the input satellite images and target map domain, the model picks up on certain distinguishable features from the satellite image and the map image to generate a map image. A Cycle-GAN doesn’t require the use of paired datasets, unlike C-GAN. This research study also briefly discusses about the various features that the Cycle-GAN applies to generate an accurate image. Various loss functions such as GAN loss, identity loss, and Cycle consistency loss that the GAN employs to improve the resolution of the image are also discussed in the study.",Accuracy;Image resolution;Navigation;Urban areas;Sociology;Generative adversarial networks;Satellite images;Generative Adversarial Network;Cycle-Consistent Adversarial Network;GAN loss;Identity Loss;Cycle Consistency Loss,"{'month': 'June', 'issn': '', 'doi': '10.1109/ICAAIC60222.2024.10575055', 'keywords': 'Accuracy;Image resolution;Navigation;Urban areas;Sociology;Generative adversarial networks;Satellite images;Generative Adversarial Network;Cycle-Consistent Adversarial Network;GAN loss;Identity Loss;Cycle Consistency Loss', 'abstract': 'With the rapid urbanization and population growth, locating suitable geographical areas to accommodate these drastic changes is paramount. This study explores the use of Generative Adversarial Networks (GANs), specifically Cycle- Consistent Generative Adversarial Networks (Cycle GAN) and Conditional GAN(C-GAN), to generate a map image from a satellite image, that is easy to read and understand. By conditioning the generator and discriminator networks on the input satellite images and target map domain, the model picks up on certain distinguishable features from the satellite image and the map image to generate a map image. A Cycle-GAN doesn’t require the use of paired datasets, unlike C-GAN. This research study also briefly discusses about the various features that the Cycle-GAN applies to generate an accurate image. Various loss functions such as GAN loss, identity loss, and Cycle consistency loss that the GAN employs to improve the resolution of the image are also discussed in the study.', 'pages': '393-398', 'number': '', 'volume': '', 'year': '2024', 'title': 'Implementation of Cycle Consistent GANs for Map to Image Translation using Deep Learning Techniques', 'booktitle': '2024 3rd International Conference on Applied Artificial Intelligence and Computing (ICAAIC)', 'author': 'Tejaswi, Majeti and Tapan, Kandikonda and Sathvik, Kunapareddi and Valiveti, Hima Bindu', 'ENTRYTYPE': 'inproceedings', 'ID': '10575055'}"
9996040,Determine the optimal Hidden Layers and Neurons in the Generative Adversarial Networks topology for the Intrusion Detection Systems,"Lamjid, Ali and Ariffin, Khairul Akram Zainol and Aziz, Mohd Juzaiddin AB and Sani, Nor Samsiah",Lamjid,10.1109/ICCR56254.2022.9996040,2022,2022 International Conference on Cyber Resilience (ICCR),"Neural Networks (NNs) are a vast field of research, particularly in terms of adjusting hyper-parameters such as hidden layers (HLs) and hidden neurons (HNs). On the other hand, (NN)s presents the main component of GAN's two elements: (1) Generator (G) and (2) Discriminator (D). It improves GAN as NN optimizes its parts. The number of Hidden Layers (HLs) and Hidden Neurons (HNs) inside the neural networks remains an important research topic despite numerous rules trying to predict the values by some directives and calculations. However, the study of the deepness and wideness of both (G) and (D) is still under-explored, and exploring this topic in GAN was neglected. Hence, this research focuses on the (NN)s contained in the original architecture of GAN and targets to clarify the relation between the number of (HL)s and (HN)s in the Generator (G) and the Discriminator (D) inside GAN. Furthermore, the goal is to determine which (NN) should have the most significant number of (HL) and (HN) in order to improve the performance of the created samples. Therefore, experiments were carried out by (1) increasing the number of (HL)s equally or differently in the G and the D and (2) increasing the number of (HN)s in each (HL) either on G or D. Deep Neural Network (DNN) is chosen as an intrusion detection system (IDS) model and the benchmark dataset KDD99 was used for application. It has been observed that a large number of (HL)s and (HN)s in the (G) improved GAN training and convergence. It means that the (G) should have more critical (HL)s and (HN)s than the (D) to generate better samples. Thus, this study provides the researchers with a vital resource to optimize the GAN's (NN)s and to process the experiments in this field by standardizing the number of (HL)s and (HN)s. The experiments carried out revealed that better training and convergence are seen when the number of (HL)s is 10 and the number of (HN)s is 1024 in the (G). The highest accuracy completed is 0.9991 which is 10 (HL)s and 1024 (HN)s in the Generator, and 2 (HL)s and 64 (HN)s in the Discriminator.",Training;Neurons;Intrusion detection;Artificial neural networks;Generative adversarial networks;Generators;Biological neural networks;Hidden Layers;Hidden neurons;Generative Adversarial Networks;Intrusion Detection System,"{'month': 'Oct', 'issn': '', 'doi': '10.1109/ICCR56254.2022.9996040', 'keywords': 'Training;Neurons;Intrusion detection;Artificial neural networks;Generative adversarial networks;Generators;Biological neural networks;Hidden Layers;Hidden neurons;Generative Adversarial Networks;Intrusion Detection System', 'abstract': ""Neural Networks (NNs) are a vast field of research, particularly in terms of adjusting hyper-parameters such as hidden layers (HLs) and hidden neurons (HNs). On the other hand, (NN)s presents the main component of GAN's two elements: (1) Generator (G) and (2) Discriminator (D). It improves GAN as NN optimizes its parts. The number of Hidden Layers (HLs) and Hidden Neurons (HNs) inside the neural networks remains an important research topic despite numerous rules trying to predict the values by some directives and calculations. However, the study of the deepness and wideness of both (G) and (D) is still under-explored, and exploring this topic in GAN was neglected. Hence, this research focuses on the (NN)s contained in the original architecture of GAN and targets to clarify the relation between the number of (HL)s and (HN)s in the Generator (G) and the Discriminator (D) inside GAN. Furthermore, the goal is to determine which (NN) should have the most significant number of (HL) and (HN) in order to improve the performance of the created samples. Therefore, experiments were carried out by (1) increasing the number of (HL)s equally or differently in the G and the D and (2) increasing the number of (HN)s in each (HL) either on G or D. Deep Neural Network (DNN) is chosen as an intrusion detection system (IDS) model and the benchmark dataset KDD99 was used for application. It has been observed that a large number of (HL)s and (HN)s in the (G) improved GAN training and convergence. It means that the (G) should have more critical (HL)s and (HN)s than the (D) to generate better samples. Thus, this study provides the researchers with a vital resource to optimize the GAN's (NN)s and to process the experiments in this field by standardizing the number of (HL)s and (HN)s. The experiments carried out revealed that better training and convergence are seen when the number of (HL)s is 10 and the number of (HN)s is 1024 in the (G). The highest accuracy completed is 0.9991 which is 10 (HL)s and 1024 (HN)s in the Generator, and 2 (HL)s and 64 (HN)s in the Discriminator."", 'pages': '1-7', 'number': '', 'volume': '', 'year': '2022', 'title': 'Determine the optimal Hidden Layers and Neurons in the Generative Adversarial Networks topology for the Intrusion Detection Systems', 'booktitle': '2022 International Conference on Cyber Resilience (ICCR)', 'author': 'Lamjid, Ali and Ariffin, Khairul Akram Zainol and Aziz, Mohd Juzaiddin AB and Sani, Nor Samsiah', 'ENTRYTYPE': 'inproceedings', 'ID': '9996040'}"
9236637,Controllable Multi-Attribute Editing of High-Resolution Face Images,"Deng, Qiyao and Li, Qi and Cao, Jie and Liu, Yunfan and Sun, Zhenan",Deng,10.1109/TIFS.2020.3033184,2021,IEEE Transactions on Information Forensics and Security,"In recent years, significant progress has been achieved in face image editing due to the success of Generative Adversarial Network (GAN). However, state-of-the-art face editing methods mainly suffer from the following two limitations: 1) they are only applicable to face images with relative low-resolutions and 2) multi-attribute face editing may generate uncontrollable changes in non-target face attribute categories. To solve these problems, we propose a novel High-Quality Generative Adversarial Network (HQ-GAN) for controllable editing of multiple face attributes in high-resolution images. HQ-GAN has two novel ideas to break the limitations of resolution and controllability correspondingly: 1) fine-grained textures and realistic details of high-resolution face images are better preserved with the aid of textural features extracted by the wavelet transform module and 2) desired multi-attribute targets of face editing are emphasized using a weighted binary cross-entropy (BCE) loss so that the influence on non-target attributes is greatly reduced. To the best of our knowledge, HQ-GAN is the first attempt to achieve continuous editing of multiple face attributes on high-resolution images of the CelebA-HQ using only 28 000 training samples. Extensive qualitative results demonstrate the superiority of the proposed method in rendering realistic high-resolution face images with accurate attribute modification, and comprehensive quantitative results show that the proposed method significantly outperforms state-of-the-art face editing methods.",Faces;Image resolution;Face recognition;Wavelet transforms;Generative adversarial networks;Gallium nitride;Computational modeling;Face attribute editing;face synthesis;generative adversarial network,"{'month': '', 'issn': '1556-6021', 'doi': '10.1109/TIFS.2020.3033184', 'keywords': 'Faces;Image resolution;Face recognition;Wavelet transforms;Generative adversarial networks;Gallium nitride;Computational modeling;Face attribute editing;face synthesis;generative adversarial network', 'abstract': 'In recent years, significant progress has been achieved in face image editing due to the success of Generative Adversarial Network (GAN). However, state-of-the-art face editing methods mainly suffer from the following two limitations: 1) they are only applicable to face images with relative low-resolutions and 2) multi-attribute face editing may generate uncontrollable changes in non-target face attribute categories. To solve these problems, we propose a novel High-Quality Generative Adversarial Network (HQ-GAN) for controllable editing of multiple face attributes in high-resolution images. HQ-GAN has two novel ideas to break the limitations of resolution and controllability correspondingly: 1) fine-grained textures and realistic details of high-resolution face images are better preserved with the aid of textural features extracted by the wavelet transform module and 2) desired multi-attribute targets of face editing are emphasized using a weighted binary cross-entropy (BCE) loss so that the influence on non-target attributes is greatly reduced. To the best of our knowledge, HQ-GAN is the first attempt to achieve continuous editing of multiple face attributes on high-resolution images of the CelebA-HQ using only 28 000 training samples. Extensive qualitative results demonstrate the superiority of the proposed method in rendering realistic high-resolution face images with accurate attribute modification, and comprehensive quantitative results show that the proposed method significantly outperforms state-of-the-art face editing methods.', 'pages': '1410-1423', 'number': '', 'volume': '16', 'year': '2021', 'title': 'Controllable Multi-Attribute Editing of High-Resolution Face Images', 'journal': 'IEEE Transactions on Information Forensics and Security', 'author': 'Deng, Qiyao and Li, Qi and Cao, Jie and Liu, Yunfan and Sun, Zhenan', 'ENTRYTYPE': 'article', 'ID': '9236637'}"
10792726,The Significance of Cultivating High-Value Patents in the Development of AI,"Jiang, Meichen and Fang, Xi",Jiang,10.1109/ICIIBMS62405.2024.10792726,2024,2024 9th International Conference on Intelligent Informatics and Biomedical Sciences (ICIIBMS),"With the rapid development of generative artificial intelligence (AI) technology, high-value patents play an increasingly important role in protecting innovation, promoting technological progress, and enhancing market competitiveness. This paper aims to explore how to effectively cultivate high-value patents in the field of generative AI and analyze their significance for technological innovation and industrial development. First, we review the basic concepts and current technological status of generative AI, conduct an in-depth analysis of high-value patents, and propose various strategies and methods for cultivating such patents. Through detailed case studies of Transformer architectures, this paper demonstrates the cultivation process and key success factors of these high-value patents. The study shows that high-value patents not only protect innovation outcomes but also promote the widespread application and commercialization of generative AI technology, providing strong support for sustainable technological development.",Industries;Patents;Technological innovation;Generative AI;Reviews;Technology transfer;Companies;Transformers;Faces;Commercialization;Generative AI;High-Value Patents;Patent Cultivation,"{'month': 'Nov', 'issn': '2189-8723', 'doi': '10.1109/ICIIBMS62405.2024.10792726', 'keywords': 'Industries;Patents;Technological innovation;Generative AI;Reviews;Technology transfer;Companies;Transformers;Faces;Commercialization;Generative AI;High-Value Patents;Patent Cultivation', 'abstract': 'With the rapid development of generative artificial intelligence (AI) technology, high-value patents play an increasingly important role in protecting innovation, promoting technological progress, and enhancing market competitiveness. This paper aims to explore how to effectively cultivate high-value patents in the field of generative AI and analyze their significance for technological innovation and industrial development. First, we review the basic concepts and current technological status of generative AI, conduct an in-depth analysis of high-value patents, and propose various strategies and methods for cultivating such patents. Through detailed case studies of Transformer architectures, this paper demonstrates the cultivation process and key success factors of these high-value patents. The study shows that high-value patents not only protect innovation outcomes but also promote the widespread application and commercialization of generative AI technology, providing strong support for sustainable technological development.', 'pages': '648-651', 'number': '', 'volume': '9', 'year': '2024', 'title': 'The Significance of Cultivating High-Value Patents in the Development of AI', 'booktitle': '2024 9th International Conference on Intelligent Informatics and Biomedical Sciences (ICIIBMS)', 'author': 'Jiang, Meichen and Fang, Xi', 'ENTRYTYPE': 'inproceedings', 'ID': '10792726'}"
10930297,LLM-Based Video Analytics Test Scenario Generation in Smart Cities,"Yilmazer, Merve and Karakose, Mehmet",Yilmazer,10.1109/IT64745.2025.10930297,2025,2025 29th International Conference on Information Technology (IT),"Rapid advances in the field of artificial intelligence have made significant contributions to the automation of software development and testing stages. Software created for use in various fields is tested with test scenarios created manually by software test experts or using test automation. Testing large-scale software with these methods complicates the testing phases because it requires increased human intervention and includes complex applications. In this study, an LLM-based scenario generation framework enhanced with prompt engineering is proposed for testing software to be used for video analysis in smart cities and smart campus areas. Thus, software test scenarios are created by strengthening large language models that are fast, flexible and have high learning ability using prompt engineering techniques. Test scenarios produced through LLM reinforced with prompt engineering techniques were evaluated with rarity and reality metrics and it was determined that more robust scenarios were produced compared to randomly generated test scenarios in the relevant field.",Software testing;Measurement;Automation;Smart cities;Large language models;Visual analytics;Software;Prompt engineering;Scenario generation;Software development management;prompt engineering;large language model;software testing;generative artificial intelligence;smart cities,"{'month': 'Feb', 'issn': '2836-3744', 'doi': '10.1109/IT64745.2025.10930297', 'keywords': 'Software testing;Measurement;Automation;Smart cities;Large language models;Visual analytics;Software;Prompt engineering;Scenario generation;Software development management;prompt engineering;large language model;software testing;generative artificial intelligence;smart cities', 'abstract': 'Rapid advances in the field of artificial intelligence have made significant contributions to the automation of software development and testing stages. Software created for use in various fields is tested with test scenarios created manually by software test experts or using test automation. Testing large-scale software with these methods complicates the testing phases because it requires increased human intervention and includes complex applications. In this study, an LLM-based scenario generation framework enhanced with prompt engineering is proposed for testing software to be used for video analysis in smart cities and smart campus areas. Thus, software test scenarios are created by strengthening large language models that are fast, flexible and have high learning ability using prompt engineering techniques. Test scenarios produced through LLM reinforced with prompt engineering techniques were evaluated with rarity and reality metrics and it was determined that more robust scenarios were produced compared to randomly generated test scenarios in the relevant field.', 'pages': '1-4', 'number': '', 'volume': '', 'year': '2025', 'title': 'LLM-Based Video Analytics Test Scenario Generation in Smart Cities', 'booktitle': '2025 29th International Conference on Information Technology (IT)', 'author': 'Yilmazer, Merve and Karakose, Mehmet', 'ENTRYTYPE': 'inproceedings', 'ID': '10930297'}"
9845345,Cyber Threat Analysis and Trustworthy Artificial Intelligence,"Wang, Shuangbao Paul and Arafin, Md Tanvir and Osuagwu, Onyema and Wandji, Ketchiozo",Wang,10.1109/CSP55486.2022.00024,2022,"2022 6th International Conference on Cryptography, Security and Privacy (CSP)","Cyber threats can cause severe damage to computing infrastructure and systems as well as data breaches that make sensitive data vulnerable to attackers and adversaries. It is therefore imperative to discover those threats and stop them before bad actors penetrating into the information systems.Threats hunting algorithms based on machine learning have shown great advantage over classical methods. Reinforcement learning models are getting more accurate for identifying not only signature-based but also behavior-based threats. Quantum mechanics brings a new dimension in improving classification speed with exponential advantage. The accuracy of the AI/ML algorithms could be affected by many factors, from algorithm, data, to prejudicial, or even intentional. As a result, AI/ML applications need to be non-biased and trustworthy.In this research, we developed a machine learning-based cyber threat detection and assessment tool. It uses two-stage (both unsupervised and supervised learning) analyzing method on 822,226 log data recorded from a web server on AWS cloud. The results show the algorithm has the ability to identify the threats with high confidence.",Privacy;Machine learning algorithms;Supervised learning;Quantum mechanics;Reinforcement learning;Data breach;Web servers;Machine Learning;Threat detection;Trustworthy AI;Data Analytics;quantum computing,"{'month': 'Jan', 'issn': '', 'doi': '10.1109/CSP55486.2022.00024', 'keywords': 'Privacy;Machine learning algorithms;Supervised learning;Quantum mechanics;Reinforcement learning;Data breach;Web servers;Machine Learning;Threat detection;Trustworthy AI;Data Analytics;quantum computing', 'abstract': 'Cyber threats can cause severe damage to computing infrastructure and systems as well as data breaches that make sensitive data vulnerable to attackers and adversaries. It is therefore imperative to discover those threats and stop them before bad actors penetrating into the information systems.Threats hunting algorithms based on machine learning have shown great advantage over classical methods. Reinforcement learning models are getting more accurate for identifying not only signature-based but also behavior-based threats. Quantum mechanics brings a new dimension in improving classification speed with exponential advantage. The accuracy of the AI/ML algorithms could be affected by many factors, from algorithm, data, to prejudicial, or even intentional. As a result, AI/ML applications need to be non-biased and trustworthy.In this research, we developed a machine learning-based cyber threat detection and assessment tool. It uses two-stage (both unsupervised and supervised learning) analyzing method on 822,226 log data recorded from a web server on AWS cloud. The results show the algorithm has the ability to identify the threats with high confidence.', 'pages': '86-90', 'number': '', 'volume': '', 'year': '2022', 'title': 'Cyber Threat Analysis and Trustworthy Artificial Intelligence', 'booktitle': '2022 6th International Conference on Cryptography, Security and Privacy (CSP)', 'author': 'Wang, Shuangbao Paul and Arafin, Md Tanvir and Osuagwu, Onyema and Wandji, Ketchiozo', 'ENTRYTYPE': 'inproceedings', 'ID': '9845345'}"
11101561,Wind Power Scenario Generation Based on Temporal Conditional Generative Adversarial Network,"Wang, Tianjun and Wang, Kai and Guo, Jiangtao and Xiao, Jingfeng and Cao, Shu and Lan, Songyan and Niu, Zhewen",Wang,10.1109/AAIEE64965.2025.11101561,2025,2025 IEEE International Symposium on the Application of Artificial Intelligence in Electrical Engineering (AAIEE),"The uncertainty of wind power output poses significant challenges to the stable operation of power systems. Artificial intelligence-based scenario generation approach offers a new way to frame uncertainty in wind power. However, generative adversarial networks (GANs) face challenges, including limited ability to effectively capture temporal information, instability during the training process, and susceptibility to mode collapse. To this end, this paper proposes a Temporal Conditional Generative Adversarial Network (TC-GAN) for renewable energy output generation. Temporal convolutional networks are embedded in both the generator and discriminator of conditional generative adversarial networks (CGAN), with the Wasserstein distance serving as the loss function for the discriminator. This design enables adversarial training to effectively capture the mapping relationships between noise and scenarios in day-ahead forecasting conditions. Furthermore, a comprehensive evaluation metric system for generated scenarios was developed, employing statistical analysis methods to assess scenario quality. Case studies demonstrate that the proposed model effectively captures temporal dependencies and enhances the accuracy of wind power scenario generation.",Training;Renewable energy sources;Uncertainty;Statistical analysis;Wind power generation;Power system stability;Generative adversarial networks;Spatiotemporal phenomena;Scenario generation;Optimization;renewable energy;uncertainty modeling;scenario generation;generative model,"{'month': 'April', 'issn': '', 'doi': '10.1109/AAIEE64965.2025.11101561', 'keywords': 'Training;Renewable energy sources;Uncertainty;Statistical analysis;Wind power generation;Power system stability;Generative adversarial networks;Spatiotemporal phenomena;Scenario generation;Optimization;renewable energy;uncertainty modeling;scenario generation;generative model', 'abstract': 'The uncertainty of wind power output poses significant challenges to the stable operation of power systems. Artificial intelligence-based scenario generation approach offers a new way to frame uncertainty in wind power. However, generative adversarial networks (GANs) face challenges, including limited ability to effectively capture temporal information, instability during the training process, and susceptibility to mode collapse. To this end, this paper proposes a Temporal Conditional Generative Adversarial Network (TC-GAN) for renewable energy output generation. Temporal convolutional networks are embedded in both the generator and discriminator of conditional generative adversarial networks (CGAN), with the Wasserstein distance serving as the loss function for the discriminator. This design enables adversarial training to effectively capture the mapping relationships between noise and scenarios in day-ahead forecasting conditions. Furthermore, a comprehensive evaluation metric system for generated scenarios was developed, employing statistical analysis methods to assess scenario quality. Case studies demonstrate that the proposed model effectively captures temporal dependencies and enhances the accuracy of wind power scenario generation.', 'pages': '544-549', 'number': '', 'volume': '', 'year': '2025', 'title': 'Wind Power Scenario Generation Based on Temporal Conditional Generative Adversarial Network', 'booktitle': '2025 IEEE International Symposium on the Application of Artificial Intelligence in Electrical Engineering (AAIEE)', 'author': 'Wang, Tianjun and Wang, Kai and Guo, Jiangtao and Xiao, Jingfeng and Cao, Shu and Lan, Songyan and Niu, Zhewen', 'ENTRYTYPE': 'inproceedings', 'ID': '11101561'}"
10529221,Enhancing Deep Reinforcement Learning: A Tutorial on Generative Diffusion Models in Network Optimization,"Du, Hongyang and Zhang, Ruichen and Liu, Yinqiu and Wang, Jiacheng and Lin, Yijing and Li, Zonghang and Niyato, Dusit and Kang, Jiawen and Xiong, Zehui and Cui, Shuguang and Ai, Bo and Zhou, Haibo and Kim, Dong In",Du,10.1109/COMST.2024.3400011,2024,IEEE Communications Surveys \& Tutorials,"Generative Diffusion Models (GDMs) have emerged as a transformative force in the realm of Generative Artificial Intelligence (GenAI), demonstrating their versatility and efficacy across various applications. The ability to model complex data distributions and generate high-quality samples has made GDMs particularly effective in tasks such as image generation and reinforcement learning. Furthermore, their iterative nature, which involves a series of noise addition and denoising steps, is a powerful and unique approach to learning and generating data. This paper serves as a comprehensive tutorial on applying GDMs in network optimization tasks. We delve into the strengths of GDMs, emphasizing their wide applicability across various domains, such as vision, text, and audio generation. We detail how GDMs can be effectively harnessed to solve complex optimization problems inherent in networks. The paper first provides a basic background of GDMs and their applications in network optimization. This is followed by a series of case studies, showcasing the integration of GDMs with Deep Reinforcement Learning (DRL), incentive mechanism design, Semantic Communications (SemCom), Internet of Vehicles (IoV) networks, etc. These case studies underscore the practicality and efficacy of GDMs in real-world scenarios, offering insights into network design. We conclude with a discussion on potential future directions for GDM research and applications, providing major insights into how they can continue to shape the future of network optimization.",Optimization;Tutorials;Computational modeling;Electronic mail;Task analysis;Data models;Artificial intelligence;Diffusion model;deep reinforcement learning;generative AI;AI-generated content;network optimization,"{'month': 'Fourthquarter', 'issn': '1553-877X', 'doi': '10.1109/COMST.2024.3400011', 'keywords': 'Optimization;Tutorials;Computational modeling;Electronic mail;Task analysis;Data models;Artificial intelligence;Diffusion model;deep reinforcement learning;generative AI;AI-generated content;network optimization', 'abstract': 'Generative Diffusion Models (GDMs) have emerged as a transformative force in the realm of Generative Artificial Intelligence (GenAI), demonstrating their versatility and efficacy across various applications. The ability to model complex data distributions and generate high-quality samples has made GDMs particularly effective in tasks such as image generation and reinforcement learning. Furthermore, their iterative nature, which involves a series of noise addition and denoising steps, is a powerful and unique approach to learning and generating data. This paper serves as a comprehensive tutorial on applying GDMs in network optimization tasks. We delve into the strengths of GDMs, emphasizing their wide applicability across various domains, such as vision, text, and audio generation. We detail how GDMs can be effectively harnessed to solve complex optimization problems inherent in networks. The paper first provides a basic background of GDMs and their applications in network optimization. This is followed by a series of case studies, showcasing the integration of GDMs with Deep Reinforcement Learning (DRL), incentive mechanism design, Semantic Communications (SemCom), Internet of Vehicles (IoV) networks, etc. These case studies underscore the practicality and efficacy of GDMs in real-world scenarios, offering insights into network design. We conclude with a discussion on potential future directions for GDM research and applications, providing major insights into how they can continue to shape the future of network optimization.', 'pages': '2611-2646', 'number': '4', 'volume': '26', 'year': '2024', 'title': 'Enhancing Deep Reinforcement Learning: A Tutorial on Generative Diffusion Models in Network Optimization', 'journal': 'IEEE Communications Surveys \\& Tutorials', 'author': 'Du, Hongyang and Zhang, Ruichen and Liu, Yinqiu and Wang, Jiacheng and Lin, Yijing and Li, Zonghang and Niyato, Dusit and Kang, Jiawen and Xiong, Zehui and Cui, Shuguang and Ai, Bo and Zhou, Haibo and Kim, Dong In', 'ENTRYTYPE': 'article', 'ID': '10529221'}"
10952212,Generative AI's Exponential Growth,"Musiol, Martin",Musiol,,2024,Generative AI: Navigating the Course to the Artificial General Intelligence Future,"Summary <p>This chapter peels back the layers and explores the underlying factors that played a pivotal role in the rise of generative AI. The rise of generative AI is being significantly propelled by its convergence with other fields\&\#x2014;a phenomenon known as technological convergence. Technological convergence is often directional, with one field exerting a greater influence on other. Cloud computing, in its essence, is the great equalizer. It has ushered in an era where computing power, once the exclusive domain of tech behemoths, is now within arm's reach of the many. Quantum computing holds the potential to transform the computational world. Big Data is the main source for modern AI systems, providing the information for machine learning (ML) to get better. The evolution in AI is driven by advanced ML algorithms, an open source culture, and a suite of tools and libraries that streamline AI development.</p>",Artificial intelligence;Generative AI;Technological innovation;Business;Investment;Games;Deep learning;Cloud computing;Vehicle dynamics;Transistors,"{'url': 'https://ieeexplore-ieee-org.crai.referencistas.com/document/10952212', 'isbn': '9781394205950', 'publisher': 'Wiley', 'issn': '', 'doi': '', 'keywords': 'Artificial intelligence;Generative AI;Technological innovation;Business;Investment;Games;Deep learning;Cloud computing;Vehicle dynamics;Transistors', 'abstract': ""Summary <p>This chapter peels back the layers and explores the underlying factors that played a pivotal role in the rise of generative AI. The rise of generative AI is being significantly propelled by its convergence with other fields\\&\\#x2014;a phenomenon known as technological convergence. Technological convergence is often directional, with one field exerting a greater influence on other. Cloud computing, in its essence, is the great equalizer. It has ushered in an era where computing power, once the exclusive domain of tech behemoths, is now within arm's reach of the many. Quantum computing holds the potential to transform the computational world. Big Data is the main source for modern AI systems, providing the information for machine learning (ML) to get better. The evolution in AI is driven by advanced ML algorithms, an open source culture, and a suite of tools and libraries that streamline AI development.</p>"", 'pages': '219-283', 'number': '', 'volume': '', 'year': '2024', 'title': ""Generative AI's Exponential Growth"", 'booktitle': 'Generative AI: Navigating the Course to the Artificial General Intelligence Future', 'author': 'Musiol, Martin', 'ENTRYTYPE': 'inbook', 'ID': '10952212'}"
11076064,Very High- to High- Resolution Imagery Transferability for Building Damage Detection Using Generative AI,"Shibli, Ali and Nascetti, Andrea and Ban, Yifang",Shibli,10.1109/JURSE60372.2025.11076064,2025,2025 Joint Urban Remote Sensing Event (JURSE),"Wildfires are a growing global concern, causing significant damage to urban infrastructure each year. This study presents a novel approach for building damage assessment using generative artificial intelligence, focusing on the transferability of high-resolution satellite imagery models to lower-resolution datasets. Our diffusion-based model is trained on the xView2 Wildfire Building Damage Benchmark, a dataset specifically designed for wildfire-induced building damage detection. The model is further evaluated on real-world wildfire incidents in Lahaina, Hawaii, and Athens, Greece, demonstrating its effectiveness in damage localization across varying spatial resolutions. With competitive performance on benchmark datasets and practical utility in real-world scenarios, this work highlights the potential of generative AI for geospatial disaster assessment and urban resilience.",Deep learning;Wildfires;Generative AI;Disasters;Buildings;Benchmark testing;Diffusion models;Satellite images;Geospatial analysis;Spatial resolution;Natural disasters;wildfire;machine learning;diffusion models;deep learning;generative artificial intelligence;satellite imagery;geospatial data,"{'month': 'May', 'issn': '2642-9535', 'doi': '10.1109/JURSE60372.2025.11076064', 'keywords': 'Deep learning;Wildfires;Generative AI;Disasters;Buildings;Benchmark testing;Diffusion models;Satellite images;Geospatial analysis;Spatial resolution;Natural disasters;wildfire;machine learning;diffusion models;deep learning;generative artificial intelligence;satellite imagery;geospatial data', 'abstract': 'Wildfires are a growing global concern, causing significant damage to urban infrastructure each year. This study presents a novel approach for building damage assessment using generative artificial intelligence, focusing on the transferability of high-resolution satellite imagery models to lower-resolution datasets. Our diffusion-based model is trained on the xView2 Wildfire Building Damage Benchmark, a dataset specifically designed for wildfire-induced building damage detection. The model is further evaluated on real-world wildfire incidents in Lahaina, Hawaii, and Athens, Greece, demonstrating its effectiveness in damage localization across varying spatial resolutions. With competitive performance on benchmark datasets and practical utility in real-world scenarios, this work highlights the potential of generative AI for geospatial disaster assessment and urban resilience.', 'pages': '1-4', 'number': '', 'volume': 'CFP25RSD-ART', 'year': '2025', 'title': 'Very High- to High- Resolution Imagery Transferability for Building Damage Detection Using Generative AI', 'booktitle': '2025 Joint Urban Remote Sensing Event (JURSE)', 'author': 'Shibli, Ali and Nascetti, Andrea and Ban, Yifang', 'ENTRYTYPE': 'inproceedings', 'ID': '11076064'}"
10981398,DS Generative AI for Supporting Teaching Activities,"Montoya Montoya, José Fabián and Lopez-Vargas, Jorge",Montoya Montoya,10.1109/EDUNINE62377.2025.10981398,2025,2025 IEEE Engineering Education World Conference (EDUNINE),"Generative Artificial Intelligence (GAI) has become significant in education, particularly for creating content, resources, and automating repetitive and timeconsuming tasks. This project explores GAI’s potential to support teachers in analyzing low-to-medium complexity programs of student’s tasks, supporting the activities of teachers. The proposed solution includes an API and web application built based on the GPT-4o Large Language Model (LLM), specifically designed for teachers. The methodology begins with a review of relevant literature review to identify scenarios where GAI have shown their potential in the educational field. Subsequently, the performance of the GPT-4o model is evaluated in the context of review and analysis of student’s source code, using the Teaching Plans which the task proposals are extracted along with their respective evaluation rubrics, determining the quality and effectiveness of generative AI within this real application.",Generative AI;Source coding;Large language models;Refining;Prototypes;Reliability engineering;Proposals;Iterative methods;Programming profession;Systematic literature review;generative artificial intelligence;education;programming;source code analysis;gpt-4o;api;web application,"{'month': 'March', 'issn': '', 'doi': '10.1109/EDUNINE62377.2025.10981398', 'keywords': 'Generative AI;Source coding;Large language models;Refining;Prototypes;Reliability engineering;Proposals;Iterative methods;Programming profession;Systematic literature review;generative artificial intelligence;education;programming;source code analysis;gpt-4o;api;web application', 'abstract': 'Generative Artificial Intelligence (GAI) has become significant in education, particularly for creating content, resources, and automating repetitive and timeconsuming tasks. This project explores GAI’s potential to support teachers in analyzing low-to-medium complexity programs of student’s tasks, supporting the activities of teachers. The proposed solution includes an API and web application built based on the GPT-4o Large Language Model (LLM), specifically designed for teachers. The methodology begins with a review of relevant literature review to identify scenarios where GAI have shown their potential in the educational field. Subsequently, the performance of the GPT-4o model is evaluated in the context of review and analysis of student’s source code, using the Teaching Plans which the task proposals are extracted along with their respective evaluation rubrics, determining the quality and effectiveness of generative AI within this real application.', 'pages': '1-4', 'number': '', 'volume': '', 'year': '2025', 'title': 'DS Generative AI for Supporting Teaching Activities', 'booktitle': '2025 IEEE Engineering Education World Conference (EDUNINE)', 'author': 'Montoya Montoya, José Fabián and Lopez-Vargas, Jorge', 'ENTRYTYPE': 'inproceedings', 'ID': '10981398'}"
8396171,Surgical workflow image generation based on generative adversarial networks,"Chen, Yuwen and Zhong, Kunhua and Wang, Fei and Wang, Hongqian and Zhao, Xueliang",Chen,10.1109/ICAIBD.2018.8396171,2018,2018 International Conference on Artificial Intelligence and Big Data (ICAIBD),"In the medical field, the labeling of surgical video data requires Expert knowledge, collecting enough numbers of marked surgical video data is difficult and time-consuming. The insufficient video data (labeled data) leads to the low generalization ability of the training model and the low accuracy of recognition. It has been recently shown that Generative Adversarial Networks (GANs) can produce synthetic images of exceptional visual fidelity. In this work, the authors propose the GAN-based method for automatic Surgical Workflow images. The theory and methodology of this paper are validated on real three surgery video datasets. It can generative effective surgical workflow images. The technology studied in this paper has broad application prospects in computer-aided surgical systems and is a core component of the artificial intelligence medical operating room in the future.",Surgery;Generators;Gallium nitride;Training;Hospitals;Biomedical imaging;Laparoscopes;surgical workflow;generative adversarial networks;CNN,"{'month': 'May', 'issn': '', 'doi': '10.1109/ICAIBD.2018.8396171', 'keywords': 'Surgery;Generators;Gallium nitride;Training;Hospitals;Biomedical imaging;Laparoscopes;surgical workflow;generative adversarial networks;CNN', 'abstract': 'In the medical field, the labeling of surgical video data requires Expert knowledge, collecting enough numbers of marked surgical video data is difficult and time-consuming. The insufficient video data (labeled data) leads to the low generalization ability of the training model and the low accuracy of recognition. It has been recently shown that Generative Adversarial Networks (GANs) can produce synthetic images of exceptional visual fidelity. In this work, the authors propose the GAN-based method for automatic Surgical Workflow images. The theory and methodology of this paper are validated on real three surgery video datasets. It can generative effective surgical workflow images. The technology studied in this paper has broad application prospects in computer-aided surgical systems and is a core component of the artificial intelligence medical operating room in the future.', 'pages': '82-86', 'number': '', 'volume': '', 'year': '2018', 'title': 'Surgical workflow image generation based on generative adversarial networks', 'booktitle': '2018 International Conference on Artificial Intelligence and Big Data (ICAIBD)', 'author': 'Chen, Yuwen and Zhong, Kunhua and Wang, Fei and Wang, Hongqian and Zhao, Xueliang', 'ENTRYTYPE': 'inproceedings', 'ID': '8396171'}"
9820158,Detection of Link Communities in Attributed Graphs via an Approximate Bayesian Generative Model,"Tang, Zehai and Hu, Lun and Pan, Xiangyu",Tang,10.1109/ICAIBD55127.2022.9820158,2022,2022 5th International Conference on Artificial Intelligence and Big Data (ICAIBD),"Attributed graphs are complex networks with non-trivial topological structures and rich node contents. Recently, a variety of algorithms have been proposed to detect meaningful communities from a given attributed graph by combining these topology and content information. However, few of them is capable of detecting the communities of links, which are better to interpret the node behaviors in forming communities. Furthermore, since nodes are possible to grouped into more than one communities, community detection upon links is natural and parameter-free to incorporate overlap, thus revealing the intrinsic organization of attributed graphs in a more reasonable way. In this work, we propose a novel Bayesian probabilistic model to approximately simulate the generative process of an attributed graph given the prior knowledge regarding the distribution of community labels over links. An efficient variational algorithm, namely VBLCD, is developed to solve the inference problem of the proposed model, thus completing the task of detecting link communities. To evaluate the performance of VBLCD, we have applied it to address two practical applications including document classification and social community detection, and also compared VBLCD with several state-of-the-art algorithms. Experimental results demonstrate the promising performance of VBLCD.",Organizations;Approximation algorithms;Probabilistic logic;Inference algorithms;Data models;Bayes methods;Classification algorithms;attributed graph;Bayesian probabilistic model;approximate generative process;community detection;clustering,"{'month': 'May', 'issn': '', 'doi': '10.1109/ICAIBD55127.2022.9820158', 'keywords': 'Organizations;Approximation algorithms;Probabilistic logic;Inference algorithms;Data models;Bayes methods;Classification algorithms;attributed graph;Bayesian probabilistic model;approximate generative process;community detection;clustering', 'abstract': 'Attributed graphs are complex networks with non-trivial topological structures and rich node contents. Recently, a variety of algorithms have been proposed to detect meaningful communities from a given attributed graph by combining these topology and content information. However, few of them is capable of detecting the communities of links, which are better to interpret the node behaviors in forming communities. Furthermore, since nodes are possible to grouped into more than one communities, community detection upon links is natural and parameter-free to incorporate overlap, thus revealing the intrinsic organization of attributed graphs in a more reasonable way. In this work, we propose a novel Bayesian probabilistic model to approximately simulate the generative process of an attributed graph given the prior knowledge regarding the distribution of community labels over links. An efficient variational algorithm, namely VBLCD, is developed to solve the inference problem of the proposed model, thus completing the task of detecting link communities. To evaluate the performance of VBLCD, we have applied it to address two practical applications including document classification and social community detection, and also compared VBLCD with several state-of-the-art algorithms. Experimental results demonstrate the promising performance of VBLCD.', 'pages': '315-320', 'number': '', 'volume': '', 'year': '2022', 'title': 'Detection of Link Communities in Attributed Graphs via an Approximate Bayesian Generative Model', 'booktitle': '2022 5th International Conference on Artificial Intelligence and Big Data (ICAIBD)', 'author': 'Tang, Zehai and Hu, Lun and Pan, Xiangyu', 'ENTRYTYPE': 'inproceedings', 'ID': '9820158'}"
9740494,A Reinforcement Learning System for Generating Instantaneous Quality Random Sequences,"Almardeny, Yahya and Benavoli, Alessio and Boujnah, Noureddine and Naredo, Enrique",Almardeny,10.1109/TAI.2022.3161893,2023,IEEE Transactions on Artificial Intelligence,"Random numbers are essential to most computer applications. Still, producing high-quality random sequences is a big challenge. Inspired by the success of artificial neural networks and reinforcement learning, we propose a novel and effective end-to-end learning system to generate pseudorandom sequences that operates under the upside-down reinforcement learning framework. It is based on manipulating the generalized information entropy metric to derive commands that instantaneously guide the agent toward the optimal random behavior. Using a wide range of evaluation tests, the proposed approach is compared against three state-of-the-art accredited pseudorandom number generators (PRNGs). The experimental results agree with our theoretical study and show that the proposed framework is a promising candidate for a wide range of applications.",Generators;Artificial intelligence;Random sequences;Reinforcement learning;NIST;Generative adversarial networks;Ciphers;Artificial intelligence (AI);pseudorandom number generators (PRNG);provably secure RNG;random;reinforcement learning;upside-down reinforcement learning (RL),"{'month': 'June', 'issn': '2691-4581', 'doi': '10.1109/TAI.2022.3161893', 'keywords': 'Generators;Artificial intelligence;Random sequences;Reinforcement learning;NIST;Generative adversarial networks;Ciphers;Artificial intelligence (AI);pseudorandom number generators (PRNG);provably secure RNG;random;reinforcement learning;upside-down reinforcement learning (RL)', 'abstract': 'Random numbers are essential to most computer applications. Still, producing high-quality random sequences is a big challenge. Inspired by the success of artificial neural networks and reinforcement learning, we propose a novel and effective end-to-end learning system to generate pseudorandom sequences that operates under the upside-down reinforcement learning framework. It is based on manipulating the generalized information entropy metric to derive commands that instantaneously guide the agent toward the optimal random behavior. Using a wide range of evaluation tests, the proposed approach is compared against three state-of-the-art accredited pseudorandom number generators (PRNGs). The experimental results agree with our theoretical study and show that the proposed framework is a promising candidate for a wide range of applications.', 'pages': '402-415', 'number': '3', 'volume': '4', 'year': '2023', 'title': 'A Reinforcement Learning System for Generating Instantaneous Quality Random Sequences', 'journal': 'IEEE Transactions on Artificial Intelligence', 'author': 'Almardeny, Yahya and Benavoli, Alessio and Boujnah, Noureddine and Naredo, Enrique', 'ENTRYTYPE': 'article', 'ID': '9740494'}"
9885879,Image Super-Resolution Based on Frequency Division Generative Adversarial Network,"Xu, Jian and Gao, Qiannan",Xu,10.1109/ICNLP55136.2022.00048,2022,2022 4th International Conference on Natural Language Processing (ICNLP),"Most supervised super-resolution (SR) algorithms require paired high-resolution (HR) and low-resolution (LR) images as training samples. However, the network structures trained by supervised algorithms don’t adapt to different image degradations and it is difficult to find paired HR and LR images in real world. In this paper, a new unsupervised image SR algorithm based on generative adversarial network (GAN) is proposed. (1) A new network that can be trained with unpaired HR and LR images is proposed. (2) An intermediate process is proposed so that network contains two GANs, the first learns LR degradation, and the second performs SR procedure. (3) The idea of frequency division training is adopted in LR degradation and SR procedures: low frequency of image is preserved and only high frequency is learned. Experimental results on different datasets show that proposed algorithm provide a better balance between visual quality of SR reconstructed images and computational cost when compared with the state-of-the-art SR algorithms.",Degradation;Training;Visualization;Adaptation models;Computational modeling;Superresolution;Frequency conversion;image super-resolution;generative adversarial network;unsupervised learning;frequency division training,"{'month': 'March', 'issn': '', 'doi': '10.1109/ICNLP55136.2022.00048', 'keywords': 'Degradation;Training;Visualization;Adaptation models;Computational modeling;Superresolution;Frequency conversion;image super-resolution;generative adversarial network;unsupervised learning;frequency division training', 'abstract': 'Most supervised super-resolution (SR) algorithms require paired high-resolution (HR) and low-resolution (LR) images as training samples. However, the network structures trained by supervised algorithms don’t adapt to different image degradations and it is difficult to find paired HR and LR images in real world. In this paper, a new unsupervised image SR algorithm based on generative adversarial network (GAN) is proposed. (1) A new network that can be trained with unpaired HR and LR images is proposed. (2) An intermediate process is proposed so that network contains two GANs, the first learns LR degradation, and the second performs SR procedure. (3) The idea of frequency division training is adopted in LR degradation and SR procedures: low frequency of image is preserved and only high frequency is learned. Experimental results on different datasets show that proposed algorithm provide a better balance between visual quality of SR reconstructed images and computational cost when compared with the state-of-the-art SR algorithms.', 'pages': '266-271', 'number': '', 'volume': '', 'year': '2022', 'title': 'Image Super-Resolution Based on Frequency Division Generative Adversarial Network', 'booktitle': '2022 4th International Conference on Natural Language Processing (ICNLP)', 'author': 'Xu, Jian and Gao, Qiannan', 'ENTRYTYPE': 'inproceedings', 'ID': '9885879'}"
10947318,Unified Visual Comparison Framework for Human and AI Paintings Using Neural Embeddings and Computational Aesthetics,"Ye, Yilin and Huang, Rong and Zhang, Kang and Zeng, Wei",Ye,10.1109/MCG.2025.3555122,2025,IEEE Computer Graphics and Applications,"To facilitate comparative analysis of artificial intelligence (AI) and human paintings, we present a unified computational framework combining neural embedding and computational aesthetic features. We first exploit CLIP embedding to provide a projected overview for human and AI painting datasets, and we next leverage computational aesthetic metrics to obtain explainable features of paintings. On that basis, we design a visual analytics system that involves distribution discrepancy measurement for quantifying dataset differences and evolutionary analysis for comparing artists with AI models. Case studies comparing three AI-generated datasets with three human paintings datasets, and analyzing the evolutionary differences between authentic Picasso paintings and AI-generated ones, show the effectiveness of our framework.",Painting;Artificial intelligence;Feature extraction;Image color analysis;Training;Brightness;Semantics;Computational modeling;Art;Generative AI,"{'month': 'March', 'issn': '1558-1756', 'doi': '10.1109/MCG.2025.3555122', 'keywords': 'Painting;Artificial intelligence;Feature extraction;Image color analysis;Training;Brightness;Semantics;Computational modeling;Art;Generative AI', 'abstract': 'To facilitate comparative analysis of artificial intelligence (AI) and human paintings, we present a unified computational framework combining neural embedding and computational aesthetic features. We first exploit CLIP embedding to provide a projected overview for human and AI painting datasets, and we next leverage computational aesthetic metrics to obtain explainable features of paintings. On that basis, we design a visual analytics system that involves distribution discrepancy measurement for quantifying dataset differences and evolutionary analysis for comparing artists with AI models. Case studies comparing three AI-generated datasets with three human paintings datasets, and analyzing the evolutionary differences between authentic Picasso paintings and AI-generated ones, show the effectiveness of our framework.', 'pages': '19-30', 'number': '2', 'volume': '45', 'year': '2025', 'title': 'Unified Visual Comparison Framework for Human and AI Paintings Using Neural Embeddings and Computational Aesthetics', 'journal': 'IEEE Computer Graphics and Applications', 'author': 'Ye, Yilin and Huang, Rong and Zhang, Kang and Zeng, Wei', 'ENTRYTYPE': 'article', 'ID': '10947318'}"
11158640,AI for Cybersecurity Threat Detection: A Machine Enabled Computing Perspective,"Mishra, Mandakani and Pradhan, Rohit Raj and Agrawalla, Kesab and Bokka, Raveendranadh",Mishra,10.1109/ASSIC64892.2025.11158640,2025,"2025 International Conference on Advancements in Smart, Secure and Intelligent Computing (ASSIC)","With the increasing sophistication of cyber threats through Advanced Persistent Threats, ransomware, and insider attacks, innovative cybersecurity strategy is required, and AI represents a crucial instrument that uses deep learning and reinforcement learning to determine, analyze, and mitigate risks. This paper discusses AI-based cybersecurity, outlining a new model for AI, which improves on threat detection and response efficiency by increasing explainability. Analyzing large datasets and recognizing attack patterns and vulnerabilities to predict them has significantly improved the anomaly detection system and threat mitigation. The model also ensures that it follows ethical standards, including bias and transparency in AI-driven security. The results of this study show the significant advancements of cybersecurity resilience by integrating AI into modern threat defense. This AI-based approach offers scalable, adaptive, and proactive security. It allows organizations to effectively be prepared to deal with increasingly changing cyber threats.",Deep learning;Explainable AI;Standards organizations;Reinforcement learning;Threat assessment;Real-time systems;Time factors;Computer security;Artificial intelligence;Anomaly detection;AI;Cybersecurity;Threat Detection;Deep Learning;Anomaly Detection;Reinforcement Learning,"{'month': 'May', 'issn': '', 'doi': '10.1109/ASSIC64892.2025.11158640', 'keywords': 'Deep learning;Explainable AI;Standards organizations;Reinforcement learning;Threat assessment;Real-time systems;Time factors;Computer security;Artificial intelligence;Anomaly detection;AI;Cybersecurity;Threat Detection;Deep Learning;Anomaly Detection;Reinforcement Learning', 'abstract': 'With the increasing sophistication of cyber threats through Advanced Persistent Threats, ransomware, and insider attacks, innovative cybersecurity strategy is required, and AI represents a crucial instrument that uses deep learning and reinforcement learning to determine, analyze, and mitigate risks. This paper discusses AI-based cybersecurity, outlining a new model for AI, which improves on threat detection and response efficiency by increasing explainability. Analyzing large datasets and recognizing attack patterns and vulnerabilities to predict them has significantly improved the anomaly detection system and threat mitigation. The model also ensures that it follows ethical standards, including bias and transparency in AI-driven security. The results of this study show the significant advancements of cybersecurity resilience by integrating AI into modern threat defense. This AI-based approach offers scalable, adaptive, and proactive security. It allows organizations to effectively be prepared to deal with increasingly changing cyber threats.', 'pages': '1-5', 'number': '', 'volume': '', 'year': '2025', 'title': 'AI for Cybersecurity Threat Detection: A Machine Enabled Computing Perspective', 'booktitle': '2025 International Conference on Advancements in Smart, Secure and Intelligent Computing (ASSIC)', 'author': 'Mishra, Mandakani and Pradhan, Rohit Raj and Agrawalla, Kesab and Bokka, Raveendranadh', 'ENTRYTYPE': 'inproceedings', 'ID': '11158640'}"
10597596,A Survey on Symbolic Knowledge Distillation of Large Language Models,"Acharya, Kamal and Velasquez, Alvaro and Song, Houbing Herbert",Acharya,10.1109/TAI.2024.3428519,2024,IEEE Transactions on Artificial Intelligence,"This survey article delves into the emerging and critical area of symbolic knowledge distillation in large language models (LLMs). As LLMs such as generative pretrained transformer-3 (GPT-3) and bidirectional encoder representations from transformers (BERT) continue to expand in scale and complexity, the challenge of effectively harnessing their extensive knowledge becomes paramount. This survey concentrates on the process of distilling the intricate, often implicit knowledge contained within these models into a more symbolic, explicit form. This transformation is crucial for enhancing the interpretability, efficiency, and applicability of LLMs. We categorize the existing research based on methodologies and applications, focusing on how symbolic knowledge distillation can be used to improve the transparency and functionality of smaller, more efficient artificial intelligence (AI) models. The survey discusses the core challenges, including maintaining the depth of knowledge in a comprehensible format, and explores the various approaches and techniques that have been developed in this field. We identify gaps in current research and potential opportunities for future advancements. This survey aims to provide a comprehensive overview of symbolic knowledge distillation in LLMs, spotlighting its significance in the progression toward more accessible and efficient AI systems.",Surveys;Computational modeling;Artificial intelligence;Training;Predictive models;Data models;Transformers;Large language models (LLMs);symbolic knowledge;symbolic knowledge distillation,"{'month': 'Dec', 'issn': '2691-4581', 'doi': '10.1109/TAI.2024.3428519', 'keywords': 'Surveys;Computational modeling;Artificial intelligence;Training;Predictive models;Data models;Transformers;Large language models (LLMs);symbolic knowledge;symbolic knowledge distillation', 'abstract': 'This survey article delves into the emerging and critical area of symbolic knowledge distillation in large language models (LLMs). As LLMs such as generative pretrained transformer-3 (GPT-3) and bidirectional encoder representations from transformers (BERT) continue to expand in scale and complexity, the challenge of effectively harnessing their extensive knowledge becomes paramount. This survey concentrates on the process of distilling the intricate, often implicit knowledge contained within these models into a more symbolic, explicit form. This transformation is crucial for enhancing the interpretability, efficiency, and applicability of LLMs. We categorize the existing research based on methodologies and applications, focusing on how symbolic knowledge distillation can be used to improve the transparency and functionality of smaller, more efficient artificial intelligence (AI) models. The survey discusses the core challenges, including maintaining the depth of knowledge in a comprehensible format, and explores the various approaches and techniques that have been developed in this field. We identify gaps in current research and potential opportunities for future advancements. This survey aims to provide a comprehensive overview of symbolic knowledge distillation in LLMs, spotlighting its significance in the progression toward more accessible and efficient AI systems.', 'pages': '5928-5948', 'number': '12', 'volume': '5', 'year': '2024', 'title': 'A Survey on Symbolic Knowledge Distillation of Large Language Models', 'journal': 'IEEE Transactions on Artificial Intelligence', 'author': 'Acharya, Kamal and Velasquez, Alvaro and Song, Houbing Herbert', 'ENTRYTYPE': 'article', 'ID': '10597596'}"
10545728,Research on Internet of Things Human-Computer Interaction System Based on Computer Artificial Intelligence Technology,"Cheng, Zitong",Cheng,10.1109/ICCECT60629.2024.10545728,2024,"2024 IEEE 2nd International Conference on Control, Electronics and Computer Technology (ICCECT)","This piece delves into the realm of the Internet of Things and explores the synergy between AI technology and human-machine interaction systems. Specifically, an analysis is conducted on the core of computer AI-driven dialogue processes and associated technologies, such as automated question answering, continuous speech recognition, and text-to-speech systems. Research indicates that AI technology greatly enhances the efficiency and accuracy of these interaction systems, particularly in handling continuous streams of natural language input and output. A deeper analysis of the automated question answering system reveals its reliance on sophisticated natural language processing and knowledge graph technologies, aiming to enhance the quality of problem analysis and response generation. Similarly, the exploration of continuous speech recognition systems demonstrates their pivotal role in converting speech information into text accurately, which is vital for achieving smooth human-machine conversations. Furthermore, the analysis of text-to-speech systems highlights the natural and fluid transformation of textual information into speech output through advanced speech synthesis technology. To ensure practicality and the actual effectiveness of the design, a series of system validation tests are proposed and implemented in this study. These tests evaluate the core performance indicators of the system, with recognition rate tests focusing on the accuracy of speech and text processing, while communication distance tests ensure stable connectivity between base stations and IoT nodes. Through this comprehensive validation, the research findings demonstrate the efficient performance and reliability of this human-machine interaction system in IoT applications.",Human computer interaction;Technological innovation;Text recognition;Speech recognition;Question answering (information retrieval);User experience;Internet of Things;Computer AI;Artificial Intelligence Technology;Internet of Things;Human-Computer Interaction Systems,"{'month': 'April', 'issn': '', 'doi': '10.1109/ICCECT60629.2024.10545728', 'keywords': 'Human computer interaction;Technological innovation;Text recognition;Speech recognition;Question answering (information retrieval);User experience;Internet of Things;Computer AI;Artificial Intelligence Technology;Internet of Things;Human-Computer Interaction Systems', 'abstract': 'This piece delves into the realm of the Internet of Things and explores the synergy between AI technology and human-machine interaction systems. Specifically, an analysis is conducted on the core of computer AI-driven dialogue processes and associated technologies, such as automated question answering, continuous speech recognition, and text-to-speech systems. Research indicates that AI technology greatly enhances the efficiency and accuracy of these interaction systems, particularly in handling continuous streams of natural language input and output. A deeper analysis of the automated question answering system reveals its reliance on sophisticated natural language processing and knowledge graph technologies, aiming to enhance the quality of problem analysis and response generation. Similarly, the exploration of continuous speech recognition systems demonstrates their pivotal role in converting speech information into text accurately, which is vital for achieving smooth human-machine conversations. Furthermore, the analysis of text-to-speech systems highlights the natural and fluid transformation of textual information into speech output through advanced speech synthesis technology. To ensure practicality and the actual effectiveness of the design, a series of system validation tests are proposed and implemented in this study. These tests evaluate the core performance indicators of the system, with recognition rate tests focusing on the accuracy of speech and text processing, while communication distance tests ensure stable connectivity between base stations and IoT nodes. Through this comprehensive validation, the research findings demonstrate the efficient performance and reliability of this human-machine interaction system in IoT applications.', 'pages': '1135-1139', 'number': '', 'volume': '', 'year': '2024', 'title': 'Research on Internet of Things Human-Computer Interaction System Based on Computer Artificial Intelligence Technology', 'booktitle': '2024 IEEE 2nd International Conference on Control, Electronics and Computer Technology (ICCECT)', 'author': 'Cheng, Zitong', 'ENTRYTYPE': 'inproceedings', 'ID': '10545728'}"
9328201,Dual-Channel Capsule Generation Adversarial Network for Hyperspectral Image Classification,"Wang, Jianing and Guo, Siying and Huang, Runhu and Li, Linhao and Zhang, Xiangrong and Jiao, Licheng",Wang,10.1109/TGRS.2020.3044312,2022,IEEE Transactions on Geoscience and Remote Sensing,"Deep learning-based methods have demonstrated significant breakthroughs in the application of hyperspectral image (HSI) classification. However, some challenging issues still exist, such as the overfitting problem caused by the limitation of training size with high-dimensional feature and the efficiency of spectral–spatial (SS) exploitation. Therefore, to efficiently model the relative position of samples within the generative adversarial network (GAN) setting, we proposed a dual-channel SS fusion capsule generative adversarial network (DcCapsGAN) for HSI classification. Dual channels (1-D-CapsGAN and 2-D-CapsGAN) are constructed by integrating the capsule network (CapsNet) with GAN for eliminating the mode collapse and gradient disappearance problem caused by traditional GAN. Meanwhile, octave convolution and multiscale convolution are integrated into the proposed model for further reducing the parameters of the CapsNet and extracting multiscale features. To further boost the classification performance, the SS channel fusion model is constructed to composite and switch the feature information of different channels, thereby facilitating the accuracy and robustness of the whole classification performance. Three commonly used HSI data sets are utilized to investigate the performance of the proposed DcCapsGAN model, and the performance of the experiment demonstrates that the proposed model can efficiently improve the classification accuracy and performance.",Feature extraction;Generative adversarial networks;Training;Generators;Gallium nitride;Convolution;Hyperspectral imaging;Capsule networks (CapsNets);generative adversarial network (GAN);hyperspectral image (HSI);multiscale convolution,"{'month': '', 'issn': '1558-0644', 'doi': '10.1109/TGRS.2020.3044312', 'keywords': 'Feature extraction;Generative adversarial networks;Training;Generators;Gallium nitride;Convolution;Hyperspectral imaging;Capsule networks (CapsNets);generative adversarial network (GAN);hyperspectral image (HSI);multiscale convolution', 'abstract': 'Deep learning-based methods have demonstrated significant breakthroughs in the application of hyperspectral image (HSI) classification. However, some challenging issues still exist, such as the overfitting problem caused by the limitation of training size with high-dimensional feature and the efficiency of spectral–spatial (SS) exploitation. Therefore, to efficiently model the relative position of samples within the generative adversarial network (GAN) setting, we proposed a dual-channel SS fusion capsule generative adversarial network (DcCapsGAN) for HSI classification. Dual channels (1-D-CapsGAN and 2-D-CapsGAN) are constructed by integrating the capsule network (CapsNet) with GAN for eliminating the mode collapse and gradient disappearance problem caused by traditional GAN. Meanwhile, octave convolution and multiscale convolution are integrated into the proposed model for further reducing the parameters of the CapsNet and extracting multiscale features. To further boost the classification performance, the SS channel fusion model is constructed to composite and switch the feature information of different channels, thereby facilitating the accuracy and robustness of the whole classification performance. Three commonly used HSI data sets are utilized to investigate the performance of the proposed DcCapsGAN model, and the performance of the experiment demonstrates that the proposed model can efficiently improve the classification accuracy and performance.', 'pages': '1-16', 'number': '', 'volume': '60', 'year': '2022', 'title': 'Dual-Channel Capsule Generation Adversarial Network for Hyperspectral Image Classification', 'journal': 'IEEE Transactions on Geoscience and Remote Sensing', 'author': 'Wang, Jianing and Guo, Siying and Huang, Runhu and Li, Linhao and Zhang, Xiangrong and Jiao, Licheng', 'ENTRYTYPE': 'article', 'ID': '9328201'}"
9968292,Deep Learning Based Online Nondestructive Defect Detection for Self-Piercing Riveted Joints in Automotive Body Manufacturing,"Chen, Shuangwu and Jin, Dong and He, Huasen and Yang, Feng and Yang, Jian",Chen,10.1109/TII.2022.3226246,2023,IEEE Transactions on Industrial Informatics,"Self-piercing riveting (SPR) is widely used for joining lightweight and dissimilar materials in automotive body manufacturing, the quality of which directly affects the safety of vehicles. However, there is still no reliable method that can be used for SPR quality control without destructive test and manual intervention. This article presents an online nondestructive SPR defect detection method based on deep learning. By learning the temporal dependencies of punch force varying with rivet displacement under different joint combinations, the proposed method can provide real-time defect alarms and avoid the enormous cost of joint dissection. We develop an SPR parameter selection mechanism to rule out the irrelevant parameters, which enhances the learning performance. For the problem of model overfitting caused by the savage imbalance of SPR data, we design a conditional generative adversarial network based data generation model. In order to accommodate the difference in defect patterns between factory and laboratory, we devise a transfer learning based model migration method, which substantially reduces the amount of labeled factory data for model training. The evaluations on real SPR data collected from two car assembly lines of Audi and NIO verify that the proposed method achieves a high detection accuracy and a low missing rate in SPR defect detection.",Force;Automobiles;Manufacturing;Data models;Inspection;Production facilities;Numerical models;Automotive body manufacturing;defect detection;nondestructive test;self-piercing riveting (SPR),"{'month': 'Aug', 'issn': '1941-0050', 'doi': '10.1109/TII.2022.3226246', 'keywords': 'Force;Automobiles;Manufacturing;Data models;Inspection;Production facilities;Numerical models;Automotive body manufacturing;defect detection;nondestructive test;self-piercing riveting (SPR)', 'abstract': 'Self-piercing riveting (SPR) is widely used for joining lightweight and dissimilar materials in automotive body manufacturing, the quality of which directly affects the safety of vehicles. However, there is still no reliable method that can be used for SPR quality control without destructive test and manual intervention. This article presents an online nondestructive SPR defect detection method based on deep learning. By learning the temporal dependencies of punch force varying with rivet displacement under different joint combinations, the proposed method can provide real-time defect alarms and avoid the enormous cost of joint dissection. We develop an SPR parameter selection mechanism to rule out the irrelevant parameters, which enhances the learning performance. For the problem of model overfitting caused by the savage imbalance of SPR data, we design a conditional generative adversarial network based data generation model. In order to accommodate the difference in defect patterns between factory and laboratory, we devise a transfer learning based model migration method, which substantially reduces the amount of labeled factory data for model training. The evaluations on real SPR data collected from two car assembly lines of Audi and NIO verify that the proposed method achieves a high detection accuracy and a low missing rate in SPR defect detection.', 'pages': '9134-9144', 'number': '8', 'volume': '19', 'year': '2023', 'title': 'Deep Learning Based Online Nondestructive Defect Detection for Self-Piercing Riveted Joints in Automotive Body Manufacturing', 'journal': 'IEEE Transactions on Industrial Informatics', 'author': 'Chen, Shuangwu and Jin, Dong and He, Huasen and Yang, Feng and Yang, Jian', 'ENTRYTYPE': 'article', 'ID': '9968292'}"
10623893,Sea Surface Temperature Prediction Method Based on Deep Generative Adversarial Network,"Wang, Jia and Zheng, Gang and Yu, Jiali and Shao, Jinliang and Zhou, Yinfei",Wang,10.1109/JSTARS.2024.3439022,2024,IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,"Sea surface temperature (SST) prediction plays an important role in ocean-related fields. Therefore, it is increasingly important to be able to make more accurate prediction of SST. In this article, we develop a deep generative adversarial network (DGAN) for generating future maps of SSTs, providing a visual method of predicting SSTs. Our DGAN model consists of a generator and a discriminator. The generator is designed to produce more realistic maps of future SSTs, which uses multiple composite layers to capture the changes of SSTs and generates clear maps of future SSTs. The discriminator uses the structure of patchGAN to obtain more SST features, and distinguishes between real and generated SST maps. In addition, we improve the loss function and perform convergence analysis, and then, obtain that minimizing the loss function is equivalent to minimizing Pearson $\chi 2$ divergence, and the relevant explanations are carried out through experiments. The generator and discriminator are training adversarially during the training stage, eventually reaching a relatively balanced state, and the DGAN is able to produce more reliable visual predictions. Finally, the effectiveness of the DGAN in the prediction of SST is verified experimentally, and it is compared with the generative model-DL model and the long short-term memory-GAN model.",Generators;Sea surface;Convergence;Sea surface temperature;Mathematical models;Training;Generative adversarial networks;Deep generative adversarial network (DGAN);future image generation;prediction;sea surface temperature (SST),"{'month': '', 'issn': '2151-1535', 'doi': '10.1109/JSTARS.2024.3439022', 'keywords': 'Generators;Sea surface;Convergence;Sea surface temperature;Mathematical models;Training;Generative adversarial networks;Deep generative adversarial network (DGAN);future image generation;prediction;sea surface temperature (SST)', 'abstract': 'Sea surface temperature (SST) prediction plays an important role in ocean-related fields. Therefore, it is increasingly important to be able to make more accurate prediction of SST. In this article, we develop a deep generative adversarial network (DGAN) for generating future maps of SSTs, providing a visual method of predicting SSTs. Our DGAN model consists of a generator and a discriminator. The generator is designed to produce more realistic maps of future SSTs, which uses multiple composite layers to capture the changes of SSTs and generates clear maps of future SSTs. The discriminator uses the structure of patchGAN to obtain more SST features, and distinguishes between real and generated SST maps. In addition, we improve the loss function and perform convergence analysis, and then, obtain that minimizing the loss function is equivalent to minimizing Pearson $\\chi 2$ divergence, and the relevant explanations are carried out through experiments. The generator and discriminator are training adversarially during the training stage, eventually reaching a relatively balanced state, and the DGAN is able to produce more reliable visual predictions. Finally, the effectiveness of the DGAN in the prediction of SST is verified experimentally, and it is compared with the generative model-DL model and the long short-term memory-GAN model.', 'pages': '14704-14711', 'number': '', 'volume': '17', 'year': '2024', 'title': 'Sea Surface Temperature Prediction Method Based on Deep Generative Adversarial Network', 'journal': 'IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing', 'author': 'Wang, Jia and Zheng, Gang and Yu, Jiali and Shao, Jinliang and Zhou, Yinfei', 'ENTRYTYPE': 'article', 'ID': '10623893'}"
9585485,Controllable Image Dataset Construction Using Conditionally Transformed Inputs in Generative Adversarial Networks,"Makhmudkhujaev, Farkhod and Kwon, Junseok and Park, In Kyu",Makhmudkhujaev,10.1109/ACCESS.2021.3122834,2021,IEEE Access,"In this paper, we tackle the well-known problem of dataset construction from the point of its generation using generative adversarial networks (GAN). As semantic information of the dataset should have a proper alignment with images, controlling the image generation process of GAN comes to the first position. Considering this, we focus on conditioning the generative process by solely utilizing conditional information to achieve reliable control over the image generation. Unlike the existing works that consider the input (noise or image) in conjunction with conditions, our work considers transforming the input directly to the conditional space by utilizing the given conditions only. By doing so, we reveal the relations between conditions to determine their distinct and reliable feature space without the impact of input information. To fully leverage the conditional information, we propose a novel architectural framework (i.e., conditional transformation) that aims to learn features only from a set of conditions for guiding a generative model by transforming the input to the generator. Such an approach enables controlling the generator by setting its inputs according to the specific conditions necessary for semantically correct image generation. Given that the framework operates at the initial stage of generation, it can be plugged into any existing generative models and trained in an end-to-end manner together with the generator. Extensive experiments on various tasks, such as novel image synthesis and image-to-image translation, demonstrate that the conditional transformation of inputs facilitates solid control over the image generation process and thus shows its applicability for use in dataset construction.",Generators;Image synthesis;Generative adversarial networks;Task analysis;Process control;Reliability;Transforms;Dataset construction;conditional image generation;generative adversarial networks;conditional transformation,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2021.3122834', 'keywords': 'Generators;Image synthesis;Generative adversarial networks;Task analysis;Process control;Reliability;Transforms;Dataset construction;conditional image generation;generative adversarial networks;conditional transformation', 'abstract': 'In this paper, we tackle the well-known problem of dataset construction from the point of its generation using generative adversarial networks (GAN). As semantic information of the dataset should have a proper alignment with images, controlling the image generation process of GAN comes to the first position. Considering this, we focus on conditioning the generative process by solely utilizing conditional information to achieve reliable control over the image generation. Unlike the existing works that consider the input (noise or image) in conjunction with conditions, our work considers transforming the input directly to the conditional space by utilizing the given conditions only. By doing so, we reveal the relations between conditions to determine their distinct and reliable feature space without the impact of input information. To fully leverage the conditional information, we propose a novel architectural framework (i.e., conditional transformation) that aims to learn features only from a set of conditions for guiding a generative model by transforming the input to the generator. Such an approach enables controlling the generator by setting its inputs according to the specific conditions necessary for semantically correct image generation. Given that the framework operates at the initial stage of generation, it can be plugged into any existing generative models and trained in an end-to-end manner together with the generator. Extensive experiments on various tasks, such as novel image synthesis and image-to-image translation, demonstrate that the conditional transformation of inputs facilitates solid control over the image generation process and thus shows its applicability for use in dataset construction.', 'pages': '144699-144712', 'number': '', 'volume': '9', 'year': '2021', 'title': 'Controllable Image Dataset Construction Using Conditionally Transformed Inputs in Generative Adversarial Networks', 'journal': 'IEEE Access', 'author': 'Makhmudkhujaev, Farkhod and Kwon, Junseok and Park, In Kyu', 'ENTRYTYPE': 'article', 'ID': '9585485'}"
10790428,Preface,,,,2024,"Toward Artificial General Intelligence: Deep Learning, Neural Networks, Generative AI",,,"{'url': 'https://ieeexplore-ieee-org.crai.referencistas.com/document/10790428', 'isbn': '9783111324166', 'publisher': 'De Gruyter', 'issn': '', 'doi': '', 'keywords': '', 'abstract': '', 'pages': 'V-VI', 'number': '', 'volume': '', 'year': '2024', 'title': 'Preface', 'booktitle': 'Toward Artificial General Intelligence: Deep Learning, Neural Networks, Generative AI', 'ENTRYTYPE': 'inbook', 'ID': '10790428'}"
10790363,Frontmatter,,,,2024,"Toward Artificial General Intelligence: Deep Learning, Neural Networks, Generative AI",,,"{'url': 'https://ieeexplore-ieee-org.crai.referencistas.com/document/10790363', 'isbn': '9783111324166', 'publisher': 'De Gruyter', 'issn': '', 'doi': '', 'keywords': '', 'abstract': '', 'pages': 'I-IV', 'number': '', 'volume': '', 'year': '2024', 'title': 'Frontmatter', 'booktitle': 'Toward Artificial General Intelligence: Deep Learning, Neural Networks, Generative AI', 'ENTRYTYPE': 'inbook', 'ID': '10790363'}"
10880526,"Use of AI with Optimization Techniques: Case Study, Challenges, and Future Trends","Mittal, Ayushi and Parul, Parul and Gupta, Charu and Tayal, Devendra K",Mittal,10.1002/9781394280735.ch11,2025,Generative Artificial Intelligence for Biomedical and Smart Health Informatics,"Summary <p>From common ailments to more intricate and rare disorders, diseases affect individuals worldwide, impacting their well\&\#x2010;being and necessitating medical attention. The application of artificial intelligence (AI) to the field of disease prediction is the beginning of a revolution in medical technology. Through the application of AI's predictive capabilities, healthcare practitioners may bring in a new era of proactive healthcare management by improving diagnostic accuracy and implementing quick preventive interventions. Utilizing mathematical algorithms to improve and optimize predictive models is a novel method in the field of disease prediction through the application of optimization techniques. This chapter conducts a thorough study of various optimization techniques like flower pollination, differential evolution, and whale optimization. A detailed case study is also conducted on these techniques when they are combined with machine learning. It also covers the challenges faced while applying these optimization techniques to medical disease datasets. A detailed section on emerging future trends is also included to give direction on what can be done with the models.</p>",Diseases;Predictive models;Prediction algorithms;Medical diagnostic imaging;Navigation;Heuristic algorithms;Accuracy;Stochastic processes;Machine learning algorithms;Linear programming,"{'url': 'https://ieeexplore-ieee-org.crai.referencistas.com/document/10880526', 'isbn': '9781394280728', 'publisher': 'IEEE', 'issn': '', 'doi': '10.1002/9781394280735.ch11', 'keywords': 'Diseases;Predictive models;Prediction algorithms;Medical diagnostic imaging;Navigation;Heuristic algorithms;Accuracy;Stochastic processes;Machine learning algorithms;Linear programming', 'abstract': ""Summary <p>From common ailments to more intricate and rare disorders, diseases affect individuals worldwide, impacting their well\\&\\#x2010;being and necessitating medical attention. The application of artificial intelligence (AI) to the field of disease prediction is the beginning of a revolution in medical technology. Through the application of AI's predictive capabilities, healthcare practitioners may bring in a new era of proactive healthcare management by improving diagnostic accuracy and implementing quick preventive interventions. Utilizing mathematical algorithms to improve and optimize predictive models is a novel method in the field of disease prediction through the application of optimization techniques. This chapter conducts a thorough study of various optimization techniques like flower pollination, differential evolution, and whale optimization. A detailed case study is also conducted on these techniques when they are combined with machine learning. It also covers the challenges faced while applying these optimization techniques to medical disease datasets. A detailed section on emerging future trends is also included to give direction on what can be done with the models.</p>"", 'pages': '209-238', 'number': '', 'volume': '', 'year': '2025', 'title': 'Use of AI with Optimization Techniques: Case Study, Challenges, and Future Trends', 'booktitle': 'Generative Artificial Intelligence for Biomedical and Smart Health Informatics', 'author': 'Mittal, Ayushi and Parul, Parul and Gupta, Charu and Tayal, Devendra K', 'ENTRYTYPE': 'inbook', 'ID': '10880526'}"
10952190,Acknowledgments,"Musiol, Martin",Musiol,,2024,Generative AI: Navigating the Course to the Artificial General Intelligence Future,,,"{'url': 'https://ieeexplore-ieee-org.crai.referencistas.com/document/10952190', 'isbn': '9781394205950', 'publisher': 'Wiley', 'issn': '', 'doi': '', 'keywords': '', 'abstract': '', 'pages': '405-406', 'number': '', 'volume': '', 'year': '2024', 'title': 'Acknowledgments', 'booktitle': 'Generative AI: Navigating the Course to the Artificial General Intelligence Future', 'author': 'Musiol, Martin', 'ENTRYTYPE': 'inbook', 'ID': '10952190'}"
10951024,About the Author,"Musiol, Martin",Musiol,,2024,Generative AI: Navigating the Course to the Artificial General Intelligence Future,,,"{'url': 'https://ieeexplore-ieee-org.crai.referencistas.com/document/10951024', 'isbn': '9781394205950', 'publisher': 'Wiley', 'issn': '', 'doi': '', 'keywords': '', 'abstract': '', 'pages': '407-407', 'number': '', 'volume': '', 'year': '2024', 'title': 'About the Author', 'booktitle': 'Generative AI: Navigating the Course to the Artificial General Intelligence Future', 'author': 'Musiol, Martin', 'ENTRYTYPE': 'inbook', 'ID': '10951024'}"
10400626,Random noise suppression of seismic data with generative adversarial network,"Wang, L. and Wang, J. and Wang, Z. and Liu, P.",Wang,10.1049/icp.2023.2968,2023,5th International Conference on Artificial Intelligence and Advanced Manufacturing (AIAM 2023),"Due to the complex nature of the seismic exploration environment, there is a significant amount of noise in the data gathering process, resulting in various factors that can affect the quality of the data. Standard denoising techniques and models are often insufficient in meeting the demand for high-quality seismic data. However, with the advancement of artificial intelligence, Generative Adversarial Network (GAN) has emerged as an effective solution for seismic data denoising. The GAN-based algorithm for random noise reduction in seismic data involves a generator and discriminator. The generator uses noisy seismic data as input to produce denoised data, while the discriminator evaluates the authenticity of the data. Through game training between the generator and discriminator, this method has been proven to successfully remove random noise from synthetic seismic data. Furthermore, experiments comparing this method to the conventional CNN method on real seismic data using evaluation indices such as data visualization and signal-to-noise ratio have demonstrated its effectiveness. This method shows great potential for future research and application in seismic data denoising.",,"{'month': 'Oct', 'issn': '', 'doi': '10.1049/icp.2023.2968', 'keywords': '', 'abstract': 'Due to the complex nature of the seismic exploration environment, there is a significant amount of noise in the data gathering process, resulting in various factors that can affect the quality of the data. Standard denoising techniques and models are often insufficient in meeting the demand for high-quality seismic data. However, with the advancement of artificial intelligence, Generative Adversarial Network (GAN) has emerged as an effective solution for seismic data denoising. The GAN-based algorithm for random noise reduction in seismic data involves a generator and discriminator. The generator uses noisy seismic data as input to produce denoised data, while the discriminator evaluates the authenticity of the data. Through game training between the generator and discriminator, this method has been proven to successfully remove random noise from synthetic seismic data. Furthermore, experiments comparing this method to the conventional CNN method on real seismic data using evaluation indices such as data visualization and signal-to-noise ratio have demonstrated its effectiveness. This method shows great potential for future research and application in seismic data denoising.', 'pages': '401-406', 'number': '', 'volume': '2023', 'year': '2023', 'title': 'Random noise suppression of seismic data with generative adversarial network', 'booktitle': '5th International Conference on Artificial Intelligence and Advanced Manufacturing (AIAM 2023)', 'author': 'Wang, L. and Wang, J. and Wang, Z. and Liu, P.', 'ENTRYTYPE': 'inproceedings', 'ID': '10400626'}"
10790418,Contents,,,,2024,"Toward Artificial General Intelligence: Deep Learning, Neural Networks, Generative AI",,,"{'url': 'https://ieeexplore-ieee-org.crai.referencistas.com/document/10790418', 'isbn': '9783111324166', 'publisher': 'De Gruyter', 'issn': '', 'doi': '', 'keywords': '', 'abstract': '', 'pages': 'VII-VIII', 'number': '', 'volume': '', 'year': '2024', 'title': 'Contents', 'booktitle': 'Toward Artificial General Intelligence: Deep Learning, Neural Networks, Generative AI', 'ENTRYTYPE': 'inbook', 'ID': '10790418'}"
10219618,Generative Iris Prior Embedded Transformer for Iris Restoration,"Huang, Yubo and Wang, Jia and Li, Peipei and Xiang, Liuyu and Li, Peigang and He, Zhaofeng",Huang,10.1109/ICME55011.2023.00094,2023,2023 IEEE International Conference on Multimedia and Expo (ICME),"Iris restoration from complexly degraded iris images, aiming to improve iris recognition performance, is a challenging problem. Due to the complex degradation, directly training a convolutional neural network (CNN) without prior cannot yield satisfactory results. In this work, we propose a generative iris prior embedded Transformer model (Gformer), in which we build a hierarchical encoder-decoder network employing Transformer block and generative iris prior. First, we tame Transformer blocks to model long-range dependencies in target images. Second, we pretrain an iris generative adversarial network (GAN) to obtain the rich iris prior, and incorporate it into the iris restoration process with our iris feature modulator. Our experiments demonstrate that the proposed Gformer outperforms state-of-the-art methods. Besides, iris recognition performance has been significantly improved after applying Gformer.",Training;Representation learning;Modulation;Transformers;Feature extraction;Generative adversarial networks;Image restoration;Iris restoration;Image restoration;Prior knowledge;Iris recognition,"{'month': 'July', 'issn': '1945-788X', 'doi': '10.1109/ICME55011.2023.00094', 'keywords': 'Training;Representation learning;Modulation;Transformers;Feature extraction;Generative adversarial networks;Image restoration;Iris restoration;Image restoration;Prior knowledge;Iris recognition', 'abstract': 'Iris restoration from complexly degraded iris images, aiming to improve iris recognition performance, is a challenging problem. Due to the complex degradation, directly training a convolutional neural network (CNN) without prior cannot yield satisfactory results. In this work, we propose a generative iris prior embedded Transformer model (Gformer), in which we build a hierarchical encoder-decoder network employing Transformer block and generative iris prior. First, we tame Transformer blocks to model long-range dependencies in target images. Second, we pretrain an iris generative adversarial network (GAN) to obtain the rich iris prior, and incorporate it into the iris restoration process with our iris feature modulator. Our experiments demonstrate that the proposed Gformer outperforms state-of-the-art methods. Besides, iris recognition performance has been significantly improved after applying Gformer.', 'pages': '510-515', 'number': '', 'volume': '', 'year': '2023', 'title': 'Generative Iris Prior Embedded Transformer for Iris Restoration', 'booktitle': '2023 IEEE International Conference on Multimedia and Expo (ICME)', 'author': 'Huang, Yubo and Wang, Jia and Li, Peipei and Xiang, Liuyu and Li, Peigang and He, Zhaofeng', 'ENTRYTYPE': 'inproceedings', 'ID': '10219618'}"
10589960,Large Language Models in Education: A Systematic Review,"Dong, Bingyu and Bai, Jie and Xu, Tao and Zhou, Yun",Dong,10.1109/CSTE62025.2024.00031,2024,2024 6th International Conference on Computer Science and Technologies in Education (CSTE),"Large Language Models (LLMs) refer to a type of generative artificial intelligence model that produces responses to natural language input. The purpose of this study is to analyze the current application status of LLMs in the field of education through a systematic review of the literature. Data were sourced from three databases: Web of Science, ERIC, and Google Scholar. The study includes 94 documents, analyzed from both qualitative and quantitative perspectives. The results show that large language models have great potential in the field of education, specifically in generating medical content, serving as an English learning assistant, assisting academic research, and evaluating the quality of tests, etc. However, there are still potential dangers such as hindering the development of critical thinking, creating academic integrity crises, and ethical and moral challenges. These findings showed the current application status of LLMs in education, laying the groundwork to inspire future research.",Ethics;Systematics;Reviews;Generative AI;Large language models;Education;Natural languages;large language model;ChatGPT;artificial intelligence;education;systematic review,"{'month': 'April', 'issn': '', 'doi': '10.1109/CSTE62025.2024.00031', 'keywords': 'Ethics;Systematics;Reviews;Generative AI;Large language models;Education;Natural languages;large language model;ChatGPT;artificial intelligence;education;systematic review', 'abstract': 'Large Language Models (LLMs) refer to a type of generative artificial intelligence model that produces responses to natural language input. The purpose of this study is to analyze the current application status of LLMs in the field of education through a systematic review of the literature. Data were sourced from three databases: Web of Science, ERIC, and Google Scholar. The study includes 94 documents, analyzed from both qualitative and quantitative perspectives. The results show that large language models have great potential in the field of education, specifically in generating medical content, serving as an English learning assistant, assisting academic research, and evaluating the quality of tests, etc. However, there are still potential dangers such as hindering the development of critical thinking, creating academic integrity crises, and ethical and moral challenges. These findings showed the current application status of LLMs in education, laying the groundwork to inspire future research.', 'pages': '131-134', 'number': '', 'volume': '', 'year': '2024', 'title': 'Large Language Models in Education: A Systematic Review', 'booktitle': '2024 6th International Conference on Computer Science and Technologies in Education (CSTE)', 'author': 'Dong, Bingyu and Bai, Jie and Xu, Tao and Zhou, Yun', 'ENTRYTYPE': 'inproceedings', 'ID': '10589960'}"
10538552,Automatic Intelligent Chronic Kidney Disease Detection in Healthcare 5.0,"Tian, Geng and Rehman, Amir and Xing, Huanlai and Feng, Li and Gulzar, Nighat and Hussain, Abid",Tian,10.1109/TrustCom60117.2023.00297,2023,"2023 IEEE 22nd International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom)","Health systems worldwide have an unprecedented opportunity to enhance healthcare service delivery due to the rapid development of emerging digital technologies. Many advancements have been made in the medical field, with deep learning proving particularly useful when applied to a large enough number of well-defined samples. Although, this aspect may make deep learning harder to implement in settings with limited-size datasets. In this study, we present a new method of chronic kidney disease detection (CKDD) by combining Generative Adversarial Networks (GAN) with Convolutional Neural Networks (CNN). Afterward, synthetic sample data was created using GAN, which enlarged the dataset. Subsequently, processing these synthetic samples, the CNN classifier was applied. According to experimental assessments, the suggested CKDD-GAN methodology accuracy is superior to without the GAN technique. Moreover, the proposed CKDD-GAN-based model outperformed with an accuracy of 98.10\%. Even though standard synthetic data samples seemed to improve classification performance, GAN-based enhancements resulted in a 2.91\% improvement. GAN implementations for detecting chronic kidney disease are highly beneficial since they also increase awareness about its possible uses in various other diseases.",Deep learning;Privacy;Medical services;Generative adversarial networks;Chronic kidney disease;Convolutional neural networks;Security;Healthcare5.0;Chronic kidney disease;Detection;IoMT;Generative Adversarial Network,"{'month': 'Nov', 'issn': '2324-9013', 'doi': '10.1109/TrustCom60117.2023.00297', 'keywords': 'Deep learning;Privacy;Medical services;Generative adversarial networks;Chronic kidney disease;Convolutional neural networks;Security;Healthcare5.0;Chronic kidney disease;Detection;IoMT;Generative Adversarial Network', 'abstract': 'Health systems worldwide have an unprecedented opportunity to enhance healthcare service delivery due to the rapid development of emerging digital technologies. Many advancements have been made in the medical field, with deep learning proving particularly useful when applied to a large enough number of well-defined samples. Although, this aspect may make deep learning harder to implement in settings with limited-size datasets. In this study, we present a new method of chronic kidney disease detection (CKDD) by combining Generative Adversarial Networks (GAN) with Convolutional Neural Networks (CNN). Afterward, synthetic sample data was created using GAN, which enlarged the dataset. Subsequently, processing these synthetic samples, the CNN classifier was applied. According to experimental assessments, the suggested CKDD-GAN methodology accuracy is superior to without the GAN technique. Moreover, the proposed CKDD-GAN-based model outperformed with an accuracy of 98.10\\%. Even though standard synthetic data samples seemed to improve classification performance, GAN-based enhancements resulted in a 2.91\\% improvement. GAN implementations for detecting chronic kidney disease are highly beneficial since they also increase awareness about its possible uses in various other diseases.', 'pages': '2134-2140', 'number': '', 'volume': '', 'year': '2023', 'title': 'Automatic Intelligent Chronic Kidney Disease Detection in Healthcare 5.0', 'booktitle': '2023 IEEE 22nd International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom)', 'author': 'Tian, Geng and Rehman, Amir and Xing, Huanlai and Feng, Li and Gulzar, Nighat and Hussain, Abid', 'ENTRYTYPE': 'inproceedings', 'ID': '10538552'}"
10578788,Online Course Improvement Through GPT-4: Monitoring Student Engagement and Dynamic FAQ Generation,"Lundström, Oxana and Maleki, Neda and Ahlgren, Fredrik",Lundström,10.1109/EDUCON60312.2024.10578788,2024,2024 IEEE Global Engineering Education Conference (EDUCON),"Artificial Intelligence (AI), specifically in language processing, is increasingly recognized as an invaluable educational tool. The Large Language Model (LLM) GPT, developed by OpenAI, is an advanced machine learning tool that utilizes deep learning for human-like text comprehension and generation. This study uses OpenAI's GPT-4 to enhance an online Internet of Things (loT) course at Linnaeus University. We analyzed 12,000+ messages on an online communication platform spanning four years. We compare traditional Natural Language Processing (NLP) techniques to Generative AI for understanding student feedback and issues, inspiring project ideas, and promoting student engagement. We provide a combined approach to monitor the sentiment or mood of the students' communications over the timeline of the course. Moreover, we show how to use LLM to refine the FAQ generation and decipher student feedback for course refinement. We demonstrate how to generate optimal prompts and prepare the data to apply LLMs effectively. Our research reinforces that strategic use of LLMs, like GPT-4, can revolutionize remote learning by lessening lecturer workload and boosting student satisfaction and engagement. Our future work aims to further leverage AI models across remote engineering education. One potential direction is developing an AI-powered bot for online platforms to facilitate real-time interaction, manage queries, encourage engagement, maintain FAQs, and enhance course outcomes.",Sentiment analysis;Generative AI;Mood;Distance learning;Large language models;Real-time systems;Internet of Things;Artificial Intelligence (AI);education;Natural Language Processing (NLP);course feedback;FAQs,"{'month': 'May', 'issn': '2165-9567', 'doi': '10.1109/EDUCON60312.2024.10578788', 'keywords': 'Sentiment analysis;Generative AI;Mood;Distance learning;Large language models;Real-time systems;Internet of Things;Artificial Intelligence (AI);education;Natural Language Processing (NLP);course feedback;FAQs', 'abstract': ""Artificial Intelligence (AI), specifically in language processing, is increasingly recognized as an invaluable educational tool. The Large Language Model (LLM) GPT, developed by OpenAI, is an advanced machine learning tool that utilizes deep learning for human-like text comprehension and generation. This study uses OpenAI's GPT-4 to enhance an online Internet of Things (loT) course at Linnaeus University. We analyzed 12,000+ messages on an online communication platform spanning four years. We compare traditional Natural Language Processing (NLP) techniques to Generative AI for understanding student feedback and issues, inspiring project ideas, and promoting student engagement. We provide a combined approach to monitor the sentiment or mood of the students' communications over the timeline of the course. Moreover, we show how to use LLM to refine the FAQ generation and decipher student feedback for course refinement. We demonstrate how to generate optimal prompts and prepare the data to apply LLMs effectively. Our research reinforces that strategic use of LLMs, like GPT-4, can revolutionize remote learning by lessening lecturer workload and boosting student satisfaction and engagement. Our future work aims to further leverage AI models across remote engineering education. One potential direction is developing an AI-powered bot for online platforms to facilitate real-time interaction, manage queries, encourage engagement, maintain FAQs, and enhance course outcomes."", 'pages': '1-6', 'number': '', 'volume': '', 'year': '2024', 'title': 'Online Course Improvement Through GPT-4: Monitoring Student Engagement and Dynamic FAQ Generation', 'booktitle': '2024 IEEE Global Engineering Education Conference (EDUCON)', 'author': 'Lundström, Oxana and Maleki, Neda and Ahlgren, Fredrik', 'ENTRYTYPE': 'inproceedings', 'ID': '10578788'}"
10759962,Development of a Virtual Patient Model for Kampo Medical Interview: New Approach for Enhancing Empathy and Understanding of Kampo Medicine Pathological Concepts,"Takata, Tomoki and Yamada, Rie and Oliveira Nzinga René, António and Xu, Kuangzhe and Fujimoto, Makoto",Takata,10.1109/SCISISIS61014.2024.10759962,2024,2024 Joint 13th International Conference on Soft Computing and Intelligent Systems and 25th International Symposium on Advanced Intelligent Systems (SCIS\&ISIS),"Global interest in complementary and alternative medicine has increased in recent years, with Kampo medicine in Japan gaining greater trust and use. Detailed patient interviews are essential in Kampo medicine, as the physician's empathy is critical to diagnostic precision. Typically, medical students develop empathy and deepen their understanding of Kampo's pathological concepts through clinical practice. However, the COVID-19 pandemic has imposed significant restrictions on clinical training. To address this challenge, we propose a novel educational approach to enhance empathy and understanding of Kampo medicine by developing a virtual patient application. This application leverages generative artificial intelligence to simulate realistic patient interactions, enabling students to practice Kampo medical interviews in a safe, controlled environment. The AI-generated conversations are designed to reflect the emotional nuances of real-life dialogue, with the virtual patients' facial expressions synchronized to these emotions, thus enhancing the realism of the training. The suggested method allows repeated practice at any time and fosters the development of essential diag-nostic and empathetic skills. While promising challenges remain in improving these simulations' accuracy, further refinements are still under consideration.",Training;COVID-19;Pathology;Emotion recognition;Pandemics;Generative AI;Medical services;Synchronization;Interviews;Medical diagnostic imaging;Kampo medical interview;Empathy;Virtual patient;Medical student;Artificial intelligence,"{'month': 'Nov', 'issn': '', 'doi': '10.1109/SCISISIS61014.2024.10759962', 'keywords': 'Training;COVID-19;Pathology;Emotion recognition;Pandemics;Generative AI;Medical services;Synchronization;Interviews;Medical diagnostic imaging;Kampo medical interview;Empathy;Virtual patient;Medical student;Artificial intelligence', 'abstract': ""Global interest in complementary and alternative medicine has increased in recent years, with Kampo medicine in Japan gaining greater trust and use. Detailed patient interviews are essential in Kampo medicine, as the physician's empathy is critical to diagnostic precision. Typically, medical students develop empathy and deepen their understanding of Kampo's pathological concepts through clinical practice. However, the COVID-19 pandemic has imposed significant restrictions on clinical training. To address this challenge, we propose a novel educational approach to enhance empathy and understanding of Kampo medicine by developing a virtual patient application. This application leverages generative artificial intelligence to simulate realistic patient interactions, enabling students to practice Kampo medical interviews in a safe, controlled environment. The AI-generated conversations are designed to reflect the emotional nuances of real-life dialogue, with the virtual patients' facial expressions synchronized to these emotions, thus enhancing the realism of the training. The suggested method allows repeated practice at any time and fosters the development of essential diag-nostic and empathetic skills. While promising challenges remain in improving these simulations' accuracy, further refinements are still under consideration."", 'pages': '1-5', 'number': '', 'volume': '', 'year': '2024', 'title': 'Development of a Virtual Patient Model for Kampo Medical Interview: New Approach for Enhancing Empathy and Understanding of Kampo Medicine Pathological Concepts', 'booktitle': '2024 Joint 13th International Conference on Soft Computing and Intelligent Systems and 25th International Symposium on Advanced Intelligent Systems (SCIS\\&ISIS)', 'author': 'Takata, Tomoki and Yamada, Rie and Oliveira Nzinga René, António and Xu, Kuangzhe and Fujimoto, Makoto', 'ENTRYTYPE': 'inproceedings', 'ID': '10759962'}"
10970365,There's no Human in Charge: Playing Chef's Hat with a Large Language Model Based Agent,"Pereira, Alexandre and Fernandes, Bruno and Barros, Pablo",Pereira,10.1109/ACIIW63320.2024.00030,2024,2024 12th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW),"Generative artificial intelligence (AI), especially large language models (LLMs), has greatly improved the creation of digital content. This study examines the use of an LLM, specifically Llama-3-8B, in playing Chef's Hat, a multiplayer card game. Traditionally, reinforcement learning (RL) agents used in this game do not perform well against human players. Our goal was to see if the LLM could play the game effectively without additional training. We integrated the Llama-3-8B model into the Chef's Hat game and compared its performance with three existing agents: Random, DQL (Deep Q-Learning), and PPO (Proximal Policy Optimization). Using special instructions, we guided the LLM on how to play the game and make decisions. The results showed that the LLM-based agent performed as well as or better than the RL agents, making smart and strategic moves based on the game rules and current situation. This study highlights the adaptability and potential of LLMs in various competitive settings, suggesting they could be used as effective autonomous agents in human-robot interactions. Future work should explore combining LLMs with other types of agents, testing them in different games, and improving the instructions given to them.",Training;Ethics;Affective computing;Generative AI;Large language models;Human-robot interaction;Games;Real-time systems;Standards;Testing;artificial intelligence;large language models;game playing;human-robot interaction,"{'month': 'Sep.', 'issn': '', 'doi': '10.1109/ACIIW63320.2024.00030', 'keywords': 'Training;Ethics;Affective computing;Generative AI;Large language models;Human-robot interaction;Games;Real-time systems;Standards;Testing;artificial intelligence;large language models;game playing;human-robot interaction', 'abstract': ""Generative artificial intelligence (AI), especially large language models (LLMs), has greatly improved the creation of digital content. This study examines the use of an LLM, specifically Llama-3-8B, in playing Chef's Hat, a multiplayer card game. Traditionally, reinforcement learning (RL) agents used in this game do not perform well against human players. Our goal was to see if the LLM could play the game effectively without additional training. We integrated the Llama-3-8B model into the Chef's Hat game and compared its performance with three existing agents: Random, DQL (Deep Q-Learning), and PPO (Proximal Policy Optimization). Using special instructions, we guided the LLM on how to play the game and make decisions. The results showed that the LLM-based agent performed as well as or better than the RL agents, making smart and strategic moves based on the game rules and current situation. This study highlights the adaptability and potential of LLMs in various competitive settings, suggesting they could be used as effective autonomous agents in human-robot interactions. Future work should explore combining LLMs with other types of agents, testing them in different games, and improving the instructions given to them."", 'pages': '149-153', 'number': '', 'volume': '', 'year': '2024', 'title': ""There's no Human in Charge: Playing Chef's Hat with a Large Language Model Based Agent"", 'booktitle': '2024 12th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)', 'author': 'Pereira, Alexandre and Fernandes, Bruno and Barros, Pablo', 'ENTRYTYPE': 'inproceedings', 'ID': '10970365'}"
11114593,A Fair and Trustworthy Remuneration Framework for AI Model Training Using DLT,"Bonnet, Severin and Di Francesco Maesa, Damiano and Loporchio, Matteo and Tietze, Frank",Bonnet,10.1109/ICBC64466.2025.11114593,2025,2025 IEEE International Conference on Blockchain and Cryptocurrency (ICBC),"Currently, artificial intelligence (AI) models – particularly those of, but not limited to, Large Language Models – are trained over large amounts of data. Training often happens with little consideration, and even less remuneration, of input data that is content protected by Intellectual Property (IP) rights (e.g., copyrights). The recent rise in sophistication and popularity of generative AI models has further highlighted this issue, as traditional IP licensing models remain largely inadequate. In this paper, we present a proof of concept for an automated, fair, and trustworthy remuneration system for AI model training data contributors leveraging Distributed Ledger Technology. We propose the use of attribution methods for rewarding the most relevant sources for any given request, and smart contracts for the enforcement of the mutually beneficial revenue-sharing agreements between the model creator and training data copyright holders.",Training;Distributed ledger;Generative AI;Large language models;Smart contracts;Training data;Intellectual property;Data models;Blockchains;Remuneration;Intellectual Property;Blockchain;Smart Contract;Artificial Intelligence;Large Language Model;Explainable AI,"{'month': 'June', 'issn': '2832-8906', 'doi': '10.1109/ICBC64466.2025.11114593', 'keywords': 'Training;Distributed ledger;Generative AI;Large language models;Smart contracts;Training data;Intellectual property;Data models;Blockchains;Remuneration;Intellectual Property;Blockchain;Smart Contract;Artificial Intelligence;Large Language Model;Explainable AI', 'abstract': 'Currently, artificial intelligence (AI) models – particularly those of, but not limited to, Large Language Models – are trained over large amounts of data. Training often happens with little consideration, and even less remuneration, of input data that is content protected by Intellectual Property (IP) rights (e.g., copyrights). The recent rise in sophistication and popularity of generative AI models has further highlighted this issue, as traditional IP licensing models remain largely inadequate. In this paper, we present a proof of concept for an automated, fair, and trustworthy remuneration system for AI model training data contributors leveraging Distributed Ledger Technology. We propose the use of attribution methods for rewarding the most relevant sources for any given request, and smart contracts for the enforcement of the mutually beneficial revenue-sharing agreements between the model creator and training data copyright holders.', 'pages': '1-5', 'number': '', 'volume': '', 'year': '2025', 'title': 'A Fair and Trustworthy Remuneration Framework for AI Model Training Using DLT', 'booktitle': '2025 IEEE International Conference on Blockchain and Cryptocurrency (ICBC)', 'author': 'Bonnet, Severin and Di Francesco Maesa, Damiano and Loporchio, Matteo and Tietze, Frank', 'ENTRYTYPE': 'inproceedings', 'ID': '11114593'}"
10937960,Robust Intrusion Detection With Combinatorial Fusion and Generative Artificial Intelligence,"Owusu, Evans and Mapkar, Mariyam and Rahouti, Mohamed and Verma, Dinesh C.",Owusu,10.1109/MC.2024.3524302,2025,Computer,"This article proposes an advanced intrusion detection system that combines combinatorial fusion analysis with generative artificial intelligence to improve anomaly detection in intelligent systems. It addresses challenges in detecting low-profile and evolving threats, especially in imbalanced datasets.",Generative AI;Computer hacking;Intrusion detection;Intelligent systems;Anomaly detection,"{'month': 'April', 'issn': '1558-0814', 'doi': '10.1109/MC.2024.3524302', 'keywords': 'Generative AI;Computer hacking;Intrusion detection;Intelligent systems;Anomaly detection', 'abstract': 'This article proposes an advanced intrusion detection system that combines combinatorial fusion analysis with generative artificial intelligence to improve anomaly detection in intelligent systems. It addresses challenges in detecting low-profile and evolving threats, especially in imbalanced datasets.', 'pages': '46-57', 'number': '4', 'volume': '58', 'year': '2025', 'title': 'Robust Intrusion Detection With Combinatorial Fusion and Generative Artificial Intelligence', 'journal': 'Computer', 'author': 'Owusu, Evans and Mapkar, Mariyam and Rahouti, Mohamed and Verma, Dinesh C.', 'ENTRYTYPE': 'article', 'ID': '10937960'}"
10812969,Generative Artificial Intelligence for Mobile Communications: A Diffusion Model Perspective,"Xu, Xiaoxia and Mu, Xidong and Liu, Yuanwei and Xing, Hong and Liu, Yue and Nallanathan, Arumugam",Xu,10.1109/MCOM.001.2400284,2025,IEEE Communications Magazine,"This article highlights the potential of a prominent generative artificial intelligence (GAI) method, namely diffusion model (DM), for mobile communications. First, we propose a DM-driven communication architecture which introduces two key paradigms, that is, conditional DM, and DM-driven deep reinforcement learning (DRL), for wireless data generation and communication management, respectively. Then, we discuss the key advantages of the DM-driven communication paradigms. To elaborate further, we explore DM-driven channel generation mechanisms for channel estimation, extrapolation, and feedback in multiple-input multiple-output (MIMO) systems. We showcase the numerical performance of conditional DM using the accurate DeepMIMO channel datasets, revealing its superiority in generating high-fidelity channels and mitigating unforeseen distribution shifts in sophisticated scenes. Furthermore, several DM-driven communication management designs promising to deal with imperfect channels and task-oriented communications are conceived. To inspire future research developments, we highlight the potential applications and open research challenges of DM-driven communications. Code is available at https://github.com/xiaoxiaxusummer/GAI\_CDMM/.",Wireless communication;Noise reduction;Channel estimation;Noise measurement;Data models;Mobile communication;Data collection;Stochastic processes;Extrapolation;Trajectory;Generative AI,"{'month': 'July', 'issn': '1558-1896', 'doi': '10.1109/MCOM.001.2400284', 'keywords': 'Wireless communication;Noise reduction;Channel estimation;Noise measurement;Data models;Mobile communication;Data collection;Stochastic processes;Extrapolation;Trajectory;Generative AI', 'abstract': 'This article highlights the potential of a prominent generative artificial intelligence (GAI) method, namely diffusion model (DM), for mobile communications. First, we propose a DM-driven communication architecture which introduces two key paradigms, that is, conditional DM, and DM-driven deep reinforcement learning (DRL), for wireless data generation and communication management, respectively. Then, we discuss the key advantages of the DM-driven communication paradigms. To elaborate further, we explore DM-driven channel generation mechanisms for channel estimation, extrapolation, and feedback in multiple-input multiple-output (MIMO) systems. We showcase the numerical performance of conditional DM using the accurate DeepMIMO channel datasets, revealing its superiority in generating high-fidelity channels and mitigating unforeseen distribution shifts in sophisticated scenes. Furthermore, several DM-driven communication management designs promising to deal with imperfect channels and task-oriented communications are conceived. To inspire future research developments, we highlight the potential applications and open research challenges of DM-driven communications. Code is available at https://github.com/xiaoxiaxusummer/GAI\\_CDMM/.', 'pages': '98-105', 'number': '7', 'volume': '63', 'year': '2025', 'title': 'Generative Artificial Intelligence for Mobile Communications: A Diffusion Model Perspective', 'journal': 'IEEE Communications Magazine', 'author': 'Xu, Xiaoxia and Mu, Xidong and Liu, Yuanwei and Xing, Hong and Liu, Yue and Nallanathan, Arumugam', 'ENTRYTYPE': 'article', 'ID': '10812969'}"
11108609,MCTA4R: Enhancing User Preference Alignment with Meta-CoT and Search Algorithms,"Huang, Ruilong and Li, Bohan and Zhao, Xinzhe and Xu, Mengfei and Wu, Wenlong and Zhu, Qi",Huang,10.1109/PRMVAI65741.2025.11108609,2025,"2025 IEEE International Conference on Pattern Recognition, Machine Vision and Artificial Intelligence (PRMVAI)","Personalized recommendation systems often struggle with accurately predicting long-term user preferences, particularly in dynamic and sparse environments. Aligning user preferences is a key issue in LLM-driven recommendation systems with generative agents. To address this, we propose the Meta-CoT-Agent4Rec (MCTA4R) framework, which integrates the Meta-CoT reasoning process with $A^{*}$ and Monte Carlo Tree Search (MCTS) algorithms. The $A^{*}$ search algorithm leverages a heuristic strategy to guide the model towards optimal recommendations by expanding nodes based on cost evaluations, ensuring relevance in structured data environments. Meanwhile, the MCTS utilizes random simulations and backpropagation to explore diverse recommendation paths, adapting to dynamic and sparse data scenarios. The framework was evaluated using the MovieLens-1M, MovieLens-10M, Amazon-Book, and Steam datasets with different interaction ratios. Experimental results demonstrate that both $\mathrm{A}^{*}$ Meta-CoT and MCTS Meta-CoT significantly outperform traditional few-shot learning methods in terms of accuracy, recall and F1 score.",Adaptation models;Monte Carlo methods;Heuristic algorithms;Machine vision;Prediction algorithms;Data models;Cognition;Pattern recognition;User preference;Recommender systems;Recommendation System;Generative Agents;User Preference Alignment;Search Algorithm,"{'month': 'June', 'issn': '', 'doi': '10.1109/PRMVAI65741.2025.11108609', 'keywords': 'Adaptation models;Monte Carlo methods;Heuristic algorithms;Machine vision;Prediction algorithms;Data models;Cognition;Pattern recognition;User preference;Recommender systems;Recommendation System;Generative Agents;User Preference Alignment;Search Algorithm', 'abstract': 'Personalized recommendation systems often struggle with accurately predicting long-term user preferences, particularly in dynamic and sparse environments. Aligning user preferences is a key issue in LLM-driven recommendation systems with generative agents. To address this, we propose the Meta-CoT-Agent4Rec (MCTA4R) framework, which integrates the Meta-CoT reasoning process with $A^{*}$ and Monte Carlo Tree Search (MCTS) algorithms. The $A^{*}$ search algorithm leverages a heuristic strategy to guide the model towards optimal recommendations by expanding nodes based on cost evaluations, ensuring relevance in structured data environments. Meanwhile, the MCTS utilizes random simulations and backpropagation to explore diverse recommendation paths, adapting to dynamic and sparse data scenarios. The framework was evaluated using the MovieLens-1M, MovieLens-10M, Amazon-Book, and Steam datasets with different interaction ratios. Experimental results demonstrate that both $\\mathrm{A}^{*}$ Meta-CoT and MCTS Meta-CoT significantly outperform traditional few-shot learning methods in terms of accuracy, recall and F1 score.', 'pages': '1-5', 'number': '', 'volume': '', 'year': '2025', 'title': 'MCTA4R: Enhancing User Preference Alignment with Meta-CoT and Search Algorithms', 'booktitle': '2025 IEEE International Conference on Pattern Recognition, Machine Vision and Artificial Intelligence (PRMVAI)', 'author': 'Huang, Ruilong and Li, Bohan and Zhao, Xinzhe and Xu, Mengfei and Wu, Wenlong and Zhu, Qi', 'ENTRYTYPE': 'inproceedings', 'ID': '11108609'}"
9421640,IInfoGAN: Improved Information Maximizing Generative Adversarial Networks,"Wang, Yan and Wang, Pujia and Sun, Boyang and He, Kai and Huang, Lan",Wang,10.1109/ICMCCE51767.2020.00326,2020,"2020 5th International Conference on Mechanical, Control and Computer Engineering (ICMCCE)","Generative Adversarial Networks (GANs) have achieved huge success in some unsupervised learning fields. There is no doubt that clustering takes a lot of weight in unsupervised algorithm. And in this paper, we raise the Improved Information Maximizing Generative Adversarial Networks (IInfoGAN) algorithm for learning discriminative classifiers from unlabeled data. The basis of our method is an math function that contains the Mutual Information (MI) and Cross Entropy of the observed examples and their predicted classification category distribution, thus enhancing the robustness of the classifier to adversarial generative models. Experiments show that the interpretable representation learned by IInfoGAN is competitive with the representation learned by existing unsupervised methods.",Training;Time series analysis;Clustering algorithms;Predictive models;Generative adversarial networks;Prediction algorithms;Entropy;component;clustering;GAN;InfoGAN;fashion-mnist,"{'month': 'Dec', 'issn': '', 'doi': '10.1109/ICMCCE51767.2020.00326', 'keywords': 'Training;Time series analysis;Clustering algorithms;Predictive models;Generative adversarial networks;Prediction algorithms;Entropy;component;clustering;GAN;InfoGAN;fashion-mnist', 'abstract': 'Generative Adversarial Networks (GANs) have achieved huge success in some unsupervised learning fields. There is no doubt that clustering takes a lot of weight in unsupervised algorithm. And in this paper, we raise the Improved Information Maximizing Generative Adversarial Networks (IInfoGAN) algorithm for learning discriminative classifiers from unlabeled data. The basis of our method is an math function that contains the Mutual Information (MI) and Cross Entropy of the observed examples and their predicted classification category distribution, thus enhancing the robustness of the classifier to adversarial generative models. Experiments show that the interpretable representation learned by IInfoGAN is competitive with the representation learned by existing unsupervised methods.', 'pages': '1487-1490', 'number': '', 'volume': '', 'year': '2020', 'title': 'IInfoGAN: Improved Information Maximizing Generative Adversarial Networks', 'booktitle': '2020 5th International Conference on Mechanical, Control and Computer Engineering (ICMCCE)', 'author': 'Wang, Yan and Wang, Pujia and Sun, Boyang and He, Kai and Huang, Lan', 'ENTRYTYPE': 'inproceedings', 'ID': '9421640'}"
10605388,Human-Generative AI Collaborative Problem Solving Who Leads and How Students Perceive the Interactions,"Zhu, Gaoxia and Sudarshan, Vidya and Kow, Jason Fok and Soon Ong, Yew",Zhu,10.1109/CAI59869.2024.00133,2024,2024 IEEE Conference on Artificial Intelligence (CAI),"This research investigates distinct human-generative AI collaboration types and students’ interaction experiences when collaborating with generative AI (i.e., ChatGPT) for problem-solving tasks and how these factors relate to students’ sense of agency and perceived collaborative problem solving. By analyzing the surveys and reflections of 79 undergraduate students, we identified three human-generative AI collaboration types: even contribution, human leads, and AI leads. Notably, our study shows that 77.21\% of students perceived they led or had even contributed to collaborative problem-solving when collaborating with ChatGPT. On the other hand, 15.19\% of the human participants indicated that the collaborations were led by ChatGPT, indicating a potential tendency for students to rely on ChatGPT. Furthermore, 67.09\% of students perceived their interaction experiences with ChatGPT to be positive or mixed. We also found a positive correlation between positive interaction experience and a sense of positive agency. The results of this study contribute to our understanding of the collaboration between students and generative AI and highlight the need to study further why some students let ChatGPT lead collaborative problem-solving and how to enhance their interaction experience through curriculum and technology design.",Surveys;Ethics;Generative AI;Federated learning;Collaboration;Lead;Chatbots;Human-generative AI collaboration;ChatGPT;problem-solving;agency;overreliance;higher education,"{'month': 'June', 'issn': '', 'doi': '10.1109/CAI59869.2024.00133', 'keywords': 'Surveys;Ethics;Generative AI;Federated learning;Collaboration;Lead;Chatbots;Human-generative AI collaboration;ChatGPT;problem-solving;agency;overreliance;higher education', 'abstract': 'This research investigates distinct human-generative AI collaboration types and students’ interaction experiences when collaborating with generative AI (i.e., ChatGPT) for problem-solving tasks and how these factors relate to students’ sense of agency and perceived collaborative problem solving. By analyzing the surveys and reflections of 79 undergraduate students, we identified three human-generative AI collaboration types: even contribution, human leads, and AI leads. Notably, our study shows that 77.21\\% of students perceived they led or had even contributed to collaborative problem-solving when collaborating with ChatGPT. On the other hand, 15.19\\% of the human participants indicated that the collaborations were led by ChatGPT, indicating a potential tendency for students to rely on ChatGPT. Furthermore, 67.09\\% of students perceived their interaction experiences with ChatGPT to be positive or mixed. We also found a positive correlation between positive interaction experience and a sense of positive agency. The results of this study contribute to our understanding of the collaboration between students and generative AI and highlight the need to study further why some students let ChatGPT lead collaborative problem-solving and how to enhance their interaction experience through curriculum and technology design.', 'pages': '680-686', 'number': '', 'volume': '', 'year': '2024', 'title': 'Human-Generative AI Collaborative Problem Solving Who Leads and How Students Perceive the Interactions', 'booktitle': '2024 IEEE Conference on Artificial Intelligence (CAI)', 'author': 'Zhu, Gaoxia and Sudarshan, Vidya and Kow, Jason Fok and Soon Ong, Yew', 'ENTRYTYPE': 'inproceedings', 'ID': '10605388'}"
11008623,Quantifying Bias in Text-to-Image Generative Models,"Vice, Jordan and Akhtar, Naveed and Hartley, Richard and Mian, Ajmal",Vice,10.1109/TDSC.2025.3572115,2025,IEEE Transactions on Dependable and Secure Computing,"Bias in text-to-image (T2I) generation can propagate unfair social representations and may be exploited to push ulterior agendas. These biases raise concerns on the dependability and fairness of models that have become widely popular and readily available for public consumption. Existing works in T2I bias analysis typically focus on social biases. We look beyond that and instead propose an evaluation methodology to quantify general bias in T2I generative models without any preconceived notion. We introduce a suite of three metrics; namely, distribution bias, Jaccard hallucination and generative miss-rate, to extensively appraise general model bias. To validate the efficacy of these metrics, we also introduce a backdoor-inspired strategy, which provides a convenient handle over the extent of bias in a model for controlled analysis. We assess T2I models implementing six widely used pipelines in this domain. Our extensive analysis covers both general and task-oriented scenarios, employing over 105 K generated images. For prior art comparison, it also encompasses social bias analysis. Moreover, we also extend our technique to analyze bias in seven popular captioned image datasets. Our experiments establish that our approach is objective, domain-agnostic and it consistently measures different forms of T2I model biases. To further research efforts into T2I model biases, we have developed an open-source web application and practical implementation of this work, which is available on HuggingFace. All relevant code is also publicly available on GitHub.",Measurement;Artificial intelligence;Analytical models;Hands;Computational modeling;Training;Text to image;Prevention and mitigation;Pipelines;Data mining;Bias;generative artificial intelligence;generative models;text-to-image generation;backdoor attacks;fairness,"{'month': 'Sep.', 'issn': '1941-0018', 'doi': '10.1109/TDSC.2025.3572115', 'keywords': 'Measurement;Artificial intelligence;Analytical models;Hands;Computational modeling;Training;Text to image;Prevention and mitigation;Pipelines;Data mining;Bias;generative artificial intelligence;generative models;text-to-image generation;backdoor attacks;fairness', 'abstract': 'Bias in text-to-image (T2I) generation can propagate unfair social representations and may be exploited to push ulterior agendas. These biases raise concerns on the dependability and fairness of models that have become widely popular and readily available for public consumption. Existing works in T2I bias analysis typically focus on social biases. We look beyond that and instead propose an evaluation methodology to quantify general bias in T2I generative models without any preconceived notion. We introduce a suite of three metrics; namely, distribution bias, Jaccard hallucination and generative miss-rate, to extensively appraise general model bias. To validate the efficacy of these metrics, we also introduce a backdoor-inspired strategy, which provides a convenient handle over the extent of bias in a model for controlled analysis. We assess T2I models implementing six widely used pipelines in this domain. Our extensive analysis covers both general and task-oriented scenarios, employing over 105 K generated images. For prior art comparison, it also encompasses social bias analysis. Moreover, we also extend our technique to analyze bias in seven popular captioned image datasets. Our experiments establish that our approach is objective, domain-agnostic and it consistently measures different forms of T2I model biases. To further research efforts into T2I model biases, we have developed an open-source web application and practical implementation of this work, which is available on HuggingFace. All relevant code is also publicly available on GitHub.', 'pages': '5658-5671', 'number': '5', 'volume': '22', 'year': '2025', 'title': 'Quantifying Bias in Text-to-Image Generative Models', 'journal': 'IEEE Transactions on Dependable and Secure Computing', 'author': 'Vice, Jordan and Akhtar, Naveed and Hartley, Richard and Mian, Ajmal', 'ENTRYTYPE': 'article', 'ID': '11008623'}"
11155671,A Systematic Literature Review Exploring AI’s Role in Enhancing Real-Time Holographic Communication,"Hart, Ahren and Shakir, Muhammad Zeeshan",Hart,10.1109/SKIMA66621.2025.11155671,2025,"2025 International Conference on Software, Knowledge, Information Management \& Applications (SKIMA)","This systematic literature review investigates the pivotal role of artificial intelligence (AI) in enhancing real-time holographic communication and immersive virtual interactions. By critically examining recent advancements in AI-driven body and facial tracking, environment generation, and realistic avatar creation, this review identifies current technological capabilities and persistent limitations within holographic communication systems. Challenges such as latency, computational demand, and seamless integration between tracking technologies and dynamic virtual environments are addressed. The synthesis highlights how AI techniques, including deep learning, reinforcement learning, and generative models, significantly enhance interactivity, responsiveness, and realism in virtual settings. The review concludes by proposing future research directions aimed at overcoming identified gaps through the integration of emerging technologies like 5G and 6G networks, emphasizing the potential transformative impact across sectors including education, healthcare, and entertainment.",Holographic Communication;Artificial Intelligence;Real-Time Body Tracking;Avatar Realism;Dynamic Environment Generation;5G/6G Networks;Immersive Virtual Interaction;Ethical Considerations;Metaverse;AI-driven Virtual Environments;Latency Reduction;Computational Optimization,"{'month': 'June', 'issn': '', 'doi': '10.1109/SKIMA66621.2025.11155671', 'keywords': 'Holographic Communication;Artificial Intelligence;Real-Time Body Tracking;Avatar Realism;Dynamic Environment Generation;5G/6G Networks;Immersive Virtual Interaction;Ethical Considerations;Metaverse;AI-driven Virtual Environments;Latency Reduction;Computational Optimization', 'abstract': 'This systematic literature review investigates the pivotal role of artificial intelligence (AI) in enhancing real-time holographic communication and immersive virtual interactions. By critically examining recent advancements in AI-driven body and facial tracking, environment generation, and realistic avatar creation, this review identifies current technological capabilities and persistent limitations within holographic communication systems. Challenges such as latency, computational demand, and seamless integration between tracking technologies and dynamic virtual environments are addressed. The synthesis highlights how AI techniques, including deep learning, reinforcement learning, and generative models, significantly enhance interactivity, responsiveness, and realism in virtual settings. The review concludes by proposing future research directions aimed at overcoming identified gaps through the integration of emerging technologies like 5G and 6G networks, emphasizing the potential transformative impact across sectors including education, healthcare, and entertainment.', 'pages': '1-6', 'number': '', 'volume': '', 'year': '2025', 'title': 'A Systematic Literature Review Exploring AI’s Role in Enhancing Real-Time Holographic Communication', 'booktitle': '2025 International Conference on Software, Knowledge, Information Management \\& Applications (SKIMA)', 'author': 'Hart, Ahren and Shakir, Muhammad Zeeshan', 'ENTRYTYPE': 'inproceedings', 'ID': '11155671'}"
10853651,Method and implementation of intelligent embedded systems based on deep learning technology,"Jing, Chuanli and Yu, Chenggong",Jing,10.1049/icp.2024.4190,2024,6th International Conference on Artificial Intelligence and Advanced Manufacturing (AIAM 2024),"With the rapid development of the Internet of Things and artificial intelligence technologies, intelligent embedded systems face increasingly complex challenges and demands. This paper proposes an intelligent embedded system approach based on deep learning technology and analyzes its implementation. First, the concept and characteristics of intelligent embedded systems, as well as the development and application of deep learning technology, are introduced. Next, the basic principles of deep neural networks and design optimization strategies for deep learning processors are described. Then, based on the analysis of application scenarios and cases of deep learning in embedded systems, an embedded system model based on Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Reinforcement Learning (RL), and Generative Adversarial Networks (GAN) have been developed. This model is applied to image processing, speech recognition, robot control, and data augmentation, and verified in real-world conditions. Finally, the paper summarizes the advantages and development trends of intelligent embedded systems based on deep learning technology and discusses their potential and challenges in the future. Furthermore, the paper highlights the importance of optimizing hardware-software integration to achieve enhanced performance, energy efficiency, and real-time processing in intelligent embedded systems.",,"{'month': 'Oct', 'issn': '', 'doi': '10.1049/icp.2024.4190', 'keywords': '', 'abstract': 'With the rapid development of the Internet of Things and artificial intelligence technologies, intelligent embedded systems face increasingly complex challenges and demands. This paper proposes an intelligent embedded system approach based on deep learning technology and analyzes its implementation. First, the concept and characteristics of intelligent embedded systems, as well as the development and application of deep learning technology, are introduced. Next, the basic principles of deep neural networks and design optimization strategies for deep learning processors are described. Then, based on the analysis of application scenarios and cases of deep learning in embedded systems, an embedded system model based on Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Reinforcement Learning (RL), and Generative Adversarial Networks (GAN) have been developed. This model is applied to image processing, speech recognition, robot control, and data augmentation, and verified in real-world conditions. Finally, the paper summarizes the advantages and development trends of intelligent embedded systems based on deep learning technology and discusses their potential and challenges in the future. Furthermore, the paper highlights the importance of optimizing hardware-software integration to achieve enhanced performance, energy efficiency, and real-time processing in intelligent embedded systems.', 'pages': '65-69', 'number': '', 'volume': '2024', 'year': '2024', 'title': 'Method and implementation of intelligent embedded systems based on deep learning technology', 'booktitle': '6th International Conference on Artificial Intelligence and Advanced Manufacturing (AIAM 2024)', 'author': 'Jing, Chuanli and Yu, Chenggong', 'ENTRYTYPE': 'inproceedings', 'ID': '10853651'}"
10752923,Generation With Nuanced Changes: Continuous Image-to-Image Translation With Adversarial Preferences,"Yao, Yinghua and Pan, Yuangang and Tsang, Ivor W. and Yao, Xin",Yao,10.1109/TAI.2024.3497915,2025,IEEE Transactions on Artificial Intelligence,"Most previous methods for continuous image-to-image translation resorted to binary attributes with restrictive description ability and thus cannot achieve satisfactory performance. Some works proposed to use fine-grained semantic information, relative attributes (RAs), preferences over pairs of images on the strength of a specified attribute. However, they still failed to reconcile both goals for smooth translation and for high-quality generation simultaneously. In this work, we propose a new model continuous translation via adversarial preferences (CTAP) to coordinate these two goals for high-quality continuous translation based on RAs. In CTAP, we simultaneously train two modules: a generator that translates an input image to the desired image with smooth nuanced changes w.r.t. the interested attributes; and a ranker that executes adversarial preferences consisting of the input image and the desired image. Particularly, adversarial preferences involve an adversarial ranking process: 1) the ranker thinks no difference between the desired image and the input image in terms of the interested attributes; 2) the generator fools the ranker to believe the attributes of its output image changes as expect compared with the input image. RAs over pairs of real images are introduced to guide the ranker to rank image pairs regarding the interested attributes only. With an effective ranker, the generator would “win” the adversarial game by producing high-quality images that present smooth changes. The experiments on two face datasets and one shoe dataset demonstrate that our CTAP achieves state-of-art results in generating high-fidelity images which exhibit smooth changes over the interested attributes.",Generators;Interpolation;Artificial intelligence;Training;Generative adversarial networks;Games;Faces;Data models;Three-dimensional displays;Semantics;Adversarial preferences;continuous image-to-image translation;generative adversarial network (GAN);relative attributes (RAs),"{'month': 'April', 'issn': '2691-4581', 'doi': '10.1109/TAI.2024.3497915', 'keywords': 'Generators;Interpolation;Artificial intelligence;Training;Generative adversarial networks;Games;Faces;Data models;Three-dimensional displays;Semantics;Adversarial preferences;continuous image-to-image translation;generative adversarial network (GAN);relative attributes (RAs)', 'abstract': 'Most previous methods for continuous image-to-image translation resorted to binary attributes with restrictive description ability and thus cannot achieve satisfactory performance. Some works proposed to use fine-grained semantic information, relative attributes (RAs), preferences over pairs of images on the strength of a specified attribute. However, they still failed to reconcile both goals for smooth translation and for high-quality generation simultaneously. In this work, we propose a new model continuous translation via adversarial preferences (CTAP) to coordinate these two goals for high-quality continuous translation based on RAs. In CTAP, we simultaneously train two modules: a generator that translates an input image to the desired image with smooth nuanced changes w.r.t. the interested attributes; and a ranker that executes adversarial preferences consisting of the input image and the desired image. Particularly, adversarial preferences involve an adversarial ranking process: 1) the ranker thinks no difference between the desired image and the input image in terms of the interested attributes; 2) the generator fools the ranker to believe the attributes of its output image changes as expect compared with the input image. RAs over pairs of real images are introduced to guide the ranker to rank image pairs regarding the interested attributes only. With an effective ranker, the generator would “win” the adversarial game by producing high-quality images that present smooth changes. The experiments on two face datasets and one shoe dataset demonstrate that our CTAP achieves state-of-art results in generating high-fidelity images which exhibit smooth changes over the interested attributes.', 'pages': '816-828', 'number': '4', 'volume': '6', 'year': '2025', 'title': 'Generation With Nuanced Changes: Continuous Image-to-Image Translation With Adversarial Preferences', 'journal': 'IEEE Transactions on Artificial Intelligence', 'author': 'Yao, Yinghua and Pan, Yuangang and Tsang, Ivor W. and Yao, Xin', 'ENTRYTYPE': 'article', 'ID': '10752923'}"
10419357,Developing Personalized Marketing Service Using Generative AI,"Lee, Gun Ho and Lee, Kyoung Jun and Jeong, Baek and Kim, Taekyung",Lee,10.1109/ACCESS.2024.3361946,2024,IEEE Access,"In today’s world, the development of social network services (SNS) like Facebook and Instagram has enabled consumers to acquire information about products through various channels. The acquisition of diverse information has led to a diversification in consumer preferences and requirements. As consumer preferences diversify and online channels expand, there is an increasing need for companies to provide personalized marketing. Among the means of personalized marketing, personalized marketing messages are a key tool that can enhance customer engagement. However, a limitation of personalized marketing message services is the cost issue associated with manually writing individual marketing messages for personalization. To solve this problem, when developing automated technology for personalized marketing messages, there were concerns about the complexity of model development and the quality of messages generated automatically. In this study, we propose the Persuasive Message Intelligence (PMI) service, which utilizes the recently prominent Large Language Model for automated individual personalized marketing messages. PMI generates marketing messages through prompt engineering based on the theory of persuasion in marketing and prior research on AI-generated messages, and validates the elements of prompts through surveys. The trial and error of researchers presented in this study, along with the know-how and rules of prompt engineering, will serve as guidelines for those who wish to develop services through prompts in the future.",Advertising;Psychology;Social networking (online);Public healthcare;History;Generative AI;Business;Generative adversarial networks;Artificial intelligence;Generative AI;personalized marketing message;persuasion theory;prompt engineering,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2024.3361946', 'keywords': 'Advertising;Psychology;Social networking (online);Public healthcare;History;Generative AI;Business;Generative adversarial networks;Artificial intelligence;Generative AI;personalized marketing message;persuasion theory;prompt engineering', 'abstract': 'In today’s world, the development of social network services (SNS) like Facebook and Instagram has enabled consumers to acquire information about products through various channels. The acquisition of diverse information has led to a diversification in consumer preferences and requirements. As consumer preferences diversify and online channels expand, there is an increasing need for companies to provide personalized marketing. Among the means of personalized marketing, personalized marketing messages are a key tool that can enhance customer engagement. However, a limitation of personalized marketing message services is the cost issue associated with manually writing individual marketing messages for personalization. To solve this problem, when developing automated technology for personalized marketing messages, there were concerns about the complexity of model development and the quality of messages generated automatically. In this study, we propose the Persuasive Message Intelligence (PMI) service, which utilizes the recently prominent Large Language Model for automated individual personalized marketing messages. PMI generates marketing messages through prompt engineering based on the theory of persuasion in marketing and prior research on AI-generated messages, and validates the elements of prompts through surveys. The trial and error of researchers presented in this study, along with the know-how and rules of prompt engineering, will serve as guidelines for those who wish to develop services through prompts in the future.', 'pages': '22394-22402', 'number': '', 'volume': '12', 'year': '2024', 'title': 'Developing Personalized Marketing Service Using Generative AI', 'journal': 'IEEE Access', 'author': 'Lee, Gun Ho and Lee, Kyoung Jun and Jeong, Baek and Kim, Taekyung', 'ENTRYTYPE': 'article', 'ID': '10419357'}"
9088157,CPGAN: Curve Clustering Architecture Based on Projected Latent Vector of Generative Adversarial Network,"Zheng, Aiyu and Cai, Jianghui and Yang, Haifeng and Zhao, Xujun",Zheng,10.1109/ACCESS.2020.2992887,2020,IEEE Access,"Although Generative Adversarial Network(GAN) has obtained remarkable achievements in the image analysis and generation, its exploration in GAN-based curve clustering is still limited. The latent space of curve data is often used for clustering. However, the distance geometry in the latent space does not reflect the inherent clusters. In this paper, we propose CPGAN(Curve Clustering Architecture based on Projected Latent Vector of Generative Adversarial Network) for the clustering of curve dataset. Firstly, a novel GAN network structure, which utilizes a projector P (composed of the transposed convolutional network) to reconstruct the latent space of curve data, is proposed. CPGAN utilizes the concatenation of discrete code and Gaussian noise as a latent vector to preserve the implicit signal and structure of the cluster. Secondly, the loss function with two regularizations for CPGAN is proposed to guarantee the robustness and effectiveness of the model. Based on these, the jointly trained projector P is used to participate in the clustering process, while the generator can be used in the generating process. Finally, the spectral dataset from the LAMOST survey, the UCI dataset, and the UCR dataset are used as experimental data to evaluate clustering performance, the robustness of CPGAN, and further application on anomalous detection. CPGAN presents higher results than other methods.",Generative adversarial networks;Gallium nitride;Generators;Convolution;Task analysis;Data models;Robustness;Machine learning;generative adversarial network;curve data;clustering;anomalous detection,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2020.2992887', 'keywords': 'Generative adversarial networks;Gallium nitride;Generators;Convolution;Task analysis;Data models;Robustness;Machine learning;generative adversarial network;curve data;clustering;anomalous detection', 'abstract': 'Although Generative Adversarial Network(GAN) has obtained remarkable achievements in the image analysis and generation, its exploration in GAN-based curve clustering is still limited. The latent space of curve data is often used for clustering. However, the distance geometry in the latent space does not reflect the inherent clusters. In this paper, we propose CPGAN(Curve Clustering Architecture based on Projected Latent Vector of Generative Adversarial Network) for the clustering of curve dataset. Firstly, a novel GAN network structure, which utilizes a projector P (composed of the transposed convolutional network) to reconstruct the latent space of curve data, is proposed. CPGAN utilizes the concatenation of discrete code and Gaussian noise as a latent vector to preserve the implicit signal and structure of the cluster. Secondly, the loss function with two regularizations for CPGAN is proposed to guarantee the robustness and effectiveness of the model. Based on these, the jointly trained projector P is used to participate in the clustering process, while the generator can be used in the generating process. Finally, the spectral dataset from the LAMOST survey, the UCI dataset, and the UCR dataset are used as experimental data to evaluate clustering performance, the robustness of CPGAN, and further application on anomalous detection. CPGAN presents higher results than other methods.', 'pages': '86765-86776', 'number': '', 'volume': '8', 'year': '2020', 'title': 'CPGAN: Curve Clustering Architecture Based on Projected Latent Vector of Generative Adversarial Network', 'journal': 'IEEE Access', 'author': 'Zheng, Aiyu and Cai, Jianghui and Yang, Haifeng and Zhao, Xujun', 'ENTRYTYPE': 'article', 'ID': '9088157'}"
9174804,Unsupervised Denoising of Optical Coherence Tomography Images With Nonlocal-Generative Adversarial Network,"Guo, Anjing and Fang, Leyuan and Qi, Min and Li, Shutao",Guo,10.1109/TIM.2020.3017036,2021,IEEE Transactions on Instrumentation and Measurement,"Deep learning for image denoising has recently attracted considerable attentions due to its excellent performance. Since most of current deep learning-based denoising models require a large number of clean images for training, it is difficult to extend them to the denoising problems when the reference clean images are hard to acquire (e.g., optical coherence tomography (OCT) images). In this article, we propose a novel unsupervised deep learning model called as nonlocal-generative adversarial network (nonlocal-GAN) for OCT image denoising, where the deep model can be trained without reference clean images. Specifically, considering that the background areas of OCT images mainly contain pure real noise samples, we creatively train a discriminator to distinguish background real noise samples from the fake noise samples generated by the denoiser, that is the generator, and then the discriminator will guide the generator for denoising. To further enhance denoising performance, we introduce a nonlocal means layer into the generator of the nonlocal-GAN model. Furthermore, since nearby several OCT B-scans have strong correlations, we also propose a nonlocal-GAN-M model to utilize the high correlations within nearby B-scans. Extensive experimental results on clinical retinal OCT images demonstrate the effectiveness and efficiency of the proposed method.",Noise reduction;Generators;Image denoising;Training;Noise measurement;Gallium nitride;Generative adversarial networks;Deep learning;generative adversarial networks (GANs);image denoising;optical coherence tomography (OCT),"{'month': '', 'issn': '1557-9662', 'doi': '10.1109/TIM.2020.3017036', 'keywords': 'Noise reduction;Generators;Image denoising;Training;Noise measurement;Gallium nitride;Generative adversarial networks;Deep learning;generative adversarial networks (GANs);image denoising;optical coherence tomography (OCT)', 'abstract': 'Deep learning for image denoising has recently attracted considerable attentions due to its excellent performance. Since most of current deep learning-based denoising models require a large number of clean images for training, it is difficult to extend them to the denoising problems when the reference clean images are hard to acquire (e.g., optical coherence tomography (OCT) images). In this article, we propose a novel unsupervised deep learning model called as nonlocal-generative adversarial network (nonlocal-GAN) for OCT image denoising, where the deep model can be trained without reference clean images. Specifically, considering that the background areas of OCT images mainly contain pure real noise samples, we creatively train a discriminator to distinguish background real noise samples from the fake noise samples generated by the denoiser, that is the generator, and then the discriminator will guide the generator for denoising. To further enhance denoising performance, we introduce a nonlocal means layer into the generator of the nonlocal-GAN model. Furthermore, since nearby several OCT B-scans have strong correlations, we also propose a nonlocal-GAN-M model to utilize the high correlations within nearby B-scans. Extensive experimental results on clinical retinal OCT images demonstrate the effectiveness and efficiency of the proposed method.', 'pages': '1-12', 'number': '', 'volume': '70', 'year': '2021', 'title': 'Unsupervised Denoising of Optical Coherence Tomography Images With Nonlocal-Generative Adversarial Network', 'journal': 'IEEE Transactions on Instrumentation and Measurement', 'author': 'Guo, Anjing and Fang, Leyuan and Qi, Min and Li, Shutao', 'ENTRYTYPE': 'article', 'ID': '9174804'}"
10374245,Multimodal Person Verification With Generative Thermal Data Augmentation,"Abdrakhmanova, Madina and Unaspekov, Timur and Varol, Huseyin Atakan",Abdrakhmanova,10.1109/TBIOM.2023.3346938,2024,"IEEE Transactions on Biometrics, Behavior, and Identity Science","The fusion of audio, visual, and thermal modalities has proven effective in developing reliable person verification systems. In this study, we enhanced multimodal person verification performance by augmenting training data using domain transfer methods. Specifically, we enriched the audio-visual-thermal SpeakingFaces dataset with a combination of real audio-visual data and synthetic thermal data from the VoxCeleb dataset. We adapted visual images in VoxCeleb to the thermal domain using CycleGAN, trained on SpeakingFaces. Our results demonstrate the positive impact of augmented training data on all unimodal and multimodal models. The score fusion of unimodal audio, unimodal visual, bimodal, and trimodal systems trained on the combined data achieved the best results on both datasets and exhibited robustness in low-illumination and noisy conditions. Our findings emphasize the importance of utilizing synthetic data, produced by generative methods, to improve deep learning model performance. To facilitate reproducibility and further research in multimodal person verification, we have made our code, pretrained models, and preprocessed dataset freely available in our GitHub repository.",Visualization;Training data;Generative adversarial networks;Feature extraction;Deep learning;Data augmentation;Identification of persons;Deep learning;multimodal fusion;multimodal learning;data augmentation;generative adversarial networks;face synthesis;person verification,"{'month': 'Jan', 'issn': '2637-6407', 'doi': '10.1109/TBIOM.2023.3346938', 'keywords': 'Visualization;Training data;Generative adversarial networks;Feature extraction;Deep learning;Data augmentation;Identification of persons;Deep learning;multimodal fusion;multimodal learning;data augmentation;generative adversarial networks;face synthesis;person verification', 'abstract': 'The fusion of audio, visual, and thermal modalities has proven effective in developing reliable person verification systems. In this study, we enhanced multimodal person verification performance by augmenting training data using domain transfer methods. Specifically, we enriched the audio-visual-thermal SpeakingFaces dataset with a combination of real audio-visual data and synthetic thermal data from the VoxCeleb dataset. We adapted visual images in VoxCeleb to the thermal domain using CycleGAN, trained on SpeakingFaces. Our results demonstrate the positive impact of augmented training data on all unimodal and multimodal models. The score fusion of unimodal audio, unimodal visual, bimodal, and trimodal systems trained on the combined data achieved the best results on both datasets and exhibited robustness in low-illumination and noisy conditions. Our findings emphasize the importance of utilizing synthetic data, produced by generative methods, to improve deep learning model performance. To facilitate reproducibility and further research in multimodal person verification, we have made our code, pretrained models, and preprocessed dataset freely available in our GitHub repository.', 'pages': '43-53', 'number': '1', 'volume': '6', 'year': '2024', 'title': 'Multimodal Person Verification With Generative Thermal Data Augmentation', 'journal': 'IEEE Transactions on Biometrics, Behavior, and Identity Science', 'author': 'Abdrakhmanova, Madina and Unaspekov, Timur and Varol, Huseyin Atakan', 'ENTRYTYPE': 'article', 'ID': '10374245'}"
10489368,Research on Time Series Data Prediction Based on STP-GAN Algorithm,"Zhang, Rui and Mariano, Vladimir Y.",Zhang,10.1109/RICAI60863.2023.10489368,2023,"2023 5th International Conference on Robotics, Intelligent Control and Artificial Intelligence (RICAI)","In this paper, we utilize the Sentiment Analysis technique in NLP field to provide the opinion influence factors for the algorithmic model by performing sentiment analysis on the text of the research report, introduce the GAN algorithm which is more compatible with the logic of the temporal data operation, and on the basis of which we add the Text Pathway, and put forward the text-assisted Generative Adversarial Network Prediction Model, STP-GAN. The model combines temporal data prediction with sentiment analysis to reduce the error of model training, and introduces the Transformer structure to improve the learning ability of temporal data features. In addition, STP-GAN utilizes BERT for fine-tuning and incorporates a lexicon to increase model adaptability. And on the basis of the proposed STP-GAN, the optimizer RMSPropW, which is more suitable for this research, is proposed, which uses the exponentially weighted moving average as the decay coefficient and utilizes weight decay to improve the training stability and training speed of the prediction model.",Training;Analytical models;Adaptation models;Sentiment analysis;Time series analysis;Predictive models;Prediction algorithms;temporal data;sentiment propensity analysis;generative adversarial model;BERT;optimizer,"{'month': 'Dec', 'issn': '', 'doi': '10.1109/RICAI60863.2023.10489368', 'keywords': 'Training;Analytical models;Adaptation models;Sentiment analysis;Time series analysis;Predictive models;Prediction algorithms;temporal data;sentiment propensity analysis;generative adversarial model;BERT;optimizer', 'abstract': 'In this paper, we utilize the Sentiment Analysis technique in NLP field to provide the opinion influence factors for the algorithmic model by performing sentiment analysis on the text of the research report, introduce the GAN algorithm which is more compatible with the logic of the temporal data operation, and on the basis of which we add the Text Pathway, and put forward the text-assisted Generative Adversarial Network Prediction Model, STP-GAN. The model combines temporal data prediction with sentiment analysis to reduce the error of model training, and introduces the Transformer structure to improve the learning ability of temporal data features. In addition, STP-GAN utilizes BERT for fine-tuning and incorporates a lexicon to increase model adaptability. And on the basis of the proposed STP-GAN, the optimizer RMSPropW, which is more suitable for this research, is proposed, which uses the exponentially weighted moving average as the decay coefficient and utilizes weight decay to improve the training stability and training speed of the prediction model.', 'pages': '819-824', 'number': '', 'volume': '', 'year': '2023', 'title': 'Research on Time Series Data Prediction Based on STP-GAN Algorithm', 'booktitle': '2023 5th International Conference on Robotics, Intelligent Control and Artificial Intelligence (RICAI)', 'author': 'Zhang, Rui and Mariano, Vladimir Y.', 'ENTRYTYPE': 'inproceedings', 'ID': '10489368'}"
10956665,AI-Based Geo Intelligent System for Attack Prediction and Virtual Simulation,"Shruthi, N Kavya and Singh Yadav, Ajit Kumar",Shruthi,10.1109/IC363308.2025.10956665,2025,"2025 International Conference on Intelligent Control, Computing and Communications (IC3)","One of the blooming technologies of AI is the metaverse, The Metaverse is a virtual environment in which users may interact with virtual things and with one another. The Metaverse can be utilized in Military GeoIntelligent systems to build a virtual training environment, allowing Defence personnel to practice in realistic circumstances without the need for costly physical resources. The military personnel can practice in a virtual training environment using the Metaverse that can be created with realistic scenarios. This can aid in enhancing their abilities, judgment, and quickness of response. For instance, pilots can practice flying in various weather conditions, soldiers can hone their tactical skills, and cyber security officials can practice responding to cyberattacks. In this paper, we have focused on AI-based technologies combined with the geo intelligence and metaverse which can jointly help in defence to model and analyse complex geographic situations, allowing defence specialists to explore numerous situations and evaluate the effectiveness of different strategies before deploying resources, with the goal of improving situational awareness, response times, and decision-making capabilities across all security-related agencies. The satellite images obtained from the geo satellites give a brief description of a variety of different types of maps and visualizations, including topographic maps, vegetation maps, and thermal maps. These types of satellite imagery include infrared images, the locations of water bodies, planes, plateaus and a number of other surface structures of earth. The imagery helps in identifying the loopholes of spotting the enemy hiding locations and the risky regions of exposure of the native soldiers. The AI model can identify such locations and predict the possible threat locations. When this kind of model is deployed in the metaverse, it can help the soldiers in being trained for war zone like situations. The predictive outcome can handle and analyse enormous data more efficiently, detect possible dangers early, and respond to them quickly. This can eventually aid in the prevention of security breaches and the protection of national interests.",Training;Solid modeling;Metaverse;Vegetation mapping;Surface structures;Satellite images;Personnel;Time factors;Intelligent systems;Computer crime;Geographic intelligent system;GeoIntelligent systems;Metaverse;Artificial intelligence,"{'month': 'Feb', 'issn': '', 'doi': '10.1109/IC363308.2025.10956665', 'keywords': 'Training;Solid modeling;Metaverse;Vegetation mapping;Surface structures;Satellite images;Personnel;Time factors;Intelligent systems;Computer crime;Geographic intelligent system;GeoIntelligent systems;Metaverse;Artificial intelligence', 'abstract': 'One of the blooming technologies of AI is the metaverse, The Metaverse is a virtual environment in which users may interact with virtual things and with one another. The Metaverse can be utilized in Military GeoIntelligent systems to build a virtual training environment, allowing Defence personnel to practice in realistic circumstances without the need for costly physical resources. The military personnel can practice in a virtual training environment using the Metaverse that can be created with realistic scenarios. This can aid in enhancing their abilities, judgment, and quickness of response. For instance, pilots can practice flying in various weather conditions, soldiers can hone their tactical skills, and cyber security officials can practice responding to cyberattacks. In this paper, we have focused on AI-based technologies combined with the geo intelligence and metaverse which can jointly help in defence to model and analyse complex geographic situations, allowing defence specialists to explore numerous situations and evaluate the effectiveness of different strategies before deploying resources, with the goal of improving situational awareness, response times, and decision-making capabilities across all security-related agencies. The satellite images obtained from the geo satellites give a brief description of a variety of different types of maps and visualizations, including topographic maps, vegetation maps, and thermal maps. These types of satellite imagery include infrared images, the locations of water bodies, planes, plateaus and a number of other surface structures of earth. The imagery helps in identifying the loopholes of spotting the enemy hiding locations and the risky regions of exposure of the native soldiers. The AI model can identify such locations and predict the possible threat locations. When this kind of model is deployed in the metaverse, it can help the soldiers in being trained for war zone like situations. The predictive outcome can handle and analyse enormous data more efficiently, detect possible dangers early, and respond to them quickly. This can eventually aid in the prevention of security breaches and the protection of national interests.', 'pages': '1-6', 'number': '', 'volume': '', 'year': '2025', 'title': 'AI-Based Geo Intelligent System for Attack Prediction and Virtual Simulation', 'booktitle': '2025 International Conference on Intelligent Control, Computing and Communications (IC3)', 'author': 'Shruthi, N Kavya and Singh Yadav, Ajit Kumar', 'ENTRYTYPE': 'inproceedings', 'ID': '10956665'}"
10877597,A Dense Residual Network-50 Model for Identification of Real and Fake Images,"Singh, Gurpreet and Guleria, Kalpna",Singh,10.1109/IC3TES62412.2024.10877597,2024,2024 Second International Conference Computational and Characterization Techniques in Engineering \& Sciences (IC3TES),"The rapid advancement of artificial intelligence has led to the creation of highly realistic synthetic images, posing significant challenges in distinguishing them from real images. This study addresses this issue by employing a ResNet-50-based model to classify AI-generated and real images. The dataset comprises 120,000 images, equally divided between REAL images from the CIFAR-10 dataset and FAKE images generated using Stable Diffusion version 1.4. The model's architecture integrates batch normalization and dropout layers to enhance training stability and reduce overfitting. Results indicate a steady improvement in training accuracy, reaching 98.87\% by the 22nd epoch, with a corresponding decrease in training loss to 0.0457. The validation accuracy stabilized at 95.59\% from the 11th epoch onwards, while validation loss fluctuated, settling at 0.1887 by the 22nd epoch. These outcomes demonstrate the model's robust capability to distinguish between real and AI-generated images, highlighting its potential for applications in digital forensics, content verification, and intellectual property protection. The integration of explainable AI techniques further enhances the model's transparency and reliability, ensuring user trust in its classifications. This research contributes to the ongoing efforts to develop reliable methods for synthetic image detection and emphasizes the importance of early detection in maintaining content authenticity.",Training;Accuracy;Explainable AI;Digital forensics;Intellectual property;Stability analysis;Reliability;Protection;Batch normalization;Overfitting;Machine learning;Deep learning;Image processing;FAKE Images;REAL Image;ResNet50 Model,"{'month': 'Nov', 'issn': '', 'doi': '10.1109/IC3TES62412.2024.10877597', 'keywords': 'Training;Accuracy;Explainable AI;Digital forensics;Intellectual property;Stability analysis;Reliability;Protection;Batch normalization;Overfitting;Machine learning;Deep learning;Image processing;FAKE Images;REAL Image;ResNet50 Model', 'abstract': ""The rapid advancement of artificial intelligence has led to the creation of highly realistic synthetic images, posing significant challenges in distinguishing them from real images. This study addresses this issue by employing a ResNet-50-based model to classify AI-generated and real images. The dataset comprises 120,000 images, equally divided between REAL images from the CIFAR-10 dataset and FAKE images generated using Stable Diffusion version 1.4. The model's architecture integrates batch normalization and dropout layers to enhance training stability and reduce overfitting. Results indicate a steady improvement in training accuracy, reaching 98.87\\% by the 22nd epoch, with a corresponding decrease in training loss to 0.0457. The validation accuracy stabilized at 95.59\\% from the 11th epoch onwards, while validation loss fluctuated, settling at 0.1887 by the 22nd epoch. These outcomes demonstrate the model's robust capability to distinguish between real and AI-generated images, highlighting its potential for applications in digital forensics, content verification, and intellectual property protection. The integration of explainable AI techniques further enhances the model's transparency and reliability, ensuring user trust in its classifications. This research contributes to the ongoing efforts to develop reliable methods for synthetic image detection and emphasizes the importance of early detection in maintaining content authenticity."", 'pages': '1-5', 'number': '', 'volume': '', 'year': '2024', 'title': 'A Dense Residual Network-50 Model for Identification of Real and Fake Images', 'booktitle': '2024 Second International Conference Computational and Characterization Techniques in Engineering \\& Sciences (IC3TES)', 'author': 'Singh, Gurpreet and Guleria, Kalpna', 'ENTRYTYPE': 'inproceedings', 'ID': '10877597'}"
9381694,Hausdorff GAN: Improving GAN Generation Quality With Hausdorff Metric,"Li, Wei and Liang, Zhixuan and Ma, Ping and Wang, Ruobei and Cui, Xiaohui and Chen, Ping",Li,10.1109/TCYB.2021.3062396,2022,IEEE Transactions on Cybernetics,"Data usually resides on a manifold, and the minimal dimension of such a manifold is called its intrinsic dimension. This fundamental data property is not considered in the generative adversarial network (GAN) model along with its its variants; such that original data and generated data often hold different intrinsic dimensions. The different intrinsic dimensions of both generated and original data may cause generated data distribution to not match original data distribution completely, and it certainly will hurt the quality of generated data. In this study, we first show that GAN is often unable to generate simulation data, holding the same intrinsic dimension as the original data with both theoretical analysis and experimental illustration. Next, we propose a new model, called Hausdorff GAN, which removes the issue of different intrinsic dimensions and introduces the Hausdorff metric into GAN training to generate higher quality data. This provides new insights into the success of Hausdorff GAN. Specifically, we utilize a mapping function to map both original and generated data into the same manifold. We then calculate the Hausdorff distance to measure the difference between the mapped original data and the mapped generated data, toward pushing generated data to the side of original data. Finally, we conduct extensive experiments (using MNIST, CIFAR10, and CelebA datasets) to demonstrate the significant performance improvement of the Hausdorff GAN in achieving the largest Inception Score and the smallest Frechet inception distance (FID) score as well as producing diverse generated data at different resolutions.",Gallium nitride;Generative adversarial networks;Generators;Measurement;Training;Manifolds;Data models;Distribution transformation;generative adversarial network (GAN);Hausdorff metric;intrinsic dimension,"{'month': 'Oct', 'issn': '2168-2275', 'doi': '10.1109/TCYB.2021.3062396', 'keywords': 'Gallium nitride;Generative adversarial networks;Generators;Measurement;Training;Manifolds;Data models;Distribution transformation;generative adversarial network (GAN);Hausdorff metric;intrinsic dimension', 'abstract': 'Data usually resides on a manifold, and the minimal dimension of such a manifold is called its intrinsic dimension. This fundamental data property is not considered in the generative adversarial network (GAN) model along with its its variants; such that original data and generated data often hold different intrinsic dimensions. The different intrinsic dimensions of both generated and original data may cause generated data distribution to not match original data distribution completely, and it certainly will hurt the quality of generated data. In this study, we first show that GAN is often unable to generate simulation data, holding the same intrinsic dimension as the original data with both theoretical analysis and experimental illustration. Next, we propose a new model, called Hausdorff GAN, which removes the issue of different intrinsic dimensions and introduces the Hausdorff metric into GAN training to generate higher quality data. This provides new insights into the success of Hausdorff GAN. Specifically, we utilize a mapping function to map both original and generated data into the same manifold. We then calculate the Hausdorff distance to measure the difference between the mapped original data and the mapped generated data, toward pushing generated data to the side of original data. Finally, we conduct extensive experiments (using MNIST, CIFAR10, and CelebA datasets) to demonstrate the significant performance improvement of the Hausdorff GAN in achieving the largest Inception Score and the smallest Frechet inception distance (FID) score as well as producing diverse generated data at different resolutions.', 'pages': '10407-10419', 'number': '10', 'volume': '52', 'year': '2022', 'title': 'Hausdorff GAN: Improving GAN Generation Quality With Hausdorff Metric', 'journal': 'IEEE Transactions on Cybernetics', 'author': 'Li, Wei and Liang, Zhixuan and Ma, Ping and Wang, Ruobei and Cui, Xiaohui and Chen, Ping', 'ENTRYTYPE': 'article', 'ID': '9381694'}"
9442206,Wireless Channel Data Augmentation for Artificial Intelligence of Things in Industrial Environment Using Generative Adversarial Networks,"Liang, Xin and Liu, Zhenyu and Chang, Haoran and Zhang, Lin",Liang,10.1109/INDIN45582.2020.9442206,2020,2020 IEEE 18th International Conference on Industrial Informatics (INDIN),"The rise of Artificial Intelligence of Things (AIoT) makes everything connected smartly, which can enrich industrial productivity. With the help of learning-based wireless communications, terminals in AIoT can communicate with each other and collaborate intelligently. However, one of the critical issues faced in AIoT deployment is the lack of available large datasets for the training of artificial intelligence algorithms in the industrial environment. In this paper, we propose a channel data augmentation algorithm for the dataset limitation cases in intelligent industrial wireless communication systems using generative adversarial networks (GAN). Specifically, we first show the performance of deep learning-based channel state information (CSI) feedback algorithm trained with different size of datasets. Then we develop a GAN for the channel data augmentation to enhance the performance of CSI feedback algorithm. Finally, we apply the enhanced approach to an insufficient dataset for the performance evaluation. Experimental results show that our method can achieve at most 3dB performance improvement than other traditional data augmentation approaches in increasing the accuracy of CSI feedback algorithm when the size of dataset is limited to 10000.",Wireless communication;Performance evaluation;Training;Productivity;Conferences;Industrial communication;Estimation;Data augmentation;GAN;industrial wireless communication;channel estimation,"{'month': 'July', 'issn': '2378-363X', 'doi': '10.1109/INDIN45582.2020.9442206', 'keywords': 'Wireless communication;Performance evaluation;Training;Productivity;Conferences;Industrial communication;Estimation;Data augmentation;GAN;industrial wireless communication;channel estimation', 'abstract': 'The rise of Artificial Intelligence of Things (AIoT) makes everything connected smartly, which can enrich industrial productivity. With the help of learning-based wireless communications, terminals in AIoT can communicate with each other and collaborate intelligently. However, one of the critical issues faced in AIoT deployment is the lack of available large datasets for the training of artificial intelligence algorithms in the industrial environment. In this paper, we propose a channel data augmentation algorithm for the dataset limitation cases in intelligent industrial wireless communication systems using generative adversarial networks (GAN). Specifically, we first show the performance of deep learning-based channel state information (CSI) feedback algorithm trained with different size of datasets. Then we develop a GAN for the channel data augmentation to enhance the performance of CSI feedback algorithm. Finally, we apply the enhanced approach to an insufficient dataset for the performance evaluation. Experimental results show that our method can achieve at most 3dB performance improvement than other traditional data augmentation approaches in increasing the accuracy of CSI feedback algorithm when the size of dataset is limited to 10000.', 'pages': '502-507', 'number': '', 'volume': '1', 'year': '2020', 'title': 'Wireless Channel Data Augmentation for Artificial Intelligence of Things in Industrial Environment Using Generative Adversarial Networks', 'booktitle': '2020 IEEE 18th International Conference on Industrial Informatics (INDIN)', 'author': 'Liang, Xin and Liu, Zhenyu and Chang, Haoran and Zhang, Lin', 'ENTRYTYPE': 'inproceedings', 'ID': '9442206'}"
10625113,Imagination Made Real: Stable Diffusion for High-Fidelity Text-to-Image Tasks,"Jadhav, Balasaheb and Jain, Manas and Jajoo, Adhip and Kadam, Devika and Kadam, Harshvardhan and Kakkad, Toshish",Jadhav,10.1109/ICSCSS60660.2024.10625113,2024,2024 2nd International Conference on Sustainable Computing and Smart Systems (ICSCSS),"By utilising the sophisticated features of diffusion models (DMs) for image synthesis, this work presents a novel method for high-fidelity text-to-image generation. Conventional deep learning approaches break down the image production process into a series of sequential denoising autoencoder applications. These applications typically operate in pixel space and result in significant computational costs due to the need for substantial GPU resources for training and inference. By exploiting the latent spaces of potent pretrained autoencoders, this method overcomes these difficulties and permits DM training with minimal computational resources while preserving great quality and adaptability. An ideal trade-off between preserving detail and reducing complexity is struck by using latent representations, which greatly improves visual fidelity over earlier techniques. Furthermore, the incorporation of cross-attention layers in the model converts diffusion models into flexible generators that can process a variety of conditioning inputs, including text and bounding boxes, so enabling convolutional high-resolution synthesis. State-of-the-art performance in image inpainting, class-specific image blending, and other tasks is demonstrated by latent diffusion models (LDMs), which also perform significantly better than pixel-based DMs in text-to-image synthesis, unrestricted image generation, and super-resolution. This research pushes the limits of what is possible in high-fidelity text-to-image applications, showcasing the adaptability and effectiveness of LDMs.",Training;Visualization;Image synthesis;Computational modeling;Superresolution;Noise reduction;Text to image;Image Generation;Deep Learning;Stable Diffusion;Latent Space;Latent Diffusion Model (LDM);Generative Adversarial Network (GAN);Contrastive Language-Image Pre-Training (CLIP),"{'month': 'July', 'issn': '', 'doi': '10.1109/ICSCSS60660.2024.10625113', 'keywords': 'Training;Visualization;Image synthesis;Computational modeling;Superresolution;Noise reduction;Text to image;Image Generation;Deep Learning;Stable Diffusion;Latent Space;Latent Diffusion Model (LDM);Generative Adversarial Network (GAN);Contrastive Language-Image Pre-Training (CLIP)', 'abstract': 'By utilising the sophisticated features of diffusion models (DMs) for image synthesis, this work presents a novel method for high-fidelity text-to-image generation. Conventional deep learning approaches break down the image production process into a series of sequential denoising autoencoder applications. These applications typically operate in pixel space and result in significant computational costs due to the need for substantial GPU resources for training and inference. By exploiting the latent spaces of potent pretrained autoencoders, this method overcomes these difficulties and permits DM training with minimal computational resources while preserving great quality and adaptability. An ideal trade-off between preserving detail and reducing complexity is struck by using latent representations, which greatly improves visual fidelity over earlier techniques. Furthermore, the incorporation of cross-attention layers in the model converts diffusion models into flexible generators that can process a variety of conditioning inputs, including text and bounding boxes, so enabling convolutional high-resolution synthesis. State-of-the-art performance in image inpainting, class-specific image blending, and other tasks is demonstrated by latent diffusion models (LDMs), which also perform significantly better than pixel-based DMs in text-to-image synthesis, unrestricted image generation, and super-resolution. This research pushes the limits of what is possible in high-fidelity text-to-image applications, showcasing the adaptability and effectiveness of LDMs.', 'pages': '773-779', 'number': '', 'volume': '', 'year': '2024', 'title': 'Imagination Made Real: Stable Diffusion for High-Fidelity Text-to-Image Tasks', 'booktitle': '2024 2nd International Conference on Sustainable Computing and Smart Systems (ICSCSS)', 'author': 'Jadhav, Balasaheb and Jain, Manas and Jajoo, Adhip and Kadam, Devika and Kadam, Harshvardhan and Kakkad, Toshish', 'ENTRYTYPE': 'inproceedings', 'ID': '10625113'}"
10649956,Generating synthetic data using GANs fusion in the digital twins model for sonars,"Połap, Dawid and Jaszcz, Antoni and Prokop, Katarzyna",Połap,10.1109/IJCNN60899.2024.10649956,2024,2024 International Joint Conference on Neural Networks (IJCNN),"Digital twins are a technology that allows for a virtual copy of a real object or process. The main goal is to map specific features to prevent problems or monitor conditions and operations. In this work, we propose a framework for a sonar system. Data acquired by sonar are most often sent to an operator or classification network to detect objects on the seabed. However, such a classifier needs a large amount of data to be properly trained. Therefore, we propose a digital twin model that uses generative adversarial networks (GANs) with feature fusion to obtain synthetic data for further processing. The proposed GAN model is based on dual generators with combining results that are passed further. The proposed technique indicates that building an artificial intelligence module by fusing real and synthetic data is important and allows for achieving high augmentation results in sonar applications.",Training;Neural networks;Sonar;Generative adversarial networks;Feature extraction;Data models;Generators;GAN;synthetic data;augmentation;feature fusion;digital twins;sonar,"{'month': 'June', 'issn': '2161-4407', 'doi': '10.1109/IJCNN60899.2024.10649956', 'keywords': 'Training;Neural networks;Sonar;Generative adversarial networks;Feature extraction;Data models;Generators;GAN;synthetic data;augmentation;feature fusion;digital twins;sonar', 'abstract': 'Digital twins are a technology that allows for a virtual copy of a real object or process. The main goal is to map specific features to prevent problems or monitor conditions and operations. In this work, we propose a framework for a sonar system. Data acquired by sonar are most often sent to an operator or classification network to detect objects on the seabed. However, such a classifier needs a large amount of data to be properly trained. Therefore, we propose a digital twin model that uses generative adversarial networks (GANs) with feature fusion to obtain synthetic data for further processing. The proposed GAN model is based on dual generators with combining results that are passed further. The proposed technique indicates that building an artificial intelligence module by fusing real and synthetic data is important and allows for achieving high augmentation results in sonar applications.', 'pages': '1-6', 'number': '', 'volume': '', 'year': '2024', 'title': 'Generating synthetic data using GANs fusion in the digital twins model for sonars', 'booktitle': '2024 International Joint Conference on Neural Networks (IJCNN)', 'author': 'Połap, Dawid and Jaszcz, Antoni and Prokop, Katarzyna', 'ENTRYTYPE': 'inproceedings', 'ID': '10649956'}"
10604540,FWBP: A New Generative Modeling Scheme,"Yu, Siqi and Jiang, Yongquan and Yang, Yan",Yu,10.1109/ICAIBD62003.2024.10604540,2024,2024 7th International Conference on Artificial Intelligence and Big Data (ICAIBD),"Generative models are often used to generate differentiated samples for images, audio and video, focusing on the diversity of the generated samples. For materials, molecules and other fields, it is more important to generate samples with targeted properties. More attention should be paid to the accuracy of the generated samples. In the spirit of exploring conditional generative neural networks, this manuscript proposes a new generative modeling scheme-FWBP, i.e., Fixed Weight Backpropagation. FWBP can effectively utilize trained prediction models and generate samples with targeted attributes by fixing the weights in prediction models and then backpropagate the properties. High-entropy alloys (HEAs) are alloys composed of multiple elements. The property design of HEAs is of great significance for practical applications. Firstly, a set of function is used to illustrate the general idea and effectiveness of FWBP. Then, FWBP is applied to the generation of HEAs with targeted properties. The accuracy of FWBP is verified in four datasets with various networks. FWBP's efficiency is also proved in the field of image generation.",Backpropagation;Accuracy;Image synthesis;Neural networks;Metals;Focusing;Predictive models;generative models;HEAs;FWBP,"{'month': 'May', 'issn': '2769-3554', 'doi': '10.1109/ICAIBD62003.2024.10604540', 'keywords': 'Backpropagation;Accuracy;Image synthesis;Neural networks;Metals;Focusing;Predictive models;generative models;HEAs;FWBP', 'abstract': ""Generative models are often used to generate differentiated samples for images, audio and video, focusing on the diversity of the generated samples. For materials, molecules and other fields, it is more important to generate samples with targeted properties. More attention should be paid to the accuracy of the generated samples. In the spirit of exploring conditional generative neural networks, this manuscript proposes a new generative modeling scheme-FWBP, i.e., Fixed Weight Backpropagation. FWBP can effectively utilize trained prediction models and generate samples with targeted attributes by fixing the weights in prediction models and then backpropagate the properties. High-entropy alloys (HEAs) are alloys composed of multiple elements. The property design of HEAs is of great significance for practical applications. Firstly, a set of function is used to illustrate the general idea and effectiveness of FWBP. Then, FWBP is applied to the generation of HEAs with targeted properties. The accuracy of FWBP is verified in four datasets with various networks. FWBP's efficiency is also proved in the field of image generation."", 'pages': '277-282', 'number': '', 'volume': '', 'year': '2024', 'title': 'FWBP: A New Generative Modeling Scheme', 'booktitle': '2024 7th International Conference on Artificial Intelligence and Big Data (ICAIBD)', 'author': 'Yu, Siqi and Jiang, Yongquan and Yang, Yan', 'ENTRYTYPE': 'inproceedings', 'ID': '10604540'}"
9402449,Drug Discovery using Generative Adversarial Network with Reinforcement Learning,"Ravindra Padalkar, Ganesh and Dinkar Patil, Shivani and Mallikarjun Hegadi, Mukta and Kailash Jaybhaye, Nikita",Ravindra Padalkar,10.1109/ICCCI50826.2021.9402449,2021,2021 International Conference on Computer Communication and Informatics (ICCCI),"A large amount of medical data is available to many of us and along with well-established deep learning algorithms, so the design of automated drug development pipelines has increased. The pipeline speeds up the drug discovery process and helps us better understand the disease. They help in planning pre-clinical lab experiments. This reduces the low productivity rate that the pharmaceutical companies are facing currently. Accurate predictions and insights are obtained by using deep learning techniques. So, this increases the need for deep learning approaches that have the potential to speed up the process, decision making, and reduce failure rates in drug discovery and development. With the fast development of computing power and enormous medical data, the project involving drug discovery have been benefited from artificial intelligence. The deep learning model knows as Generative Adversarial Network (GAN) with reinforcement learning is used to solve the problem.",Drugs;Deep learning;Pipelines;Reinforcement learning;Generative adversarial networks;Data models;Gallium nitride;Convolution Neural Network;Generative Adversarial Network;Recurrent Neural Network;Reinforcement Learning,"{'month': 'Jan', 'issn': '2329-7190', 'doi': '10.1109/ICCCI50826.2021.9402449', 'keywords': 'Drugs;Deep learning;Pipelines;Reinforcement learning;Generative adversarial networks;Data models;Gallium nitride;Convolution Neural Network;Generative Adversarial Network;Recurrent Neural Network;Reinforcement Learning', 'abstract': 'A large amount of medical data is available to many of us and along with well-established deep learning algorithms, so the design of automated drug development pipelines has increased. The pipeline speeds up the drug discovery process and helps us better understand the disease. They help in planning pre-clinical lab experiments. This reduces the low productivity rate that the pharmaceutical companies are facing currently. Accurate predictions and insights are obtained by using deep learning techniques. So, this increases the need for deep learning approaches that have the potential to speed up the process, decision making, and reduce failure rates in drug discovery and development. With the fast development of computing power and enormous medical data, the project involving drug discovery have been benefited from artificial intelligence. The deep learning model knows as Generative Adversarial Network (GAN) with reinforcement learning is used to solve the problem.', 'pages': '1-3', 'number': '', 'volume': '', 'year': '2021', 'title': 'Drug Discovery using Generative Adversarial Network with Reinforcement Learning', 'booktitle': '2021 International Conference on Computer Communication and Informatics (ICCCI)', 'author': 'Ravindra Padalkar, Ganesh and Dinkar Patil, Shivani and Mallikarjun Hegadi, Mukta and Kailash Jaybhaye, Nikita', 'ENTRYTYPE': 'inproceedings', 'ID': '9402449'}"
10309274,"How Generative AI Helped Me Imagine a Better Robot: It Didn't Give Me Schematics, But It Did Boost My Creativity","Broo, Didem Gürdür",Broo,10.1109/MSPEC.2023.10309274,2023,IEEE Spectrum,"This year, 2023, will probably be remembered as the year of generative AI. It is still an open question whether generative AI will change our lives for the better. One thing is certain, though: New artificial-intelligence tools are being unveiled rapidly and will continue for some time to come. And engineers have much to gain from experimenting with them and incorporating them into their design process. • That's already happening in certain spheres. For Aston Martin's DBR22 concept car, designers relied on AI that's integrated into Divergent Technologies' digital 3D software to optimize the shape and layout of the rear subframe components. The rear subframe has an organic, skeletal look, enabled by the AI exploration of forms. The actual components were produced through additive manufacturing. Aston Martin says that this method substantially reduced the weight of the components while maintaining their rigidity. The company plans to use this same design and manufacturing process in upcoming low-volume vehicle models. • Other examples of AI-aided design can be found in NASA's space hardware, including planetary instruments, space telescopes, and the Mars Sample Return mission. NASA engineer Ryan McClelland says that the new AI-generated designs may “look somewhat alien and weird,” but they tolerate higher structural loads while weighing less than conventional components do. Also, they take a fraction of the time to design compared to traditional components. McClelland calls these new designs “evolved structures.” The phrase refers to how the AI software iterates through design mutations and converges on high-performing designs.",Artificial intelligence;Generative adversarial networks;Space vehicles;Space missions;Aerospace electronics;Three-dimensional printing;Three-dimensional displays;Rigidity;Design engineering,"{'month': 'November', 'issn': '1939-9340', 'doi': '10.1109/MSPEC.2023.10309274', 'keywords': 'Artificial intelligence;Generative adversarial networks;Space vehicles;Space missions;Aerospace electronics;Three-dimensional printing;Three-dimensional displays;Rigidity;Design engineering', 'abstract': ""This year, 2023, will probably be remembered as the year of generative AI. It is still an open question whether generative AI will change our lives for the better. One thing is certain, though: New artificial-intelligence tools are being unveiled rapidly and will continue for some time to come. And engineers have much to gain from experimenting with them and incorporating them into their design process. • That's already happening in certain spheres. For Aston Martin's DBR22 concept car, designers relied on AI that's integrated into Divergent Technologies' digital 3D software to optimize the shape and layout of the rear subframe components. The rear subframe has an organic, skeletal look, enabled by the AI exploration of forms. The actual components were produced through additive manufacturing. Aston Martin says that this method substantially reduced the weight of the components while maintaining their rigidity. The company plans to use this same design and manufacturing process in upcoming low-volume vehicle models. • Other examples of AI-aided design can be found in NASA's space hardware, including planetary instruments, space telescopes, and the Mars Sample Return mission. NASA engineer Ryan McClelland says that the new AI-generated designs may “look somewhat alien and weird,” but they tolerate higher structural loads while weighing less than conventional components do. Also, they take a fraction of the time to design compared to traditional components. McClelland calls these new designs “evolved structures.” The phrase refers to how the AI software iterates through design mutations and converges on high-performing designs."", 'pages': '44-50', 'number': '11', 'volume': '60', 'year': '2023', 'title': ""How Generative AI Helped Me Imagine a Better Robot: It Didn't Give Me Schematics, But It Did Boost My Creativity"", 'journal': 'IEEE Spectrum', 'author': 'Broo, Didem Gürdür', 'ENTRYTYPE': 'article', 'ID': '10309274'}"
11036309,Using Generative AI for Neurofeedback Content Personalization,"Shuler, Todd L. and Ostrowski, David Alfred",Shuler,10.1109/ICSC64641.2025.00053,2025,2025 19th International Conference on Semantic Computing (ICSC),"This paper presents a comprehensive framework for integrating Large Language Models (LLMs) into neurofeedback systems. By leveraging real-time electroencephalogram (EEG) data processing, advanced machine learning, and multimodal feedback mechanisms, the framework offers a transformative approach to personalized neurofeedback. Technical validation and statistical analysis highlight system scalability and empirical reproducibility, addressing critical performance metrics such as latency, accuracy, and resource efficiency. The framework emphasizes the convergence of traditional neurofeedback principles with modern AI capabilities, demonstrating significant potential for applications in mental health, cognitive enhancement, and neurological rehabilitation.",Generative AI;Statistical analysis;Large language models;Scalability;Semantics;Mental health;Machine learning;Electroencephalography;Reproducibility of results;Neurofeedback;Adaptive Feedback;Artificial Intelligence in Neuroscience;Brain-Computer Interfaces (BCI);Cognitive Enhancement;Electroencephalogram (EEG) Generative AI;Large Language Models (LLM);Machine Learning;Mental Health;Multi-Modal Feedback;Neurofeedback;Optimization Framework,"{'month': 'Feb', 'issn': '2472-9671', 'doi': '10.1109/ICSC64641.2025.00053', 'keywords': 'Generative AI;Statistical analysis;Large language models;Scalability;Semantics;Mental health;Machine learning;Electroencephalography;Reproducibility of results;Neurofeedback;Adaptive Feedback;Artificial Intelligence in Neuroscience;Brain-Computer Interfaces (BCI);Cognitive Enhancement;Electroencephalogram (EEG) Generative AI;Large Language Models (LLM);Machine Learning;Mental Health;Multi-Modal Feedback;Neurofeedback;Optimization Framework', 'abstract': 'This paper presents a comprehensive framework for integrating Large Language Models (LLMs) into neurofeedback systems. By leveraging real-time electroencephalogram (EEG) data processing, advanced machine learning, and multimodal feedback mechanisms, the framework offers a transformative approach to personalized neurofeedback. Technical validation and statistical analysis highlight system scalability and empirical reproducibility, addressing critical performance metrics such as latency, accuracy, and resource efficiency. The framework emphasizes the convergence of traditional neurofeedback principles with modern AI capabilities, demonstrating significant potential for applications in mental health, cognitive enhancement, and neurological rehabilitation.', 'pages': '306-309', 'number': '', 'volume': '', 'year': '2025', 'title': 'Using Generative AI for Neurofeedback Content Personalization', 'booktitle': '2025 19th International Conference on Semantic Computing (ICSC)', 'author': 'Shuler, Todd L. and Ostrowski, David Alfred', 'ENTRYTYPE': 'inproceedings', 'ID': '11036309'}"
10166608,A Method for Generating Subsynchronous Oscillation Data of Power System Based on Wasserstein Generative Adversarial Network,"Zheng, Yuhang and Miao, Miao and Peng, Xiangjia and Lei, Jiaxing and Feng, Shuang",Zheng,10.1109/CIEEC58067.2023.10166608,2023,2023 IEEE 6th International Electrical and Energy Conference (CIEEC),"With a large number of power electronic devices connected to the power grid, the problem of subsynchronous oscillation caused by the interaction between the converter and the power grid seriously threatens the safe and stable operation of the power system. The existing research methods of subsynchronous oscillation, especially the artificial intelligence methods, mostly rely on a large amount of oscillation data, but in practice, such data is very scarce. Therefore, this paper proposes a method to generate subsynchronous oscillation data in power systems based on Wasserstein generative adversarial network (WGAN). In order to improve the quality of the generated samples and solve the problem of training instability, the JS distance is replaced by Wasserstein distance. Taking the oscillation data in the direct-driven permanent-magnet wind farm system as a practical example, the generated data are analyzed by dynamic time warping (DTW) and frequency to damping ratio methods. The results show that the data generated by the proposed method is consistent with the characteristics of oscillation data and has certain advantages in data quality.",Training;Time-frequency analysis;Simulation;Power system dynamics;Power system stability;Wind farms;Generative adversarial networks;generative adversarial networks;subsynchronous oscillation;data generation;Wasserstein distance,"{'month': 'May', 'issn': '', 'doi': '10.1109/CIEEC58067.2023.10166608', 'keywords': 'Training;Time-frequency analysis;Simulation;Power system dynamics;Power system stability;Wind farms;Generative adversarial networks;generative adversarial networks;subsynchronous oscillation;data generation;Wasserstein distance', 'abstract': 'With a large number of power electronic devices connected to the power grid, the problem of subsynchronous oscillation caused by the interaction between the converter and the power grid seriously threatens the safe and stable operation of the power system. The existing research methods of subsynchronous oscillation, especially the artificial intelligence methods, mostly rely on a large amount of oscillation data, but in practice, such data is very scarce. Therefore, this paper proposes a method to generate subsynchronous oscillation data in power systems based on Wasserstein generative adversarial network (WGAN). In order to improve the quality of the generated samples and solve the problem of training instability, the JS distance is replaced by Wasserstein distance. Taking the oscillation data in the direct-driven permanent-magnet wind farm system as a practical example, the generated data are analyzed by dynamic time warping (DTW) and frequency to damping ratio methods. The results show that the data generated by the proposed method is consistent with the characteristics of oscillation data and has certain advantages in data quality.', 'pages': '3702-3707', 'number': '', 'volume': '', 'year': '2023', 'title': 'A Method for Generating Subsynchronous Oscillation Data of Power System Based on Wasserstein Generative Adversarial Network', 'booktitle': '2023 IEEE 6th International Electrical and Energy Conference (CIEEC)', 'author': 'Zheng, Yuhang and Miao, Miao and Peng, Xiangjia and Lei, Jiaxing and Feng, Shuang', 'ENTRYTYPE': 'inproceedings', 'ID': '10166608'}"
10897060,The Future of GenAI in Cybersecurity,"Islam, Mohammad Rubyet",Islam,10.1002/9781394279326.ch11,2025,"Generative AI, Cybersecurity, and Ethics","Summary <p>This chapter explores the future of generative artificial intelligence (GenAI) in cybersecurity, emphasizing emerging trends and future challenges. The chapter highlights the promise of GenAI in enhancing digital defenses through advanced predictive models and simulations while addressing complex ethical questions. It concludes with the integration of automated security protocols, deepfake detection, adaptive threat modeling, and artificial intelligence (AI)\&\#x2010;driven security education. The chapter also delves into ethical considerations, regulatory compliance, and the importance of inclusivity and continuous adaptation in the development and deployment of GenAI technologies. The chapter concludes with a call for ethical stewardship, global cooperation, and a commitment to fostering a secure and ethically sound digital future.</p>",Ethics;Computer security;Security;Deepfakes;Artificial intelligence;Training;Biological system modeling;Adaptation models;Predictive models;Market research,"{'url': 'https://ieeexplore-ieee-org.crai.referencistas.com/document/10897060', 'isbn': '9781394279319', 'publisher': 'Wiley', 'issn': '', 'doi': '10.1002/9781394279326.ch11', 'keywords': 'Ethics;Computer security;Security;Deepfakes;Artificial intelligence;Training;Biological system modeling;Adaptation models;Predictive models;Market research', 'abstract': 'Summary <p>This chapter explores the future of generative artificial intelligence (GenAI) in cybersecurity, emphasizing emerging trends and future challenges. The chapter highlights the promise of GenAI in enhancing digital defenses through advanced predictive models and simulations while addressing complex ethical questions. It concludes with the integration of automated security protocols, deepfake detection, adaptive threat modeling, and artificial intelligence (AI)\\&\\#x2010;driven security education. The chapter also delves into ethical considerations, regulatory compliance, and the importance of inclusivity and continuous adaptation in the development and deployment of GenAI technologies. The chapter concludes with a call for ethical stewardship, global cooperation, and a commitment to fostering a secure and ethically sound digital future.</p>', 'pages': '273-291', 'number': '', 'volume': '', 'year': '2025', 'title': 'The Future of GenAI in Cybersecurity', 'booktitle': 'Generative AI, Cybersecurity, and Ethics', 'author': 'Islam, Mohammad Rubyet', 'ENTRYTYPE': 'inbook', 'ID': '10897060'}"
10793762,A Novel Approach for Automated Segmentation of Left Ventricle Based on Bidirectional Myocardium to Endocardium Translation Using Generative Adversarial Network,"Fatima, Noreen and Afrakhteh, Sajjad and Demi, Libertario",Fatima,10.1109/UFFC-JS60046.2024.10793762,2024,"2024 IEEE Ultrasonics, Ferroelectrics, and Frequency Control Joint Symposium (UFFC-JS)","In echocardiography, accurate segmentation of cardiac structures, particularly left ventricle (LV), is crucial for clinical diagnosis. However, manual segmentation is user-intensive and prone to variability among experts due to intricate anatomical details and imaging artifacts. Our aim is to propose an artificial intelligence (AI)-based technique to automatically segment the LV structure, enhancing accuracy and reducing segmentation time and subjectivity of manual segmentation. We propose a novel Bidirectional generative adversarial network (Bi-GAN) for automated segmentation of LV structures. Specifically, we utilize Bi-GAN for segmenting the endocardium region from available myocardium region and vice versa. The adversarial training, Bi-GAN minimizes the losses and produces the target domain segmentation. The analysis is conducted on the cardiac acquisitions for the multi-structure ultrasound segmentation (CAMUS) dataset, comprising 900 echocardiographic training images and 100 testing images in 4CH views during both end-diastolic (ED) and end-systolic (ES) phases. Results showed a mean Dice of 0.97 for both the ED and ES phases, along with an MAE of 1.90. Conversely, when generating myocardium segmentations from the endocardium, our method attained a mean Dice of 0.88 and MAE of 5.05. Hence, the proposed technique achieves competitive results comparable to the state-of-the-art in LV segmentation, requiring only one set of LV masks (either endocardium or myocardium) as input.",Training;Image segmentation;Accuracy;Ultrasonic imaging;Translation;Imaging;Manuals;Myocardium;Generative adversarial networks;Testing;Deep Learning;Segmentation;Generative Adversarial Network;Echocardiography,"{'month': 'Sep.', 'issn': '2375-0448', 'doi': '10.1109/UFFC-JS60046.2024.10793762', 'keywords': 'Training;Image segmentation;Accuracy;Ultrasonic imaging;Translation;Imaging;Manuals;Myocardium;Generative adversarial networks;Testing;Deep Learning;Segmentation;Generative Adversarial Network;Echocardiography', 'abstract': 'In echocardiography, accurate segmentation of cardiac structures, particularly left ventricle (LV), is crucial for clinical diagnosis. However, manual segmentation is user-intensive and prone to variability among experts due to intricate anatomical details and imaging artifacts. Our aim is to propose an artificial intelligence (AI)-based technique to automatically segment the LV structure, enhancing accuracy and reducing segmentation time and subjectivity of manual segmentation. We propose a novel Bidirectional generative adversarial network (Bi-GAN) for automated segmentation of LV structures. Specifically, we utilize Bi-GAN for segmenting the endocardium region from available myocardium region and vice versa. The adversarial training, Bi-GAN minimizes the losses and produces the target domain segmentation. The analysis is conducted on the cardiac acquisitions for the multi-structure ultrasound segmentation (CAMUS) dataset, comprising 900 echocardiographic training images and 100 testing images in 4CH views during both end-diastolic (ED) and end-systolic (ES) phases. Results showed a mean Dice of 0.97 for both the ED and ES phases, along with an MAE of 1.90. Conversely, when generating myocardium segmentations from the endocardium, our method attained a mean Dice of 0.88 and MAE of 5.05. Hence, the proposed technique achieves competitive results comparable to the state-of-the-art in LV segmentation, requiring only one set of LV masks (either endocardium or myocardium) as input.', 'pages': '1-4', 'number': '', 'volume': '', 'year': '2024', 'title': 'A Novel Approach for Automated Segmentation of Left Ventricle Based on Bidirectional Myocardium to Endocardium Translation Using Generative Adversarial Network', 'booktitle': '2024 IEEE Ultrasonics, Ferroelectrics, and Frequency Control Joint Symposium (UFFC-JS)', 'author': 'Fatima, Noreen and Afrakhteh, Sajjad and Demi, Libertario', 'ENTRYTYPE': 'inproceedings', 'ID': '10793762'}"
10498169,Dynamic Capturing of Facial Expression and 3D Animation Generation based on Generative Adversarial Network,"Li, Jian",Li,10.1109/ICICACS60521.2024.10498169,2024,2024 International Conference on Integrated Circuits and Communication Systems (ICICACS),"In recent years, interaction between human and computer based on the functions of computer software is distant from meeting human needs for computer use through development of computer and Artificial Intelligence (AI). People expect a more convenient and faster human-computer interface. In the mobile network environment, the accuracy of related image matching algorithms is affected by factors such as bandwidth uncertainty and channel interference, resulting in significant limitations in image feature matching. The deep learning approach of Generative Adversarial Network (GAN) is proposed for dynamic capturing of 3D animation effect generation by using facial expressions. The feature extraction approach utilized the histogram (HOG) and utilizes the GAN in classification for dynamic capture of animation effects. OpenGL and C++ are employed for 3D animation to simulate the rendering. The outcomes exhibit that the face detection approach has attained good performance in both accuracy and speed.",Deep learning;Histograms;Three-dimensional displays;Heuristic algorithms;C++ languages;Animation;Generative adversarial networks;Animation effect;Deep Learning;Dynamic Capturing;Generative Adversarial Network;and Histogram,"{'month': 'Feb', 'issn': '', 'doi': '10.1109/ICICACS60521.2024.10498169', 'keywords': 'Deep learning;Histograms;Three-dimensional displays;Heuristic algorithms;C++ languages;Animation;Generative adversarial networks;Animation effect;Deep Learning;Dynamic Capturing;Generative Adversarial Network;and Histogram', 'abstract': 'In recent years, interaction between human and computer based on the functions of computer software is distant from meeting human needs for computer use through development of computer and Artificial Intelligence (AI). People expect a more convenient and faster human-computer interface. In the mobile network environment, the accuracy of related image matching algorithms is affected by factors such as bandwidth uncertainty and channel interference, resulting in significant limitations in image feature matching. The deep learning approach of Generative Adversarial Network (GAN) is proposed for dynamic capturing of 3D animation effect generation by using facial expressions. The feature extraction approach utilized the histogram (HOG) and utilizes the GAN in classification for dynamic capture of animation effects. OpenGL and C++ are employed for 3D animation to simulate the rendering. The outcomes exhibit that the face detection approach has attained good performance in both accuracy and speed.', 'pages': '1-5', 'number': '', 'volume': '', 'year': '2024', 'title': 'Dynamic Capturing of Facial Expression and 3D Animation Generation based on Generative Adversarial Network', 'booktitle': '2024 International Conference on Integrated Circuits and Communication Systems (ICICACS)', 'author': 'Li, Jian', 'ENTRYTYPE': 'inproceedings', 'ID': '10498169'}"
9525515,Dual-Energy CT Image Super-resolution via Generative Adversarial Network,"Zhong, XinYi and Wang, YiZhong and Cai, AiLong and Liang, NingNing and Li, Lei and Yan, Bin",Zhong,10.1109/AIEA53260.2021.00079,2021,2021 International Conference on Artificial Intelligence and Electromechanical Automation (AIEA),"Photon counting detector obtains CT images from multiple energy bins, and acquires X-ray intensity data of different energy bins through one X-ray exposure. However, the spatial resolution of the reconstructed image will decrease, and the image will be blurred due to the low photon count in the narrow energy box width, quantum noise and the response problem of detector cells. Deep learning is gradually applied to medical images to reduce noise or improve resolution, which has exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images to high-resolution (HR) images. Inspired by the cycle-GAN, we propose a novel network model which realize the mapping of HR images to LR images for Dual-Energy CT (DECT) reconstruction. Experimental results show that the reconstructed image has significant improvements in peak signal-to-noise ratio (PSNR) and root mean square error (RMSE). Compared with the traditional super-resolution reconstruction method, this method has better experimental results.",PSNR;Computed tomography;Superresolution;Detectors;Reconstruction algorithms;Spatial resolution;Root mean square;Dual-Energy CT;super resolution;deep learning;generative adversarial network,"{'month': 'May', 'issn': '', 'doi': '10.1109/AIEA53260.2021.00079', 'keywords': 'PSNR;Computed tomography;Superresolution;Detectors;Reconstruction algorithms;Spatial resolution;Root mean square;Dual-Energy CT;super resolution;deep learning;generative adversarial network', 'abstract': 'Photon counting detector obtains CT images from multiple energy bins, and acquires X-ray intensity data of different energy bins through one X-ray exposure. However, the spatial resolution of the reconstructed image will decrease, and the image will be blurred due to the low photon count in the narrow energy box width, quantum noise and the response problem of detector cells. Deep learning is gradually applied to medical images to reduce noise or improve resolution, which has exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images to high-resolution (HR) images. Inspired by the cycle-GAN, we propose a novel network model which realize the mapping of HR images to LR images for Dual-Energy CT (DECT) reconstruction. Experimental results show that the reconstructed image has significant improvements in peak signal-to-noise ratio (PSNR) and root mean square error (RMSE). Compared with the traditional super-resolution reconstruction method, this method has better experimental results.', 'pages': '343-347', 'number': '', 'volume': '', 'year': '2021', 'title': 'Dual-Energy CT Image Super-resolution via Generative Adversarial Network', 'booktitle': '2021 International Conference on Artificial Intelligence and Electromechanical Automation (AIEA)', 'author': 'Zhong, XinYi and Wang, YiZhong and Cai, AiLong and Liang, NingNing and Li, Lei and Yan, Bin', 'ENTRYTYPE': 'inproceedings', 'ID': '9525515'}"
10774771,Artificial Intelligence for Detecting Cyber Attacks in Deepfake \& Identity Theft,"Lokhande, Meghana and Raut, Prajot and Gawali, Kiran and Ahirrao, Mrudul and Bhande, Abhishek",Lokhande,10.1109/ICCUBEA61740.2024.10774771,2024,"2024 8th International Conference on Computing, Communication, Control and Automation (ICCUBEA)","In today's era of digital world and evolving cyber threats, this research paper presents a unified exploration of innovative techniques harnessing the power of artificial intelligence and blockchain to combat deepfake attacks and identity theft. These intertwined challenges demand holistic solutions that transcend traditional boundaries. As the digital landscape is increasingly infiltrated by deepfake technology, concerns surrounding the authenticity of digital content are reaching a critical juncture. Deepfake attacks, capable of generating persuasive yet false imagery and videos, pose a grave societal threat. They undermine trust in media, perpetuate misinformation, and raise the specter of identity theft. Image processing techniques for deepfake detection aim to distinguish real from manipulated content by leveraging advances in AI. Meanwhile, the application of AI and machine learning in deepfake detection has yielded promising results, enhancing our capacity to discern authentic media from forgeries. The research converges on a proactive approach, introducing a pioneering framework that integrates AI and blockchain technology. This paper proposes an Artificial Intelligence-based protection framework, leveraging unsupervised pre-training techniques and Dense Neural Networks (DNN), to combat identity impersonation attacks, particularly the Clone ID attack directed at the Routing Protocol for Low Power and Lossy Networks (RPL). The research investigates the potential of blockchain, including Smart Contracts to combat the deepfake problem by verifying digital media's history and provenance.",Deep learning;Deepfakes;Identity theft;Prevention and mitigation;Neural networks;Smart contracts;Impersonation attacks;Routing protocols;Blockchains;Protection;Deepfake detection;Convolutional Neural Networks;Blockchain technology;Digital content authenticity;Image and video analysis;Threat detection;Artificial Intelligence;Media integrity,"{'month': 'Aug', 'issn': '2771-1358', 'doi': '10.1109/ICCUBEA61740.2024.10774771', 'keywords': 'Deep learning;Deepfakes;Identity theft;Prevention and mitigation;Neural networks;Smart contracts;Impersonation attacks;Routing protocols;Blockchains;Protection;Deepfake detection;Convolutional Neural Networks;Blockchain technology;Digital content authenticity;Image and video analysis;Threat detection;Artificial Intelligence;Media integrity', 'abstract': ""In today's era of digital world and evolving cyber threats, this research paper presents a unified exploration of innovative techniques harnessing the power of artificial intelligence and blockchain to combat deepfake attacks and identity theft. These intertwined challenges demand holistic solutions that transcend traditional boundaries. As the digital landscape is increasingly infiltrated by deepfake technology, concerns surrounding the authenticity of digital content are reaching a critical juncture. Deepfake attacks, capable of generating persuasive yet false imagery and videos, pose a grave societal threat. They undermine trust in media, perpetuate misinformation, and raise the specter of identity theft. Image processing techniques for deepfake detection aim to distinguish real from manipulated content by leveraging advances in AI. Meanwhile, the application of AI and machine learning in deepfake detection has yielded promising results, enhancing our capacity to discern authentic media from forgeries. The research converges on a proactive approach, introducing a pioneering framework that integrates AI and blockchain technology. This paper proposes an Artificial Intelligence-based protection framework, leveraging unsupervised pre-training techniques and Dense Neural Networks (DNN), to combat identity impersonation attacks, particularly the Clone ID attack directed at the Routing Protocol for Low Power and Lossy Networks (RPL). The research investigates the potential of blockchain, including Smart Contracts to combat the deepfake problem by verifying digital media's history and provenance."", 'pages': '1-6', 'number': '', 'volume': '', 'year': '2024', 'title': 'Artificial Intelligence for Detecting Cyber Attacks in Deepfake \\& Identity Theft', 'booktitle': '2024 8th International Conference on Computing, Communication, Control and Automation (ICCUBEA)', 'author': 'Lokhande, Meghana and Raut, Prajot and Gawali, Kiran and Ahirrao, Mrudul and Bhande, Abhishek', 'ENTRYTYPE': 'inproceedings', 'ID': '10774771'}"
11005231,Artificial Intelligence Assisted Side-Channel Analysis: Security Assessment Challenges,"Annadurai, Suganya and Meenakshi, M",Annadurai,10.1109/WiSPNET64060.2025.11005231,2025,2025 International Conference on Wireless Communications Signal Processing and Networking (WiSPNET),"Side-channel attack exploits the information leakage through physical medium to retrieve the secret key used by the cryptographic module. Hence, resistance to such attack has been brought as a mandate for the cryptographic module that are deployed in critical applications. Various countermeasures are being developed to make cryptographic implementations more robust against side-channel attacks. However, the current security assessment standard, which is based on statistical methods will not estimate the efficacy of these countermeasures. Assessing the efficacy of a countermeasure using Artificial Intelligence (AI) techniques or machine learning will be helpful to evaluate the side-channel resistant implementations and will help to streamline the evaluation process. In this paper we outline the datasets available for deep learning on public domain and the dataset created by us along with the recent advancement in AI assisted side-channel attacks that are explored by the research community. Further, this paper discusses the challenges in the security assessment methods using artificial intelligence techniques.",Deep learning;Training;Resistance;Wireless communication;Statistical analysis;Signal processing algorithms;Side-channel attacks;Optimization;Standards;Testing;side-channel analysis;artificial intelligence;countermeasures;security assessment methods,"{'month': 'March', 'issn': '', 'doi': '10.1109/WiSPNET64060.2025.11005231', 'keywords': 'Deep learning;Training;Resistance;Wireless communication;Statistical analysis;Signal processing algorithms;Side-channel attacks;Optimization;Standards;Testing;side-channel analysis;artificial intelligence;countermeasures;security assessment methods', 'abstract': 'Side-channel attack exploits the information leakage through physical medium to retrieve the secret key used by the cryptographic module. Hence, resistance to such attack has been brought as a mandate for the cryptographic module that are deployed in critical applications. Various countermeasures are being developed to make cryptographic implementations more robust against side-channel attacks. However, the current security assessment standard, which is based on statistical methods will not estimate the efficacy of these countermeasures. Assessing the efficacy of a countermeasure using Artificial Intelligence (AI) techniques or machine learning will be helpful to evaluate the side-channel resistant implementations and will help to streamline the evaluation process. In this paper we outline the datasets available for deep learning on public domain and the dataset created by us along with the recent advancement in AI assisted side-channel attacks that are explored by the research community. Further, this paper discusses the challenges in the security assessment methods using artificial intelligence techniques.', 'pages': '1-5', 'number': '', 'volume': '', 'year': '2025', 'title': 'Artificial Intelligence Assisted Side-Channel Analysis: Security Assessment Challenges', 'booktitle': '2025 International Conference on Wireless Communications Signal Processing and Networking (WiSPNET)', 'author': 'Annadurai, Suganya and Meenakshi, M', 'ENTRYTYPE': 'inproceedings', 'ID': '11005231'}"
10974319,Vision-and-Language Navigation Generative Pretrained Transformer,"Guo, Shijie and Zhu, Ming and Zhang, Jiawei and Min, Hang and Zhu, Wei",Guo,10.1109/AIHCIR65563.2024.00013,2024,"2024 3rd International Conference on Artificial Intelligence, Human-Computer Interaction and Robotics (AIHCIR)","In the domain of Vision-and-Language Navigation (VLN), agents are given the task of navigating through real-world environments, guided by verbal instructions. This poses a significant challenge in maintaining adherence to the provided linguistic instructions throughout the navigation process. Tra-ditional approaches typically employ encoders to meticulously track previous locations and actions, which, in turn, escalates the complexity of the model and the consumption of resources. Our approach introduces the Vision-and-Language Navigation Gen-erative Pretrained Transformer (VLN-GPT), which leverages a transformer decoder model (GPT2) to capture dependencies in the trajectory sequence, eliminating the requirement for separate modules to encode historical data. This strategy facilitates direct access to historical information via the trajectory sequence, thereby improving efficiency. Evaluations conducted on the VLN dataset demonstrate that our VLN-GPT model outperforms the more complex encoder-dependent models, highlighting its effectiveness in the field.",Human computer interaction;Navigation;Decision making;Reinforcement learning;Linguistics;Transformers;Trajectory;Decoding;History;Robots;Vision-and-Language Navigation;Generative Pretrained Transformer;Reinforcement Learning,"{'month': 'Nov', 'issn': '', 'doi': '10.1109/AIHCIR65563.2024.00013', 'keywords': 'Human computer interaction;Navigation;Decision making;Reinforcement learning;Linguistics;Transformers;Trajectory;Decoding;History;Robots;Vision-and-Language Navigation;Generative Pretrained Transformer;Reinforcement Learning', 'abstract': 'In the domain of Vision-and-Language Navigation (VLN), agents are given the task of navigating through real-world environments, guided by verbal instructions. This poses a significant challenge in maintaining adherence to the provided linguistic instructions throughout the navigation process. Tra-ditional approaches typically employ encoders to meticulously track previous locations and actions, which, in turn, escalates the complexity of the model and the consumption of resources. Our approach introduces the Vision-and-Language Navigation Gen-erative Pretrained Transformer (VLN-GPT), which leverages a transformer decoder model (GPT2) to capture dependencies in the trajectory sequence, eliminating the requirement for separate modules to encode historical data. This strategy facilitates direct access to historical information via the trajectory sequence, thereby improving efficiency. Evaluations conducted on the VLN dataset demonstrate that our VLN-GPT model outperforms the more complex encoder-dependent models, highlighting its effectiveness in the field.', 'pages': '38-42', 'number': '', 'volume': '', 'year': '2024', 'title': 'Vision-and-Language Navigation Generative Pretrained Transformer', 'booktitle': '2024 3rd International Conference on Artificial Intelligence, Human-Computer Interaction and Robotics (AIHCIR)', 'author': 'Guo, Shijie and Zhu, Ming and Zhang, Jiawei and Min, Hang and Zhu, Wei', 'ENTRYTYPE': 'inproceedings', 'ID': '10974319'}"
11108954,Application of AI in Lightning and Thunderstorm Forecasting: A Vision for the Future,"Gomes, Chandima",Gomes,10.1109/APL65034.2025.11108954,2025,2025 13th Asia-Pacific International Conference on Lightning (APL),"The application of artificial intelligence (AI) in lightning forecasting and thunderstorm tracking is rapidly transforming atmospheric science by enhancing predictive accuracy and real-time monitoring capabilities. Traditional meteorological models rely on numerical weather prediction (NWP) techniques, which, despite their effectiveness, are computationally expensive and often lack the precision required for high-impact weather events. AI-driven approaches, particularly deep learning, machine learning, and neural networks, offer novel solutions by processing vast datasets from satellites, ground-based lightning detection networks, and radar systems with unparalleled speed and efficiency. This paper explores the integration of AI methodologies with conventional meteorological frameworks to improve short-term lightning forecasts and real-time thunderstorm tracking. We discuss recent advancements in AI-based convective storm prediction, the role of ensemble learning in minimizing forecast uncertainties, and the application of generative models for synthesizing realistic weather scenarios. Additionally, we highlight case studies demonstrating the efficacy of AI-driven nowcasting systems in reducing lightning-related casualties and infrastructure damage. The challenges associated with AI deployment in meteorology, including data biases, model interpretability, and computational constraints, are critically examined. Future research directions emphasize the need for hybrid models that blend AI with physics-based approaches, the potential of edge computing for real-time processing, and the prospects of AI-assisted early warning systems in mitigating lightning hazards. By bridging AI innovations with meteorological science, this study envisions a transformative shift in lightning forecasting, paving the way for more resilient disaster management strategies in a changing climate.",Computational modeling;Lightning;Weather forecasting;Predictive models;Alarm systems;Radar tracking;Real-time systems;Data models;Artificial intelligence;Meteorology;AI in meteorology;lightning prediction models;thunderstorm nowcasting;machine learning in weather forecasting;AI-based early warning systems,"{'month': 'June', 'issn': '', 'doi': '10.1109/APL65034.2025.11108954', 'keywords': 'Computational modeling;Lightning;Weather forecasting;Predictive models;Alarm systems;Radar tracking;Real-time systems;Data models;Artificial intelligence;Meteorology;AI in meteorology;lightning prediction models;thunderstorm nowcasting;machine learning in weather forecasting;AI-based early warning systems', 'abstract': 'The application of artificial intelligence (AI) in lightning forecasting and thunderstorm tracking is rapidly transforming atmospheric science by enhancing predictive accuracy and real-time monitoring capabilities. Traditional meteorological models rely on numerical weather prediction (NWP) techniques, which, despite their effectiveness, are computationally expensive and often lack the precision required for high-impact weather events. AI-driven approaches, particularly deep learning, machine learning, and neural networks, offer novel solutions by processing vast datasets from satellites, ground-based lightning detection networks, and radar systems with unparalleled speed and efficiency. This paper explores the integration of AI methodologies with conventional meteorological frameworks to improve short-term lightning forecasts and real-time thunderstorm tracking. We discuss recent advancements in AI-based convective storm prediction, the role of ensemble learning in minimizing forecast uncertainties, and the application of generative models for synthesizing realistic weather scenarios. Additionally, we highlight case studies demonstrating the efficacy of AI-driven nowcasting systems in reducing lightning-related casualties and infrastructure damage. The challenges associated with AI deployment in meteorology, including data biases, model interpretability, and computational constraints, are critically examined. Future research directions emphasize the need for hybrid models that blend AI with physics-based approaches, the potential of edge computing for real-time processing, and the prospects of AI-assisted early warning systems in mitigating lightning hazards. By bridging AI innovations with meteorological science, this study envisions a transformative shift in lightning forecasting, paving the way for more resilient disaster management strategies in a changing climate.', 'pages': '228-233', 'number': '', 'volume': '', 'year': '2025', 'title': 'Application of AI in Lightning and Thunderstorm Forecasting: A Vision for the Future', 'booktitle': '2025 13th Asia-Pacific International Conference on Lightning (APL)', 'author': 'Gomes, Chandima', 'ENTRYTYPE': 'inproceedings', 'ID': '11108954'}"
10678704,Generative AI for Research Data Processing: Lessons Learnt From Three Use Cases,"Mitra, Modhurita and de Vos, Martine G. and Cortinovis, Nicola and Ometto, Dawa",Mitra,10.1109/e-Science62913.2024.10678704,2024,2024 IEEE 20th International Conference on e-Science (e-Science),"There has been enormous interest in generative AI since ChatGPT was launched in 2022. However, there are concerns about the accuracy and consistency of the outputs of generative AI. We have carried out an exploratory study on the application of this new technology in research data processing. We identified tasks for which rule-based or traditional machine learning approaches were difficult to apply, and then performed these tasks using generative AI.We demonstrate the feasibility of using the generative AI model Claude 3 Opus in three research projects involving complex data processing tasks:1)Information extraction: We extract plant species names from historical seedlists (catalogues of seeds) published by botanical gardens.2)Natural language understanding: We extract certain data points (name of drug, name of health indication, relative effectiveness, cost-effectiveness, etc.) from documents published by Health Technology Assessment organisations in the EU.3)Text classification: We assign industry codes to projects on the crowdfunding website Kickstarter.We share the lessons we learnt from these use cases: How to determine if generative AI is an appropriate tool for a given data processing task, and if so, how to maximise the accuracy and consistency of the results obtained.",Industries;Drugs;Accuracy;Crowdfunding;Codes;Generative AI;Machine learning;Generative AI;Large Language Models;artificial intelligence;data processing;accuracy of results;consistency of results;reliability of research method,"{'month': 'Sep.', 'issn': '2325-3703', 'doi': '10.1109/e-Science62913.2024.10678704', 'keywords': 'Industries;Drugs;Accuracy;Crowdfunding;Codes;Generative AI;Machine learning;Generative AI;Large Language Models;artificial intelligence;data processing;accuracy of results;consistency of results;reliability of research method', 'abstract': 'There has been enormous interest in generative AI since ChatGPT was launched in 2022. However, there are concerns about the accuracy and consistency of the outputs of generative AI. We have carried out an exploratory study on the application of this new technology in research data processing. We identified tasks for which rule-based or traditional machine learning approaches were difficult to apply, and then performed these tasks using generative AI.We demonstrate the feasibility of using the generative AI model Claude 3 Opus in three research projects involving complex data processing tasks:1)Information extraction: We extract plant species names from historical seedlists (catalogues of seeds) published by botanical gardens.2)Natural language understanding: We extract certain data points (name of drug, name of health indication, relative effectiveness, cost-effectiveness, etc.) from documents published by Health Technology Assessment organisations in the EU.3)Text classification: We assign industry codes to projects on the crowdfunding website Kickstarter.We share the lessons we learnt from these use cases: How to determine if generative AI is an appropriate tool for a given data processing task, and if so, how to maximise the accuracy and consistency of the results obtained.', 'pages': '1-10', 'number': '', 'volume': '', 'year': '2024', 'title': 'Generative AI for Research Data Processing: Lessons Learnt From Three Use Cases', 'booktitle': '2024 IEEE 20th International Conference on e-Science (e-Science)', 'author': 'Mitra, Modhurita and de Vos, Martine G. and Cortinovis, Nicola and Ometto, Dawa', 'ENTRYTYPE': 'inproceedings', 'ID': '10678704'}"
11137298,"Data Engineering for Intelligent Systems and Generative AI: Architectures, Pipelines, and Strategy","Murri, Srinivas",Murri,10.1109/SATC65530.2025.11137298,2025,"2025 1st International Conference on Secure IoT, Assured and Trusted Computing (SATC)","Integrating generative AI and data engineering greatly enhances intelligent systems by addressing scalability, effectiveness, and real-time efficiency issues. Data engineering establishes the foundation to handle large and complex datasets, while generative AI streamlines these processes by automating repetitive tasks, detecting anomalies, and optimizing query performance. This paper discusses the essential concepts of data pipelines, including modularity, scalability, and automation, which are necessary to support large-scale data environments. Key components of a data pipeline, such as data sources, data flows, processing, and monitoring, are examined to enhance the data preparation process for analysis. This research contributes to the growing evidence showing how generative AI transforms data workflow optimization, particularly for machine learning (ML) pipelines and natural language processing (NLP). The literature review explores innovative developments in generative AI, NLP, and data engineering technologies, highlighting various use cases. Several studies demonstrate that generative AI can reduce costs, improve data quality, and simplify complex data transformations. Additionally, challenges related to data privacy, ethical considerations, and computational expenses are addressed, with future research directions proposed to enhance data pipeline efficiency and AI-driven processes. The findings of this study provide a clearer understanding of how data engineering practices can be combined with AI technologies to support innovation and create more efficient intelligent systems.",Trusted computing;Generative AI;Scalability;Data integrity;Pipelines;Computer architecture;Transforms;Data engineering;Natural language processing;Intelligent systems;Data Engineering;Generative AI;Intelligent Systems;Artificial Intelligence;Data Pipeline;Architectures;lifecycle;Data Quality,"{'month': 'Feb', 'issn': '', 'doi': '10.1109/SATC65530.2025.11137298', 'keywords': 'Trusted computing;Generative AI;Scalability;Data integrity;Pipelines;Computer architecture;Transforms;Data engineering;Natural language processing;Intelligent systems;Data Engineering;Generative AI;Intelligent Systems;Artificial Intelligence;Data Pipeline;Architectures;lifecycle;Data Quality', 'abstract': 'Integrating generative AI and data engineering greatly enhances intelligent systems by addressing scalability, effectiveness, and real-time efficiency issues. Data engineering establishes the foundation to handle large and complex datasets, while generative AI streamlines these processes by automating repetitive tasks, detecting anomalies, and optimizing query performance. This paper discusses the essential concepts of data pipelines, including modularity, scalability, and automation, which are necessary to support large-scale data environments. Key components of a data pipeline, such as data sources, data flows, processing, and monitoring, are examined to enhance the data preparation process for analysis. This research contributes to the growing evidence showing how generative AI transforms data workflow optimization, particularly for machine learning (ML) pipelines and natural language processing (NLP). The literature review explores innovative developments in generative AI, NLP, and data engineering technologies, highlighting various use cases. Several studies demonstrate that generative AI can reduce costs, improve data quality, and simplify complex data transformations. Additionally, challenges related to data privacy, ethical considerations, and computational expenses are addressed, with future research directions proposed to enhance data pipeline efficiency and AI-driven processes. The findings of this study provide a clearer understanding of how data engineering practices can be combined with AI technologies to support innovation and create more efficient intelligent systems.', 'pages': '1-5', 'number': '', 'volume': '', 'year': '2025', 'title': 'Data Engineering for Intelligent Systems and Generative AI: Architectures, Pipelines, and Strategy', 'booktitle': '2025 1st International Conference on Secure IoT, Assured and Trusted Computing (SATC)', 'author': 'Murri, Srinivas', 'ENTRYTYPE': 'inproceedings', 'ID': '11137298'}"
10844093,Mathematical Modeling of Cyberattack Defense Mechanism Using Hybrid Transfer Learning With Snow Ablation Optimization Algorithm in Critical Infrastructures,"Ishak, Mohamad Khairi",Ishak,10.1109/ACCESS.2025.3530931,2025,IEEE Access,"Cybersecurity is a significant topic that has turned into an efficient one at present owing to the increasing dependency on interconnected methods and technology. As digitalization upsurges, the requirement for cybersecurity measures becomes even more vital for numerous networks. To certify the security of critical infrastructures, numerous cyber security solutions must be taken together and the essential infrastructure must be developed. Industrial control methods are one of the most vital aspects of the cybersecurity of critical infrastructures. It is possible to entirely stop these physically located devices’ operations and do substantial destruction with a cyberattack. Whereas AI solutions are being utilized in numerous fields, cyber security has started to become one of the concentrated fields of artificial intelligence (AI) domain. Consequently, there are many studies on identifying cyberattacks by utilizing AI methods. It is probable to utilize AI to help and support cybersecurity solutions to develop cybersecurity of significant infrastructures. This study develops a Cyberattack Defense Mechanism using Hybrid Transfer Learning with Snow Ablation Optimization Algorithm (CDMHTL-SAOA) technique in Critical infrastructures. The main cause of the CDMHTL-SAOA model is to improve the cyber security maturity level of critical infrastructures and inspect both traditional cyberattack and AI approaches. Primarily, the data normalization process can be implemented to scale the raw data into a uniform format. In addition, the snow ablation optimization (SAO) algorithm can be exploited for the optimum choice of feature subsets. For the cybersecurity classification process, the presented CDMHTL-SAOA technique applies the hybrid of convolutional neural network and bi-directional long short-term memory (CNN-BiLSTM) method. Eventually, the parameter choice of the CNN-BiLSTM technique has been implemented by the design of the hippopotamus optimization algorithm (HOA). To represent the better solution of the CDMHTL-SAOA classifier, a simulation validation can be tested on a benchmark database and the solutions are measured for various aspects. The simulation outcomes certified the improved execution of the CDMHTL-SAOA method over other techniques.",Snow;Computer security;Security;Cyberattack;Artificial intelligence;Optimization;Critical infrastructure;Classification algorithms;Transfer learning;Vectors;Cyberattack;hybrid transfer learning;snow ablation optimization;hyperparameter selection;data normalization,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2025.3530931', 'keywords': 'Snow;Computer security;Security;Cyberattack;Artificial intelligence;Optimization;Critical infrastructure;Classification algorithms;Transfer learning;Vectors;Cyberattack;hybrid transfer learning;snow ablation optimization;hyperparameter selection;data normalization', 'abstract': 'Cybersecurity is a significant topic that has turned into an efficient one at present owing to the increasing dependency on interconnected methods and technology. As digitalization upsurges, the requirement for cybersecurity measures becomes even more vital for numerous networks. To certify the security of critical infrastructures, numerous cyber security solutions must be taken together and the essential infrastructure must be developed. Industrial control methods are one of the most vital aspects of the cybersecurity of critical infrastructures. It is possible to entirely stop these physically located devices’ operations and do substantial destruction with a cyberattack. Whereas AI solutions are being utilized in numerous fields, cyber security has started to become one of the concentrated fields of artificial intelligence (AI) domain. Consequently, there are many studies on identifying cyberattacks by utilizing AI methods. It is probable to utilize AI to help and support cybersecurity solutions to develop cybersecurity of significant infrastructures. This study develops a Cyberattack Defense Mechanism using Hybrid Transfer Learning with Snow Ablation Optimization Algorithm (CDMHTL-SAOA) technique in Critical infrastructures. The main cause of the CDMHTL-SAOA model is to improve the cyber security maturity level of critical infrastructures and inspect both traditional cyberattack and AI approaches. Primarily, the data normalization process can be implemented to scale the raw data into a uniform format. In addition, the snow ablation optimization (SAO) algorithm can be exploited for the optimum choice of feature subsets. For the cybersecurity classification process, the presented CDMHTL-SAOA technique applies the hybrid of convolutional neural network and bi-directional long short-term memory (CNN-BiLSTM) method. Eventually, the parameter choice of the CNN-BiLSTM technique has been implemented by the design of the hippopotamus optimization algorithm (HOA). To represent the better solution of the CDMHTL-SAOA classifier, a simulation validation can be tested on a benchmark database and the solutions are measured for various aspects. The simulation outcomes certified the improved execution of the CDMHTL-SAOA method over other techniques.', 'pages': '13329-13340', 'number': '', 'volume': '13', 'year': '2025', 'title': 'Mathematical Modeling of Cyberattack Defense Mechanism Using Hybrid Transfer Learning With Snow Ablation Optimization Algorithm in Critical Infrastructures', 'journal': 'IEEE Access', 'author': 'Ishak, Mohamad Khairi', 'ENTRYTYPE': 'article', 'ID': '10844093'}"
10930479,A Survey of Large Language Model for Drug Research and Development,"Guo, Huijie and Xing, Xudong and Zhou, Yongjie and Jiang, Wenjiao and Chen, Xiaoyi and Wang, Ting and Jiang, Zixuan and Wang, Yibing and Hou, Junyan and Jiang, Yukun and Xu, Jianzhen",Guo,10.1109/ACCESS.2025.3552256,2025,IEEE Access,"Drug research and development (drug R\&D) is a sophisticated, cost-intensive, and time-consuming procedure with historically low success rates. The advent of Artificial Intelligence (AI) technologies has introduced innovative methods into drug R\&D, particularly by leveraging AI capabilities. Large language models (LLMs), a breakthrough in generative AI, have revolutionized drug discovery. With their extensive datasets, numerous parameters, and strong multitasking abilities, LLMs have significantly improved efficiency across various related domains, providing unparalleled support to drug R\&D. These models have facilitated a deeper understanding of intricate disease mechanisms and the identification of novel therapeutic strategies, ushering in a new era in drug development and clinical applications. As a result, the advancement of LLMs is poised to drive significant transformations in drug R\&D, emphasizing the importance of effectively leveraging this technology. This review provides insights into the architecture and characteristics of LLMs, explores their applications in drug R\&D, and highlights their research implications in bioinformatics data, including proteins, genes, and chemical compounds. Furthermore, it investigates the practical strategies of LLMs in drug discovery, drug repositioning, and clinical inquiries, presenting an innovative approach to research and future advancements in this field.",Drugs;Research and development;Artificial intelligence;Clinical trials;Costs;Biology;Drug discovery;Safety;Diseases;Speech recognition;Large model;artificial intelligence;transformer;drug research and development,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2025.3552256', 'keywords': 'Drugs;Research and development;Artificial intelligence;Clinical trials;Costs;Biology;Drug discovery;Safety;Diseases;Speech recognition;Large model;artificial intelligence;transformer;drug research and development', 'abstract': 'Drug research and development (drug R\\&D) is a sophisticated, cost-intensive, and time-consuming procedure with historically low success rates. The advent of Artificial Intelligence (AI) technologies has introduced innovative methods into drug R\\&D, particularly by leveraging AI capabilities. Large language models (LLMs), a breakthrough in generative AI, have revolutionized drug discovery. With their extensive datasets, numerous parameters, and strong multitasking abilities, LLMs have significantly improved efficiency across various related domains, providing unparalleled support to drug R\\&D. These models have facilitated a deeper understanding of intricate disease mechanisms and the identification of novel therapeutic strategies, ushering in a new era in drug development and clinical applications. As a result, the advancement of LLMs is poised to drive significant transformations in drug R\\&D, emphasizing the importance of effectively leveraging this technology. This review provides insights into the architecture and characteristics of LLMs, explores their applications in drug R\\&D, and highlights their research implications in bioinformatics data, including proteins, genes, and chemical compounds. Furthermore, it investigates the practical strategies of LLMs in drug discovery, drug repositioning, and clinical inquiries, presenting an innovative approach to research and future advancements in this field.', 'pages': '51110-51129', 'number': '', 'volume': '13', 'year': '2025', 'title': 'A Survey of Large Language Model for Drug Research and Development', 'journal': 'IEEE Access', 'author': 'Guo, Huijie and Xing, Xudong and Zhou, Yongjie and Jiang, Wenjiao and Chen, Xiaoyi and Wang, Ting and Jiang, Zixuan and Wang, Yibing and Hou, Junyan and Jiang, Yukun and Xu, Jianzhen', 'ENTRYTYPE': 'article', 'ID': '10930479'}"
10743442,Embedding Based Sensitive Element Injection against Text-to-Image Generative Models,"Jiang, Benrui and Chen, Kan and Hou, Guanyu and Chen, Xiying and He, Jiaming",Jiang,10.1109/ICSP62122.2024.10743442,2024,2024 9th International Conference on Intelligent Computing and Signal Processing (ICSP),"Text-to-image technique has exploded the research on artificial intelligence, and also deep learning technique has received widespread attentions. This technique is an emerging direction of deep learning. It is becoming increasingly popular among researchers and the public. Unfortunately, we found that text-to-image technique has certain security issues, especially adversarial and backdoor attacks. In our work, we explore a novel attack paradigm for the text-to-image scenarios. By our attack, we will use target embeddings to manipulate the user embeddings to generate malicious images. We designed a framework to verify our attack, and the experimental result shows that the efficiency of our attack is 95\%, this data proves the effectiveness of our experiments.",Deep learning;Text to image;Signal processing;Security;Text-to-image;Adversarial Attacks;Text Encoder;Transformer,"{'month': 'April', 'issn': '', 'doi': '10.1109/ICSP62122.2024.10743442', 'keywords': 'Deep learning;Text to image;Signal processing;Security;Text-to-image;Adversarial Attacks;Text Encoder;Transformer', 'abstract': 'Text-to-image technique has exploded the research on artificial intelligence, and also deep learning technique has received widespread attentions. This technique is an emerging direction of deep learning. It is becoming increasingly popular among researchers and the public. Unfortunately, we found that text-to-image technique has certain security issues, especially adversarial and backdoor attacks. In our work, we explore a novel attack paradigm for the text-to-image scenarios. By our attack, we will use target embeddings to manipulate the user embeddings to generate malicious images. We designed a framework to verify our attack, and the experimental result shows that the efficiency of our attack is 95\\%, this data proves the effectiveness of our experiments.', 'pages': '453-456', 'number': '', 'volume': '', 'year': '2024', 'title': 'Embedding Based Sensitive Element Injection against Text-to-Image Generative Models', 'booktitle': '2024 9th International Conference on Intelligent Computing and Signal Processing (ICSP)', 'author': 'Jiang, Benrui and Chen, Kan and Hou, Guanyu and Chen, Xiying and He, Jiaming', 'ENTRYTYPE': 'inproceedings', 'ID': '10743442'}"
11025027,"The Use of Generative AI in Workplace: Driving Factors, Barriers, and Benefits","Samudra, Bima Surya and Baihaqi, Imam and Isnaini, Fadila",Samudra,10.1109/TEMSCON-ASPAC62480.2024.11025027,2024,2024 IEEE Technology \& Engineering Management Conference - Asia Pacific (TEMSCON-ASPAC),"This study investigates the intention to use generative artificial intelligence (Gen AI) in the workplace based on the Unified Theory of Acceptance and Use of Technology (UTAUT) model. Survey was conducted using an online platform and randomly distributed amongst office workers in Indonesia, gathered 150 responses. Data were analyzed using Partial Least Squares Structural Equation Modeling (PLS-SEM). The results show that performance expectations and facilitating conditions significantly influence behavioral intentions to adopt Gen AI, while ease of use and social influence show limited impacts. Organizations should focus on developing infrastructure and providing technical support to better use in workplaces, thus increasing the adoption and satisfaction of users or workers with Gen AI technology.",Training;Surveys;Productivity;Adaptation models;Generative AI;Employment;Mathematical models;Regulation;Manufacturing;Artificial intelligence;Artificial Intelligence (AI);UTAUT;Manufacturing;Service,"{'month': 'Sep.', 'issn': '', 'doi': '10.1109/TEMSCON-ASPAC62480.2024.11025027', 'keywords': 'Training;Surveys;Productivity;Adaptation models;Generative AI;Employment;Mathematical models;Regulation;Manufacturing;Artificial intelligence;Artificial Intelligence (AI);UTAUT;Manufacturing;Service', 'abstract': 'This study investigates the intention to use generative artificial intelligence (Gen AI) in the workplace based on the Unified Theory of Acceptance and Use of Technology (UTAUT) model. Survey was conducted using an online platform and randomly distributed amongst office workers in Indonesia, gathered 150 responses. Data were analyzed using Partial Least Squares Structural Equation Modeling (PLS-SEM). The results show that performance expectations and facilitating conditions significantly influence behavioral intentions to adopt Gen AI, while ease of use and social influence show limited impacts. Organizations should focus on developing infrastructure and providing technical support to better use in workplaces, thus increasing the adoption and satisfaction of users or workers with Gen AI technology.', 'pages': '1-7', 'number': '', 'volume': '', 'year': '2024', 'title': 'The Use of Generative AI in Workplace: Driving Factors, Barriers, and Benefits', 'booktitle': '2024 IEEE Technology \\& Engineering Management Conference - Asia Pacific (TEMSCON-ASPAC)', 'author': 'Samudra, Bima Surya and Baihaqi, Imam and Isnaini, Fadila', 'ENTRYTYPE': 'inproceedings', 'ID': '11025027'}"
10982345,Glossary,"Bergeret, Olivier and Abbasi, Asif and Farvault, Joel",Bergeret,,2025,GenAI on AWS: A Practical Approach to Building Generative AI Applications on AWS,,,"{'url': 'https://ieeexplore-ieee-org.crai.referencistas.com/document/10982345', 'isbn': '9781394281305', 'publisher': 'Wiley', 'issn': '', 'doi': '', 'keywords': '', 'abstract': '', 'pages': '323-332', 'number': '', 'volume': '', 'year': '2025', 'title': 'Glossary', 'booktitle': 'GenAI on AWS: A Practical Approach to Building Generative AI Applications on AWS', 'author': 'Bergeret, Olivier and Abbasi, Asif and Farvault, Joel', 'ENTRYTYPE': 'inbook', 'ID': '10982345'}"
10803506,Variational Disentangled Generative Model for Individualized Treatment Effect Estimation,"Teng, Jiayi and Yang, Fan and Zhang, Qingke and Zhao, Yaoyao and Li, Na and Li, Qian",Teng,10.1109/MedAI62885.2024.00089,2024,2024 IEEE International Conference on Medical Artificial Intelligence (MedAI),"Estimating individualized treatment effect (ITE) from observational data is a challenging problem due to the lack of counterfactual outcomes and the presence of selection bias. Existing ITE estimation methods, which primarily focus on disentangled representation learning, have achieved significant success in estimating treatment effects. However, precisely learning disentangled latent factors remains an open problem. In this paper, we propose a Variational Disentangled Generative Model (VDGM) based on a collaborative strategy of Variational Auto-Encoder (VAE), Generative Adversarial Net (GAN), and Mutual Information Network Estimation. VDGM infers latent factors from observed covariates by VAE, disentangling factors into three disjoint sets corresponding to the instrumental, confounding, and adjustment factors. The disentangled latent factors can then be used to balance the distribution discrepancy between treatment and control groups, thereby reducing selection bias. Furthermore, VDGM addresses information loss by preserving useful predictive information under the regularization of a mutual information estimator. It also uses mutual information estimators to maintain the independence of disentangled latent factors. Additionally, our model uses the GAN framework to generate counterfactual outcomes for ITE estimation. Experimental results on synthetic and real-world datasets show that the proposed method outperforms the baselines in achieving low errors in ITE estimation.",Instruments;Disentangled representation learning;Estimation;Collaboration;Generative adversarial networks;Artificial intelligence;Mutual information;Individualized treatment effect;Disentangled representation learning;Mutual information network estimation,"{'month': 'Nov', 'issn': '', 'doi': '10.1109/MedAI62885.2024.00089', 'keywords': 'Instruments;Disentangled representation learning;Estimation;Collaboration;Generative adversarial networks;Artificial intelligence;Mutual information;Individualized treatment effect;Disentangled representation learning;Mutual information network estimation', 'abstract': 'Estimating individualized treatment effect (ITE) from observational data is a challenging problem due to the lack of counterfactual outcomes and the presence of selection bias. Existing ITE estimation methods, which primarily focus on disentangled representation learning, have achieved significant success in estimating treatment effects. However, precisely learning disentangled latent factors remains an open problem. In this paper, we propose a Variational Disentangled Generative Model (VDGM) based on a collaborative strategy of Variational Auto-Encoder (VAE), Generative Adversarial Net (GAN), and Mutual Information Network Estimation. VDGM infers latent factors from observed covariates by VAE, disentangling factors into three disjoint sets corresponding to the instrumental, confounding, and adjustment factors. The disentangled latent factors can then be used to balance the distribution discrepancy between treatment and control groups, thereby reducing selection bias. Furthermore, VDGM addresses information loss by preserving useful predictive information under the regularization of a mutual information estimator. It also uses mutual information estimators to maintain the independence of disentangled latent factors. Additionally, our model uses the GAN framework to generate counterfactual outcomes for ITE estimation. Experimental results on synthetic and real-world datasets show that the proposed method outperforms the baselines in achieving low errors in ITE estimation.', 'pages': '635-644', 'number': '', 'volume': '', 'year': '2024', 'title': 'Variational Disentangled Generative Model for Individualized Treatment Effect Estimation', 'booktitle': '2024 IEEE International Conference on Medical Artificial Intelligence (MedAI)', 'author': 'Teng, Jiayi and Yang, Fan and Zhang, Qingke and Zhao, Yaoyao and Li, Na and Li, Qian', 'ENTRYTYPE': 'inproceedings', 'ID': '10803506'}"
10968513,Validation of Generative Visual Solutions Using Prompt Engineering and Caption Based Visual Reasoning Models,"Arora, Manali and Garg, Chirag and Mangla, Deepanshu",Arora,10.1109/ICMLAS64557.2025.10968513,2025,2025 International Conference on Machine Learning and Autonomous Systems (ICMLAS),"We introduce the novel approach of validation of artificially generated images which helps to validate the images based on the prompt given for the generated image. Existing methods involve generation of images based on the prompts, diffusion of images and modification of images, but fail to determine the correctness of the generated image with respect to the generated content and prompt given at user end. The prompt used for generating the image using generative artificial intelligence solutions can be comprehensive and can hold more than single perspective. To address this issue while validating computer visual solutions, we propose a method for the validation of generative visual solutions using prompt engineering and caption based visual reasoning models. The proposed solution determines the different perspectives and comprehensiveness of the prompts based on entities and attributes and then, multiple test cases are formed considering different perspectives, in more detailed and comprehensive format. Hence, proposed solution validates the generated image based on the text prompt engineered for comprehensive understanding based on the complexity of the prompt suitable for visual reasoning models.",Visualization;Scalability;Computational modeling;Machine learning;Reliability engineering;Distortion;Cognition;Reflection;Prompt engineering;Testing;Visual reasoning;Prompt Engineering;LLM;Generative AI;Validation,"{'month': 'March', 'issn': '', 'doi': '10.1109/ICMLAS64557.2025.10968513', 'keywords': 'Visualization;Scalability;Computational modeling;Machine learning;Reliability engineering;Distortion;Cognition;Reflection;Prompt engineering;Testing;Visual reasoning;Prompt Engineering;LLM;Generative AI;Validation', 'abstract': 'We introduce the novel approach of validation of artificially generated images which helps to validate the images based on the prompt given for the generated image. Existing methods involve generation of images based on the prompts, diffusion of images and modification of images, but fail to determine the correctness of the generated image with respect to the generated content and prompt given at user end. The prompt used for generating the image using generative artificial intelligence solutions can be comprehensive and can hold more than single perspective. To address this issue while validating computer visual solutions, we propose a method for the validation of generative visual solutions using prompt engineering and caption based visual reasoning models. The proposed solution determines the different perspectives and comprehensiveness of the prompts based on entities and attributes and then, multiple test cases are formed considering different perspectives, in more detailed and comprehensive format. Hence, proposed solution validates the generated image based on the text prompt engineered for comprehensive understanding based on the complexity of the prompt suitable for visual reasoning models.', 'pages': '537-543', 'number': '', 'volume': '', 'year': '2025', 'title': 'Validation of Generative Visual Solutions Using Prompt Engineering and Caption Based Visual Reasoning Models', 'booktitle': '2025 International Conference on Machine Learning and Autonomous Systems (ICMLAS)', 'author': 'Arora, Manali and Garg, Chirag and Mangla, Deepanshu', 'ENTRYTYPE': 'inproceedings', 'ID': '10968513'}"
11100283,Comparison of AWS Architectures for Scalable and Cost-Efficient Retrieval-Augmented Generation,"Stojanović, Dimitrije and Vidaković, Luka and Pavković, Bogdan and Četić, Nenad and Krunić, Momčilo",Stojanović,10.1109/ZINC65316.2025.11100283,2025,2025 IEEE Zooming Innovation in Consumer Technologies Conference (ZINC),"Large Language Models (LLMs) require up-to-date and domain-specific knowledge to generate accurate responses. As demand for generative Artificial intelligence (AI) applications grows, there is a need for Retrieval-Augmented Generation (RAG) architectures that can dynamically scale and efficiently manage resources. Conventional deployments on Amazon EC2 face challenges in scalability, cost efficiency, and operational complexity, making it difficult to adapt quickly to unpredictable workloads. Another approach is serverless RAG architecture on AWS that leverages Lambda, Amazon S3, DynamoDB, and API Gateway to automate scaling, reduce management overhead, and implement a cost-effective, pay-per-use model. Our evaluation demonstrates that a serverless approach can give savings of up to 87\% for loads of 10000 requests per hour compared to EC2 instances while meeting the performance and efficiency requirements of modern AI applications. Serverless architecture establishes a pathway for developing more resilient and scalable cloud-based generative AI systems.",Technological innovation;Generative AI;Scalability;Large language models;Retrieval augmented generation;Serverless computing;Computer architecture;Logic gates;Zinc;Load modeling;Cloud computing;Serverless computing;Retrieval-Augmented Generation;Scalability;Generative AI,"{'month': 'May', 'issn': '2995-2689', 'doi': '10.1109/ZINC65316.2025.11100283', 'keywords': 'Technological innovation;Generative AI;Scalability;Large language models;Retrieval augmented generation;Serverless computing;Computer architecture;Logic gates;Zinc;Load modeling;Cloud computing;Serverless computing;Retrieval-Augmented Generation;Scalability;Generative AI', 'abstract': 'Large Language Models (LLMs) require up-to-date and domain-specific knowledge to generate accurate responses. As demand for generative Artificial intelligence (AI) applications grows, there is a need for Retrieval-Augmented Generation (RAG) architectures that can dynamically scale and efficiently manage resources. Conventional deployments on Amazon EC2 face challenges in scalability, cost efficiency, and operational complexity, making it difficult to adapt quickly to unpredictable workloads. Another approach is serverless RAG architecture on AWS that leverages Lambda, Amazon S3, DynamoDB, and API Gateway to automate scaling, reduce management overhead, and implement a cost-effective, pay-per-use model. Our evaluation demonstrates that a serverless approach can give savings of up to 87\\% for loads of 10000 requests per hour compared to EC2 instances while meeting the performance and efficiency requirements of modern AI applications. Serverless architecture establishes a pathway for developing more resilient and scalable cloud-based generative AI systems.', 'pages': '20-24', 'number': '', 'volume': '', 'year': '2025', 'title': 'Comparison of AWS Architectures for Scalable and Cost-Efficient Retrieval-Augmented Generation', 'booktitle': '2025 IEEE Zooming Innovation in Consumer Technologies Conference (ZINC)', 'author': 'Stojanović, Dimitrije and Vidaković, Luka and Pavković, Bogdan and Četić, Nenad and Krunić, Momčilo', 'ENTRYTYPE': 'inproceedings', 'ID': '11100283'}"
10828732,Deep Learning Model Design for Blood Cancer Prediction through AI-Driven Strategies,"Juliet, A. Hency",Juliet,10.1109/IC3I61595.2024.10828732,2024,2024 7th International Conference on Contemporary Computing and Informatics (IC3I),"The study seeks to create and utilize sophisticated deep learning model namely Convolutional Neural Network (CNN) designed for image classification purposes. It’s a custom CNN architecture tailored for the task, to classify images of blood cells into eight distinct categories. It comprises several layers, starting with convolutional layers followed by max-pooling layers, and fully connected layers empowered by AI technique, aiming to precisely forecast different forms of blood cancer using a dataset comprising images of blood cells. Ensuring the consistency and quality of input data, especially in medical imaging datasets susceptible to variations affecting model performance, poses a significant challenge; improving the interpretability and explainability of deep learning models is crucial for their reliable incorporation into clinical decision-making processes for blood cancer prediction. Utilize sophisticated deep learning model design methodologies guided by Artificial Intelligence approaches, tailored to address the unique characteristics of the blood cell image dataset, in order to surmount the outlined challenges. The suggested system provides improved precision and effectiveness in predicting blood cancer by employing advanced deep learning models guided by customized Artificial Intelligence techniques. Throughout the span of 50 epochs, the model’s performance was assessed based on its training and validation accuracy, gradually enhancing until reaching final rates of about 90.31\% for training and 90\% for validation.",Deep learning;Training;Accuracy;Cells (biology);Predictive models;Data models;Convolutional neural networks;Artificial intelligence;Blood;Cancer;Deep Learning Model Design;Convolutional Neural Network;Blood Cancer Prediction;AI-Driven Strategies;Image Classification;Custom Architecture;Max-pooling;Clinical Decision-making,"{'month': 'Sep.', 'issn': '', 'doi': '10.1109/IC3I61595.2024.10828732', 'keywords': 'Deep learning;Training;Accuracy;Cells (biology);Predictive models;Data models;Convolutional neural networks;Artificial intelligence;Blood;Cancer;Deep Learning Model Design;Convolutional Neural Network;Blood Cancer Prediction;AI-Driven Strategies;Image Classification;Custom Architecture;Max-pooling;Clinical Decision-making', 'abstract': 'The study seeks to create and utilize sophisticated deep learning model namely Convolutional Neural Network (CNN) designed for image classification purposes. It’s a custom CNN architecture tailored for the task, to classify images of blood cells into eight distinct categories. It comprises several layers, starting with convolutional layers followed by max-pooling layers, and fully connected layers empowered by AI technique, aiming to precisely forecast different forms of blood cancer using a dataset comprising images of blood cells. Ensuring the consistency and quality of input data, especially in medical imaging datasets susceptible to variations affecting model performance, poses a significant challenge; improving the interpretability and explainability of deep learning models is crucial for their reliable incorporation into clinical decision-making processes for blood cancer prediction. Utilize sophisticated deep learning model design methodologies guided by Artificial Intelligence approaches, tailored to address the unique characteristics of the blood cell image dataset, in order to surmount the outlined challenges. The suggested system provides improved precision and effectiveness in predicting blood cancer by employing advanced deep learning models guided by customized Artificial Intelligence techniques. Throughout the span of 50 epochs, the model’s performance was assessed based on its training and validation accuracy, gradually enhancing until reaching final rates of about 90.31\\% for training and 90\\% for validation.', 'pages': '60-65', 'number': '', 'volume': '7', 'year': '2024', 'title': 'Deep Learning Model Design for Blood Cancer Prediction through AI-Driven Strategies', 'booktitle': '2024 7th International Conference on Contemporary Computing and Informatics (IC3I)', 'author': 'Juliet, A. Hency', 'ENTRYTYPE': 'inproceedings', 'ID': '10828732'}"
11019305,GenAI Workforce Evaluation System,"Rani, P. Shobha and Neela, K. and Chandanavalli, P. and B, Ashwini and K, Sachin and R, Vijay Sai Raj",Rani,10.1109/ICCCT63501.2025.11019305,2025,2025 International Conference on Computing and Communication Technologies (ICCCT),"A widely used approach in emotional analysis is the BOW model, which is also popular in traditional theme modeling. However, an important limitation to the bow model is that it often considers two contrasting emotional texts as the same due to dependence on individual terms. This results in a change in polarity that the machine learning models struggle to handle effectively. We recommend integrating a semantic analysis program with a relevant dividing line to overcome this challenge to improve accuracy. By taking advantage of a generative AI-based model, we can expand emotions and increase classification. Transformer-based architecture as generic AI models offers advanced abilities in understanding relevant meanings and emotional shades, leading to more accurate emotions and analysis results. Implementation of Generic AI in emotional analysis can revolutionize emotional applications, including the recommended system, customer response evaluation and social media tracking. Applying AI-controlled methods will enable decision-making in different domains and improve user experiences, more accurate in reference-u-extensional emotional classification, reference income-ownable emotion classification.",Analytical models;Accuracy;Social networking (online);Generative AI;Semantics;Decision making;Machine learning;Computer architecture;Transformers;Communications technology;Semantic analysis;Generative AI;Emotional classification;Social media tracking,"{'month': 'April', 'issn': '2995-3197', 'doi': '10.1109/ICCCT63501.2025.11019305', 'keywords': 'Analytical models;Accuracy;Social networking (online);Generative AI;Semantics;Decision making;Machine learning;Computer architecture;Transformers;Communications technology;Semantic analysis;Generative AI;Emotional classification;Social media tracking', 'abstract': 'A widely used approach in emotional analysis is the BOW model, which is also popular in traditional theme modeling. However, an important limitation to the bow model is that it often considers two contrasting emotional texts as the same due to dependence on individual terms. This results in a change in polarity that the machine learning models struggle to handle effectively. We recommend integrating a semantic analysis program with a relevant dividing line to overcome this challenge to improve accuracy. By taking advantage of a generative AI-based model, we can expand emotions and increase classification. Transformer-based architecture as generic AI models offers advanced abilities in understanding relevant meanings and emotional shades, leading to more accurate emotions and analysis results. Implementation of Generic AI in emotional analysis can revolutionize emotional applications, including the recommended system, customer response evaluation and social media tracking. Applying AI-controlled methods will enable decision-making in different domains and improve user experiences, more accurate in reference-u-extensional emotional classification, reference income-ownable emotion classification.', 'pages': '1-5', 'number': '', 'volume': '', 'year': '2025', 'title': 'GenAI Workforce Evaluation System', 'booktitle': '2025 International Conference on Computing and Communication Technologies (ICCCT)', 'author': 'Rani, P. Shobha and Neela, K. and Chandanavalli, P. and B, Ashwini and K, Sachin and R, Vijay Sai Raj', 'ENTRYTYPE': 'inproceedings', 'ID': '11019305'}"
10274928,Assisting students to compose music with deep neural networks and aesthetic measurement feedback,"Paroiu, Razvan and Trausan-Matu, Stefan",Paroiu,10.1109/RoEduNet60162.2023.10274928,2023,2023 22nd RoEduNet Conference: Networking in Education and Research (RoEduNet),"This paper introduces a novel approach that enables students to compose music using deep neural networks while simultaneously gaining insights into the functionality of various mathematically computed aesthetic measures. While there are existing applications that allow users to compose music with artificial intelligence, they often lack the feedback, which may be provided by newly developed mathematical aesthetic measurement techniques. Our approach bridges this gap by providing users with objective feedback for their generated melodies, fostering a better understanding of the underlying measurement methods. The application was evaluated by multiple participants, which made us aware of both the strengths and limitations of the application.",Bridges;Atmospheric measurements;Education;Music;Artificial neural networks;Measurement techniques;Gain measurement;computer music generation;artificial intelligence;neural networks;aesthetic measurement;deep learning,"{'month': 'Sep.', 'issn': '2247-5443', 'doi': '10.1109/RoEduNet60162.2023.10274928', 'keywords': 'Bridges;Atmospheric measurements;Education;Music;Artificial neural networks;Measurement techniques;Gain measurement;computer music generation;artificial intelligence;neural networks;aesthetic measurement;deep learning', 'abstract': 'This paper introduces a novel approach that enables students to compose music using deep neural networks while simultaneously gaining insights into the functionality of various mathematically computed aesthetic measures. While there are existing applications that allow users to compose music with artificial intelligence, they often lack the feedback, which may be provided by newly developed mathematical aesthetic measurement techniques. Our approach bridges this gap by providing users with objective feedback for their generated melodies, fostering a better understanding of the underlying measurement methods. The application was evaluated by multiple participants, which made us aware of both the strengths and limitations of the application.', 'pages': '1-5', 'number': '', 'volume': '', 'year': '2023', 'title': 'Assisting students to compose music with deep neural networks and aesthetic measurement feedback', 'booktitle': '2023 22nd RoEduNet Conference: Networking in Education and Research (RoEduNet)', 'author': 'Paroiu, Razvan and Trausan-Matu, Stefan', 'ENTRYTYPE': 'inproceedings', 'ID': '10274928'}"
10879254,Bidirectional Local–Global Interaction-Guided GAN With Discriminator Gradient Gap Regularization for Bearing Fault Diagnosis With Unbalanced Datasets,"Zhang, Zhihang and Yuan, Xianfeng and Ye, Tianyi and Zhu, Weijie and Zhou, Fengyu",Zhang,10.1109/JSEN.2025.3538320,2025,IEEE Sensors Journal,"In complex industrial environments, mechanical devices predominantly operate under normal conditions, resulting in a shortage of available fault samples. This imbalance significantly impedes the effectiveness of intelligent fault diagnosis approaches. To overcome the challenge, an innovative generative adversarial network (GAN) model is proposed in this study, termed the bidirectional local–global interaction-guided GAN with discriminator gradient gap regularization (Bi-Interaction GAN). First, the generator is designed to incorporate a bidirectional interaction mechanism that accounts for the interaction between local and global features of vibration signals, effectively capturing the deep relationships between local and global information in the bearing vibration signals with limited datasets. Subsequently, the stability of the model is enhanced by introducing a novel loss function that integrates discriminator gradient gap regularization with the Wasserstein distance. This approach aims to minimize the gradient discrepancy between real and generated samples in the discriminator, thereby facilitating a more stable training process. Extensive comparative experiments are carried out to validate the proposed approach using a widely utilized dataset and realistic experimental configuration. The results indicate that the Bi-Interaction GAN outperforms existing state-of-the-art (SOTA) GAN models in generating superior-quality samples while achieving high classification accuracy.",Generative adversarial networks;Fault diagnosis;Training;Vibrations;Vectors;Hands;Generators;Costs;Artificial intelligence;Accuracy;Bidirectional local-global interaction;discriminator gradient gap regularization;generative adversarial network (GAN);imbalanced fault diagnosis,"{'month': 'March', 'issn': '1558-1748', 'doi': '10.1109/JSEN.2025.3538320', 'keywords': 'Generative adversarial networks;Fault diagnosis;Training;Vibrations;Vectors;Hands;Generators;Costs;Artificial intelligence;Accuracy;Bidirectional local-global interaction;discriminator gradient gap regularization;generative adversarial network (GAN);imbalanced fault diagnosis', 'abstract': 'In complex industrial environments, mechanical devices predominantly operate under normal conditions, resulting in a shortage of available fault samples. This imbalance significantly impedes the effectiveness of intelligent fault diagnosis approaches. To overcome the challenge, an innovative generative adversarial network (GAN) model is proposed in this study, termed the bidirectional local–global interaction-guided GAN with discriminator gradient gap regularization (Bi-Interaction GAN). First, the generator is designed to incorporate a bidirectional interaction mechanism that accounts for the interaction between local and global features of vibration signals, effectively capturing the deep relationships between local and global information in the bearing vibration signals with limited datasets. Subsequently, the stability of the model is enhanced by introducing a novel loss function that integrates discriminator gradient gap regularization with the Wasserstein distance. This approach aims to minimize the gradient discrepancy between real and generated samples in the discriminator, thereby facilitating a more stable training process. Extensive comparative experiments are carried out to validate the proposed approach using a widely utilized dataset and realistic experimental configuration. The results indicate that the Bi-Interaction GAN outperforms existing state-of-the-art (SOTA) GAN models in generating superior-quality samples while achieving high classification accuracy.', 'pages': '10498-10511', 'number': '6', 'volume': '25', 'year': '2025', 'title': 'Bidirectional Local–Global Interaction-Guided GAN With Discriminator Gradient Gap Regularization for Bearing Fault Diagnosis With Unbalanced Datasets', 'journal': 'IEEE Sensors Journal', 'author': 'Zhang, Zhihang and Yuan, Xianfeng and Ye, Tianyi and Zhu, Weijie and Zhou, Fengyu', 'ENTRYTYPE': 'article', 'ID': '10879254'}"
9361182,Efficient 3D Neural Networks with Support Vector Machine for Hippocampus Segmentation,"Chen, Yue and Yang, Xirui and Cheng, Kun and Li, Yi and Liu, Zhiwen and Shi, Yonggang",Chen,10.1109/ICAICE51518.2020.00071,2020,2020 International Conference on Artificial Intelligence and Computer Engineering (ICAICE),"Accurate segmentation of the hippocampal and its subfields from the brain magnetic resonance imaging (MRI), which is a prerequisite for volume measurement, plays a significant role in the clinical diagnosis and treatment of many neurodegenerative diseases. It is of great significance for the precise segmentation of the hippocampus and its sub-regions.In this paper, we proposed a hippocampal subfields segmentation approach based on support vector machine (SVM) combined 3D convolutional neural network (3D CNN) and generative adversarial network (GAN). In the 3D CNN-SVM model, the representative features processed by the 3D CNN are input into the SVM. SVM is trained with the features to achieve the voxel classification of the image, and the segmentation results are obtained. In the 3D GAN-SVM model, we use the generator to segment and use the 3D CNN-SVM network we proposed as the discriminator.The experiments has performed on the dataset obtained from Center for Imaging of Neurodegenerative Diseases (CIND) in San Francisco, USA. The segmentation dice similarity coefficients (DSCs) of the 3D CNN-SVM for CAI, CA2, DG, CA3, Head, Tail, SUB, ERC and PHG in hippocampal subfields are respectively 0.930, 0.926, 0.977, 0.967, 0.931, 0.905, 0.981, 0.870 and 0.911. It demonstrates that combining 3D CNN and SVM achieves a significant improvement in the accuracy of all the hippocampal subfields, and outperforms the existing methods based on the CNN. The DSCs of 3D GAN-SVM are higher, which are respectively 0.989, 0.965, 0.986, 0.977, 0.975, 0.993, 0.818, 0.985 and 0.994. The effect of the GAN-SVM model is also significantly better than that of pure GAN, and the segmentation accuracy has reached the best level on this dataset.Neural network can extract representative features, but it mainly relies on extracting features from a large number of accurately labeled datasets. Most medical datasets are small and difficult to obtain. SVM is more suitable for classification of small datasets, so we combine SVM and neural network to effectively improve the segmentation accuracy of hippocampus in brain MRI images.",Support vector machines;Image segmentation;Three-dimensional displays;Magnetic resonance imaging;Feature extraction;Generative adversarial networks;Hippocampus;hippocampal subfields segmentation;3D convolutional network;support vector machine;generative adversarial,"{'month': 'Oct', 'issn': '', 'doi': '10.1109/ICAICE51518.2020.00071', 'keywords': 'Support vector machines;Image segmentation;Three-dimensional displays;Magnetic resonance imaging;Feature extraction;Generative adversarial networks;Hippocampus;hippocampal subfields segmentation;3D convolutional network;support vector machine;generative adversarial', 'abstract': 'Accurate segmentation of the hippocampal and its subfields from the brain magnetic resonance imaging (MRI), which is a prerequisite for volume measurement, plays a significant role in the clinical diagnosis and treatment of many neurodegenerative diseases. It is of great significance for the precise segmentation of the hippocampus and its sub-regions.In this paper, we proposed a hippocampal subfields segmentation approach based on support vector machine (SVM) combined 3D convolutional neural network (3D CNN) and generative adversarial network (GAN). In the 3D CNN-SVM model, the representative features processed by the 3D CNN are input into the SVM. SVM is trained with the features to achieve the voxel classification of the image, and the segmentation results are obtained. In the 3D GAN-SVM model, we use the generator to segment and use the 3D CNN-SVM network we proposed as the discriminator.The experiments has performed on the dataset obtained from Center for Imaging of Neurodegenerative Diseases (CIND) in San Francisco, USA. The segmentation dice similarity coefficients (DSCs) of the 3D CNN-SVM for CAI, CA2, DG, CA3, Head, Tail, SUB, ERC and PHG in hippocampal subfields are respectively 0.930, 0.926, 0.977, 0.967, 0.931, 0.905, 0.981, 0.870 and 0.911. It demonstrates that combining 3D CNN and SVM achieves a significant improvement in the accuracy of all the hippocampal subfields, and outperforms the existing methods based on the CNN. The DSCs of 3D GAN-SVM are higher, which are respectively 0.989, 0.965, 0.986, 0.977, 0.975, 0.993, 0.818, 0.985 and 0.994. The effect of the GAN-SVM model is also significantly better than that of pure GAN, and the segmentation accuracy has reached the best level on this dataset.Neural network can extract representative features, but it mainly relies on extracting features from a large number of accurately labeled datasets. Most medical datasets are small and difficult to obtain. SVM is more suitable for classification of small datasets, so we combine SVM and neural network to effectively improve the segmentation accuracy of hippocampus in brain MRI images.', 'pages': '337-341', 'number': '', 'volume': '', 'year': '2020', 'title': 'Efficient 3D Neural Networks with Support Vector Machine for Hippocampus Segmentation', 'booktitle': '2020 International Conference on Artificial Intelligence and Computer Engineering (ICAICE)', 'author': 'Chen, Yue and Yang, Xirui and Cheng, Kun and Li, Yi and Liu, Zhiwen and Shi, Yonggang', 'ENTRYTYPE': 'inproceedings', 'ID': '9361182'}"
10108162,A Study on Neural Style Transfer Methods for Images,"Liao, Junzhe",Liao,10.1109/ICBAR58199.2022.00019,2022,"2022 2nd International Conference on Big Data, Artificial Intelligence and Risk Management (ICBAR)","Image style transfer techniques have been around for almost twenty years. In particular, the rise of deep learning has gradually influenced traditional image transfer techniques, eventually giving rise to new neural image transfer techniques. This paper covers an introduction to traditional style transfer techniques, including Stroke-based rendering, image filtering, image analogy and texture synthesis, as well as a categorization of various current classic neural image style transfer methods including slow neural method, fast neural method and generative adversarial networks (GAN) based neural style transfer method. In addition, this paper covers commonly used training datasets for style transfer. Finally, the challenges of neural image style transfer are discussed, and the possible future research directions are contained.",Deep learning;Training;Big Data;Rendering (computer graphics);Generative adversarial networks;Image filtering;Risk management;neural style transfer;deep learning;convolutional neural network;image rendering;generative adversarial nets;texture synthesis,"{'month': 'Nov', 'issn': '', 'doi': '10.1109/ICBAR58199.2022.00019', 'keywords': 'Deep learning;Training;Big Data;Rendering (computer graphics);Generative adversarial networks;Image filtering;Risk management;neural style transfer;deep learning;convolutional neural network;image rendering;generative adversarial nets;texture synthesis', 'abstract': 'Image style transfer techniques have been around for almost twenty years. In particular, the rise of deep learning has gradually influenced traditional image transfer techniques, eventually giving rise to new neural image transfer techniques. This paper covers an introduction to traditional style transfer techniques, including Stroke-based rendering, image filtering, image analogy and texture synthesis, as well as a categorization of various current classic neural image style transfer methods including slow neural method, fast neural method and generative adversarial networks (GAN) based neural style transfer method. In addition, this paper covers commonly used training datasets for style transfer. Finally, the challenges of neural image style transfer are discussed, and the possible future research directions are contained.', 'pages': '60-64', 'number': '', 'volume': '', 'year': '2022', 'title': 'A Study on Neural Style Transfer Methods for Images', 'booktitle': '2022 2nd International Conference on Big Data, Artificial Intelligence and Risk Management (ICBAR)', 'author': 'Liao, Junzhe', 'ENTRYTYPE': 'inproceedings', 'ID': '10108162'}"
10760979,Utilizing Hybrid-Deep Learning for Autism Spectrum Disorder Detection in Children via Facial Emotion Recognition,"Balasubramani, Jagadesh and Surendran, R",Balasubramani,10.1109/ICSSAS64001.2024.10760979,2024,2024 2nd International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS),"Autism Spectrum Disorder (ASD) is a neurodevelopmental disorder that significantly impacts social interaction and communication. Early detection and intervention are crucial for optimal outcomes. This research investigates the use of deep learning techniques to identify ASD based on facial emotion analysis. The proposed approach leverages Self-Attention-Based Progressive Generative Adversarial Networks (SA-PGAN) optimized with the Gorilla Troops Optimizer (GTO) to accurately recognize subtle facial cues associated with ASD. The model is trained on a dataset of facial images from autistic children, undergoing extensive preprocessing to ensure data quality and consistency. The performance of the proposed model is evaluated using various metrics, including accuracy, precision, recall, F1-score, and computational time. The results demonstrate significant improvements over state-of-the-art methods, such as Random Forest (RF), Support Vector Machine (SVM), and Convolutional Neural Networks (CNN). This research contributes to the development of advanced AI-powered tools for early ASD detection, enabling timely interventions and improving the quality of life for individuals with ASD.",Support vector machines;Radio frequency;Measurement;Autism;Accuracy;Computational modeling;Face recognition;Generative adversarial networks;Convolutional neural networks;Random forests;Deep Learning;Autism Spectrum Disorder (ASD);Self-Attention-Based Progressive Generative Adversarial Networks (SA-PGAN);Gorilla Troops Optimizer (GTO);Facial Landmark Analysis,"{'month': 'Oct', 'issn': '', 'doi': '10.1109/ICSSAS64001.2024.10760979', 'keywords': 'Support vector machines;Radio frequency;Measurement;Autism;Accuracy;Computational modeling;Face recognition;Generative adversarial networks;Convolutional neural networks;Random forests;Deep Learning;Autism Spectrum Disorder (ASD);Self-Attention-Based Progressive Generative Adversarial Networks (SA-PGAN);Gorilla Troops Optimizer (GTO);Facial Landmark Analysis', 'abstract': 'Autism Spectrum Disorder (ASD) is a neurodevelopmental disorder that significantly impacts social interaction and communication. Early detection and intervention are crucial for optimal outcomes. This research investigates the use of deep learning techniques to identify ASD based on facial emotion analysis. The proposed approach leverages Self-Attention-Based Progressive Generative Adversarial Networks (SA-PGAN) optimized with the Gorilla Troops Optimizer (GTO) to accurately recognize subtle facial cues associated with ASD. The model is trained on a dataset of facial images from autistic children, undergoing extensive preprocessing to ensure data quality and consistency. The performance of the proposed model is evaluated using various metrics, including accuracy, precision, recall, F1-score, and computational time. The results demonstrate significant improvements over state-of-the-art methods, such as Random Forest (RF), Support Vector Machine (SVM), and Convolutional Neural Networks (CNN). This research contributes to the development of advanced AI-powered tools for early ASD detection, enabling timely interventions and improving the quality of life for individuals with ASD.', 'pages': '487-492', 'number': '', 'volume': '', 'year': '2024', 'title': 'Utilizing Hybrid-Deep Learning for Autism Spectrum Disorder Detection in Children via Facial Emotion Recognition', 'booktitle': '2024 2nd International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS)', 'author': 'Balasubramani, Jagadesh and Surendran, R', 'ENTRYTYPE': 'inproceedings', 'ID': '10760979'}"
9339146,Spectrum Waterfall Completion in Jamming Enviroment: A General Adversarial Networks Method,"Cai, Yuan and Song, Fei and Xu, Yifan and Liu, Xiaodu and Zhang, Xiao and Han, Hao",Cai,10.1109/ITAIC49862.2020.9339146,2020,2020 IEEE 9th Joint International Information Technology and Artificial Intelligence Conference (ITAIC),"It is very important for completing spectrum data in the wireless communication system. The traditional completion method mainly considered the missing completion operation of sparse data. To solve this problem, we proposed an efficient algorithm based on generative adversarial network (GAN), which can automatically mine the relationship of the data and complete the missing data accurately. The proposed method uses the sensing spectrum waterfall as the input sample of the network structure. We adopt a pre-classification module and generation module based on GAN to generate incomplete spectrum data in multiple jamming patterns. Simulation results show that the proposed algorithm can effectively complete the missing data, and its performance is better than that of the data completion algorithm without a clssifier.",Wireless communication;Simulation;Generative adversarial networks;Generators;Sensors;Jamming;Gallium nitride;Spectrum waterfall completion;generative adversarial network;wireless communication,"{'month': 'Dec', 'issn': '2693-2865', 'doi': '10.1109/ITAIC49862.2020.9339146', 'keywords': 'Wireless communication;Simulation;Generative adversarial networks;Generators;Sensors;Jamming;Gallium nitride;Spectrum waterfall completion;generative adversarial network;wireless communication', 'abstract': 'It is very important for completing spectrum data in the wireless communication system. The traditional completion method mainly considered the missing completion operation of sparse data. To solve this problem, we proposed an efficient algorithm based on generative adversarial network (GAN), which can automatically mine the relationship of the data and complete the missing data accurately. The proposed method uses the sensing spectrum waterfall as the input sample of the network structure. We adopt a pre-classification module and generation module based on GAN to generate incomplete spectrum data in multiple jamming patterns. Simulation results show that the proposed algorithm can effectively complete the missing data, and its performance is better than that of the data completion algorithm without a clssifier.', 'pages': '1661-1665', 'number': '', 'volume': '9', 'year': '2020', 'title': 'Spectrum Waterfall Completion in Jamming Enviroment: A General Adversarial Networks Method', 'booktitle': '2020 IEEE 9th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)', 'author': 'Cai, Yuan and Song, Fei and Xu, Yifan and Liu, Xiaodu and Zhang, Xiao and Han, Hao', 'ENTRYTYPE': 'inproceedings', 'ID': '9339146'}"
10957177,Research on Distribution Grid Line Defect Identification and Classification Methods in UAV Inspection,"Liao, Bin and Guo, Zipei and Liao, Maotian and Shen, Xiaonan and Zou, Yu",Liao,10.1109/ICEAAI64185.2025.10957177,2025,2025 International Conference on Electrical Automation and Artificial Intelligence (ICEAAI),"With the development of the power system, the application of UAVs in distribution network inspection is becoming more and more widespread, therefore, this paper carries out research on the algorithms for distribution network line defects, solves the problems of fewer samples for acceptance of line engineering defects, poorer training effect, and lower model recognition accuracy, and researches a method for the identification and categorization of line defects in distribution networks. Firstly, through the sample data enhancement technique, the generalization ability of the model is improved, overfitting is reduced, and the robustness of the model to the input data is enhanced; secondly, the Generative Adversarial Network (GAN) is introduced to generate the antagonistic samples, which significantly reduces the amount of computational effort; finally, in combination with the Transformer architecture, an algorithm applicable to the recognition of line defects of power distribution grids is proposed, which improves the algorithm's performance in the complex scenario. The experimental results show that the method in this paper has significant improvement in the accuracy of defect recognition, computational efficiency and anti-interference ability, which provides a new technical solution for distribution network line defect detection.",Accuracy;Computational modeling;Atmospheric modeling;Distribution networks;Inspection;Generative adversarial networks;Transformers;Autonomous aerial vehicles;Data models;Robustness;unmanned aircraft inspection;distribution network line;defect recognition;generative adversarial network,"{'month': 'Jan', 'issn': '', 'doi': '10.1109/ICEAAI64185.2025.10957177', 'keywords': 'Accuracy;Computational modeling;Atmospheric modeling;Distribution networks;Inspection;Generative adversarial networks;Transformers;Autonomous aerial vehicles;Data models;Robustness;unmanned aircraft inspection;distribution network line;defect recognition;generative adversarial network', 'abstract': ""With the development of the power system, the application of UAVs in distribution network inspection is becoming more and more widespread, therefore, this paper carries out research on the algorithms for distribution network line defects, solves the problems of fewer samples for acceptance of line engineering defects, poorer training effect, and lower model recognition accuracy, and researches a method for the identification and categorization of line defects in distribution networks. Firstly, through the sample data enhancement technique, the generalization ability of the model is improved, overfitting is reduced, and the robustness of the model to the input data is enhanced; secondly, the Generative Adversarial Network (GAN) is introduced to generate the antagonistic samples, which significantly reduces the amount of computational effort; finally, in combination with the Transformer architecture, an algorithm applicable to the recognition of line defects of power distribution grids is proposed, which improves the algorithm's performance in the complex scenario. The experimental results show that the method in this paper has significant improvement in the accuracy of defect recognition, computational efficiency and anti-interference ability, which provides a new technical solution for distribution network line defect detection."", 'pages': '144-148', 'number': '', 'volume': '', 'year': '2025', 'title': 'Research on Distribution Grid Line Defect Identification and Classification Methods in UAV Inspection', 'booktitle': '2025 International Conference on Electrical Automation and Artificial Intelligence (ICEAAI)', 'author': 'Liao, Bin and Guo, Zipei and Liao, Maotian and Shen, Xiaonan and Zou, Yu', 'ENTRYTYPE': 'inproceedings', 'ID': '10957177'}"
10920590,Multi-Sensor Data Fusion and Augmentation for Imbalanced Fault Diagnosis of Bearings,"Fan, Zhongding and Liu, Xianzeng and Cao, Zheng and Wang, Hang and Zhou, Yuanyuan and Liu, Yongbin",Fan,10.1109/ICSMD64214.2024.10920590,2024,"2024 International Conference on Sensing, Measurement \& Data Analytics in the era of Artificial Intelligence (ICSMD)","Stable operation of bearing is required, while on the other side, it can lead to insufficient fault data collection and imbalanced data, which further deteriorates the performance of deep learning(DL) methods in the fault diagnosis of bearings. In this paper, a novel method combining multi-sensor data fusion and augmentation for bearing imbalanced fault diagnosis is proposed. First, the limited real fault samples are fed into the one-dimensional Wasserstein generative adversarial network with gradient penalty (1D-WGAN-GP) to generate the fake samples for augmenting the fault data. Then, the features of augmented data from different sensors are extracted and fused by using the proposed multi-branch one-dimensional convolutional neural network (1D-MCNN). Finally, imbalanced fault diagnosis of bearings is achieved based on the fused features. The performance of the proposed method is verified experimentally. The results show that, compared to existing methods the proposed method can be used to fuse fault features from multiple sensors, and effectively enhance the fault data, thereby achieving excellent accuracy and stability under the imbalanced data of bearing.",Fault diagnosis;Vibrations;Accuracy;Data integration;Sensor fusion;Sensor phenomena and characterization;Feature extraction;Generative adversarial networks;Data augmentation;Data mining;Bearing;Imbalanced fault diagnosis;Generative adversarial network;Data augmentation;Features fusion,"{'month': 'Oct', 'issn': '', 'doi': '10.1109/ICSMD64214.2024.10920590', 'keywords': 'Fault diagnosis;Vibrations;Accuracy;Data integration;Sensor fusion;Sensor phenomena and characterization;Feature extraction;Generative adversarial networks;Data augmentation;Data mining;Bearing;Imbalanced fault diagnosis;Generative adversarial network;Data augmentation;Features fusion', 'abstract': 'Stable operation of bearing is required, while on the other side, it can lead to insufficient fault data collection and imbalanced data, which further deteriorates the performance of deep learning(DL) methods in the fault diagnosis of bearings. In this paper, a novel method combining multi-sensor data fusion and augmentation for bearing imbalanced fault diagnosis is proposed. First, the limited real fault samples are fed into the one-dimensional Wasserstein generative adversarial network with gradient penalty (1D-WGAN-GP) to generate the fake samples for augmenting the fault data. Then, the features of augmented data from different sensors are extracted and fused by using the proposed multi-branch one-dimensional convolutional neural network (1D-MCNN). Finally, imbalanced fault diagnosis of bearings is achieved based on the fused features. The performance of the proposed method is verified experimentally. The results show that, compared to existing methods the proposed method can be used to fuse fault features from multiple sensors, and effectively enhance the fault data, thereby achieving excellent accuracy and stability under the imbalanced data of bearing.', 'pages': '1-5', 'number': '', 'volume': '', 'year': '2024', 'title': 'Multi-Sensor Data Fusion and Augmentation for Imbalanced Fault Diagnosis of Bearings', 'booktitle': '2024 International Conference on Sensing, Measurement \\& Data Analytics in the era of Artificial Intelligence (ICSMD)', 'author': 'Fan, Zhongding and Liu, Xianzeng and Cao, Zheng and Wang, Hang and Zhou, Yuanyuan and Liu, Yongbin', 'ENTRYTYPE': 'inproceedings', 'ID': '10920590'}"
10108310,Multitrack Symbolic Music Composition with LSTM-GAN,"Huang, Jiashu",Huang,10.1109/ICBAR58199.2022.00014,2022,"2022 2nd International Conference on Big Data, Artificial Intelligence and Risk Management (ICBAR)","Music generation is becoming a heated subject in the field of machine learning. Particularly, in the direction of multitrack, sequential, and symbolic music composition, methods based on generative adversarial networks (GAN) such as MuseGAN and BinaryMuseGAN have shown outstanding performance and have been adopted by many researchers. However, there are still issues in musicality and temporal correlation that need to be addressed. In this paper, an LSTM-based GAN is proposed in order to capture more temporal information from real-world music samples. One bar is seen as the smallest unit and fed into the LSTM network in the discriminator. Additionally, a processor is added between the generator and the discriminator to extract important information from the data. This LSTM-GAN scheme helps the discriminator find important differentiators more quickly and hence improves the ability of the generator to fabricate authentic data. The proposed method is validated via experiments, where the generated data shows similarities with the authentic ones.",Heating systems;Correlation;Machine learning;Big Data;Generative adversarial networks;Generators;Risk management;music generation;generative adversarial network;generator;discriminator;LSTM,"{'month': 'Nov', 'issn': '', 'doi': '10.1109/ICBAR58199.2022.00014', 'keywords': 'Heating systems;Correlation;Machine learning;Big Data;Generative adversarial networks;Generators;Risk management;music generation;generative adversarial network;generator;discriminator;LSTM', 'abstract': 'Music generation is becoming a heated subject in the field of machine learning. Particularly, in the direction of multitrack, sequential, and symbolic music composition, methods based on generative adversarial networks (GAN) such as MuseGAN and BinaryMuseGAN have shown outstanding performance and have been adopted by many researchers. However, there are still issues in musicality and temporal correlation that need to be addressed. In this paper, an LSTM-based GAN is proposed in order to capture more temporal information from real-world music samples. One bar is seen as the smallest unit and fed into the LSTM network in the discriminator. Additionally, a processor is added between the generator and the discriminator to extract important information from the data. This LSTM-GAN scheme helps the discriminator find important differentiators more quickly and hence improves the ability of the generator to fabricate authentic data. The proposed method is validated via experiments, where the generated data shows similarities with the authentic ones.', 'pages': '35-40', 'number': '', 'volume': '', 'year': '2022', 'title': 'Multitrack Symbolic Music Composition with LSTM-GAN', 'booktitle': '2022 2nd International Conference on Big Data, Artificial Intelligence and Risk Management (ICBAR)', 'author': 'Huang, Jiashu', 'ENTRYTYPE': 'inproceedings', 'ID': '10108310'}"
10934534,Weighted Strategy Optimization Approach for Discrete Sequence Generation,"Jiang, Xinran and Li, Daiwei and Zhang, Haiqing and Zhou, Ying and Liu, Jialing and Xiang, Xiaoming",Jiang,10.1109/AIIM64537.2024.10934534,2024,2024 4th International Symposium on Artificial Intelligence and Intelligent Manufacturing (AIIM),"Generative Adversarial Networks (GANs) perform well in continuous data generation, but face challenges in discrete sequence generation due to the non-differentiability of backpropagation. SeqGAN addresses this issue by utilizing the policy gradient method; however, it still suffers from limitations such as low efficiency, inadequate reward utilization, and a tendency to converge to local optima. This paper proposes a discrete sequence generation model, W-SeqGAN, which is based on Weighted Policy Optimization (WPO). This model incorporates WPO into the training process of the generator. Its dynamic weighting mechanism ena bles the generator to focus on samples with strong discriminator feedback or those that are difficult to generate, thereby enhancing the generation quality. Experiments were conducted using synthe tic data, and the Negative Log Likelihood (NLL) was adopted as t he evaluation metric. The results indicate that W-SeqGAN outper forms traditional SeqGAN and other baseline models in terms of generation quality and training efficiency.",Training;Measurement;Gradient methods;Data collection;Generative adversarial networks;Generators;Stability analysis;Manufacturing;Optimization;Faces;Generative Adversarial Networks;Sequence Generation;Weighted Policy Optimization,"{'month': 'Dec', 'issn': '', 'doi': '10.1109/AIIM64537.2024.10934534', 'keywords': 'Training;Measurement;Gradient methods;Data collection;Generative adversarial networks;Generators;Stability analysis;Manufacturing;Optimization;Faces;Generative Adversarial Networks;Sequence Generation;Weighted Policy Optimization', 'abstract': 'Generative Adversarial Networks (GANs) perform well in continuous data generation, but face challenges in discrete sequence generation due to the non-differentiability of backpropagation. SeqGAN addresses this issue by utilizing the policy gradient method; however, it still suffers from limitations such as low efficiency, inadequate reward utilization, and a tendency to converge to local optima. This paper proposes a discrete sequence generation model, W-SeqGAN, which is based on Weighted Policy Optimization (WPO). This model incorporates WPO into the training process of the generator. Its dynamic weighting mechanism ena bles the generator to focus on samples with strong discriminator feedback or those that are difficult to generate, thereby enhancing the generation quality. Experiments were conducted using synthe tic data, and the Negative Log Likelihood (NLL) was adopted as t he evaluation metric. The results indicate that W-SeqGAN outper forms traditional SeqGAN and other baseline models in terms of generation quality and training efficiency.', 'pages': '843-846', 'number': '', 'volume': '', 'year': '2024', 'title': 'Weighted Strategy Optimization Approach for Discrete Sequence Generation', 'booktitle': '2024 4th International Symposium on Artificial Intelligence and Intelligent Manufacturing (AIIM)', 'author': 'Jiang, Xinran and Li, Daiwei and Zhang, Haiqing and Zhou, Ying and Liu, Jialing and Xiang, Xiaoming', 'ENTRYTYPE': 'inproceedings', 'ID': '10934534'}"
10575170,Rate of Penetration Prediction using Batch Normalized Deep Elman Neural Network,"Paroha, Abhay Dutt",Paroha,10.1109/ICAAIC60222.2024.10575170,2024,2024 3rd International Conference on Applied Artificial Intelligence and Computing (ICAAIC),"Exploration, drilling, and extraction are basic components of the upstream process in the oil and gas industry. Rate of Penetration (ROP) is an essential component of the drilling process because it dictates the rate at which materials fracture to enlarge the aperture. For time and money savings, early ROP prediction is critical, but current methods are defective. To enhance ROP prediction, BG-DEL, an innovative method, combines Batch Normalization with a Deep Elman Neural Network based on the Gaussian Error Linear Unit. Following the proper preparation of the data, the subsequent operations are executed: clustering using WFFC, value generation utilizing LRF-GAN, and correlation calculation. To evaluate a classifier that has been trained to predict ROP using database values, this study employs L2CO method for parameter selection. The experimental evaluation yields substantial advantages for the upstream sector of the oil and gas industry. It demonstrates a 98.9\% accuracy rate and a correlation coefficient of 0.95. Keywords-Rate of Penetration (ROP), Ward Farthest First Clustering (WFFC), Linear Regression Function based Generative Adversarial Network (LRF-GAN), Laplace Crossover Operator based Coot Optimization (L2CO), Batch Normalized.",Drilling;Correlation coefficient;Accuracy;Databases;Neural networks;Linear regression;Generative adversarial networks;Rate of Penetration (ROP);Ward Farthest First Clustering (WFFC);Linear Regression Function based Generative Adversarial Network (LRF-GAN);Laplace Crossover Operator based Coot Optimization (L2CO);Batch Normalized,"{'month': 'June', 'issn': '', 'doi': '10.1109/ICAAIC60222.2024.10575170', 'keywords': 'Drilling;Correlation coefficient;Accuracy;Databases;Neural networks;Linear regression;Generative adversarial networks;Rate of Penetration (ROP);Ward Farthest First Clustering (WFFC);Linear Regression Function based Generative Adversarial Network (LRF-GAN);Laplace Crossover Operator based Coot Optimization (L2CO);Batch Normalized', 'abstract': 'Exploration, drilling, and extraction are basic components of the upstream process in the oil and gas industry. Rate of Penetration (ROP) is an essential component of the drilling process because it dictates the rate at which materials fracture to enlarge the aperture. For time and money savings, early ROP prediction is critical, but current methods are defective. To enhance ROP prediction, BG-DEL, an innovative method, combines Batch Normalization with a Deep Elman Neural Network based on the Gaussian Error Linear Unit. Following the proper preparation of the data, the subsequent operations are executed: clustering using WFFC, value generation utilizing LRF-GAN, and correlation calculation. To evaluate a classifier that has been trained to predict ROP using database values, this study employs L2CO method for parameter selection. The experimental evaluation yields substantial advantages for the upstream sector of the oil and gas industry. It demonstrates a 98.9\\% accuracy rate and a correlation coefficient of 0.95. Keywords-Rate of Penetration (ROP), Ward Farthest First Clustering (WFFC), Linear Regression Function based Generative Adversarial Network (LRF-GAN), Laplace Crossover Operator based Coot Optimization (L2CO), Batch Normalized.', 'pages': '34-40', 'number': '', 'volume': '', 'year': '2024', 'title': 'Rate of Penetration Prediction using Batch Normalized Deep Elman Neural Network', 'booktitle': '2024 3rd International Conference on Applied Artificial Intelligence and Computing (ICAAIC)', 'author': 'Paroha, Abhay Dutt', 'ENTRYTYPE': 'inproceedings', 'ID': '10575170'}"
9797676,Image Translation based on Attention Residual GAN,"Zhang, Minghao and He, Lingmin and Wang, Xiuhui",Zhang,10.1109/ICAICE54393.2021.00156,2021,2021 2nd International Conference on Artificial Intelligence and Computer Engineering (ICAICE),"Using Generative Adversarial Networks (GAN) to translate images is a significant field in computer vision. There are partial distortion, artifacts and detail loss in the images generated by current image translation algorithms. In order to solve this problem, this paper adds attention-based residual neural network to the generator of GAN. Attention-based residual neural network can improve the representation ability of the generator by weighting the channels of the feature map. Experiment results on the Facades dataset show that Attention Residual GAN can translate images with excellent quality.",Training;Degradation;Deep learning;Computer vision;Neural networks;Generative adversarial networks;Distortion;image translation;Generative Adversarial Networks;residual neural network;attention mechanism,"{'month': 'Nov', 'issn': '', 'doi': '10.1109/ICAICE54393.2021.00156', 'keywords': 'Training;Degradation;Deep learning;Computer vision;Neural networks;Generative adversarial networks;Distortion;image translation;Generative Adversarial Networks;residual neural network;attention mechanism', 'abstract': 'Using Generative Adversarial Networks (GAN) to translate images is a significant field in computer vision. There are partial distortion, artifacts and detail loss in the images generated by current image translation algorithms. In order to solve this problem, this paper adds attention-based residual neural network to the generator of GAN. Attention-based residual neural network can improve the representation ability of the generator by weighting the channels of the feature map. Experiment results on the Facades dataset show that Attention Residual GAN can translate images with excellent quality.', 'pages': '802-805', 'number': '', 'volume': '', 'year': '2021', 'title': 'Image Translation based on Attention Residual GAN', 'booktitle': '2021 2nd International Conference on Artificial Intelligence and Computer Engineering (ICAICE)', 'author': 'Zhang, Minghao and He, Lingmin and Wang, Xiuhui', 'ENTRYTYPE': 'inproceedings', 'ID': '9797676'}"
10933659,Color Image Information Hiding Based on Attention GAN,"Li, Jia and Cheng, Xien and Luan, Wenye",Li,10.1109/ICCBD-AI65562.2024.00057,2024,"2024 5th International Conference on Computer, Big Data and Artificial Intelligence (ICCBD+AI)","Image steganography aims to embed secret information into images to achieve data steganography. Aiming at the problems of insufficient feature expression ability, poor model flexibility, and poor quality of steganographic images in existing methods, this paper proposes an image steganography method based on attention generative adversarial networks. By introducing a combination of global attention mechanism, adaptive convolution module, and deep convolution block, the feature expression ability and the accuracy of image restoration are improved, while a customized discriminator is used to ensure the quality and authenticity of the steganographic image. The adaptive convolution module is able to dynamically adjust the size and number of convolution kernels according to the features of the input image, which effectively improves the flexibility and generalization ability of the model and reduces the consumption of computational resources. In this paper, experimental validation is carried out on COCO, CelebA and ImageNet datasets, and the results show that the method has obvious advantages over existing deep learning steganography techniques in terms of steganographic capacity, security, robustness and imperceptibility.",Steganography;Adaptation models;Visualization;Attention mechanisms;Accuracy;Convolution;Generative adversarial networks;Robustness;Stability analysis;Image restoration;Image Steganography;Generative Adversarial Network;Encoder-Decoder Network;Attention Mechanism,"{'month': 'Nov', 'issn': '', 'doi': '10.1109/ICCBD-AI65562.2024.00057', 'keywords': 'Steganography;Adaptation models;Visualization;Attention mechanisms;Accuracy;Convolution;Generative adversarial networks;Robustness;Stability analysis;Image restoration;Image Steganography;Generative Adversarial Network;Encoder-Decoder Network;Attention Mechanism', 'abstract': 'Image steganography aims to embed secret information into images to achieve data steganography. Aiming at the problems of insufficient feature expression ability, poor model flexibility, and poor quality of steganographic images in existing methods, this paper proposes an image steganography method based on attention generative adversarial networks. By introducing a combination of global attention mechanism, adaptive convolution module, and deep convolution block, the feature expression ability and the accuracy of image restoration are improved, while a customized discriminator is used to ensure the quality and authenticity of the steganographic image. The adaptive convolution module is able to dynamically adjust the size and number of convolution kernels according to the features of the input image, which effectively improves the flexibility and generalization ability of the model and reduces the consumption of computational resources. In this paper, experimental validation is carried out on COCO, CelebA and ImageNet datasets, and the results show that the method has obvious advantages over existing deep learning steganography techniques in terms of steganographic capacity, security, robustness and imperceptibility.', 'pages': '301-305', 'number': '', 'volume': '', 'year': '2024', 'title': 'Color Image Information Hiding Based on Attention GAN', 'booktitle': '2024 5th International Conference on Computer, Big Data and Artificial Intelligence (ICCBD+AI)', 'author': 'Li, Jia and Cheng, Xien and Luan, Wenye', 'ENTRYTYPE': 'inproceedings', 'ID': '10933659'}"
9904077,Industrial Character Image Motion Deblurring and Target Region Dynamic Location Method,"Zhang, Xiao and Chen, Ming and Wu, Dongliu",Zhang,10.1109/PRAI55851.2022.9904077,2022,2022 5th International Conference on Pattern Recognition and Artificial Intelligence (PRAI),"For one commodity, characters are printed on a specific area of the packing paper, recording the product ID. Because the machine needs to find the particular area for character recognition, we propose an automatic location method of character area. This location method can discover the region of interest more accurately without additional manual operation. Due to subtle camera vibrations and the factory line’s abrupt speed changes, slightly blurred images will be collected in image acquisition in industrial applications. In image motion deblurring, the protection of texture details is often ignored by previous methods, but it is vital for character recognition. Therefore, we propose a variant of Generative Adversarial Networks, which can preserve the texture details better and run faster in motion deblurring. The results of our method are better in terms of texture detail preservation and texture edge clarity. Its resulting images have more apparent details and edges, which is the key to achieving better effect during the character recognization.",Vibrations;Target recognition;Image edge detection;Packaging;Network architecture;Generative adversarial networks;Production facilities;automatic location;motion deblurring;character texture details;channel-recurrent network;generative adversarial networks,"{'month': 'Aug', 'issn': '', 'doi': '10.1109/PRAI55851.2022.9904077', 'keywords': 'Vibrations;Target recognition;Image edge detection;Packaging;Network architecture;Generative adversarial networks;Production facilities;automatic location;motion deblurring;character texture details;channel-recurrent network;generative adversarial networks', 'abstract': 'For one commodity, characters are printed on a specific area of the packing paper, recording the product ID. Because the machine needs to find the particular area for character recognition, we propose an automatic location method of character area. This location method can discover the region of interest more accurately without additional manual operation. Due to subtle camera vibrations and the factory line’s abrupt speed changes, slightly blurred images will be collected in image acquisition in industrial applications. In image motion deblurring, the protection of texture details is often ignored by previous methods, but it is vital for character recognition. Therefore, we propose a variant of Generative Adversarial Networks, which can preserve the texture details better and run faster in motion deblurring. The results of our method are better in terms of texture detail preservation and texture edge clarity. Its resulting images have more apparent details and edges, which is the key to achieving better effect during the character recognization.', 'pages': '658-663', 'number': '', 'volume': '', 'year': '2022', 'title': 'Industrial Character Image Motion Deblurring and Target Region Dynamic Location Method', 'booktitle': '2022 5th International Conference on Pattern Recognition and Artificial Intelligence (PRAI)', 'author': 'Zhang, Xiao and Chen, Ming and Wu, Dongliu', 'ENTRYTYPE': 'inproceedings', 'ID': '9904077'}"
11160513,Research on Cervical Cell Image Classification Method Based on Convolutional Neural Network,"Zhao, Yanli and Xiong, Junna",Zhao,10.1109/AIEA66061.2025.11160513,2025,2025 6th International Conference on Artificial Intelligence and Electromechanical Automation (AIEA),"Given the scarcity of cervical cell image samples and their low classification accuracy, a deep learning-based intelligent analysis method is proposed for the task of cervical cytopathology image classification. By introducing deep convolutional generative adversarial network (DCGAN) for data enhancement, the problem of insufficient medical image samples is effectively solved, and the generated images significantly outperform the comparison methods in PSNR (23.69) and SSIM (0.939) metrics. Adopting ResNet101 as a classification network and utilizing its residual linkage structure to effectively extract multi-scale features, it achieves $\mathbf{9 9. 1 5 \\%}$ precision and $\mathbf{9 9. 3 0 \\%}$ recall on the Herlev dataset, which outperforms comparative models such as VGG and ResNet50. The experimental results show that the method performs well in both normal cell (up to 99.97 \% recall) and lesion cell (down to 98.17 \% precision) classification tasks, providing a reliable technical solution for computer-aided diagnosis of cervical disease screening.",Computational modeling;Data enhancement;Feature extraction;Generative adversarial networks;Data models;Lesions;Convolutional neural networks;Reliability;Image classification;Residual neural networks;Convolutional Neural Networks;Generative Adversarial Networks;Cervical Cell Classification;Deep Learning,"{'month': 'Aug', 'issn': '', 'doi': '10.1109/AIEA66061.2025.11160513', 'keywords': 'Computational modeling;Data enhancement;Feature extraction;Generative adversarial networks;Data models;Lesions;Convolutional neural networks;Reliability;Image classification;Residual neural networks;Convolutional Neural Networks;Generative Adversarial Networks;Cervical Cell Classification;Deep Learning', 'abstract': 'Given the scarcity of cervical cell image samples and their low classification accuracy, a deep learning-based intelligent analysis method is proposed for the task of cervical cytopathology image classification. By introducing deep convolutional generative adversarial network (DCGAN) for data enhancement, the problem of insufficient medical image samples is effectively solved, and the generated images significantly outperform the comparison methods in PSNR (23.69) and SSIM (0.939) metrics. Adopting ResNet101 as a classification network and utilizing its residual linkage structure to effectively extract multi-scale features, it achieves $\\mathbf{9 9. 1 5 \\\\%}$ precision and $\\mathbf{9 9. 3 0 \\\\%}$ recall on the Herlev dataset, which outperforms comparative models such as VGG and ResNet50. The experimental results show that the method performs well in both normal cell (up to 99.97 \\% recall) and lesion cell (down to 98.17 \\% precision) classification tasks, providing a reliable technical solution for computer-aided diagnosis of cervical disease screening.', 'pages': '211-215', 'number': '', 'volume': '', 'year': '2025', 'title': 'Research on Cervical Cell Image Classification Method Based on Convolutional Neural Network', 'booktitle': '2025 6th International Conference on Artificial Intelligence and Electromechanical Automation (AIEA)', 'author': 'Zhao, Yanli and Xiong, Junna', 'ENTRYTYPE': 'inproceedings', 'ID': '11160513'}"
11022295,Research on Speech Enhancement of Vocal Music Auxiliary Teaching System Based on Time-Frequency Fusion and Multi-Channel Attention Mechanism,"Lu, Ji and Wu, Minjun",Lu,10.1109/ACAIT63902.2024.11022295,2024,2024 8th Asian Conference on Artificial Intelligence Technology (ACAIT),"In this paper, the quality of speech signals collected in acoustic teaching scenarios is further improved. A dual-microphone speech enhancement method based on time-frequency fusion and deep learning algorithm is proposed to obtain higher quality speech information. Using dual-microphone speech enhancement algorithm based on general sidelobe canceller as the basic speech enhancement algorithm, time-frequency masking and adversarial generative networks are introduced. Then, the noise removal level of the algorithm is further improved, and the quality of the enhanced speech is further improved. Simulation results prove that compared with other types of speech enhancement algorithms, the constructed speech enhancement algorithm has better performance in different scenarios in terms of mean opinion score, signal-to-noise ratio, perceptual evaluation of speech quality, short-time objective intelligibility and log spectral distance. Therefore, the constructed dual-microphone speech enhancement method based on time-frequency fusion and deep learning algorithm has good performance, can effectively improve the quality of the enhanced speech, and can be applied to the actual vocal music teaching scene to help improve the teaching quality, witch has high feasibility.",Deep learning;Time-frequency analysis;Attention mechanisms;Simulation;Education;Speech enhancement;Generative adversarial networks;Microphone arrays;Optimization;Signal to noise ratio;vocal music teaching;speech enhancement;microphone array;time-frequency masking;generative adversarial network,"{'month': 'Nov', 'issn': '', 'doi': '10.1109/ACAIT63902.2024.11022295', 'keywords': 'Deep learning;Time-frequency analysis;Attention mechanisms;Simulation;Education;Speech enhancement;Generative adversarial networks;Microphone arrays;Optimization;Signal to noise ratio;vocal music teaching;speech enhancement;microphone array;time-frequency masking;generative adversarial network', 'abstract': 'In this paper, the quality of speech signals collected in acoustic teaching scenarios is further improved. A dual-microphone speech enhancement method based on time-frequency fusion and deep learning algorithm is proposed to obtain higher quality speech information. Using dual-microphone speech enhancement algorithm based on general sidelobe canceller as the basic speech enhancement algorithm, time-frequency masking and adversarial generative networks are introduced. Then, the noise removal level of the algorithm is further improved, and the quality of the enhanced speech is further improved. Simulation results prove that compared with other types of speech enhancement algorithms, the constructed speech enhancement algorithm has better performance in different scenarios in terms of mean opinion score, signal-to-noise ratio, perceptual evaluation of speech quality, short-time objective intelligibility and log spectral distance. Therefore, the constructed dual-microphone speech enhancement method based on time-frequency fusion and deep learning algorithm has good performance, can effectively improve the quality of the enhanced speech, and can be applied to the actual vocal music teaching scene to help improve the teaching quality, witch has high feasibility.', 'pages': '1542-1547', 'number': '', 'volume': '', 'year': '2024', 'title': 'Research on Speech Enhancement of Vocal Music Auxiliary Teaching System Based on Time-Frequency Fusion and Multi-Channel Attention Mechanism', 'booktitle': '2024 8th Asian Conference on Artificial Intelligence Technology (ACAIT)', 'author': 'Lu, Ji and Wu, Minjun', 'ENTRYTYPE': 'inproceedings', 'ID': '11022295'}"
10165280,Business systems KPI anomaly detection based on an adversarial variational autoencoder,"Gao, Dequan and Yang, Meng and Feng, Bao and Luo, Wang and Bai, Dongxia",Gao,10.1109/ICIBA56860.2023.10165280,2023,"2023 IEEE 3rd International Conference on Information Technology, Big Data and Artificial Intelligence (ICIBA)","An anomaly KPI detection method is proposed based on an adversarial variational autoencoder (AVAE) for business systems. The proposed AVAE model is a combination of conditional variational autoencoder (CVAE) and generative adversarial network (GAN). Firstly, to effectively extract the periodic characteristics of KPI data, time information is introduced into the CVAE network as the input condition. Secondly, the discriminator of GAN network is introduced on the basis of CVAE to improve the sample inference ability of the generated network. Finally, CVAE, whose encoder can better obtain the mapping from real samples to hidden space, is used as a generator. Experimental results show the effectiveness of the proposed method.",Systems operation;Maintenance engineering;Generative adversarial networks;Generators;Distance measurement;Data models;Information technology;anomaly detection;business system;variational autoencoder;generative adversarial network;KPI,"{'month': 'May', 'issn': '', 'doi': '10.1109/ICIBA56860.2023.10165280', 'keywords': 'Systems operation;Maintenance engineering;Generative adversarial networks;Generators;Distance measurement;Data models;Information technology;anomaly detection;business system;variational autoencoder;generative adversarial network;KPI', 'abstract': 'An anomaly KPI detection method is proposed based on an adversarial variational autoencoder (AVAE) for business systems. The proposed AVAE model is a combination of conditional variational autoencoder (CVAE) and generative adversarial network (GAN). Firstly, to effectively extract the periodic characteristics of KPI data, time information is introduced into the CVAE network as the input condition. Secondly, the discriminator of GAN network is introduced on the basis of CVAE to improve the sample inference ability of the generated network. Finally, CVAE, whose encoder can better obtain the mapping from real samples to hidden space, is used as a generator. Experimental results show the effectiveness of the proposed method.', 'pages': '1702-1706', 'number': '', 'volume': '3', 'year': '2023', 'title': 'Business systems KPI anomaly detection based on an adversarial variational autoencoder', 'booktitle': '2023 IEEE 3rd International Conference on Information Technology, Big Data and Artificial Intelligence (ICIBA)', 'author': 'Gao, Dequan and Yang, Meng and Feng, Bao and Luo, Wang and Bai, Dongxia', 'ENTRYTYPE': 'inproceedings', 'ID': '10165280'}"
11064529,Enhanced Rice Leaf Diseases Diagnosis Using Deep Learning Model with GAN-Based Synthetic Data Augmentation,"Jain, Shaveta and Kumar, Rajneesh and Agrawal, Kushagra",Jain,10.1109/ICCSAI64074.2025.11064529,2025,"2025 3rd International Conference on Communication, Security, and Artificial Intelligence (ICCSAI)","Rice is a widely cultivated crop and a crucial staple food in many South Asian nations. Diseases that affect rice leaves can significantly decrease crop yields, but early identification can help mitigate their effects. Recently, machine learning has been applied to address the issue of disease spread. A major challenge, however, is the scarcity of balanced diseased leaf images compared to healthy ones, which limits the effectiveness of model training due to the need for extensive datasets. To overcome this limitation, Generative Adversarial Networks (GANs) have been increasingly used to generate synthetic images that closely mimic real ones. This approach has gained popularity in identifying leaf diseases, although research focusing specifically on rice diseases is still limited. In this research, GAN is utilized as a data augmentation method to create synthetic images for balancing Paddy doctor dataset. Subsequently, a hybrid model and Modified ResNet50 are employed for paddy disease classification. The experimental findings reveal that with image augmentation deep learning models learns well and performs best, achieving an accuracy of 99.92\% by hybrid model and 98.8\% by Modified ResNet50 which is 1\% – 10\% better when compared with existing models.",Deep learning;Training;Accuracy;Computational modeling;Generative adversarial networks;Data augmentation;Data models;Diseases;Residual neural networks;Synthetic data;Rice leaf Disease;Generative AI;GANs;CNN;InceptionV3;Modified ResNet50,"{'month': 'April', 'issn': '', 'doi': '10.1109/ICCSAI64074.2025.11064529', 'keywords': 'Deep learning;Training;Accuracy;Computational modeling;Generative adversarial networks;Data augmentation;Data models;Diseases;Residual neural networks;Synthetic data;Rice leaf Disease;Generative AI;GANs;CNN;InceptionV3;Modified ResNet50', 'abstract': 'Rice is a widely cultivated crop and a crucial staple food in many South Asian nations. Diseases that affect rice leaves can significantly decrease crop yields, but early identification can help mitigate their effects. Recently, machine learning has been applied to address the issue of disease spread. A major challenge, however, is the scarcity of balanced diseased leaf images compared to healthy ones, which limits the effectiveness of model training due to the need for extensive datasets. To overcome this limitation, Generative Adversarial Networks (GANs) have been increasingly used to generate synthetic images that closely mimic real ones. This approach has gained popularity in identifying leaf diseases, although research focusing specifically on rice diseases is still limited. In this research, GAN is utilized as a data augmentation method to create synthetic images for balancing Paddy doctor dataset. Subsequently, a hybrid model and Modified ResNet50 are employed for paddy disease classification. The experimental findings reveal that with image augmentation deep learning models learns well and performs best, achieving an accuracy of 99.92\\% by hybrid model and 98.8\\% by Modified ResNet50 which is 1\\% – 10\\% better when compared with existing models.', 'pages': '1334-1339', 'number': '', 'volume': '3', 'year': '2025', 'title': 'Enhanced Rice Leaf Diseases Diagnosis Using Deep Learning Model with GAN-Based Synthetic Data Augmentation', 'booktitle': '2025 3rd International Conference on Communication, Security, and Artificial Intelligence (ICCSAI)', 'author': 'Jain, Shaveta and Kumar, Rajneesh and Agrawal, Kushagra', 'ENTRYTYPE': 'inproceedings', 'ID': '11064529'}"
10475264,SNRGAN: The Semi Noise Reduction GAN for Image Denoising,"Momen-Tayefeh, Mehrshad and Momen-Tayefeh, Mehrdad and Hasheminasab, Fatemeh Zahra and Ghahramani, S. AmirAli Gh.",Momen-Tayefeh,10.1109/AISP61396.2024.10475264,2024,2024 20th CSI International Symposium on Artificial Intelligence and Signal Processing (AISP),"Conventional noise reduction methods often fail to effectively handle high levels of noise, leading to artifacts and distortions. This paper proposes a Generative Adversarial Network (GAN) approach for noise reduction with low complexity. The proposed Semi Noise Reduction GAN (SNRGAN) effectively learns the underlying patterns of noise and generates denoised versions of noisy images, even with different noise levels. Training our model on three diverse datasets yielded admissible results, as evidenced by superior PSNR and NMSE scores. Furthermore, our model excelled in both subjective evaluations and objective metrics and its efficacy in handling elevated noise levels positions it as a promising solution for real-world applications.",Training;Image quality;Quantization (signal);Noise reduction;Neural networks;Generative adversarial networks;Distortion;Generative Adversarial Networks;Noise Reduction;Convolutional Neural Networks,"{'month': 'Feb', 'issn': '2640-5768', 'doi': '10.1109/AISP61396.2024.10475264', 'keywords': 'Training;Image quality;Quantization (signal);Noise reduction;Neural networks;Generative adversarial networks;Distortion;Generative Adversarial Networks;Noise Reduction;Convolutional Neural Networks', 'abstract': 'Conventional noise reduction methods often fail to effectively handle high levels of noise, leading to artifacts and distortions. This paper proposes a Generative Adversarial Network (GAN) approach for noise reduction with low complexity. The proposed Semi Noise Reduction GAN (SNRGAN) effectively learns the underlying patterns of noise and generates denoised versions of noisy images, even with different noise levels. Training our model on three diverse datasets yielded admissible results, as evidenced by superior PSNR and NMSE scores. Furthermore, our model excelled in both subjective evaluations and objective metrics and its efficacy in handling elevated noise levels positions it as a promising solution for real-world applications.', 'pages': '1-5', 'number': '', 'volume': '', 'year': '2024', 'title': 'SNRGAN: The Semi Noise Reduction GAN for Image Denoising', 'booktitle': '2024 20th CSI International Symposium on Artificial Intelligence and Signal Processing (AISP)', 'author': 'Momen-Tayefeh, Mehrshad and Momen-Tayefeh, Mehrdad and Hasheminasab, Fatemeh Zahra and Ghahramani, S. AmirAli Gh.', 'ENTRYTYPE': 'inproceedings', 'ID': '10475264'}"
9288175,Dialog Driven Face Construction using GANs,"Vijay, Malaika and Meghana, Meghana and Aklecha, Nishant and Srinath, Ramamoorthy",Vijay,10.1109/ICTAI50040.2020.00104,2020,2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI),"This paper presents an end-to-end pipeline using Generative Adversarial Networks (GANs) for face construction based on speech-based descriptions, and iterative editing of the generated image to arrive at a close approximation of the expected face. We propose a dialog-based interaction with the system where the user and system take turns providing descriptions and generating images respectively. A rule-based Natural Language (NL) Parser is used to extract facial attribute descriptors from textual descriptions, MSG-Style GAN (Multi-Scale Gradient Style GAN) for face generation, and Attribute GAN (AttGAN) for facial attribute manipulation.",Pipelines;Tools;Generative adversarial networks;Feature extraction;Generators;Faces;Facial features;Generative Adversarial Network;Convolutional Neural Network;Face Construction;Image Generation;Natural Language Parser,"{'month': 'Nov', 'issn': '2375-0197', 'doi': '10.1109/ICTAI50040.2020.00104', 'keywords': 'Pipelines;Tools;Generative adversarial networks;Feature extraction;Generators;Faces;Facial features;Generative Adversarial Network;Convolutional Neural Network;Face Construction;Image Generation;Natural Language Parser', 'abstract': 'This paper presents an end-to-end pipeline using Generative Adversarial Networks (GANs) for face construction based on speech-based descriptions, and iterative editing of the generated image to arrive at a close approximation of the expected face. We propose a dialog-based interaction with the system where the user and system take turns providing descriptions and generating images respectively. A rule-based Natural Language (NL) Parser is used to extract facial attribute descriptors from textual descriptions, MSG-Style GAN (Multi-Scale Gradient Style GAN) for face generation, and Attribute GAN (AttGAN) for facial attribute manipulation.', 'pages': '647-652', 'number': '', 'volume': '', 'year': '2020', 'title': 'Dialog Driven Face Construction using GANs', 'booktitle': '2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI)', 'author': 'Vijay, Malaika and Meghana, Meghana and Aklecha, Nishant and Srinath, Ramamoorthy', 'ENTRYTYPE': 'inproceedings', 'ID': '9288175'}"
9688141,Estimation of Centroid Position of Silicon Particles Based on GAN and Visual Tracking,"Sun, Yifan and Huang, Peng and Jiao, Wentao and Xiaoqiang, Sima and Zhang, Peng and Wang, Li",Sun,10.1109/ICIBA52610.2021.9688141,2021,"2021 IEEE 2nd International Conference on Information Technology, Big Data and Artificial Intelligence (ICIBA)","In this paper, we used generative adversarial network (GAN) and visual tracking to detect the growth of silicon particles. To solve the problem of fewer data sets, we use the Wasserstein-GAN (WGAN) model to expand the data set. From the loss functions of the generator and discriminator, the quality of the data generated by the model is high. The position of the center of mass of the silica particles during the melting process was determined by the extracted tracking mark results, and the motion trajectory of the center of mass of the silica was given.",Silicon compounds;Visualization;Target tracking;Noise reduction;Generative adversarial networks;Data models;Silicon;generative adversarial networks;visual tracking;silicon particles;mathematical modeling,"{'month': 'Dec', 'issn': '', 'doi': '10.1109/ICIBA52610.2021.9688141', 'keywords': 'Silicon compounds;Visualization;Target tracking;Noise reduction;Generative adversarial networks;Data models;Silicon;generative adversarial networks;visual tracking;silicon particles;mathematical modeling', 'abstract': 'In this paper, we used generative adversarial network (GAN) and visual tracking to detect the growth of silicon particles. To solve the problem of fewer data sets, we use the Wasserstein-GAN (WGAN) model to expand the data set. From the loss functions of the generator and discriminator, the quality of the data generated by the model is high. The position of the center of mass of the silica particles during the melting process was determined by the extracted tracking mark results, and the motion trajectory of the center of mass of the silica was given.', 'pages': '695-699', 'number': '', 'volume': '2', 'year': '2021', 'title': 'Estimation of Centroid Position of Silicon Particles Based on GAN and Visual Tracking', 'booktitle': '2021 IEEE 2nd International Conference on Information Technology, Big Data and Artificial Intelligence (ICIBA)', 'author': 'Sun, Yifan and Huang, Peng and Jiao, Wentao and Xiaoqiang, Sima and Zhang, Peng and Wang, Li', 'ENTRYTYPE': 'inproceedings', 'ID': '9688141'}"
9743093,COVID-19 Patterns Identification using Advanced Machine Learning and Deep Neural Network Implementation,"Rao, V. Chandra Shekhar and Venkatramulu, S and Phridviraj, M S B and Pratapagiri, Sreenivas and Madugula, Sujatha and Kiran, Siripuri",Rao,10.1109/ICAIS53314.2022.9743093,2022,2022 Second International Conference on Artificial Intelligence and Smart Energy (ICAIS),"Predictive analysis and Therapeutic analysis are solitaries in the field of investigation with huge dimensions, which predict a range of diseases. Recently, the effect of the Covid-19 is massive and the virus constantly is mutating. The Generative Adversarial Network (GAN) is the modern efficiency technique used by advanced neural networks to analyze data in cavernous environment. This paper reviews the issues in datasets of Covid, that enables patients to be diagnosed and predicted. The GANs are used to produce, transform, and view datasets profound that trends in medical database. The general prediction research can be highly performing with the incorporation of GANs in comparison with classical neural networks in multiple layers. This research manuscript is projected so that the prediction of mining and the exploration of information can be done more effectively.",COVID-19;Planets;Neural networks;Transforms;Production;Generative adversarial networks;Market research;Data Analytics;Network for Generative Adversarial;GAN;Medical Diagnosis,"{'month': 'Feb', 'issn': '', 'doi': '10.1109/ICAIS53314.2022.9743093', 'keywords': 'COVID-19;Planets;Neural networks;Transforms;Production;Generative adversarial networks;Market research;Data Analytics;Network for Generative Adversarial;GAN;Medical Diagnosis', 'abstract': 'Predictive analysis and Therapeutic analysis are solitaries in the field of investigation with huge dimensions, which predict a range of diseases. Recently, the effect of the Covid-19 is massive and the virus constantly is mutating. The Generative Adversarial Network (GAN) is the modern efficiency technique used by advanced neural networks to analyze data in cavernous environment. This paper reviews the issues in datasets of Covid, that enables patients to be diagnosed and predicted. The GANs are used to produce, transform, and view datasets profound that trends in medical database. The general prediction research can be highly performing with the incorporation of GANs in comparison with classical neural networks in multiple layers. This research manuscript is projected so that the prediction of mining and the exploration of information can be done more effectively.', 'pages': '240-243', 'number': '', 'volume': '', 'year': '2022', 'title': 'COVID-19 Patterns Identification using Advanced Machine Learning and Deep Neural Network Implementation', 'booktitle': '2022 Second International Conference on Artificial Intelligence and Smart Energy (ICAIS)', 'author': 'Rao, V. Chandra Shekhar and Venkatramulu, S and Phridviraj, M S B and Pratapagiri, Sreenivas and Madugula, Sujatha and Kiran, Siripuri', 'ENTRYTYPE': 'inproceedings', 'ID': '9743093'}"
10730244,Skip-CBAM Based Unsupervised Anomaly Detection of Fused Magnesia Furnace,"Wang, Yuxin and Liu, Qiang",Wang,10.1109/IAI63275.2024.10730244,2024,2024 6th International Conference on Industrial Artificial Intelligence (IAI),"Abnormal ultra-high temperature semi-molten conditions in a fused magnesia furnace (FMF) can lead to furnace leakage. To ensure high detection performance, existing models require sufficient expert-labeled training data for supervised learning. However, the above model has limited ability to identify unknown abnormal working conditions, and label production is time-consuming and expensive. This paper suggests an unsupervised detection method based on skip-connected generative adversarial network (GAN) to address this issue. Specifically, to adapt to the strong interference caused by irregular changes in images caused by irregular water mist and flicker scenes, existing methods remove interference through multi-step processing for detection, which is difficult to apply in practice. In contrast, we use a reconstruction model of convolutional block attention module (CBAM) with skip connections for end-to-end computation, which improves performance while being more concise. This method determines abnormal areas through reconstruction and distinguishes abnormal working conditions by comparing abnormal scores. Finally, this method is compared with existing unsupervised and supervised methods on actual FMF images, and the application results demonstrate the effectiveness of this method.",Employee welfare;Adaptation models;Furnaces;Training data;Interference;Generative adversarial networks;Data models;Magnesium;Image reconstruction;Anomaly detection;Fused magnesia furnace (FMF);unsupervised learning;generative adversarial network (GAN);anomaly detection,"{'month': 'Aug', 'issn': '', 'doi': '10.1109/IAI63275.2024.10730244', 'keywords': 'Employee welfare;Adaptation models;Furnaces;Training data;Interference;Generative adversarial networks;Data models;Magnesium;Image reconstruction;Anomaly detection;Fused magnesia furnace (FMF);unsupervised learning;generative adversarial network (GAN);anomaly detection', 'abstract': 'Abnormal ultra-high temperature semi-molten conditions in a fused magnesia furnace (FMF) can lead to furnace leakage. To ensure high detection performance, existing models require sufficient expert-labeled training data for supervised learning. However, the above model has limited ability to identify unknown abnormal working conditions, and label production is time-consuming and expensive. This paper suggests an unsupervised detection method based on skip-connected generative adversarial network (GAN) to address this issue. Specifically, to adapt to the strong interference caused by irregular changes in images caused by irregular water mist and flicker scenes, existing methods remove interference through multi-step processing for detection, which is difficult to apply in practice. In contrast, we use a reconstruction model of convolutional block attention module (CBAM) with skip connections for end-to-end computation, which improves performance while being more concise. This method determines abnormal areas through reconstruction and distinguishes abnormal working conditions by comparing abnormal scores. Finally, this method is compared with existing unsupervised and supervised methods on actual FMF images, and the application results demonstrate the effectiveness of this method.', 'pages': '1-6', 'number': '', 'volume': '', 'year': '2024', 'title': 'Skip-CBAM Based Unsupervised Anomaly Detection of Fused Magnesia Furnace', 'booktitle': '2024 6th International Conference on Industrial Artificial Intelligence (IAI)', 'author': 'Wang, Yuxin and Liu, Qiang', 'ENTRYTYPE': 'inproceedings', 'ID': '10730244'}"
10934487,A Deep Learning-Based Non-Photorealistic Rendering (NPR) Generation Method,"Zhou, Wei and Cang, Minnan",Zhou,10.1109/AIIM64537.2024.10934487,2024,2024 4th International Symposium on Artificial Intelligence and Intelligent Manufacturing (AIIM),"With the advancement of deep learning technologies, non-photorealistic rendering (NPR) techniques have made significant breakthroughs in the field of computer graphics. This paper proposes a deep learning-based framework for NPR generation, combining convolutional neural networks (CNNs) and generative adversarial networks (GANs), aimed at automatically generating graphics in various artistic styles, such as sketches, ink paintings, and oil paintings. Through image style transfer and feature learning, the framework optimizes the preservation of details and enhances the artistic effects of the generated images. Experimental results demonstrate that the proposed method significantly outperforms traditional algorithms in terms of style transfer quality, generation efficiency, and adaptability. It shows strong potential for wide applications in animation, game design, and virtual reality, providing robust technical support for digital art creation.",Deep learning;Solid modeling;Digital art;Games;Rendering (computer graphics);Generative adversarial networks;Animation;Convolutional neural networks;Optimization;Painting;Non-Photorealistic Rendering;Deep Learning;Convolutional Neural Networks;Generative Adversarial Networks (GAN),"{'month': 'Dec', 'issn': '', 'doi': '10.1109/AIIM64537.2024.10934487', 'keywords': 'Deep learning;Solid modeling;Digital art;Games;Rendering (computer graphics);Generative adversarial networks;Animation;Convolutional neural networks;Optimization;Painting;Non-Photorealistic Rendering;Deep Learning;Convolutional Neural Networks;Generative Adversarial Networks (GAN)', 'abstract': 'With the advancement of deep learning technologies, non-photorealistic rendering (NPR) techniques have made significant breakthroughs in the field of computer graphics. This paper proposes a deep learning-based framework for NPR generation, combining convolutional neural networks (CNNs) and generative adversarial networks (GANs), aimed at automatically generating graphics in various artistic styles, such as sketches, ink paintings, and oil paintings. Through image style transfer and feature learning, the framework optimizes the preservation of details and enhances the artistic effects of the generated images. Experimental results demonstrate that the proposed method significantly outperforms traditional algorithms in terms of style transfer quality, generation efficiency, and adaptability. It shows strong potential for wide applications in animation, game design, and virtual reality, providing robust technical support for digital art creation.', 'pages': '981-984', 'number': '', 'volume': '', 'year': '2024', 'title': 'A Deep Learning-Based Non-Photorealistic Rendering (NPR) Generation Method', 'booktitle': '2024 4th International Symposium on Artificial Intelligence and Intelligent Manufacturing (AIIM)', 'author': 'Zhou, Wei and Cang, Minnan', 'ENTRYTYPE': 'inproceedings', 'ID': '10934487'}"
10860191,Research on Anomaly Detection Model for Power Industrial Control Networks Based on Bidirectional Recurrent Neural Networks,"Liu, Chao and Yang, Tingting and Wang, Yu and Ren, Bin and Wang, Yanzhao",Liu,10.1109/AIVRV63595.2024.10860191,2024,"2024 4th International Conference on Artificial Intelligence, Virtual Reality and Visualization","In response to the current challenges of low accuracy in detecting abnormal data traffic and insufficient sample space in power industrial control networks, we propose a novel approach. Our method involves leveraging bidirectional recurrent neural networks(Bi-RNN) with long short-term memory (LSTM) architecture for anomaly traffic detection. Additionally, we introduce a sample augmentation technique based on generative adversarial networks (GAN) to address the limited sample size issue. We train our model on training datasets with temporal characteristics and evaluate its performance on testing datasets, achieving a prediction accuracy exceeding 99.6\%. Moreover, through the sample augmentation model, we expand the dataset by over 100,000 samples.",Training;Solid modeling;Protocols;Accuracy;Recurrent neural networks;Industrial control;Generative adversarial networks;Long short term memory;Streams;Testing;Bidirectional Recurrent Neural Networks;Power Industrial Control Networks;Anomaly Traffic Detection;Generative Adversarial Networks,"{'month': 'Nov', 'issn': '', 'doi': '10.1109/AIVRV63595.2024.10860191', 'keywords': 'Training;Solid modeling;Protocols;Accuracy;Recurrent neural networks;Industrial control;Generative adversarial networks;Long short term memory;Streams;Testing;Bidirectional Recurrent Neural Networks;Power Industrial Control Networks;Anomaly Traffic Detection;Generative Adversarial Networks', 'abstract': 'In response to the current challenges of low accuracy in detecting abnormal data traffic and insufficient sample space in power industrial control networks, we propose a novel approach. Our method involves leveraging bidirectional recurrent neural networks(Bi-RNN) with long short-term memory (LSTM) architecture for anomaly traffic detection. Additionally, we introduce a sample augmentation technique based on generative adversarial networks (GAN) to address the limited sample size issue. We train our model on training datasets with temporal characteristics and evaluate its performance on testing datasets, achieving a prediction accuracy exceeding 99.6\\%. Moreover, through the sample augmentation model, we expand the dataset by over 100,000 samples.', 'pages': '171-175', 'number': '', 'volume': '', 'year': '2024', 'title': 'Research on Anomaly Detection Model for Power Industrial Control Networks Based on Bidirectional Recurrent Neural Networks', 'booktitle': '2024 4th International Conference on Artificial Intelligence, Virtual Reality and Visualization', 'author': 'Liu, Chao and Yang, Tingting and Wang, Yu and Ren, Bin and Wang, Yanzhao', 'ENTRYTYPE': 'inproceedings', 'ID': '10860191'}"
10664887,Using the Swin-Transformer for Real \& Fake Data Recognition in PC-Model,"Park, Jiyoon",Park,10.1109/ISEC61299.2024.10664887,2024,2024 IEEE Integrated STEM Education Conference (ISEC),"Recently, due to the rapid development of generative AI technologies, the use of AI-generated images has increased significantly, making the distinction between real and fake images crucial. Generative images may be used in various ways such as data training and fast image generation, but a potential for misuse, such as in Deep fake or spreading false information, still exists. This study explores a novel model using the architecture ofSwin-Transformer to distinguish between fake and real images generated based on CNN (Convolutional Neural Networks) and GAN (Generative Adversarial Networks). The Swin-Transformer, a successor model of Vision in Transformer (ViT), applies the structure of the Transformer, which has shown outstanding performance in natural language processing, to the field of images and demonstrates excellent pixel-level segmentation performance. Real and fake images require detailed pixel-level analysis, in which the Swin-Transformer exhibits higher accuracy. Improving the performance of distinguishing between real and fake images is expected to set limits on indiscreet image generation, bringing further effects such as preventing the indiscriminate use of AI images through program-based discrimination/legal sanctions.",Training;Image segmentation;Image synthesis;Face recognition;Transformers;Generative adversarial networks;Natural language processing;Artificial Intelligence;Convolution Neural Network;Generative Adversarial Network;Real\&Fake,"{'month': 'March', 'issn': '2473-7623', 'doi': '10.1109/ISEC61299.2024.10664887', 'keywords': 'Training;Image segmentation;Image synthesis;Face recognition;Transformers;Generative adversarial networks;Natural language processing;Artificial Intelligence;Convolution Neural Network;Generative Adversarial Network;Real\\&Fake', 'abstract': 'Recently, due to the rapid development of generative AI technologies, the use of AI-generated images has increased significantly, making the distinction between real and fake images crucial. Generative images may be used in various ways such as data training and fast image generation, but a potential for misuse, such as in Deep fake or spreading false information, still exists. This study explores a novel model using the architecture ofSwin-Transformer to distinguish between fake and real images generated based on CNN (Convolutional Neural Networks) and GAN (Generative Adversarial Networks). The Swin-Transformer, a successor model of Vision in Transformer (ViT), applies the structure of the Transformer, which has shown outstanding performance in natural language processing, to the field of images and demonstrates excellent pixel-level segmentation performance. Real and fake images require detailed pixel-level analysis, in which the Swin-Transformer exhibits higher accuracy. Improving the performance of distinguishing between real and fake images is expected to set limits on indiscreet image generation, bringing further effects such as preventing the indiscriminate use of AI images through program-based discrimination/legal sanctions.', 'pages': '01-05', 'number': '', 'volume': '', 'year': '2024', 'title': 'Using the Swin-Transformer for Real \\& Fake Data Recognition in PC-Model', 'booktitle': '2024 IEEE Integrated STEM Education Conference (ISEC)', 'author': 'Park, Jiyoon', 'ENTRYTYPE': 'inproceedings', 'ID': '10664887'}"
11076342,A Comprehensive Survey of Generative AI Agents: Transforming Predictive Demand Forecasting and Supply Chain Optimization Strategies,"Puvvadi, Meghana and Arava, Sai Kumar and Raut, Atharva S and Santoria, Adarsh and Prasanna Chennupati, Sesha Sai and Vardhan Puvvadi, Harsha",Puvvadi,10.1109/ICISS63372.2025.11076342,2025,2025 7th International Conference on Intelligent Sustainable Systems (ICISS),"Modern supply chains are increasingly complex, requiring intelligent AI-driven systems to optimize supply allocation dynamically in response to fluctuating demand. This review highlights the significance of AI in supply chain management, examining recent advancements in predictive analytics, machine learning, and optimization techniques for demand forecasting and resource allocation. By critically analyzing AI methodologies—including deep neural networks, reinforcement learning, constraint programming, and hybrid models—we assess their impact on supply chain agility, operational efficiency, and profitability. Through an extensive review of literature and industry case studies, this work benchmarks AI-driven supply chain solutions, demonstrating that AI-based planners enhance order fulfillment rates by 15-20\%, increase revenue by 1015\%, and improve demand-fluctuation resilience by over 20\%, significantly outperforming conventional rule-based methods. Furthermore, AI enables real-time decision-making, reducing computational overhead and response time in dynamic market conditions. Despite these advancements, challenges such as data limitations, computational constraints, model interpretability, and sustainability remain significant barriers to adoption. This survey identifies these limitations and outlines critical research directions, including self-adaptive AI models, scalable parallel computing frameworks, and sustainability-driven optimization strategies, to ensure AI-powered supply chains remain resilient, efficient, and adaptable in evolving market environments.",Surveys;Adaptation models;Supply chain management;Computational modeling;Supply chains;Demand forecasting;Resource management;Artificial intelligence;Sustainable development;Optimization;Large Language Models;Generative AI;AI-driven Supply Chains;Reinforcement Learning;Autonomous Supply Chain Agents;Demand-Supply Optimization;Inventory Forecasting;Deep Learning in Logistics,"{'month': 'March', 'issn': '', 'doi': '10.1109/ICISS63372.2025.11076342', 'keywords': 'Surveys;Adaptation models;Supply chain management;Computational modeling;Supply chains;Demand forecasting;Resource management;Artificial intelligence;Sustainable development;Optimization;Large Language Models;Generative AI;AI-driven Supply Chains;Reinforcement Learning;Autonomous Supply Chain Agents;Demand-Supply Optimization;Inventory Forecasting;Deep Learning in Logistics', 'abstract': 'Modern supply chains are increasingly complex, requiring intelligent AI-driven systems to optimize supply allocation dynamically in response to fluctuating demand. This review highlights the significance of AI in supply chain management, examining recent advancements in predictive analytics, machine learning, and optimization techniques for demand forecasting and resource allocation. By critically analyzing AI methodologies—including deep neural networks, reinforcement learning, constraint programming, and hybrid models—we assess their impact on supply chain agility, operational efficiency, and profitability. Through an extensive review of literature and industry case studies, this work benchmarks AI-driven supply chain solutions, demonstrating that AI-based planners enhance order fulfillment rates by 15-20\\%, increase revenue by 1015\\%, and improve demand-fluctuation resilience by over 20\\%, significantly outperforming conventional rule-based methods. Furthermore, AI enables real-time decision-making, reducing computational overhead and response time in dynamic market conditions. Despite these advancements, challenges such as data limitations, computational constraints, model interpretability, and sustainability remain significant barriers to adoption. This survey identifies these limitations and outlines critical research directions, including self-adaptive AI models, scalable parallel computing frameworks, and sustainability-driven optimization strategies, to ensure AI-powered supply chains remain resilient, efficient, and adaptable in evolving market environments.', 'pages': '546-553', 'number': '', 'volume': '', 'year': '2025', 'title': 'A Comprehensive Survey of Generative AI Agents: Transforming Predictive Demand Forecasting and Supply Chain Optimization Strategies', 'booktitle': '2025 7th International Conference on Intelligent Sustainable Systems (ICISS)', 'author': 'Puvvadi, Meghana and Arava, Sai Kumar and Raut, Atharva S and Santoria, Adarsh and Prasanna Chennupati, Sesha Sai and Vardhan Puvvadi, Harsha', 'ENTRYTYPE': 'inproceedings', 'ID': '11076342'}"
9832721,HyperViTGAN: Semisupervised Generative Adversarial Network With Transformer for Hyperspectral Image Classification,"He, Ziping and Xia, Kewen and Ghamisi, Pedram and Hu, Yuhen and Fan, Shurui and Zu, Baokai",He,10.1109/JSTARS.2022.3192127,2022,IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,"Generative adversarial networks (GANs) have achieved many excellent results in hyperspectral image (HSI) classification in recent years, as GANs can effectively solve the dilemma of limited training samples in HSI classification. However, due to the class imbalance problem of HSI data, GANs always associate minority-class samples with fake label. To address this issue, we first propose a semisupervised generative adversarial network incorporating a transformer, called HyperViTGAN. The proposed HyperViTGAN is designed with an external semisupervised classifier to avoid self-contradiction when the discriminator performs both classification and discrimination tasks. The generator and discriminator with skip connection are utilized to generate HSI patches by adversarial learning. The proposed HyperViTGAN captures semantic context and low-level textures to reduce the loss of critical information. In addition, the generalization ability of the HyperViTGAN is improved through the use of data augmentation. Experimental results on three well-known HSI datasets, Houston 2013, Indian Pines 2010, and Xuzhou, show that the proposed model achieves competitive HSI classification performance in comparison with the current state-of-the-art classification models.",Generative adversarial networks;Transformers;Task analysis;Data models;Hyperspectral imaging;Generators;Training;Generative adversarial network (GAN);hyperspectral image (HSI) classification;semisupervised learning;transformer,"{'month': '', 'issn': '2151-1535', 'doi': '10.1109/JSTARS.2022.3192127', 'keywords': 'Generative adversarial networks;Transformers;Task analysis;Data models;Hyperspectral imaging;Generators;Training;Generative adversarial network (GAN);hyperspectral image (HSI) classification;semisupervised learning;transformer', 'abstract': 'Generative adversarial networks (GANs) have achieved many excellent results in hyperspectral image (HSI) classification in recent years, as GANs can effectively solve the dilemma of limited training samples in HSI classification. However, due to the class imbalance problem of HSI data, GANs always associate minority-class samples with fake label. To address this issue, we first propose a semisupervised generative adversarial network incorporating a transformer, called HyperViTGAN. The proposed HyperViTGAN is designed with an external semisupervised classifier to avoid self-contradiction when the discriminator performs both classification and discrimination tasks. The generator and discriminator with skip connection are utilized to generate HSI patches by adversarial learning. The proposed HyperViTGAN captures semantic context and low-level textures to reduce the loss of critical information. In addition, the generalization ability of the HyperViTGAN is improved through the use of data augmentation. Experimental results on three well-known HSI datasets, Houston 2013, Indian Pines 2010, and Xuzhou, show that the proposed model achieves competitive HSI classification performance in comparison with the current state-of-the-art classification models.', 'pages': '6053-6068', 'number': '', 'volume': '15', 'year': '2022', 'title': 'HyperViTGAN: Semisupervised Generative Adversarial Network With Transformer for Hyperspectral Image Classification', 'journal': 'IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing', 'author': 'He, Ziping and Xia, Kewen and Ghamisi, Pedram and Hu, Yuhen and Fan, Shurui and Zu, Baokai', 'ENTRYTYPE': 'article', 'ID': '9832721'}"
10265249,Steganography Embedding Cost Learning With Generative Multi-Adversarial Network,"Huang, Dongxia and Luo, Weiqi and Liu, Minglin and Tang, Weixuan and Huang, Jiwu",Huang,10.1109/TIFS.2023.3318939,2024,IEEE Transactions on Information Forensics and Security,"Since the generative adversarial network (GAN) was proposed by Ian Goodfellow et al. in 2014, it has been widely used in various fields. However, there are only a few works related to image steganography so far. Existing GAN-based steganographic methods mainly focus on the design of generator, and just assign a relatively poorer steganalyzer in discriminator, which inevitably limits the performances of their models. In this paper, we propose a novel Steganographic method based on Generative Multi-Adversarial Network (Steg-GMAN) to enhance steganography security. Specifically, we first employ multiple steganalyzers rather than a single steganalyzer like existing methods to enhance the performance of discriminator. Furthermore, in order to balance the capabilities of the generator and the discriminator during training stage, we propose an adaptive way to update the parameters of the proposed GAN model according to the discriminant ability of different steganalyzers. In each iteration, we just update the poorest one among all steganalyzers in discriminator, while update the generator with the gradients derived from the strongest one. In this way, the performance of generator and discriminator can be gradually improved, so as to avoid training failure caused by gradient vanishing. Extensive comparative results show that the proposed method can achieve state-of-the-art results compared with the traditional steganography and the modern GAN-based steganographic methods. In addition, a large number of ablation experiments verify the rationality of the proposed model.",Generators;Generative adversarial networks;Steganography;Costs;Security;Training;Distortion;Steganography;generative adversarial network;steganalysis,"{'month': '', 'issn': '1556-6021', 'doi': '10.1109/TIFS.2023.3318939', 'keywords': 'Generators;Generative adversarial networks;Steganography;Costs;Security;Training;Distortion;Steganography;generative adversarial network;steganalysis', 'abstract': 'Since the generative adversarial network (GAN) was proposed by Ian Goodfellow et al. in 2014, it has been widely used in various fields. However, there are only a few works related to image steganography so far. Existing GAN-based steganographic methods mainly focus on the design of generator, and just assign a relatively poorer steganalyzer in discriminator, which inevitably limits the performances of their models. In this paper, we propose a novel Steganographic method based on Generative Multi-Adversarial Network (Steg-GMAN) to enhance steganography security. Specifically, we first employ multiple steganalyzers rather than a single steganalyzer like existing methods to enhance the performance of discriminator. Furthermore, in order to balance the capabilities of the generator and the discriminator during training stage, we propose an adaptive way to update the parameters of the proposed GAN model according to the discriminant ability of different steganalyzers. In each iteration, we just update the poorest one among all steganalyzers in discriminator, while update the generator with the gradients derived from the strongest one. In this way, the performance of generator and discriminator can be gradually improved, so as to avoid training failure caused by gradient vanishing. Extensive comparative results show that the proposed method can achieve state-of-the-art results compared with the traditional steganography and the modern GAN-based steganographic methods. In addition, a large number of ablation experiments verify the rationality of the proposed model.', 'pages': '15-29', 'number': '', 'volume': '19', 'year': '2024', 'title': 'Steganography Embedding Cost Learning With Generative Multi-Adversarial Network', 'journal': 'IEEE Transactions on Information Forensics and Security', 'author': 'Huang, Dongxia and Luo, Weiqi and Liu, Minglin and Tang, Weixuan and Huang, Jiwu', 'ENTRYTYPE': 'article', 'ID': '10265249'}"
10607852,Generative AI in the Advancement of Viral Therapeutics for Predicting and Targeting Immune-Evasive SARS-CoV-2 Mutations,"Bist, Prem Singh and Tayara, Hilal and Chong, Kil To",Bist,10.1109/JBHI.2024.3432649,2024,IEEE Journal of Biomedical and Health Informatics,"The emergence of immune-evasive mutations in the SARS-CoV-2 spike protein is consistently challenging existing vaccines and therapies, making precise prediction of their escape potential a critical imperative. Artificial Intelligence(AI) holds great promise for deciphering the intricate language of protein. Here, we employed a Generative Adversarial Network to decipher the hidden escape pathways within the spike protein by generating spikes that closely resemble natural ones. Through comprehensive analysis, we demonstrated that generated sequences capture natural escape characteristics. Moreover, incorporating these sequences into an AI-based escape prediction model significantly enhanced its performance, achieving a 7\% increase in detecting natural escape mutations on the experimentally validated Greaney dataset. Similar improvements were observed on other datasets, demonstrating the model's generalizability. Precisely predicting immune-evasive spikes not only enables the design of strategically targeted therapies but also has the potential to expedite future viral therapeutics. This breakthrough carries profound implications for shaping a more resilient future against viral threats.",Proteins;Coronaviruses;Protein engineering;Training;Computational modeling;Medical treatment;Vaccines;Immune-evasive mutations;SARS-CoV-2 spike protein;artificial intelligence (AI);generative adversarial network (GAN);escape prediction model,"{'month': 'Nov', 'issn': '2168-2208', 'doi': '10.1109/JBHI.2024.3432649', 'keywords': 'Proteins;Coronaviruses;Protein engineering;Training;Computational modeling;Medical treatment;Vaccines;Immune-evasive mutations;SARS-CoV-2 spike protein;artificial intelligence (AI);generative adversarial network (GAN);escape prediction model', 'abstract': ""The emergence of immune-evasive mutations in the SARS-CoV-2 spike protein is consistently challenging existing vaccines and therapies, making precise prediction of their escape potential a critical imperative. Artificial Intelligence(AI) holds great promise for deciphering the intricate language of protein. Here, we employed a Generative Adversarial Network to decipher the hidden escape pathways within the spike protein by generating spikes that closely resemble natural ones. Through comprehensive analysis, we demonstrated that generated sequences capture natural escape characteristics. Moreover, incorporating these sequences into an AI-based escape prediction model significantly enhanced its performance, achieving a 7\\% increase in detecting natural escape mutations on the experimentally validated Greaney dataset. Similar improvements were observed on other datasets, demonstrating the model's generalizability. Precisely predicting immune-evasive spikes not only enables the design of strategically targeted therapies but also has the potential to expedite future viral therapeutics. This breakthrough carries profound implications for shaping a more resilient future against viral threats."", 'pages': '6974-6982', 'number': '11', 'volume': '28', 'year': '2024', 'title': 'Generative AI in the Advancement of Viral Therapeutics for Predicting and Targeting Immune-Evasive SARS-CoV-2 Mutations', 'journal': 'IEEE Journal of Biomedical and Health Informatics', 'author': 'Bist, Prem Singh and Tayara, Hilal and Chong, Kil To', 'ENTRYTYPE': 'article', 'ID': '10607852'}"
10721579,Artificial Intelligence-Based Kidney Segmentation With Modified Cycle-Consistent Generative Adversarial Network and Appearance-Based Shape Prior,"Sharaby, Israa and Magdy Balaha, Hossam and Alksas, Ahmed and Mahmoud, Ali and Abou El-Ghar, Mohamed and Khalil, Ashraf and Ghazal, Mohammed and Contractor, Sohail and El-Baz, Ayman",Sharaby,10.1109/ACCESS.2024.3483661,2024,IEEE Access,"This study presents an innovative deep learning framework for kidney segmentation in magnetic resonance imaging (MRI) data. The framework integrates both kidney appearance and prior shape information using a residual cycle-consistent generative adversarial network (CycleGAN). An appearance-based shape prior model is developed, utilizing iso-circular contours generated from the kidney centroid and employing the fast marching level sets method for shape extraction. By utilizing the kidney centroid and matching cross-circular iso-circular contours’ appearance, the proposed appearance-based shape prior model remains invariant to translation, rotation, and scaling, eliminating the need for alignment. Additionally, a novel weighted loss function, the H-Loss, is introduced to enhance segmentation performance and prevent overfitting. The proposed approach is tested on 34 blood-oxygen-level-dependent (BOLD) grafts from patients in our kidney transplant program, achieving an average dice score of 92\%. These promising results validate the effectiveness of the approach, with optimized hyperparameters ensuring high segmentation quality.",Kidney;Image segmentation;Tumors;Cancer;Generative adversarial networks;Training;Medical diagnostic imaging;Level set;Computer architecture;Shape measurement;Appearance-based shape prior;CycleGAN;generative adversarial network;kidney segmentation,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2024.3483661', 'keywords': 'Kidney;Image segmentation;Tumors;Cancer;Generative adversarial networks;Training;Medical diagnostic imaging;Level set;Computer architecture;Shape measurement;Appearance-based shape prior;CycleGAN;generative adversarial network;kidney segmentation', 'abstract': 'This study presents an innovative deep learning framework for kidney segmentation in magnetic resonance imaging (MRI) data. The framework integrates both kidney appearance and prior shape information using a residual cycle-consistent generative adversarial network (CycleGAN). An appearance-based shape prior model is developed, utilizing iso-circular contours generated from the kidney centroid and employing the fast marching level sets method for shape extraction. By utilizing the kidney centroid and matching cross-circular iso-circular contours’ appearance, the proposed appearance-based shape prior model remains invariant to translation, rotation, and scaling, eliminating the need for alignment. Additionally, a novel weighted loss function, the H-Loss, is introduced to enhance segmentation performance and prevent overfitting. The proposed approach is tested on 34 blood-oxygen-level-dependent (BOLD) grafts from patients in our kidney transplant program, achieving an average dice score of 92\\%. These promising results validate the effectiveness of the approach, with optimized hyperparameters ensuring high segmentation quality.', 'pages': '162536-162548', 'number': '', 'volume': '12', 'year': '2024', 'title': 'Artificial Intelligence-Based Kidney Segmentation With Modified Cycle-Consistent Generative Adversarial Network and Appearance-Based Shape Prior', 'journal': 'IEEE Access', 'author': 'Sharaby, Israa and Magdy Balaha, Hossam and Alksas, Ahmed and Mahmoud, Ali and Abou El-Ghar, Mohamed and Khalil, Ashraf and Ghazal, Mohammed and Contractor, Sohail and El-Baz, Ayman', 'ENTRYTYPE': 'article', 'ID': '10721579'}"
11060533,Integrating Generative Artificial Intelligence in Assessment Generation for Higher Education: Computer Science Use Case,"Kituku, Benson and Araka, Eric and Muuro, Elizaphan",Kituku,10.23919/IST-Africa67297.2025.11060533,2025,2025 IST-Africa Conference (IST-Africa),"The overburdened lecturers today face the dual challenge of monitoring online attendance and ensuring active student engagement, while also receiving instant feedback on concept comprehension during live lectures. They are further pressed to extend practical problem-solving experiences beyond the classroom and deliver higher-order thinking assessments - all without increasing their workload. In response, this paper presents a one-year experiment involving 590 students across seven computer science units. The study integrated generative AI-generated questions into live lectures, weekly discussion forums, and both formative and summative exams. The findings reveal that, with ethical use and proper human-in-the-loop, AI can significantly boost student engagement, promptly rectify misconceptions, and foster collaborative learning outside traditional settings and offer diverse question styles. However, given potential pitfalls such as low-quality outputs and overreliance, instructors must adhere to best practices and maintain rigorous oversight to ensure that assessments remain balanced, engaging, and of high quality, eventually benefiting both educators and learners.",Computer science;Generative AI;Federated learning;Education;Human in the loop;Problem-solving;Monitoring;Best practices;Faces;Testing;Generative AI;Assessment;Multiple choice questions;Higher education;Computer Science and Prompt,"{'month': 'May', 'issn': '2576-8581', 'doi': '10.23919/IST-Africa67297.2025.11060533', 'keywords': 'Computer science;Generative AI;Federated learning;Education;Human in the loop;Problem-solving;Monitoring;Best practices;Faces;Testing;Generative AI;Assessment;Multiple choice questions;Higher education;Computer Science and Prompt', 'abstract': 'The overburdened lecturers today face the dual challenge of monitoring online attendance and ensuring active student engagement, while also receiving instant feedback on concept comprehension during live lectures. They are further pressed to extend practical problem-solving experiences beyond the classroom and deliver higher-order thinking assessments - all without increasing their workload. In response, this paper presents a one-year experiment involving 590 students across seven computer science units. The study integrated generative AI-generated questions into live lectures, weekly discussion forums, and both formative and summative exams. The findings reveal that, with ethical use and proper human-in-the-loop, AI can significantly boost student engagement, promptly rectify misconceptions, and foster collaborative learning outside traditional settings and offer diverse question styles. However, given potential pitfalls such as low-quality outputs and overreliance, instructors must adhere to best practices and maintain rigorous oversight to ensure that assessments remain balanced, engaging, and of high quality, eventually benefiting both educators and learners.', 'pages': '1-10', 'number': '', 'volume': '', 'year': '2025', 'title': 'Integrating Generative Artificial Intelligence in Assessment Generation for Higher Education: Computer Science Use Case', 'booktitle': '2025 IST-Africa Conference (IST-Africa)', 'author': 'Kituku, Benson and Araka, Eric and Muuro, Elizaphan', 'ENTRYTYPE': 'inproceedings', 'ID': '11060533'}"
10457279,Face Recognition for Soldiers with Bulletproof Helmets Using Generative Networks and Data Augmentation,"Choi, Minseo and Shin, Wunjong and Pyo, Heesu and Song, Wootaek and Jung, Byungjun and Shin, Minwoo and Paik, Joonki",Choi,10.1109/ICEIC61013.2024.10457279,2024,"2024 International Conference on Electronics, Information, and Communication (ICEIC)","Face recognition (FR) is a key technology for a wide range of applications, including privacy-preserving security, education, entertainment, and emotion recognition. However, existing FR methods require a large amount of data to train, and their performance can be degraded by factors such as bulletproof helmets worn by soldiers. This paper proposes a new FR method for soldiers with bulletproof helmets that uses generative networks and data augmentation techniques to effectively increase the amount of training data. The proposed method first generates synthetic face images of soldiers with bulletproof helmets using a generative network. Then, it uses data augmentation techniques to further increase the diversity of the training data. The proposed method is evaluated on both our own dataset of soldiers with bulletproof helmets and a public dataset. The results show that the proposed method outperforms existing FR methods on both datasets, especially in the presence of bulletproof helmets. Overall, the proposed method is a promising new approach for FR for soldiers with bulletproof helmets. It can be used to improve the performance of FR systems in military applications, such as soldier identification and authentication.",Emotion recognition;Head;Face recognition;Supervised learning;Training data;Entertainment industry;Data augmentation;Face Recognition;Generative Model;Data Augmentation,"{'month': 'Jan', 'issn': '2767-7699', 'doi': '10.1109/ICEIC61013.2024.10457279', 'keywords': 'Emotion recognition;Head;Face recognition;Supervised learning;Training data;Entertainment industry;Data augmentation;Face Recognition;Generative Model;Data Augmentation', 'abstract': 'Face recognition (FR) is a key technology for a wide range of applications, including privacy-preserving security, education, entertainment, and emotion recognition. However, existing FR methods require a large amount of data to train, and their performance can be degraded by factors such as bulletproof helmets worn by soldiers. This paper proposes a new FR method for soldiers with bulletproof helmets that uses generative networks and data augmentation techniques to effectively increase the amount of training data. The proposed method first generates synthetic face images of soldiers with bulletproof helmets using a generative network. Then, it uses data augmentation techniques to further increase the diversity of the training data. The proposed method is evaluated on both our own dataset of soldiers with bulletproof helmets and a public dataset. The results show that the proposed method outperforms existing FR methods on both datasets, especially in the presence of bulletproof helmets. Overall, the proposed method is a promising new approach for FR for soldiers with bulletproof helmets. It can be used to improve the performance of FR systems in military applications, such as soldier identification and authentication.', 'pages': '1-2', 'number': '', 'volume': '', 'year': '2024', 'title': 'Face Recognition for Soldiers with Bulletproof Helmets Using Generative Networks and Data Augmentation', 'booktitle': '2024 International Conference on Electronics, Information, and Communication (ICEIC)', 'author': 'Choi, Minseo and Shin, Wunjong and Pyo, Heesu and Song, Wootaek and Jung, Byungjun and Shin, Minwoo and Paik, Joonki', 'ENTRYTYPE': 'inproceedings', 'ID': '10457279'}"
10810680,Explainable Artifacts for Synthetic Western Blot Source Attribution,"Cardenuto, João P. and Mandelli, Sara and Moreira, Daniel and Bestagini, Paolo and Delp, Edward and Rocha, Anderson",Cardenuto,10.1109/WIFS61860.2024.10810680,2024,2024 IEEE International Workshop on Information Forensics and Security (WIFS),"Recent advancements in artificial intelligence have enabled generative models to produce synthetic scientific images that are indistinguishable from pristine ones, posing a challenge even for expert scientists habituated to working with such content. When exploited by organizations known as paper mills, which systematically generate fraudulent articles, these technologies can significantly contribute to the spread of misinformation about ungrounded science, potentially undermining trust in scientific research. While previous studies have explored black-box solutions, such as Convolutional Neural Networks, for identifying synthetic content, only some have addressed the challenge of generalizing across different models and providing insight into the artifacts in synthetic images that inform the detection process. This study aims to identify explainable artifacts generated by state-of-the-art generative models (e.g., Generative Adversarial Networks and Diffusion Models) and leverage them for open-set identification and source attribution (i.e., pointing to the model that created the image).",Forensics;Conferences;Paper mills;Closed box;Organizations;Generative adversarial networks;Diffusion models;Security;Convolutional neural networks;Fake news;Western blots;synthetically generated images;image forensics;source attribution;scientific integrity,"{'month': 'Dec', 'issn': '2157-4774', 'doi': '10.1109/WIFS61860.2024.10810680', 'keywords': 'Forensics;Conferences;Paper mills;Closed box;Organizations;Generative adversarial networks;Diffusion models;Security;Convolutional neural networks;Fake news;Western blots;synthetically generated images;image forensics;source attribution;scientific integrity', 'abstract': 'Recent advancements in artificial intelligence have enabled generative models to produce synthetic scientific images that are indistinguishable from pristine ones, posing a challenge even for expert scientists habituated to working with such content. When exploited by organizations known as paper mills, which systematically generate fraudulent articles, these technologies can significantly contribute to the spread of misinformation about ungrounded science, potentially undermining trust in scientific research. While previous studies have explored black-box solutions, such as Convolutional Neural Networks, for identifying synthetic content, only some have addressed the challenge of generalizing across different models and providing insight into the artifacts in synthetic images that inform the detection process. This study aims to identify explainable artifacts generated by state-of-the-art generative models (e.g., Generative Adversarial Networks and Diffusion Models) and leverage them for open-set identification and source attribution (i.e., pointing to the model that created the image).', 'pages': '1-6', 'number': '', 'volume': '', 'year': '2024', 'title': 'Explainable Artifacts for Synthetic Western Blot Source Attribution', 'booktitle': '2024 IEEE International Workshop on Information Forensics and Security (WIFS)', 'author': 'Cardenuto, João P. and Mandelli, Sara and Moreira, Daniel and Bestagini, Paolo and Delp, Edward and Rocha, Anderson', 'ENTRYTYPE': 'inproceedings', 'ID': '10810680'}"
10131946,Correction of Banding Errors in Satellite Images With Generative Adversarial Networks (GAN),"Paola, Zárate L. and Jesús, López S. and Christian, Arroyo H. and Sonia, Rincón U.",Paola,10.1109/ACCESS.2023.3279265,2023,IEEE Access,"This research proposes an innovative method for correcting banding errors in satellite images based on Generative Adversarial Networks (GAN). Small satellites are frequently launched into space to obtain images that can be used in scientific or military research, commercial activities, and urban planning, among other applications. However, its small cameras are more susceptible to radiometric, geometric errors, and other distortions caused by atmospheric interference. The proposed method was compared to the conventional correction technique using experimental data, showing the similar performance (92.64\% and 90.05\% accuracy, respectively). These experimental results suggest that generative models utilizing Artificial Intelligence (AI) techniques, specifically Deep Learning, are getting closer to achieving automatic correction close to conventional methods. Advantages of the GAN models include automating the task of correcting banding in satellite images, reducing the required time, and facilitating the processing without requiring prior technical knowledge in handling Geographic Information Systems (GIS). Potentially, this technique could represent a valuable tool for satellite image processing, improving the accuracy of the results and making the process more efficient. The research is particularly relevant to the field of remote sensing and can have practical applications in various industries.",Satellite broadcasting;Generative adversarial networks;Generators;Training;Radiometry;Remote sensing;Image coding;Artificial neural network;deep learning;generative adversarial network;satellite images;radiometric error;banding,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2023.3279265', 'keywords': 'Satellite broadcasting;Generative adversarial networks;Generators;Training;Radiometry;Remote sensing;Image coding;Artificial neural network;deep learning;generative adversarial network;satellite images;radiometric error;banding', 'abstract': 'This research proposes an innovative method for correcting banding errors in satellite images based on Generative Adversarial Networks (GAN). Small satellites are frequently launched into space to obtain images that can be used in scientific or military research, commercial activities, and urban planning, among other applications. However, its small cameras are more susceptible to radiometric, geometric errors, and other distortions caused by atmospheric interference. The proposed method was compared to the conventional correction technique using experimental data, showing the similar performance (92.64\\% and 90.05\\% accuracy, respectively). These experimental results suggest that generative models utilizing Artificial Intelligence (AI) techniques, specifically Deep Learning, are getting closer to achieving automatic correction close to conventional methods. Advantages of the GAN models include automating the task of correcting banding in satellite images, reducing the required time, and facilitating the processing without requiring prior technical knowledge in handling Geographic Information Systems (GIS). Potentially, this technique could represent a valuable tool for satellite image processing, improving the accuracy of the results and making the process more efficient. The research is particularly relevant to the field of remote sensing and can have practical applications in various industries.', 'pages': '51960-51970', 'number': '', 'volume': '11', 'year': '2023', 'title': 'Correction of Banding Errors in Satellite Images With Generative Adversarial Networks (GAN)', 'journal': 'IEEE Access', 'author': 'Paola, Zárate L. and Jesús, López S. and Christian, Arroyo H. and Sonia, Rincón U.', 'ENTRYTYPE': 'article', 'ID': '10131946'}"
10609905,Exploring Image Similarity through Generative Language Models: A Comparative Study of GPT-4 with Word Embeddings and Traditional Approaches,"Malla, Alejandro and Omwenga, Maxwell M. and Kumar Bera, Pallav",Malla,10.1109/eIT60633.2024.10609905,2024,2024 IEEE International Conference on Electro Information Technology (eIT),"In this article, we propose a novel approach for determining image similarity, leveraging advancements in generative artificial intelligence. At the heart of our method is the use of OpenAI’s GPT-4 large language model for generating image captions, combined with the Ada v2 word embedding model for semantic analysis. This technique involves creating textual descriptions of images via GPT-4 and subsequently computing cosine similarity of these descriptions using Ada v2 word embeddings. We compare this innovative approach with traditional image similarity methods, with a particular focus on the VGG16 neural network approach, employing the DISC21 dataset for our analysis. Preliminary results demonstrate the promising potential of this method in the field of image similarity assessment. The paper delves into both the advantages and current limitations of our approach, including constraints like rate limits in experimentation and the rapidly evolving capabilities of language models in vision tasks. Our findings indicate a trajectory towards improved outcomes as these models continue to advance, underscoring the growing intersection of language and vision models in artificial intelligence for applications like image similarity evaluation.",Heart;Analytical models;Generative AI;Large language models;Computational modeling;Semantics;Neural networks;Image Similarity Assessment;Generative Language Models (GPT-4);Word Embeddings (Ada v2);Deep Learning in Computer Vision (VGG16);Semantic Analysis,"{'month': 'May', 'issn': '2154-0373', 'doi': '10.1109/eIT60633.2024.10609905', 'keywords': 'Heart;Analytical models;Generative AI;Large language models;Computational modeling;Semantics;Neural networks;Image Similarity Assessment;Generative Language Models (GPT-4);Word Embeddings (Ada v2);Deep Learning in Computer Vision (VGG16);Semantic Analysis', 'abstract': 'In this article, we propose a novel approach for determining image similarity, leveraging advancements in generative artificial intelligence. At the heart of our method is the use of OpenAI’s GPT-4 large language model for generating image captions, combined with the Ada v2 word embedding model for semantic analysis. This technique involves creating textual descriptions of images via GPT-4 and subsequently computing cosine similarity of these descriptions using Ada v2 word embeddings. We compare this innovative approach with traditional image similarity methods, with a particular focus on the VGG16 neural network approach, employing the DISC21 dataset for our analysis. Preliminary results demonstrate the promising potential of this method in the field of image similarity assessment. The paper delves into both the advantages and current limitations of our approach, including constraints like rate limits in experimentation and the rapidly evolving capabilities of language models in vision tasks. Our findings indicate a trajectory towards improved outcomes as these models continue to advance, underscoring the growing intersection of language and vision models in artificial intelligence for applications like image similarity evaluation.', 'pages': '275-279', 'number': '', 'volume': '', 'year': '2024', 'title': 'Exploring Image Similarity through Generative Language Models: A Comparative Study of GPT-4 with Word Embeddings and Traditional Approaches', 'booktitle': '2024 IEEE International Conference on Electro Information Technology (eIT)', 'author': 'Malla, Alejandro and Omwenga, Maxwell M. and Kumar Bera, Pallav', 'ENTRYTYPE': 'inproceedings', 'ID': '10609905'}"
8995243,Short-Spoken Language Intent Classification with Conditional Sequence Generative Adversarial Network,"Zhou, Xinyu and Peng, Yang",Zhou,10.1109/ICTAI.2019.00261,2019,2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI),"As one of the most thrilling tasks of natural language understanding (NLU), intent classification in a dialogue system has received a great deal of attention in both industry and academia. The major limiting factor on intent classification is the lack of tagged data. To solve it, in this paper, we propose a conditional sequence generative adversarial network (cSeq-GAN) for intent classification of short-spoken language, in which we simultaneously train a generative model and a discriminative model for two tasks, one to distinguish the generated text from the real spoken one, while the other to predict its intent category. More reliable tagged data obtained by the generator greatly improves the performance of the intent classification task. Extensive experiments on both Air Travel Information System (ATIS) and our selling robot dialogue system for insurance industries demonstrate that our cSeq-GAN achieves competitive classification accuracy with other state-of-art methods of text classification.",GAN;Dialogue System;Intent Classification,"{'month': 'Nov', 'issn': '2375-0197', 'doi': '10.1109/ICTAI.2019.00261', 'keywords': 'GAN;Dialogue System;Intent Classification', 'abstract': 'As one of the most thrilling tasks of natural language understanding (NLU), intent classification in a dialogue system has received a great deal of attention in both industry and academia. The major limiting factor on intent classification is the lack of tagged data. To solve it, in this paper, we propose a conditional sequence generative adversarial network (cSeq-GAN) for intent classification of short-spoken language, in which we simultaneously train a generative model and a discriminative model for two tasks, one to distinguish the generated text from the real spoken one, while the other to predict its intent category. More reliable tagged data obtained by the generator greatly improves the performance of the intent classification task. Extensive experiments on both Air Travel Information System (ATIS) and our selling robot dialogue system for insurance industries demonstrate that our cSeq-GAN achieves competitive classification accuracy with other state-of-art methods of text classification.', 'pages': '1753-1756', 'number': '', 'volume': '', 'year': '2019', 'title': 'Short-Spoken Language Intent Classification with Conditional Sequence Generative Adversarial Network', 'booktitle': '2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI)', 'author': 'Zhou, Xinyu and Peng, Yang', 'ENTRYTYPE': 'inproceedings', 'ID': '8995243'}"
10920549,Occlusion-Aware Segmentation Via RCF-Pix2Pix Generative Network,"An, Congying and Wu, Jingjing and Zhang, Huanlong",An,10.1109/ICSMD64214.2024.10920549,2024,"2024 International Conference on Sensing, Measurement \& Data Analytics in the era of Artificial Intelligence (ICSMD)","Segmentingimage objects overlapped by other objects is challenging, because the shapes in occluded areas are unknown, and occlusion boundaries typically have no distinction from real object contours. Unlike traditional segmentation methods using convolutional networks, we explored the segmentation capabilities of generative networks in occluded areas and proposed a new edge-guided network architecture (RCF-Pix2Pix). The network simulates the human thought process for estimating the shape of occluded areas, using edge and overall contour information from visible parts to infer the shape of the occluded portion. RCF-Pix2Pix enhances the edge features by integrating edge information. It also uses edge contours as conditional input for the discriminator, guiding the network to predict contours in invisible areas. Moreover, by combining MSE-SSIM-L1 loss and edge loss, it improves the accuracy of target segmentation and stabilizes the quality of segmented images, making it more effective in handling complex imaging tasks. Experimental results show that our method has achieved significant improvements on the chip dataset, with an mIOU of 98.9\%, Boundary IoU of 86.6\% (+10.4\%), and also achieved significant accuracy improvements in the D2SA dataset tests. This confirms the effectiveness and practical value of the RCF-Pix2Pix network in dealing with complex obstruction situations. Code is available at: https://github.com/CongyingAn/RCF-Pix2Pix-Generative-Network.",Training;Image segmentation;Accuracy;Shape;Image edge detection;Noise;Production;Network architecture;Generators;Surface treatment;RCF-Pix2Pix;Occluded areas;conditional information;Edge,"{'month': 'Oct', 'issn': '', 'doi': '10.1109/ICSMD64214.2024.10920549', 'keywords': 'Training;Image segmentation;Accuracy;Shape;Image edge detection;Noise;Production;Network architecture;Generators;Surface treatment;RCF-Pix2Pix;Occluded areas;conditional information;Edge', 'abstract': 'Segmentingimage objects overlapped by other objects is challenging, because the shapes in occluded areas are unknown, and occlusion boundaries typically have no distinction from real object contours. Unlike traditional segmentation methods using convolutional networks, we explored the segmentation capabilities of generative networks in occluded areas and proposed a new edge-guided network architecture (RCF-Pix2Pix). The network simulates the human thought process for estimating the shape of occluded areas, using edge and overall contour information from visible parts to infer the shape of the occluded portion. RCF-Pix2Pix enhances the edge features by integrating edge information. It also uses edge contours as conditional input for the discriminator, guiding the network to predict contours in invisible areas. Moreover, by combining MSE-SSIM-L1 loss and edge loss, it improves the accuracy of target segmentation and stabilizes the quality of segmented images, making it more effective in handling complex imaging tasks. Experimental results show that our method has achieved significant improvements on the chip dataset, with an mIOU of 98.9\\%, Boundary IoU of 86.6\\% (+10.4\\%), and also achieved significant accuracy improvements in the D2SA dataset tests. This confirms the effectiveness and practical value of the RCF-Pix2Pix network in dealing with complex obstruction situations. Code is available at: https://github.com/CongyingAn/RCF-Pix2Pix-Generative-Network.', 'pages': '1-6', 'number': '', 'volume': '', 'year': '2024', 'title': 'Occlusion-Aware Segmentation Via RCF-Pix2Pix Generative Network', 'booktitle': '2024 International Conference on Sensing, Measurement \\& Data Analytics in the era of Artificial Intelligence (ICSMD)', 'author': 'An, Congying and Wu, Jingjing and Zhang, Huanlong', 'ENTRYTYPE': 'inproceedings', 'ID': '10920549'}"
10974174,Generative AI-Based Tool for Brute Forcing IoT Devices' Default Credentials,"Al Rawi, Anas and Jabeur, Nafaa and Anwar, Raja Waseem",Al Rawi,10.1109/AIR63653.2024.00015,2024,2024 Artificial Intelligence Revolutions (AIR),"This study beneficially uses the power of generative AI to search for vendor-specific default credentials and uses them to brute force IoT devices logins. IoT devices have a diverse set of open ports used for accessing and configuration. With the increased usage of IoT devices, keeping all devices' ports wellsecured is overwhelming and costly, especially for SMEs. Using a variety of methods to approach the problem, this research studied IoT attacks, characteristics, IoT penetration tools, and small to medium size enterprises (SMEs) requirements to produce an automated solution. Findings indicated that a lot of IoT devices are still configured with default credentials making the networks they are connected to vulnerable attacks. The solution presented, is a script that integrates OpenAI GPT to search for default credentials, Nmap to scan for open ports, and Hydra to attack the device. The tool is implemented to assess some specific widely used ports. To detect vulnerable IoT devices and report them to the user, the tool analyses login pages available on ports 80 and 443 to search for the brand and model of the IoT device. The output is used for the default credentials GPT search. Despite its ability to shortlist the dictionary for a brute force list, it should be tested on an experiential environment that includes different IoT simulators with several open ports on changed credentials and default ones. Then verified its functionality on a real IoT network. Further research could explore implementing machine learning to thoroughly analyse IoT device firmware.",Analytical models;Dictionaries;Generative AI;Force;Machine learning;Internet of Things;Microprogramming;Generative AI;IoT Device;Default Credentials;Brute Force;SMEs;Python Script,"{'month': 'Oct', 'issn': '', 'doi': '10.1109/AIR63653.2024.00015', 'keywords': 'Analytical models;Dictionaries;Generative AI;Force;Machine learning;Internet of Things;Microprogramming;Generative AI;IoT Device;Default Credentials;Brute Force;SMEs;Python Script', 'abstract': ""This study beneficially uses the power of generative AI to search for vendor-specific default credentials and uses them to brute force IoT devices logins. IoT devices have a diverse set of open ports used for accessing and configuration. With the increased usage of IoT devices, keeping all devices' ports wellsecured is overwhelming and costly, especially for SMEs. Using a variety of methods to approach the problem, this research studied IoT attacks, characteristics, IoT penetration tools, and small to medium size enterprises (SMEs) requirements to produce an automated solution. Findings indicated that a lot of IoT devices are still configured with default credentials making the networks they are connected to vulnerable attacks. The solution presented, is a script that integrates OpenAI GPT to search for default credentials, Nmap to scan for open ports, and Hydra to attack the device. The tool is implemented to assess some specific widely used ports. To detect vulnerable IoT devices and report them to the user, the tool analyses login pages available on ports 80 and 443 to search for the brand and model of the IoT device. The output is used for the default credentials GPT search. Despite its ability to shortlist the dictionary for a brute force list, it should be tested on an experiential environment that includes different IoT simulators with several open ports on changed credentials and default ones. Then verified its functionality on a real IoT network. Further research could explore implementing machine learning to thoroughly analyse IoT device firmware."", 'pages': '102-108', 'number': '', 'volume': '', 'year': '2024', 'title': ""Generative AI-Based Tool for Brute Forcing IoT Devices' Default Credentials"", 'booktitle': '2024 Artificial Intelligence Revolutions (AIR)', 'author': 'Al Rawi, Anas and Jabeur, Nafaa and Anwar, Raja Waseem', 'ENTRYTYPE': 'inproceedings', 'ID': '10974174'}"
10951427,Parsing Machine Learning and Deep Learning,"Minnick, Chris",Minnick,,2024,Coding with AI For Dummies,"Summary <p>This chapter covers some of the basics of AI and machine learning. Machine learning is a type of AI that focuses on developing and using computer systems that can learn and adapt without following explicit instructions. Deep learning is a type of machine learning based on artificial neural networks. Natural\&\#x2010;language processing (NLP) is the branch of AI concerned with giving computers the capability to understand human language in written and spoken form. NLP can be further divided into two subsets: natural\&\#x2010;language understanding and natural\&\#x2010;language generation. Transformer models use a self\&\#x2010;attention mechanism to find dependencies between inputs and outputs. AI chatbots are language models tuned for conversation. The standard ChatGPT response to even a simple question reads like a high school book report, containing an introduction, an analysis of an issue from multiple viewpoints, and a summary.</p>",Biological neural networks;Dogs;Neurons;Deep learning;Generative AI;Artificial neural networks;Writing;Encoding;Training;Shape,"{'url': 'https://ieeexplore-ieee-org.crai.referencistas.com/document/10951427', 'isbn': '9781394249152', 'publisher': 'Wiley', 'issn': '', 'doi': '', 'keywords': 'Biological neural networks;Dogs;Neurons;Deep learning;Generative AI;Artificial neural networks;Writing;Encoding;Training;Shape', 'abstract': 'Summary <p>This chapter covers some of the basics of AI and machine learning. Machine learning is a type of AI that focuses on developing and using computer systems that can learn and adapt without following explicit instructions. Deep learning is a type of machine learning based on artificial neural networks. Natural\\&\\#x2010;language processing (NLP) is the branch of AI concerned with giving computers the capability to understand human language in written and spoken form. NLP can be further divided into two subsets: natural\\&\\#x2010;language understanding and natural\\&\\#x2010;language generation. Transformer models use a self\\&\\#x2010;attention mechanism to find dependencies between inputs and outputs. AI chatbots are language models tuned for conversation. The standard ChatGPT response to even a simple question reads like a high school book report, containing an introduction, an analysis of an issue from multiple viewpoints, and a summary.</p>', 'pages': '37-56', 'number': '', 'volume': '', 'year': '2024', 'title': 'Parsing Machine Learning and Deep Learning', 'booktitle': 'Coding with AI For Dummies', 'author': 'Minnick, Chris', 'ENTRYTYPE': 'inbook', 'ID': '10951427'}"
10908263,A Robust and Trustworthy Intrusion Detection System Using Adversarial Machine Learning and XAI,"Tai, Nguyen Ngoc and Tai, Nguyen Ngoc and Tan, Nguyen Duc and Tan, Nguyen Duc and To, Trong-Nghia and To, Trong-Nghia and Duy, Phan The and Duy, Phan The and Pham, Van-Hau and Pham, Van-Hau",Tai,10.1109/ATC63255.2024.10908263,2024,2024 International Conference on Advanced Technologies for Communications (ATC),"Network attacks are increasingly sophisticated. Advances in Artificial Intelligence (AI), particularly deep learning, have improved intrusion detection systems (IDS). However, deep learning (DL) in cybersecurity faces challenges, such as imbalanced training data, vulnerability to adversarial attacks and a lack of transparency regarding AI systems. To address these problems, we developed the RobustAdvTrain (Robustness Adversarial Training) framework to train IDS models for high accuracy and resilience against adversarial attacks. This framework provides explainable AI for transparent IDS predictions. We propose the sAoEGAN (self-Attention on Explanation Generative Adversarial Network) model, which combines explainable AI and self-attention mechanisms to generate high-quality adversarial samples. Our approach improves intrusion detection, resilience to adversarial samples, and transparency in deep learning-based IDS systems.",Training;Deep learning;Explainable AI;Computational modeling;Scalability;Memory management;Intrusion detection;Training data;Generative adversarial networks;Resilience;Intrusion Detection System;Adversarial Machine Learning;Explainable AI,"{'month': 'Oct', 'issn': '2162-1039', 'doi': '10.1109/ATC63255.2024.10908263', 'keywords': 'Training;Deep learning;Explainable AI;Computational modeling;Scalability;Memory management;Intrusion detection;Training data;Generative adversarial networks;Resilience;Intrusion Detection System;Adversarial Machine Learning;Explainable AI', 'abstract': 'Network attacks are increasingly sophisticated. Advances in Artificial Intelligence (AI), particularly deep learning, have improved intrusion detection systems (IDS). However, deep learning (DL) in cybersecurity faces challenges, such as imbalanced training data, vulnerability to adversarial attacks and a lack of transparency regarding AI systems. To address these problems, we developed the RobustAdvTrain (Robustness Adversarial Training) framework to train IDS models for high accuracy and resilience against adversarial attacks. This framework provides explainable AI for transparent IDS predictions. We propose the sAoEGAN (self-Attention on Explanation Generative Adversarial Network) model, which combines explainable AI and self-attention mechanisms to generate high-quality adversarial samples. Our approach improves intrusion detection, resilience to adversarial samples, and transparency in deep learning-based IDS systems.', 'pages': '407-412', 'number': '', 'volume': '', 'year': '2024', 'title': 'A Robust and Trustworthy Intrusion Detection System Using Adversarial Machine Learning and XAI', 'booktitle': '2024 International Conference on Advanced Technologies for Communications (ATC)', 'author': 'Tai, Nguyen Ngoc and Tai, Nguyen Ngoc and Tan, Nguyen Duc and Tan, Nguyen Duc and To, Trong-Nghia and To, Trong-Nghia and Duy, Phan The and Duy, Phan The and Pham, Van-Hau and Pham, Van-Hau', 'ENTRYTYPE': 'inproceedings', 'ID': '10908263'}"
9823913,Role of DNN in CyberSecurity: A Review,"Thakur, Pooja and Soni, Bikash Kumar and Agarwal, Divyansh",Thakur,10.1109/ICACITE53722.2022.9823913,2022,2022 2nd International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE),"The advent of the cyberspace had us living in the lap of luxury, connecting us to every part of the world. It has also become a dependable depot for storing all forms data. This has also led to an upsurge in the number of cyber threats in the past few years. Data breaches, Identity theft, Cyberterrorism and many other forms of threats have been affecting innumerable individuals and organizations. Cybercriminals have been constantly redefining their skills to pose even greater threat to the world. Cybersecurity experts are constantly trying to evade such failures; still the current security methods have been proven not to be much effective on many fronts. The addition of the potentials of Artificial Intelligence could serve as a boon for this purpose as this machine based form of intelligence can use its abilities to enhance different forms of pre-existing cybersecurity methods and probably also to create some new methods. Artificial Intelligence has been a revolutionary creation. The key intent of this paper is to examine various features of A.I. which can be blended with other security methods to create a more secure environment and make it a crucial weapon in our depository to act as a counter against the cyber threats.",Technological innovation;Social networking (online);Pandemics;Weapons;Neural networks;Multimedia Web sites;Organizations;Cyber-security;Cyber threats;Generative Adversarial Networks;DNN,"{'month': 'April', 'issn': '', 'doi': '10.1109/ICACITE53722.2022.9823913', 'keywords': 'Technological innovation;Social networking (online);Pandemics;Weapons;Neural networks;Multimedia Web sites;Organizations;Cyber-security;Cyber threats;Generative Adversarial Networks;DNN', 'abstract': 'The advent of the cyberspace had us living in the lap of luxury, connecting us to every part of the world. It has also become a dependable depot for storing all forms data. This has also led to an upsurge in the number of cyber threats in the past few years. Data breaches, Identity theft, Cyberterrorism and many other forms of threats have been affecting innumerable individuals and organizations. Cybercriminals have been constantly redefining their skills to pose even greater threat to the world. Cybersecurity experts are constantly trying to evade such failures; still the current security methods have been proven not to be much effective on many fronts. The addition of the potentials of Artificial Intelligence could serve as a boon for this purpose as this machine based form of intelligence can use its abilities to enhance different forms of pre-existing cybersecurity methods and probably also to create some new methods. Artificial Intelligence has been a revolutionary creation. The key intent of this paper is to examine various features of A.I. which can be blended with other security methods to create a more secure environment and make it a crucial weapon in our depository to act as a counter against the cyber threats.', 'pages': '947-950', 'number': '', 'volume': '', 'year': '2022', 'title': 'Role of DNN in CyberSecurity: A Review', 'booktitle': '2022 2nd International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE)', 'author': 'Thakur, Pooja and Soni, Bikash Kumar and Agarwal, Divyansh', 'ENTRYTYPE': 'inproceedings', 'ID': '9823913'}"
10767756,Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap,"Wu, Xingyu and Wu, Sheng-Hao and Wu, Jibin and Feng, Liang and Tan, Kay Chen",Wu,10.1109/TEVC.2024.3506731,2025,IEEE Transactions on Evolutionary Computation,"Large language models (LLMs) have not only revolutionized natural language processing but also extended their prowess to various domains, marking a significant stride toward artificial general intelligence. The interplay between LLMs and evolutionary algorithms (EAs), despite differing in objectives and methodologies, share a common pursuit of applicability in complex problems. Meanwhile, EA can provide an optimization framework for LLM’s further enhancement under closed box settings, empowering LLM with flexible global search capacities. On the other hand, the abundant domain knowledge inherent in LLMs could enable EA to conduct more intelligent searches. Furthermore, the text processing and generative capabilities of LLMs would aid in deploying EAs across a wide range of tasks. Based on these complementary advantages, this article provides a thorough review and a forward-looking roadmap, categorizing the reciprocal inspiration into two main avenues: 1) LLM-enhanced EA and 2) EA-enhanced LLM. Some integrated synergy methods are further introduced to exemplify the complementarity between LLMs and EAs in diverse scenarios, including code generation, software engineering, neural architecture search, and various generation tasks. As the first comprehensive review focused on the EA research in the era of LLMs, this article provides a foundational stepping stone for understanding the collaborative potential of LLMs and EAs. The identified challenges and future directions offer guidance for researchers and practitioners to unlock the full potential of this innovative collaboration in propelling advancements in optimization and artificial intelligence. We have created a GitHub repository to index the relevant papers: https://github.com/wuxingyu-ai/LLM4EC.",Optimization;Closed box;Reviews;Evolutionary computation;Codes;Search problems;Collaboration;Surveys;Software engineering;Prompt engineering;Algorithm generation;evolutionary algorithm (EA);large language model (LLM);neural architecture search (NAS);optimization problem;prompt engineering,"{'month': 'April', 'issn': '1941-0026', 'doi': '10.1109/TEVC.2024.3506731', 'keywords': 'Optimization;Closed box;Reviews;Evolutionary computation;Codes;Search problems;Collaboration;Surveys;Software engineering;Prompt engineering;Algorithm generation;evolutionary algorithm (EA);large language model (LLM);neural architecture search (NAS);optimization problem;prompt engineering', 'abstract': 'Large language models (LLMs) have not only revolutionized natural language processing but also extended their prowess to various domains, marking a significant stride toward artificial general intelligence. The interplay between LLMs and evolutionary algorithms (EAs), despite differing in objectives and methodologies, share a common pursuit of applicability in complex problems. Meanwhile, EA can provide an optimization framework for LLM’s further enhancement under closed box settings, empowering LLM with flexible global search capacities. On the other hand, the abundant domain knowledge inherent in LLMs could enable EA to conduct more intelligent searches. Furthermore, the text processing and generative capabilities of LLMs would aid in deploying EAs across a wide range of tasks. Based on these complementary advantages, this article provides a thorough review and a forward-looking roadmap, categorizing the reciprocal inspiration into two main avenues: 1) LLM-enhanced EA and 2) EA-enhanced LLM. Some integrated synergy methods are further introduced to exemplify the complementarity between LLMs and EAs in diverse scenarios, including code generation, software engineering, neural architecture search, and various generation tasks. As the first comprehensive review focused on the EA research in the era of LLMs, this article provides a foundational stepping stone for understanding the collaborative potential of LLMs and EAs. The identified challenges and future directions offer guidance for researchers and practitioners to unlock the full potential of this innovative collaboration in propelling advancements in optimization and artificial intelligence. We have created a GitHub repository to index the relevant papers: https://github.com/wuxingyu-ai/LLM4EC.', 'pages': '534-554', 'number': '2', 'volume': '29', 'year': '2025', 'title': 'Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap', 'journal': 'IEEE Transactions on Evolutionary Computation', 'author': 'Wu, Xingyu and Wu, Sheng-Hao and Wu, Jibin and Feng, Liang and Tan, Kay Chen', 'ENTRYTYPE': 'article', 'ID': '10767756'}"
11005557,Collaborative AI Dysarthric Speech Recognition System With Data Augmentation Using Generative Adversarial Neural Network,"He, Yibo and Seng, Kah Phooi and Ang, Li-Minn",He,10.1109/TNSRE.2025.3570383,2025,IEEE Transactions on Neural Systems and Rehabilitation Engineering,"This paper proposes a novel collaborative dysarthric speech recognition system designed to convert dysarthric speech into non-dysarthric speech to enhance the robustness of automatic speech recognition (ASR) systems fine-tuned for dysarthric speech. The system employs an innovative three-stage data augmentation framework: The first stage collaboratively augments the training dataset by generating static data and high-quality synthetic speech samples using a natural text-to-speech model (Tacotron2). The second stage applies a tempo perturbation technique that simulates the natural variation of speech rhythms by adjusting the playback tempo to improve the model’s adaptability to varying speech speeds. The third stage integrates the Inception-ResNet module with a temporal masking strategy using an enhanced CycleGAN-based conversion model to efficiently map conformal and non-conformal phonological features while preserving the overall speech structure and resolving temporal irregularities. Experiments conducted on the UASpeech corpus demonstrate a significant reduction in the word error rate (WER) compared to the baseline approach. Specifically, the three-stage data enhancement process achieves a reduction in the WER for the fine-tuned Wav2Vec2-XLSR and Whisper-Tiny models by 9.81\% and 6.56\%, respectively, with an average WER of 13.58\% for the best performing system. These results highlight the effectiveness of the collaborative framework in improving the accuracy and naturalness of speech recognition for dysarthria, thereby offering individuals with dysarthria a more natural and intelligible communication experience.",Speech recognition;Data augmentation;Perturbation methods;Data models;Speech enhancement;Collaboration;Artificial intelligence;Training;Robustness;Generative adversarial networks;Collaborative AI;data augmentation;generative adversarial networks (GANs);deep learning;speech recognition;dysarthric speech,"{'month': '', 'issn': '1558-0210', 'doi': '10.1109/TNSRE.2025.3570383', 'keywords': 'Speech recognition;Data augmentation;Perturbation methods;Data models;Speech enhancement;Collaboration;Artificial intelligence;Training;Robustness;Generative adversarial networks;Collaborative AI;data augmentation;generative adversarial networks (GANs);deep learning;speech recognition;dysarthric speech', 'abstract': 'This paper proposes a novel collaborative dysarthric speech recognition system designed to convert dysarthric speech into non-dysarthric speech to enhance the robustness of automatic speech recognition (ASR) systems fine-tuned for dysarthric speech. The system employs an innovative three-stage data augmentation framework: The first stage collaboratively augments the training dataset by generating static data and high-quality synthetic speech samples using a natural text-to-speech model (Tacotron2). The second stage applies a tempo perturbation technique that simulates the natural variation of speech rhythms by adjusting the playback tempo to improve the model’s adaptability to varying speech speeds. The third stage integrates the Inception-ResNet module with a temporal masking strategy using an enhanced CycleGAN-based conversion model to efficiently map conformal and non-conformal phonological features while preserving the overall speech structure and resolving temporal irregularities. Experiments conducted on the UASpeech corpus demonstrate a significant reduction in the word error rate (WER) compared to the baseline approach. Specifically, the three-stage data enhancement process achieves a reduction in the WER for the fine-tuned Wav2Vec2-XLSR and Whisper-Tiny models by 9.81\\% and 6.56\\%, respectively, with an average WER of 13.58\\% for the best performing system. These results highlight the effectiveness of the collaborative framework in improving the accuracy and naturalness of speech recognition for dysarthria, thereby offering individuals with dysarthria a more natural and intelligible communication experience.', 'pages': '2097-2111', 'number': '', 'volume': '33', 'year': '2025', 'title': 'Collaborative AI Dysarthric Speech Recognition System With Data Augmentation Using Generative Adversarial Neural Network', 'journal': 'IEEE Transactions on Neural Systems and Rehabilitation Engineering', 'author': 'He, Yibo and Seng, Kah Phooi and Ang, Li-Minn', 'ENTRYTYPE': 'article', 'ID': '11005557'}"
10472376,Stability in Radiomics Analysis: Advancements and Challenges,"Chaddad, Ahmad",Chaddad,10.1109/Healthcom56612.2023.10472376,2023,"2023 IEEE International Conference on E-health Networking, Application \& Services (Healthcom)","Radiomics has emerged as a rapidly expanding field within precision medicine, offering promising applications for cancer diagnosis, classification, prognosis, prediction, and evaluation. However, the clinical viability of radiomics depends on the stability of its features and models. This paper provides a comprehensive analysis of recent advances in the assessment of the stability, repeatability, and reproducibility of radiomics. It explores various methodologies that aim to improve these performance metrics while addressing the challenges and limitations encountered in clinical applications. The survey emphasizes the critical importance of establishing unified standards that can harmonize data acquisition and analysis practices across multiple institutions. By promoting standardized approaches, this seeks to enable the delivery of more reliable and stable radiomic data, thus improving the overall stability of models and facilitating widespread adoption in clinical settings. Additionally, this paper explores the integration of artificial intelligence (AI) and medical imaging, recognizing it as a potential avenue for fostering secure data sharing while protecting patient privacy. Using AI techniques, healthcare professionals can collaborate and share insights more efficiently, accelerating the progress of radiomics research and its translation into clinical practice. By addressing these critical aspects, our work contributes to the continuous development and broader implementation of radiomics within the field of precision medicine.",Analytical models;Precision medicine;Stability criteria;Feature extraction;Reproducibility of results;Reliability;Artificial intelligence;Radiomics;stability;medical imaging;AI,"{'month': 'Dec', 'issn': '', 'doi': '10.1109/Healthcom56612.2023.10472376', 'keywords': 'Analytical models;Precision medicine;Stability criteria;Feature extraction;Reproducibility of results;Reliability;Artificial intelligence;Radiomics;stability;medical imaging;AI', 'abstract': 'Radiomics has emerged as a rapidly expanding field within precision medicine, offering promising applications for cancer diagnosis, classification, prognosis, prediction, and evaluation. However, the clinical viability of radiomics depends on the stability of its features and models. This paper provides a comprehensive analysis of recent advances in the assessment of the stability, repeatability, and reproducibility of radiomics. It explores various methodologies that aim to improve these performance metrics while addressing the challenges and limitations encountered in clinical applications. The survey emphasizes the critical importance of establishing unified standards that can harmonize data acquisition and analysis practices across multiple institutions. By promoting standardized approaches, this seeks to enable the delivery of more reliable and stable radiomic data, thus improving the overall stability of models and facilitating widespread adoption in clinical settings. Additionally, this paper explores the integration of artificial intelligence (AI) and medical imaging, recognizing it as a potential avenue for fostering secure data sharing while protecting patient privacy. Using AI techniques, healthcare professionals can collaborate and share insights more efficiently, accelerating the progress of radiomics research and its translation into clinical practice. By addressing these critical aspects, our work contributes to the continuous development and broader implementation of radiomics within the field of precision medicine.', 'pages': '1-5', 'number': '', 'volume': '', 'year': '2023', 'title': 'Stability in Radiomics Analysis: Advancements and Challenges', 'booktitle': '2023 IEEE International Conference on E-health Networking, Application \\& Services (Healthcom)', 'author': 'Chaddad, Ahmad', 'ENTRYTYPE': 'inproceedings', 'ID': '10472376'}"
10880572,Deep Learning Model for Resolution Enhancement of Biomedical Images for Biometrics,"RaviKrishna, Bhallamudi and Reddy, Madireddy Vijay and Soni, Mukesh and Byeon, Haewon and Pande, Sagar D. and Rusho, Maher A.",RaviKrishna,10.1002/9781394280735.ch16,2025,Generative Artificial Intelligence for Biomedical and Smart Health Informatics,"Summary <p>In the fields of precision biometrics and healthcare, high\&\#x2010;resolution biometric images play a crucial role as objective proof for accurate illness diagnosis. However, due to limitations in hardware resolution and scanning duration, real\&\#x2010;time acquisition of high\&\#x2010;resolution biomedical images poses challenges. Classic image super\&\#x2010;resolution reconstruction (SRR) algorithms suffer from difficulties in estimating model parameters, resulting in blurry and unrealistic reconstructed images, making them unsuitable for biomedical images. To address this issue, this chapter proposes a sparse\&\#x2010;coding nonlocal attention dual\&\#x2010;network. By employing sparse\&\#x2010;coding nonlocal attention mechanisms, Gaussian constraints, and parameter sharing strategies in the up\&\#x2010;sampling and down\&\#x2010;sampling dual branches, SRR of biomedical images is achieved. It has a high signal\&\#x2010;to\&\#x2010;noise ratio of 30.84\&\#x2009;dB and a structural identity of 0.914 for the rebuilt biomedical images. The research shows that the suggested method not only correctly reconstructs details at a high frequency in biomedical images but it also improves modeling efficiency with lightweight sparse\&\#x2010;coding nonlocal attention mechanisms. This makes it a useful method for reconstructing biomedical images at very high resolutions in biometrics.</p>",Feature extraction;Image reconstruction;Hafnium;Biological system modeling;Biometrics;Biomedical imaging;Data mining;Superresolution;Semantics;Training,"{'url': 'https://ieeexplore-ieee-org.crai.referencistas.com/document/10880572', 'isbn': '9781394280728', 'publisher': 'IEEE', 'issn': '', 'doi': '10.1002/9781394280735.ch16', 'keywords': 'Feature extraction;Image reconstruction;Hafnium;Biological system modeling;Biometrics;Biomedical imaging;Data mining;Superresolution;Semantics;Training', 'abstract': 'Summary <p>In the fields of precision biometrics and healthcare, high\\&\\#x2010;resolution biometric images play a crucial role as objective proof for accurate illness diagnosis. However, due to limitations in hardware resolution and scanning duration, real\\&\\#x2010;time acquisition of high\\&\\#x2010;resolution biomedical images poses challenges. Classic image super\\&\\#x2010;resolution reconstruction (SRR) algorithms suffer from difficulties in estimating model parameters, resulting in blurry and unrealistic reconstructed images, making them unsuitable for biomedical images. To address this issue, this chapter proposes a sparse\\&\\#x2010;coding nonlocal attention dual\\&\\#x2010;network. By employing sparse\\&\\#x2010;coding nonlocal attention mechanisms, Gaussian constraints, and parameter sharing strategies in the up\\&\\#x2010;sampling and down\\&\\#x2010;sampling dual branches, SRR of biomedical images is achieved. It has a high signal\\&\\#x2010;to\\&\\#x2010;noise ratio of 30.84\\&\\#x2009;dB and a structural identity of 0.914 for the rebuilt biomedical images. The research shows that the suggested method not only correctly reconstructs details at a high frequency in biomedical images but it also improves modeling efficiency with lightweight sparse\\&\\#x2010;coding nonlocal attention mechanisms. This makes it a useful method for reconstructing biomedical images at very high resolutions in biometrics.</p>', 'pages': '321-341', 'number': '', 'volume': '', 'year': '2025', 'title': 'Deep Learning Model for Resolution Enhancement of Biomedical Images for Biometrics', 'booktitle': 'Generative Artificial Intelligence for Biomedical and Smart Health Informatics', 'author': 'RaviKrishna, Bhallamudi and Reddy, Madireddy Vijay and Soni, Mukesh and Byeon, Haewon and Pande, Sagar D. and Rusho, Maher A.', 'ENTRYTYPE': 'inbook', 'ID': '10880572'}"
10935540,Development and Classroom Application Research of Q\&A System Based on LangChain + LLMs,"Zhang, Bingge and Shao, Xiuting and Song, Ziwen",Zhang,10.1109/ITME63426.2024.00142,2024,2024 14th International Conference on Information Technology in Medicine and Education (ITME),"As China emphasizes intelligent education, the application of Artificial Intelligence (AI) in this field is becoming increasingly widespread. Q\&A systems, as one of the key technologies of AI, are gradually being integrated into classroom teaching. The application plays a significant role in improving teaching quality and promoting personalized learning. However, the application process faces several problems, such as insufficient digital literacy, the inapplicability of traditional assessment methods, and the backwardness of educational concepts and methods. Moreover, in the era of information overload, the information provided by intelligent Q\&A systems may be inaccurate. To address these problems, this study developed an intelligent Q\&A system based on LangChain framework and LLMs. The system creates a web interface using Streamlit, integrates with LLMs, and enables generative Q\&A based on the content of PDF documents. In addition, the study further explores the application of the system in classroom teaching, aiming to change teachers' mindset, promote teachers' teaching innovation, enhance students' personalized learning experience, alleviate the problem of digital literacy deficit, and promote equity and change in education. This study not only provides a new teaching aid for educators, but also provides a practical reference for the in-depth development of smart education.",Technological innovation;Education;Educational technology;Portable document format;Digital intelligence;Artificial intelligence;Information technology;Faces;component;LangChain;Digital literacy;LLMs;Q\&A system,"{'month': 'Sep.', 'issn': '2474-3828', 'doi': '10.1109/ITME63426.2024.00142', 'keywords': 'Technological innovation;Education;Educational technology;Portable document format;Digital intelligence;Artificial intelligence;Information technology;Faces;component;LangChain;Digital literacy;LLMs;Q\\&A system', 'abstract': ""As China emphasizes intelligent education, the application of Artificial Intelligence (AI) in this field is becoming increasingly widespread. Q\\&A systems, as one of the key technologies of AI, are gradually being integrated into classroom teaching. The application plays a significant role in improving teaching quality and promoting personalized learning. However, the application process faces several problems, such as insufficient digital literacy, the inapplicability of traditional assessment methods, and the backwardness of educational concepts and methods. Moreover, in the era of information overload, the information provided by intelligent Q\\&A systems may be inaccurate. To address these problems, this study developed an intelligent Q\\&A system based on LangChain framework and LLMs. The system creates a web interface using Streamlit, integrates with LLMs, and enables generative Q\\&A based on the content of PDF documents. In addition, the study further explores the application of the system in classroom teaching, aiming to change teachers' mindset, promote teachers' teaching innovation, enhance students' personalized learning experience, alleviate the problem of digital literacy deficit, and promote equity and change in education. This study not only provides a new teaching aid for educators, but also provides a practical reference for the in-depth development of smart education."", 'pages': '698-702', 'number': '', 'volume': '', 'year': '2024', 'title': 'Development and Classroom Application Research of Q\\&A System Based on LangChain + LLMs', 'booktitle': '2024 14th International Conference on Information Technology in Medicine and Education (ITME)', 'author': 'Zhang, Bingge and Shao, Xiuting and Song, Ziwen', 'ENTRYTYPE': 'inproceedings', 'ID': '10935540'}"
11086491,Sel4FT: Annotation Selection for Pretraining-Finetuning With Distribution Shift,"Lu, Han and Xie, Yichen and Ding, Mingyu and Zhan, Wei and Yang, Xiaokang and Tomizuka, Masayoshi and Yan, Junchi",Lu,10.1109/TPAMI.2025.3591018,2025,IEEE Transactions on Pattern Analysis and Machine Intelligence,"The pretraining-finetuning paradigm has become dominant in computer vision, yet strategically exploiting limited annotation budgets during finetuning remains unexplored. We introduce active finetuning—a novel task for selecting the most informative samples to annotate within this paradigm. We propose Sel4FT, a unified annotation selection framework that optimizes a parametric model in continuous feature space to identify a subset preserving the entire pool's distribution while maintaining diversity. To address distribution shifts from data augmentation, we develop Sel4FT++ with augmentation-aware selection mechanisms. We theoretically prove our approach minimizes the Earth Mover's Distance between selected subset and full data pool. Our framework eliminates iterative retraining and annotation process during selection, providing an efficient solution for real-world deployment. Extensive experiments on image classification, long-tailed recognition, and semantic segmentation demonstrate state-of-the-art performance with over 100× speedup compared to existing methods. Code is released at https://github.com/yichen928/ActiveFT.",Training;Annotations;Data augmentation;Active learning;Optimization;Data models;Training data;Artificial intelligence;Parametric statistics;Heavily-tailed distribution;Active learning;continuous space optimization;pretraining-finetuning,"{'month': '', 'issn': '1939-3539', 'doi': '10.1109/TPAMI.2025.3591018', 'keywords': 'Training;Annotations;Data augmentation;Active learning;Optimization;Data models;Training data;Artificial intelligence;Parametric statistics;Heavily-tailed distribution;Active learning;continuous space optimization;pretraining-finetuning', 'abstract': ""The pretraining-finetuning paradigm has become dominant in computer vision, yet strategically exploiting limited annotation budgets during finetuning remains unexplored. We introduce active finetuning—a novel task for selecting the most informative samples to annotate within this paradigm. We propose Sel4FT, a unified annotation selection framework that optimizes a parametric model in continuous feature space to identify a subset preserving the entire pool's distribution while maintaining diversity. To address distribution shifts from data augmentation, we develop Sel4FT++ with augmentation-aware selection mechanisms. We theoretically prove our approach minimizes the Earth Mover's Distance between selected subset and full data pool. Our framework eliminates iterative retraining and annotation process during selection, providing an efficient solution for real-world deployment. Extensive experiments on image classification, long-tailed recognition, and semantic segmentation demonstrate state-of-the-art performance with over 100× speedup compared to existing methods. Code is released at https://github.com/yichen928/ActiveFT."", 'pages': '1-16', 'number': '', 'volume': '', 'year': '2025', 'title': 'Sel4FT: Annotation Selection for Pretraining-Finetuning With Distribution Shift', 'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence', 'author': 'Lu, Han and Xie, Yichen and Ding, Mingyu and Zhan, Wei and Yang, Xiaokang and Tomizuka, Masayoshi and Yan, Junchi', 'ENTRYTYPE': 'article', 'ID': '11086491'}"
10912343,LLMs as Debate Partners: Utilizing Genetic Algorithms and Adversarial Search for Adaptive Arguments,"Aryan, Prakash",Aryan,10.1109/ICEI64305.2024.10912343,2024,2024 IEEE Conference on Engineering Informatics (ICEI),"This paper introduces DebateBrawl, an innovative AI-powered debate platform that integrates Large Language Models (LLMs), Genetic Algorithms (GA), and Adversarial Search (AS) to create an adaptive and engaging debating experience. DebateBrawl addresses the limitations of traditional LLMs in strategic planning by incorporating evolutionary optimization and game-theoretic techniques. The system demonstrates remarkable performance in generating coherent, contextually relevant arguments while adapting its strategy in real-time. Experimental results involving 23 debates show balanced outcomes between AI and human participants, with the AI system achieving an average score of ${2. 7 2}$ compared to the human average of ${2. 6 7}$ out of 10. User feedback indicates significant improvements in debating skills and a highly satisfactory learning experience, with 85\% of users reporting improved debating abilities and ${7 8 \\%}$ finding the AI opponent appropriately challenging. The system’s ability to maintain high factual accuracy (92\% compared to ${7 8 \\%}$ in human-only debates) while generating diverse arguments addresses critical concerns in AI-assisted discourse. DebateBrawl not only serves as an effective educational tool but also contributes to the broader goal of improving public discourse through AI-assisted argumentation. The paper discusses the ethical implications of AI in persuasive contexts and outlines the measures implemented to ensure responsible development and deployment of the system, including robust fact-checking mechanisms and transparency in decision-making processes.",Training;Ethics;Visualization;Large language models;Decision making;Strategic planning;Problem-solving;Artificial intelligence;Research and development;Genetic algorithms;Machine Learning;Deep Learning;Generative AI;Large Language Models;Genetic Algorithms;Adversarial Search,"{'month': 'Nov', 'issn': '', 'doi': '10.1109/ICEI64305.2024.10912343', 'keywords': 'Training;Ethics;Visualization;Large language models;Decision making;Strategic planning;Problem-solving;Artificial intelligence;Research and development;Genetic algorithms;Machine Learning;Deep Learning;Generative AI;Large Language Models;Genetic Algorithms;Adversarial Search', 'abstract': 'This paper introduces DebateBrawl, an innovative AI-powered debate platform that integrates Large Language Models (LLMs), Genetic Algorithms (GA), and Adversarial Search (AS) to create an adaptive and engaging debating experience. DebateBrawl addresses the limitations of traditional LLMs in strategic planning by incorporating evolutionary optimization and game-theoretic techniques. The system demonstrates remarkable performance in generating coherent, contextually relevant arguments while adapting its strategy in real-time. Experimental results involving 23 debates show balanced outcomes between AI and human participants, with the AI system achieving an average score of ${2. 7 2}$ compared to the human average of ${2. 6 7}$ out of 10. User feedback indicates significant improvements in debating skills and a highly satisfactory learning experience, with 85\\% of users reporting improved debating abilities and ${7 8 \\\\%}$ finding the AI opponent appropriately challenging. The system’s ability to maintain high factual accuracy (92\\% compared to ${7 8 \\\\%}$ in human-only debates) while generating diverse arguments addresses critical concerns in AI-assisted discourse. DebateBrawl not only serves as an effective educational tool but also contributes to the broader goal of improving public discourse through AI-assisted argumentation. The paper discusses the ethical implications of AI in persuasive contexts and outlines the measures implemented to ensure responsible development and deployment of the system, including robust fact-checking mechanisms and transparency in decision-making processes.', 'pages': '1-11', 'number': '', 'volume': '', 'year': '2024', 'title': 'LLMs as Debate Partners: Utilizing Genetic Algorithms and Adversarial Search for Adaptive Arguments', 'booktitle': '2024 IEEE Conference on Engineering Informatics (ICEI)', 'author': 'Aryan, Prakash', 'ENTRYTYPE': 'inproceedings', 'ID': '10912343'}"
10193894,Artificial Intelligence in Data Analysis for Open-Source Investigations,"Rădoi, Teodor-Cristian",Rădoi,10.1109/ECAI58194.2023.10193894,2023,"2023 15th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)","The developed OSINT platform organizes data hierarchically and integrates a GPT model for faster, easier large data processing. Users can interact with the GPT model through natural language communication with a virtual agent for data processing commands. Open source investigations face challenges like vast data volumes and incorrect information. To address these issues, a real-time data processing tool is needed. Our OSINT platform integrates a GPT model trained to learn from data, enhancing the efficiency of open source investigations. We assessed various natural language processing models, focusing on the benefits of pretraining, fine-tuning., and generative models in open source investigations. GPT models excel due to pretraining on extensive text data, allowing fine-tuning for specific tasks and domains., giving investigators a robust tool for text analysis. The generative nature of GPT models benefits OSINT investigations by producing human-like text for extracting insights and identifying patterns. Fine-tuning enables customization to specific domains or topics., increasing accuracy and reliability while reducing time and effort in data analysis. In conclusion., our OSINT platform presents an innovative solution for open source investigations by incorporating a GPT model for efficient information processing. The Davinci model by OpenAI outperforms other evaluated models., enhancing investigation efficiency and maintaining grammatical accuracy. This work highlights the significance of natural language processing models in open source investigations and paves the way for future research.",Analytical models;Data analysis;Data models;Natural language processing;Real-time systems;Reliability;Data mining;GPT;BERT;Artificial Intelligence;Open Source Intelligence;Information,"{'month': 'June', 'issn': '', 'doi': '10.1109/ECAI58194.2023.10193894', 'keywords': 'Analytical models;Data analysis;Data models;Natural language processing;Real-time systems;Reliability;Data mining;GPT;BERT;Artificial Intelligence;Open Source Intelligence;Information', 'abstract': 'The developed OSINT platform organizes data hierarchically and integrates a GPT model for faster, easier large data processing. Users can interact with the GPT model through natural language communication with a virtual agent for data processing commands. Open source investigations face challenges like vast data volumes and incorrect information. To address these issues, a real-time data processing tool is needed. Our OSINT platform integrates a GPT model trained to learn from data, enhancing the efficiency of open source investigations. We assessed various natural language processing models, focusing on the benefits of pretraining, fine-tuning., and generative models in open source investigations. GPT models excel due to pretraining on extensive text data, allowing fine-tuning for specific tasks and domains., giving investigators a robust tool for text analysis. The generative nature of GPT models benefits OSINT investigations by producing human-like text for extracting insights and identifying patterns. Fine-tuning enables customization to specific domains or topics., increasing accuracy and reliability while reducing time and effort in data analysis. In conclusion., our OSINT platform presents an innovative solution for open source investigations by incorporating a GPT model for efficient information processing. The Davinci model by OpenAI outperforms other evaluated models., enhancing investigation efficiency and maintaining grammatical accuracy. This work highlights the significance of natural language processing models in open source investigations and paves the way for future research.', 'pages': '1-6', 'number': '', 'volume': '', 'year': '2023', 'title': 'Artificial Intelligence in Data Analysis for Open-Source Investigations', 'booktitle': '2023 15th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)', 'author': 'Rădoi, Teodor-Cristian', 'ENTRYTYPE': 'inproceedings', 'ID': '10193894'}"
11077880,3 Understanding Brain Connectivity: From Synapses to Networks,,,,2025,Brain Networks in Neuroscience: Personalization Unveiled Via Artificial Intelligence,"This book is an in-depth exploration of brain networks, providing a comprehensive understanding of their structures, functions, and implications for personalization through artificial intelligence. Readers will gain insights into the intricate workings of the brain, making this book an indispensable resource for those seeking a thorough grasp of neuroscience concepts. It offers the seamless integration of neuroscience principles with artificial intelligence applications. The book bridges these two domains, elucidating how advancements in AI draw inspiration from the complexities of the human brain. This interdisciplinary approach sets the book apart, offering readers a holistic view of cutting-edge technologies. Readers can expect practical applications and real-world case studies that illustrate the tangible benefits of the concepts discussed. From personalized healthcare solutions to adaptive learning systems, the book goes beyond theory, empowering readers to apply knowledge in diverse domains. This practical emphasis enhances the book\&\#x2019;s relevance for professionals and researchers alike. The inclusion of online enhancements, such as interactive visualizations, downloadable supplementary materials, and engaging video content, transforms the reading experience into an interactive learning journey. This added value distinguishes the book by providing readers with hands-on tools to deepen their understanding and apply newfound knowledge. This book doesn\&\#x2019;t just dwell on current technologies; it takes readers into the future by exploring emerging trends at the intersection of neuroscience and artificial intelligence. By delving into potential breakthroughs and innovations, the book equips readers with insights that are forward-thinking and relevant in an ever-evolving technological landscape.",,"{'url': 'https://ieeexplore-ieee-org.crai.referencistas.com/document/11077880', 'isbn': '9788770047357', 'publisher': 'River Publishers', 'issn': '', 'doi': '', 'keywords': '', 'abstract': 'This book is an in-depth exploration of brain networks, providing a comprehensive understanding of their structures, functions, and implications for personalization through artificial intelligence. Readers will gain insights into the intricate workings of the brain, making this book an indispensable resource for those seeking a thorough grasp of neuroscience concepts. It offers the seamless integration of neuroscience principles with artificial intelligence applications. The book bridges these two domains, elucidating how advancements in AI draw inspiration from the complexities of the human brain. This interdisciplinary approach sets the book apart, offering readers a holistic view of cutting-edge technologies. Readers can expect practical applications and real-world case studies that illustrate the tangible benefits of the concepts discussed. From personalized healthcare solutions to adaptive learning systems, the book goes beyond theory, empowering readers to apply knowledge in diverse domains. This practical emphasis enhances the book\\&\\#x2019;s relevance for professionals and researchers alike. The inclusion of online enhancements, such as interactive visualizations, downloadable supplementary materials, and engaging video content, transforms the reading experience into an interactive learning journey. This added value distinguishes the book by providing readers with hands-on tools to deepen their understanding and apply newfound knowledge. This book doesn\\&\\#x2019;t just dwell on current technologies; it takes readers into the future by exploring emerging trends at the intersection of neuroscience and artificial intelligence. By delving into potential breakthroughs and innovations, the book equips readers with insights that are forward-thinking and relevant in an ever-evolving technological landscape.', 'pages': '41-66', 'number': '', 'volume': '', 'year': '2025', 'title': '3 Understanding Brain Connectivity: From Synapses to Networks', 'booktitle': 'Brain Networks in Neuroscience: Personalization Unveiled Via Artificial Intelligence', 'ENTRYTYPE': 'inbook', 'ID': '11077880'}"
10902408,MIN: Moiré Inpainting Network With Position Adaptive Mask for 3-D Height Reconstruction,"Kim, Tae-Jung and Ha, Min-Ho and Arshad, Saba and Park, Tae-Hyoung",Kim,10.1109/ACCESS.2025.3545748,2025,IEEE Access,"In the AI-driven computer vision industry, height measurement of Printed Circuit Board images typically relies on laser or Moiré methods. In this paper, we focus on the Moiré method, known for its high accuracy and fast measurement speed. However, when using Moiré method, shadows and light reflections are generated on Printed Circuit Board surface that cause significant errors in height measurement. To address this problem, we propose a Moiré Inpainting Network, which integrates the Moiré method with an image inpainting model architecture. Our approach leverages a Generative Adversarial Network to accurately identify and reconstruct shadow and reflection regions. The network takes 2D Printed Circuit Board Moiré images as input and outputs heights of Printed Circuit Board. We evaluate performance using Height Reconstruction Rate, Shadow Reconstruction Rate, and Reflection Reconstruction Rate, metrics we define in this paper. Comparative experiments show that our method outperforms state-of-the-art inpainting models for Moiré images, proving its effectiveness in computer vision applications. Moreover, we achieve a reasonable inference time, enabling real-time deployment in Printed Circuit Board manufacturing.",Image reconstruction;Printed circuits;Reflection;Generative adversarial networks;Context modeling;Height measurement;Integrated circuit modeling;Computational modeling;Solid modeling;Adaptive systems;Artificial intelligence;computer vision;generative adversarial networks;image inpainting;anomaly detection;Moiré;printed circuit board,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2025.3545748', 'keywords': 'Image reconstruction;Printed circuits;Reflection;Generative adversarial networks;Context modeling;Height measurement;Integrated circuit modeling;Computational modeling;Solid modeling;Adaptive systems;Artificial intelligence;computer vision;generative adversarial networks;image inpainting;anomaly detection;Moiré;printed circuit board', 'abstract': 'In the AI-driven computer vision industry, height measurement of Printed Circuit Board images typically relies on laser or Moiré methods. In this paper, we focus on the Moiré method, known for its high accuracy and fast measurement speed. However, when using Moiré method, shadows and light reflections are generated on Printed Circuit Board surface that cause significant errors in height measurement. To address this problem, we propose a Moiré Inpainting Network, which integrates the Moiré method with an image inpainting model architecture. Our approach leverages a Generative Adversarial Network to accurately identify and reconstruct shadow and reflection regions. The network takes 2D Printed Circuit Board Moiré images as input and outputs heights of Printed Circuit Board. We evaluate performance using Height Reconstruction Rate, Shadow Reconstruction Rate, and Reflection Reconstruction Rate, metrics we define in this paper. Comparative experiments show that our method outperforms state-of-the-art inpainting models for Moiré images, proving its effectiveness in computer vision applications. Moreover, we achieve a reasonable inference time, enabling real-time deployment in Printed Circuit Board manufacturing.', 'pages': '37501-37513', 'number': '', 'volume': '13', 'year': '2025', 'title': 'MIN: Moiré Inpainting Network With Position Adaptive Mask for 3-D Height Reconstruction', 'journal': 'IEEE Access', 'author': 'Kim, Tae-Jung and Ha, Min-Ho and Arshad, Saba and Park, Tae-Hyoung', 'ENTRYTYPE': 'article', 'ID': '10902408'}"
11167566,A Novel CSRU-GAN Framework for Polyp Segmentation,"Navaneeth, P G and Kar, Mithun Kumar",Navaneeth,10.1109/CONIT65521.2025.11167566,2025,2025 5th International Conference on Intelligent Technologies (CONIT),"Polyp segmentation plays a crucial role in the early detection and diagnosis of fatal diseases such as colorectal cancer during colonoscopy procedures. Timely diagnosis of such catastrophic diseases could assist medical experts and potentially avert impending crisis Over the years, image segmentation in biomedical applications has been undergoing a massive surge especially with the introduction of the U-Net architecture in 2015 and the FCN (Fully Convolutional Network). This architecture went on to become a traditional thresholding and edge-detection technique. Several years hence, more sophisticated deep learning models of the U-Net such as Attention U-Net and ResU-Net were introduced. These architectures have significantly enhanced segmentation accuracy by capturing complex spatial hierarchies. However, conventional encoder-decoder frameworks still face limitations in capturing long-range dependencies and generating fine-grained boundaries in challenging scenarios. In order to tackle the aforementioned issues, several GAN (Generative Adversarial Network) models were introduced to enhance the quality of segmentation masks. These architectures massively outperformed the conventional architectures in terms of dice and IOU scores which are touted to be the primordial metrics designed to evaluate the authenticity of the masks generated by the models. This paper attempts to propose a new Conditional Generative Adversarial Network model called CSRU-GAN (Conditional SWIN ResU-Net Generative Adversarial Network) which intends to use the ability of the residual connections inspired from the ResNet architecture to clip the vanishing gradient problem, the ability of SWIN transformers to capture hierarchical features by making use of shifted windows and attention mechanism. Lastly, it makes use of the adversarial training of the CGAN (Conditional Generative Adversarial Network) while also taking into account the ground truth information. The proposed model when tested on the CVC database performed admirably in terms of the Dice and IOU (Intersection Over Union) scores which were found out to be $\mathbf{9 1. 2 1 \\%}$ and $\mathbf{8 3. 2 9 \\%}$ respectively.",Training;Deep learning;Representation learning;Image segmentation;Accuracy;Colonoscopy;Generative adversarial networks;Transformers;Generators;Surges;U-Net;ResNet;ResU-Net;SWIN transformers;GAN;CVC dataset;dice score;IOU,"{'month': 'June', 'issn': '', 'doi': '10.1109/CONIT65521.2025.11167566', 'keywords': 'Training;Deep learning;Representation learning;Image segmentation;Accuracy;Colonoscopy;Generative adversarial networks;Transformers;Generators;Surges;U-Net;ResNet;ResU-Net;SWIN transformers;GAN;CVC dataset;dice score;IOU', 'abstract': 'Polyp segmentation plays a crucial role in the early detection and diagnosis of fatal diseases such as colorectal cancer during colonoscopy procedures. Timely diagnosis of such catastrophic diseases could assist medical experts and potentially avert impending crisis Over the years, image segmentation in biomedical applications has been undergoing a massive surge especially with the introduction of the U-Net architecture in 2015 and the FCN (Fully Convolutional Network). This architecture went on to become a traditional thresholding and edge-detection technique. Several years hence, more sophisticated deep learning models of the U-Net such as Attention U-Net and ResU-Net were introduced. These architectures have significantly enhanced segmentation accuracy by capturing complex spatial hierarchies. However, conventional encoder-decoder frameworks still face limitations in capturing long-range dependencies and generating fine-grained boundaries in challenging scenarios. In order to tackle the aforementioned issues, several GAN (Generative Adversarial Network) models were introduced to enhance the quality of segmentation masks. These architectures massively outperformed the conventional architectures in terms of dice and IOU scores which are touted to be the primordial metrics designed to evaluate the authenticity of the masks generated by the models. This paper attempts to propose a new Conditional Generative Adversarial Network model called CSRU-GAN (Conditional SWIN ResU-Net Generative Adversarial Network) which intends to use the ability of the residual connections inspired from the ResNet architecture to clip the vanishing gradient problem, the ability of SWIN transformers to capture hierarchical features by making use of shifted windows and attention mechanism. Lastly, it makes use of the adversarial training of the CGAN (Conditional Generative Adversarial Network) while also taking into account the ground truth information. The proposed model when tested on the CVC database performed admirably in terms of the Dice and IOU (Intersection Over Union) scores which were found out to be $\\mathbf{9 1. 2 1 \\\\%}$ and $\\mathbf{8 3. 2 9 \\\\%}$ respectively.', 'pages': '1-7', 'number': '', 'volume': '', 'year': '2025', 'title': 'A Novel CSRU-GAN Framework for Polyp Segmentation', 'booktitle': '2025 5th International Conference on Intelligent Technologies (CONIT)', 'author': 'Navaneeth, P G and Kar, Mithun Kumar', 'ENTRYTYPE': 'inproceedings', 'ID': '11167566'}"
10890943,VALL-T: Decoder-Only Generative Transducer for Robust and Decoding-Controllable Text-to-Speech,"Du, Chenpeng and Guo, Yiwei and Wang, Hankun and Yang, Yifan and Niu, Zhikang and Wang, Shuai and Zhang, Hui and Chen, Xie and Yu, Kai",Du,10.1109/ICASSP49660.2025.10890943,2025,"ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","Recent TTS models with decoder-only Transformer architecture, such as SPEAR-TTS and VALL-E, achieve impressive naturalness and demonstrate the ability for zero-shot adaptation given a speech prompt. However, such decoder-only TTS models lack monotonic alignment constraints, sometimes leading to hallucination issues such as mispronunciation, word skipping and repeating. To address this limitation, we propose VALL-T, a generative Transducer model that introduces shifting relative position embeddings for input phoneme sequence, explicitly indicating the monotonic generation process while maintaining the architecture of decoder-only Transformer. Consequently, VALL-T retains the capability of prompt-based zero-shot adaptation and demonstrates better robustness against hallucinations with a relative reduction of 28.3\% in the word error rate. The audio samples are available at https://cpdu.github.io/vallt.",Training;Adaptation models;Transducers;Error analysis;Posterior probability;Signal processing;Transformers;Robustness;Text to speech;Speech processing;transducer;text-to-speech;decoder-only;hallucination,"{'month': 'April', 'issn': '2379-190X', 'doi': '10.1109/ICASSP49660.2025.10890943', 'keywords': 'Training;Adaptation models;Transducers;Error analysis;Posterior probability;Signal processing;Transformers;Robustness;Text to speech;Speech processing;transducer;text-to-speech;decoder-only;hallucination', 'abstract': 'Recent TTS models with decoder-only Transformer architecture, such as SPEAR-TTS and VALL-E, achieve impressive naturalness and demonstrate the ability for zero-shot adaptation given a speech prompt. However, such decoder-only TTS models lack monotonic alignment constraints, sometimes leading to hallucination issues such as mispronunciation, word skipping and repeating. To address this limitation, we propose VALL-T, a generative Transducer model that introduces shifting relative position embeddings for input phoneme sequence, explicitly indicating the monotonic generation process while maintaining the architecture of decoder-only Transformer. Consequently, VALL-T retains the capability of prompt-based zero-shot adaptation and demonstrates better robustness against hallucinations with a relative reduction of 28.3\\% in the word error rate. The audio samples are available at https://cpdu.github.io/vallt.', 'pages': '1-5', 'number': '', 'volume': '', 'year': '2025', 'title': 'VALL-T: Decoder-Only Generative Transducer for Robust and Decoding-Controllable Text-to-Speech', 'booktitle': 'ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)', 'author': 'Du, Chenpeng and Guo, Yiwei and Wang, Hankun and Yang, Yifan and Niu, Zhikang and Wang, Shuai and Zhang, Hui and Chen, Xie and Yu, Kai', 'ENTRYTYPE': 'inproceedings', 'ID': '10890943'}"
10739326,Automated Generation and Evaluation of MultipleChoice Quizzes using Langchain and Gemini LLM,"Pawar, Pratik and Dube, Raghav and Joshi, Amogh and Gulhane, Zinee and Patil, Ratna",Pawar,10.1109/ICEECT61758.2024.10739326,2024,2024 International Conference on Electrical Electronics and Computing Technologies (ICEECT),"The research study investigates the use of cutting-edge technologies like Langchain and Gemini AI for the automated creation and assessment of multiple-choice questions (MCQs). Although producing multiple-choice questions (MCQs) has traditionally been done by hand and required a lot of work, advances in artificial intelligence (AI) and natural language processing (NLP) have opened up new possibilities. The creation and implementation of an MCQ generator—which uses Gemini AI to generate MCQs and Langchain for rapid engineering are covered in this paper. Customizing prompts for prompt engineering, chaining prompts with the Gemini AI model, and utilizing OpenAI's GPT-3.5 and multiple Language Learning Models (LLMs) to assess the created MCQs are the steps in the process. The study intends to assess these models' efficiency in handling intricate queries, producing appropriate answers, and examining the caliber of the MCQs that are produced.",Measurement;Computers;Analytical models;Computational modeling;Educational technology;Natural language processing;Generators;Prompt engineering;mcq generator;large language models;natural language processing;google gemini pro;langchain;generative ai,"{'month': 'Aug', 'issn': '', 'doi': '10.1109/ICEECT61758.2024.10739326', 'keywords': 'Measurement;Computers;Analytical models;Computational modeling;Educational technology;Natural language processing;Generators;Prompt engineering;mcq generator;large language models;natural language processing;google gemini pro;langchain;generative ai', 'abstract': ""The research study investigates the use of cutting-edge technologies like Langchain and Gemini AI for the automated creation and assessment of multiple-choice questions (MCQs). Although producing multiple-choice questions (MCQs) has traditionally been done by hand and required a lot of work, advances in artificial intelligence (AI) and natural language processing (NLP) have opened up new possibilities. The creation and implementation of an MCQ generator—which uses Gemini AI to generate MCQs and Langchain for rapid engineering are covered in this paper. Customizing prompts for prompt engineering, chaining prompts with the Gemini AI model, and utilizing OpenAI's GPT-3.5 and multiple Language Learning Models (LLMs) to assess the created MCQs are the steps in the process. The study intends to assess these models' efficiency in handling intricate queries, producing appropriate answers, and examining the caliber of the MCQs that are produced."", 'pages': '1-7', 'number': '', 'volume': '1', 'year': '2024', 'title': 'Automated Generation and Evaluation of MultipleChoice Quizzes using Langchain and Gemini LLM', 'booktitle': '2024 International Conference on Electrical Electronics and Computing Technologies (ICEECT)', 'author': 'Pawar, Pratik and Dube, Raghav and Joshi, Amogh and Gulhane, Zinee and Patil, Ratna', 'ENTRYTYPE': 'inproceedings', 'ID': '10739326'}"
10780708,Hybrid Dual-Channel Input Image Tampering Detection for Scientific Papers,"Shi, Jiayi and E, Haihong and Liu, Jianhua and Hu, Tianyi and Qiao, Xiaodong and Ding, Junpeng and Huang, Jiayu",Shi,10.1109/BESC64747.2024.10780708,2024,2024 11th International Conference on Behavioural and Social Computing (BESC),"With the development of science and technology, the application of digital image processing and artificial intelligence technology is becoming more and more widespread, which makes it easier and more difficult to detect forged academic images, and academic images have become a high incidence of scientific and technological journal paper forgery. To ensure the authenticity of academic images and scientific reproducibility of experiments, we developed a network called MultiScopeNet. It combines RGB and DCT streams, performs multi-resolution fusion at each layer, and can comprehensively analyze spatial and frequency domain features of images. We also created a new dataset based on an existing tamper detection dataset by applying an image complementation model to train and validate our model against academic misconduct of tampering using generatively forged images. MultiScopeNet significantly outperforms existing state-of-the-art models in dealing with image tampering at different resolutions and sizes.",Training;Visualization;Frequency-domain analysis;Transform coding;Transforms;Reproducibility of results;Discrete cosine transforms;Detection algorithms;Spatial resolution;Streams;image tampering;generative image tampering dataset;image detection for scientific papers,"{'month': 'Aug', 'issn': '2689-8284', 'doi': '10.1109/BESC64747.2024.10780708', 'keywords': 'Training;Visualization;Frequency-domain analysis;Transform coding;Transforms;Reproducibility of results;Discrete cosine transforms;Detection algorithms;Spatial resolution;Streams;image tampering;generative image tampering dataset;image detection for scientific papers', 'abstract': 'With the development of science and technology, the application of digital image processing and artificial intelligence technology is becoming more and more widespread, which makes it easier and more difficult to detect forged academic images, and academic images have become a high incidence of scientific and technological journal paper forgery. To ensure the authenticity of academic images and scientific reproducibility of experiments, we developed a network called MultiScopeNet. It combines RGB and DCT streams, performs multi-resolution fusion at each layer, and can comprehensively analyze spatial and frequency domain features of images. We also created a new dataset based on an existing tamper detection dataset by applying an image complementation model to train and validate our model against academic misconduct of tampering using generatively forged images. MultiScopeNet significantly outperforms existing state-of-the-art models in dealing with image tampering at different resolutions and sizes.', 'pages': '1-6', 'number': '', 'volume': '', 'year': '2024', 'title': 'Hybrid Dual-Channel Input Image Tampering Detection for Scientific Papers', 'booktitle': '2024 11th International Conference on Behavioural and Social Computing (BESC)', 'author': 'Shi, Jiayi and E, Haihong and Liu, Jianhua and Hu, Tianyi and Qiao, Xiaodong and Ding, Junpeng and Huang, Jiayu', 'ENTRYTYPE': 'inproceedings', 'ID': '10780708'}"
11150277,A Generative Virtual Leader Network Framework for Enhanced Synchronization and Disturbance Resistance in Multi-Agent Systems,"Zhang, Meng and Chen, Yihang and Wang, Xue and Cui, Tong and Ren, Yan and Chen, Xinyu",Zhang,10.1109/YAC66630.2025.11150277,2025,2025 40th Youth Academic Annual Conference of Chinese Association of Automation (YAC),"This paper introduces a novel control method for multi-agent systems by integrating a Generative Virtual Leader Network (GVLN) to improve synchronization speed and disturbance resistance. Unlike traditional single-layer complex network-based methods, the proposed approach leverages a multi-layer synchronization framework, where the GVLN provides auxiliary control to the follower network. Simulation results demonstrate that this method significantly reduces synchronization time and system oscillations while enhancing robustness against disturbances. Experiments on chaotic systems confirm the superiority of the proposed approach in achieving faster state coordination and improved stability. These findings offer new insights into complex network synchronization and practical multi-agent system control.",Resistance;Simulation;Complex networks;Nonhomogeneous media;Control systems;Robustness;Stability analysis;Synchronization;Oscillators;Multi-agent systems;Complex Network;Multi-Agent;State Following;Virtual Leader Network,"{'month': 'May', 'issn': '2837-8601', 'doi': '10.1109/YAC66630.2025.11150277', 'keywords': 'Resistance;Simulation;Complex networks;Nonhomogeneous media;Control systems;Robustness;Stability analysis;Synchronization;Oscillators;Multi-agent systems;Complex Network;Multi-Agent;State Following;Virtual Leader Network', 'abstract': 'This paper introduces a novel control method for multi-agent systems by integrating a Generative Virtual Leader Network (GVLN) to improve synchronization speed and disturbance resistance. Unlike traditional single-layer complex network-based methods, the proposed approach leverages a multi-layer synchronization framework, where the GVLN provides auxiliary control to the follower network. Simulation results demonstrate that this method significantly reduces synchronization time and system oscillations while enhancing robustness against disturbances. Experiments on chaotic systems confirm the superiority of the proposed approach in achieving faster state coordination and improved stability. These findings offer new insights into complex network synchronization and practical multi-agent system control.', 'pages': '3087-3092', 'number': '', 'volume': '', 'year': '2025', 'title': 'A Generative Virtual Leader Network Framework for Enhanced Synchronization and Disturbance Resistance in Multi-Agent Systems', 'booktitle': '2025 40th Youth Academic Annual Conference of Chinese Association of Automation (YAC)', 'author': 'Zhang, Meng and Chen, Yihang and Wang, Xue and Cui, Tong and Ren, Yan and Chen, Xinyu', 'ENTRYTYPE': 'inproceedings', 'ID': '11150277'}"
9403820,Architectural Facade Recognition and Generation through Generative Adversarial Networks,"Yu, Qiu and Malaeb, Jamal and Ma, Wenjun",Yu,10.1109/ICBASE51474.2020.00072,2020,2020 International Conference on Big Data \& Artificial Intelligence \& Software Engineering (ICBASE),"With the development of artificial intelligence technology, the ideas of machine learning have been introduced into the field of design in recent years. The research methods of “AI + Architecture” have brought new ideas for solving traditional problems. Generative Adversarial Network (GAN) is a machine learning model for image generation. Pix2pix is an improved version of GAN, which is specially designed to learn and generate pairs of image data with similar characteristics. In this study, Pix2pix is applied to the recognition and generation of building facade. The purpose is to explore the feasibility of using image generation technology to achieve rapid recognition and generation of building facade based on pix2pix. This paper also discusses the application scenarios of this technology. The existing building façade datasets and the self-made Chinese traditional building datasets are used to test and verify that pix2pix under different types of datasets can nicely identify and generate facade images. Then we summarize a set of working methods based on GAN to realize the overall or local reconstruction design of the facade, so as to provide new ideas for the improvement of the efficiency of related industries and the expansion of teaching tools.",Industries;Image recognition;Image synthesis;Buildings;Education;Machine learning;Tools;facade;GAN;image recognition;image generation,"{'month': 'Oct', 'issn': '', 'doi': '10.1109/ICBASE51474.2020.00072', 'keywords': 'Industries;Image recognition;Image synthesis;Buildings;Education;Machine learning;Tools;facade;GAN;image recognition;image generation', 'abstract': 'With the development of artificial intelligence technology, the ideas of machine learning have been introduced into the field of design in recent years. The research methods of “AI + Architecture” have brought new ideas for solving traditional problems. Generative Adversarial Network (GAN) is a machine learning model for image generation. Pix2pix is an improved version of GAN, which is specially designed to learn and generate pairs of image data with similar characteristics. In this study, Pix2pix is applied to the recognition and generation of building facade. The purpose is to explore the feasibility of using image generation technology to achieve rapid recognition and generation of building facade based on pix2pix. This paper also discusses the application scenarios of this technology. The existing building façade datasets and the self-made Chinese traditional building datasets are used to test and verify that pix2pix under different types of datasets can nicely identify and generate facade images. Then we summarize a set of working methods based on GAN to realize the overall or local reconstruction design of the facade, so as to provide new ideas for the improvement of the efficiency of related industries and the expansion of teaching tools.', 'pages': '310-316', 'number': '', 'volume': '', 'year': '2020', 'title': 'Architectural Facade Recognition and Generation through Generative Adversarial Networks', 'booktitle': '2020 International Conference on Big Data \\& Artificial Intelligence \\& Software Engineering (ICBASE)', 'author': 'Yu, Qiu and Malaeb, Jamal and Ma, Wenjun', 'ENTRYTYPE': 'inproceedings', 'ID': '9403820'}"
10163173,,"Sawarkar, Kunal",Sawarkar,,2022,Deep Learning with PyTorch Lightning: Swiftly build high-performance Artificial Intelligence (AI) models using Python,"Build, train, and deploy deep learning models quickly and accurately to improve your productivity using PyTorch Lightning WrapperKey FeaturesBecome well-versed with PyTorch Lightning and learn how to implement it in various applicationsSpeed up your research using PyTorch Lightning by creating new loss functions, and architecturesTrain and build new DL applications for images, audio, video, structured and unstructured dataBook DescriptionBuilding and implementing deep learning (DL) is becoming a key skill for those who want to be at the forefront of progress.But with so much information and complex study materials out there, getting started with DL can feel quite overwhelming. Written by an AI thought leader, Deep Learning with PyTorch Lightning helps researchers build their first DL models quickly and easily without getting stuck on the complexities. With its help, you’ll be able to maximize productivity for DL projects while ensuring full flexibility – from model formulation to implementation. Throughout this book, you’ll learn how to configure PyTorch Lightning on a cloud platform, understand the architectural components, and explore how they are configured to build various industry solutions. You’ll build a neural network architecture, deploy an application from scratch, and see how you can expand it based on your specific needs, beyond what the framework can provide. In the later chapters, you’ll also learn how to implement capabilities to build and train various models like Convolutional Neural Nets (CNN), Natural Language Processing (NLP), Time Series, Self-Supervised Learning, Semi-Supervised Learning, Generative Adversarial Network (GAN) using PyTorch Lightning. By the end of this book, you’ll be able to build and deploy DL models with confidence.What you will learnCustomize models that are built for different datasets, model architecturesUnderstand a variety of DL models from image recognition, NLP to time seriesCreate advanced DL models to write poems (Semi-Supervised) or create fake images (GAN)Learn to train on unlabelled images using Self-Supervised Contrastive LearningLearn to use pre-trained models using transfer learning to save computeMake use of out-of-the-box SOTA model architectures using Lightning FlashExplore techniques for model deployment \& scoring using ONNX formatRun and tune DL models in a multi-GPU environment using mixed-mode precisionsWho this book is forIf you’re a data scientist curious about deep learning but don't know where to start or feel intimidated by the complexities of large neural networks, then this book is for you. Expert data scientists making the transition from other DL frameworks to PyTorch will also find plenty of useful information in this book, as will researchers interested in using PyTorch Lightning as a reference guide. To get started, you’ll need a solid grasp on Python; the book will teach you the rest",,"{'url': 'https://ieeexplore-ieee-org.crai.referencistas.com/document/10163173', 'isbn': '9781800569270', 'publisher': 'Packt Publishing', 'issn': '', 'doi': '', 'keywords': '', 'abstract': ""Build, train, and deploy deep learning models quickly and accurately to improve your productivity using PyTorch Lightning WrapperKey FeaturesBecome well-versed with PyTorch Lightning and learn how to implement it in various applicationsSpeed up your research using PyTorch Lightning by creating new loss functions, and architecturesTrain and build new DL applications for images, audio, video, structured and unstructured dataBook DescriptionBuilding and implementing deep learning (DL) is becoming a key skill for those who want to be at the forefront of progress.But with so much information and complex study materials out there, getting started with DL can feel quite overwhelming. Written by an AI thought leader, Deep Learning with PyTorch Lightning helps researchers build their first DL models quickly and easily without getting stuck on the complexities. With its help, you’ll be able to maximize productivity for DL projects while ensuring full flexibility – from model formulation to implementation. Throughout this book, you’ll learn how to configure PyTorch Lightning on a cloud platform, understand the architectural components, and explore how they are configured to build various industry solutions. You’ll build a neural network architecture, deploy an application from scratch, and see how you can expand it based on your specific needs, beyond what the framework can provide. In the later chapters, you’ll also learn how to implement capabilities to build and train various models like Convolutional Neural Nets (CNN), Natural Language Processing (NLP), Time Series, Self-Supervised Learning, Semi-Supervised Learning, Generative Adversarial Network (GAN) using PyTorch Lightning. By the end of this book, you’ll be able to build and deploy DL models with confidence.What you will learnCustomize models that are built for different datasets, model architecturesUnderstand a variety of DL models from image recognition, NLP to time seriesCreate advanced DL models to write poems (Semi-Supervised) or create fake images (GAN)Learn to train on unlabelled images using Self-Supervised Contrastive LearningLearn to use pre-trained models using transfer learning to save computeMake use of out-of-the-box SOTA model architectures using Lightning FlashExplore techniques for model deployment \\& scoring using ONNX formatRun and tune DL models in a multi-GPU environment using mixed-mode precisionsWho this book is forIf you’re a data scientist curious about deep learning but don't know where to start or feel intimidated by the complexities of large neural networks, then this book is for you. Expert data scientists making the transition from other DL frameworks to PyTorch will also find plenty of useful information in this book, as will researchers interested in using PyTorch Lightning as a reference guide. To get started, you’ll need a solid grasp on Python; the book will teach you the rest"", 'pages': '', 'number': '', 'volume': '', 'year': '2022', 'booktitle': 'Deep Learning with PyTorch Lightning: Swiftly build high-performance Artificial Intelligence (AI) models using Python', 'author': 'Sawarkar, Kunal', 'ENTRYTYPE': 'book', 'ID': '10163173'}"
10276257,"New Deep Learning Models for Medical Imaging: Deep Belief Network, GAN, Autoencoder","Gupta, Richa and Kumar Shukla, Surendra and Tripathi, Vikas",Gupta,10.1109/ICOSEC58147.2023.10276257,2023,2023 4th International Conference on Smart Electronics and Communication (ICOSEC),"Deep learning (DL) is being utilized in different clinical imaging process and has accomplished promising results. Nonetheless, incorporating DL in clinical imaging also presents significant challenges. This research study presents the characteristics of clinical imaging that features both clinical requirements and specialized challenges and depicts how the patterns in DL are resolving these issues. This study presents a few contextual investigations that are commonly found, including advanced pathology and respiratory, chest, brain, and ophthalmology imaging. Rather than presenting a thorough writing review, this study highlights some notable exploration aspects associated with the contextual analysis. This study is concluded with a discussion and stating some possible future research directions.",Deep learning;Pathology;Image resolution;Writing;Generative adversarial networks;Brain modeling;Ophthalmology;Deep Learning;Medical images;Deep belief network (DBN);Generative Adversarial Network GAN;Autoencoder,"{'month': 'Sep.', 'issn': '', 'doi': '10.1109/ICOSEC58147.2023.10276257', 'keywords': 'Deep learning;Pathology;Image resolution;Writing;Generative adversarial networks;Brain modeling;Ophthalmology;Deep Learning;Medical images;Deep belief network (DBN);Generative Adversarial Network GAN;Autoencoder', 'abstract': 'Deep learning (DL) is being utilized in different clinical imaging process and has accomplished promising results. Nonetheless, incorporating DL in clinical imaging also presents significant challenges. This research study presents the characteristics of clinical imaging that features both clinical requirements and specialized challenges and depicts how the patterns in DL are resolving these issues. This study presents a few contextual investigations that are commonly found, including advanced pathology and respiratory, chest, brain, and ophthalmology imaging. Rather than presenting a thorough writing review, this study highlights some notable exploration aspects associated with the contextual analysis. This study is concluded with a discussion and stating some possible future research directions.', 'pages': '907-913', 'number': '', 'volume': '', 'year': '2023', 'title': 'New Deep Learning Models for Medical Imaging: Deep Belief Network, GAN, Autoencoder', 'booktitle': '2023 4th International Conference on Smart Electronics and Communication (ICOSEC)', 'author': 'Gupta, Richa and Kumar Shukla, Surendra and Tripathi, Vikas', 'ENTRYTYPE': 'inproceedings', 'ID': '10276257'}"
11136724,Exploring Ethical Awareness and Learning Impact: A Study on the Use of ChatGPT and Generative AI in Higher Education,"Jahani, Masoumeh and Baruah, Bidyut and Ward, Tony",Jahani,10.1109/EAEEIE65428.2025.11136724,2025,2025 34th Annual Conference of the European Association for Education in Electrical and Information Engineering (EAEEIE),"The rapid adoption of generative AI (GenAI) tools like ChatGPT is reshaping higher education by transforming how students access information, engage with content, and develop academic skills. These tools offer personalised feedback, encourage self-directed learning, and support knowledge construction. However, their increasing use also raises concerns about academic integrity, ethical awareness, and potential impacts on critical thinking.This study examines the use of GenAI tools among postgraduate students in the Engineering Management programme at the University of York. Employing a mixed-methods approach, data were collected via a survey of 42 students and semi-structured interviews with 15 academic staff. The research explores student motivations, ethical perceptions, and the influence of GenAI on academic development.Findings indicate that over 90\% of students use GenAI primarily for formative tasks such as research, idea generation, and writing support, driven by goals of efficiency, conceptual understanding, and overcoming language barriers. While many perceive GenAI as empowering independent learning, concerns about over-reliance and ethical implications persist. Institutional support and policy clarity were found lacking, highlighting the need for inclusive digital literacy training and clear guidelines.Academic staff stress the importance of aligning AI use with learning objectives and enhancing faculty training. To maximise benefits while preserving academic integrity, a balanced, human-centred approach to GenAI integration is essential, combining innovation with ethical responsibility.",Training;Surveys;Ethics;Technological innovation;Generative AI;Learning (artificial intelligence);Writing;Chatbots;Digital intelligence;Interviews;Artificial Intelligence;GenAI;AI-based learning;ChatGPT;Higher Education;Engineering Education;Ethical Awareness;AI in Assessment;Academic Integrity,"{'month': 'June', 'issn': '2472-7687', 'doi': '10.1109/EAEEIE65428.2025.11136724', 'keywords': 'Training;Surveys;Ethics;Technological innovation;Generative AI;Learning (artificial intelligence);Writing;Chatbots;Digital intelligence;Interviews;Artificial Intelligence;GenAI;AI-based learning;ChatGPT;Higher Education;Engineering Education;Ethical Awareness;AI in Assessment;Academic Integrity', 'abstract': 'The rapid adoption of generative AI (GenAI) tools like ChatGPT is reshaping higher education by transforming how students access information, engage with content, and develop academic skills. These tools offer personalised feedback, encourage self-directed learning, and support knowledge construction. However, their increasing use also raises concerns about academic integrity, ethical awareness, and potential impacts on critical thinking.This study examines the use of GenAI tools among postgraduate students in the Engineering Management programme at the University of York. Employing a mixed-methods approach, data were collected via a survey of 42 students and semi-structured interviews with 15 academic staff. The research explores student motivations, ethical perceptions, and the influence of GenAI on academic development.Findings indicate that over 90\\% of students use GenAI primarily for formative tasks such as research, idea generation, and writing support, driven by goals of efficiency, conceptual understanding, and overcoming language barriers. While many perceive GenAI as empowering independent learning, concerns about over-reliance and ethical implications persist. Institutional support and policy clarity were found lacking, highlighting the need for inclusive digital literacy training and clear guidelines.Academic staff stress the importance of aligning AI use with learning objectives and enhancing faculty training. To maximise benefits while preserving academic integrity, a balanced, human-centred approach to GenAI integration is essential, combining innovation with ethical responsibility.', 'pages': '1-5', 'number': '', 'volume': '', 'year': '2025', 'title': 'Exploring Ethical Awareness and Learning Impact: A Study on the Use of ChatGPT and Generative AI in Higher Education', 'booktitle': '2025 34th Annual Conference of the European Association for Education in Electrical and Information Engineering (EAEEIE)', 'author': 'Jahani, Masoumeh and Baruah, Bidyut and Ward, Tony', 'ENTRYTYPE': 'inproceedings', 'ID': '11136724'}"
10823299,A Comprehensive Survey on Phytopathogen Surveillance with Modern Artificial Intelligence Practices,"G, Kaleeswari and R, Sundarrajan",G,10.1109/ICICNIS64247.2024.10823299,2024,2024 International Conference on IoT Based Control Networks and Intelligent Systems (ICICNIS),"The most important aspect of modern agriculture is the detection of plant diseases with the goal of improving crop quality and output. This survey paper investigates modern approaches of deep learning algorithms, Explainable AI and Federated Learning to plant disease diagnosis. The background and broadening of plant disease detection are briefly reviewed at the outset of the study, with a focus on the importance of precise and effective identification techniques. The multivariate normal DL neural network (MNDLNN) classifier and Customized CNN are the most robust feature extraction and classification techniques among the examined technologies. Using Higher-Order Whitened Singular Value Decomposition (HOWSVD), complex data is processed in a way that highlights distinct patterns and features for easy data classification and also increases accuracy. The E-GreenNet generates disease classification features based on a MobileNetV3. The ResNet9 and Improved Vision Transformer models perform better when processing complicated visual data. Spectral Generative Adversarial Neural Network (DSGAN2) and Lite Multikernel Depthwise Convolutions architecture are suitable for real-time applications because they provide promising results with less computational overhead. These findings' implications point to a possible move toward disease detection systems that are easier to use and more effective, which would improve crop management and food security. The development of more affordable solutions for small scale farmers, the integration of Embedded systems for real-time monitoring, and the improvement of these models for increased precision are some future directions. This survey highlights the transformative potential of combining deep learning with other techniques like Federated Learning in advancing agricultural practices.",Surveys;Deep learning;Plant diseases;Visualization;Federated learning;Surveillance;Neural networks;Crops;Transformers;Real-time systems;Deep learning;Federated learning;Explainable AI;Real-time detection,"{'month': 'Dec', 'issn': '', 'doi': '10.1109/ICICNIS64247.2024.10823299', 'keywords': 'Surveys;Deep learning;Plant diseases;Visualization;Federated learning;Surveillance;Neural networks;Crops;Transformers;Real-time systems;Deep learning;Federated learning;Explainable AI;Real-time detection', 'abstract': ""The most important aspect of modern agriculture is the detection of plant diseases with the goal of improving crop quality and output. This survey paper investigates modern approaches of deep learning algorithms, Explainable AI and Federated Learning to plant disease diagnosis. The background and broadening of plant disease detection are briefly reviewed at the outset of the study, with a focus on the importance of precise and effective identification techniques. The multivariate normal DL neural network (MNDLNN) classifier and Customized CNN are the most robust feature extraction and classification techniques among the examined technologies. Using Higher-Order Whitened Singular Value Decomposition (HOWSVD), complex data is processed in a way that highlights distinct patterns and features for easy data classification and also increases accuracy. The E-GreenNet generates disease classification features based on a MobileNetV3. The ResNet9 and Improved Vision Transformer models perform better when processing complicated visual data. Spectral Generative Adversarial Neural Network (DSGAN2) and Lite Multikernel Depthwise Convolutions architecture are suitable for real-time applications because they provide promising results with less computational overhead. These findings' implications point to a possible move toward disease detection systems that are easier to use and more effective, which would improve crop management and food security. The development of more affordable solutions for small scale farmers, the integration of Embedded systems for real-time monitoring, and the improvement of these models for increased precision are some future directions. This survey highlights the transformative potential of combining deep learning with other techniques like Federated Learning in advancing agricultural practices."", 'pages': '1491-1496', 'number': '', 'volume': '', 'year': '2024', 'title': 'A Comprehensive Survey on Phytopathogen Surveillance with Modern Artificial Intelligence Practices', 'booktitle': '2024 International Conference on IoT Based Control Networks and Intelligent Systems (ICICNIS)', 'author': 'G, Kaleeswari and R, Sundarrajan', 'ENTRYTYPE': 'inproceedings', 'ID': '10823299'}"
10791156,Deepfake Face Images: Explainable Detection using Deep Neural Networks and Class Activation Mapping,"Sugiantoro, Bambang",Sugiantoro,10.1109/ISCT62336.2024.10791156,2024,2024 IEEE International Symposium on Consumer Technology (ISCT),"Our study presents an improved method for detecting deepfake content using advanced explainable artificial intelligence (XAI) techniques. We focused on enhancing deep neural networks, specifically Residual Network (ResNet) models such as ResNet50V2, ResNet101V2, and ResNet152V2, with Gradient-weighted Class Activation Mapping (Grad-CAM) to more accurately differentiate between real and artificial faces. The addition of XAI principles through Grad-CAM not only increases the detection accuracy but also makes the decision-making process of the models transparent, fostering trust, and making the technology more accessible for real-world applications. We evaluated our models using the FFHQ dataset, which comprises a vast array of real and fake facial images. The results demonstrated significant improvements in both precision and recall rates across all models with the integration of Grad-CAM. Specifically, the enhanced ResNet50V2 model achieved a precision of 87\% for fake images and 94\% for real images, with recall rates of 94\% for fake images and 86\% for real images, resulting in f1 scores of 90\% for both classes. The ResNet101V2 and ResNet152V2 models with Grad-CAM also showed notable improvements, with the ResNet101V2 + Grad-CAM model reaching a precision of 87\% for fake and 96\% for real, and the ResNet152V2 + Grad-CAM model achieving a precision of 90\% for fake and 92\% for real, both with high recall and f1 scores, highlighting the precision and reliability of the method. Our approach not only addresses the challenge of detecting deepfakes with high accuracy but also balances the model complexity with computational efficiency. Despite some limitations, such as data set biases and occasional misclassifications, our method significantly advances digital media authentication and shows promising prospects for identity and security verification. Future work will focus on refining these models, emphasizing the importance of XAI, and exploring their application to broader image-classification challenges to strengthen defenses against the evolving threat of deepfake technology.",Training;Deepfakes;Accuracy;Explainable AI;Computational modeling;Decision making;Media;Complexity theory;Faces;Testing;Deepfake;Detection;Explainable Artificial Intelligence;Grad-CAM;ResNet,"{'month': 'Aug', 'issn': '2159-1423', 'doi': '10.1109/ISCT62336.2024.10791156', 'keywords': 'Training;Deepfakes;Accuracy;Explainable AI;Computational modeling;Decision making;Media;Complexity theory;Faces;Testing;Deepfake;Detection;Explainable Artificial Intelligence;Grad-CAM;ResNet', 'abstract': 'Our study presents an improved method for detecting deepfake content using advanced explainable artificial intelligence (XAI) techniques. We focused on enhancing deep neural networks, specifically Residual Network (ResNet) models such as ResNet50V2, ResNet101V2, and ResNet152V2, with Gradient-weighted Class Activation Mapping (Grad-CAM) to more accurately differentiate between real and artificial faces. The addition of XAI principles through Grad-CAM not only increases the detection accuracy but also makes the decision-making process of the models transparent, fostering trust, and making the technology more accessible for real-world applications. We evaluated our models using the FFHQ dataset, which comprises a vast array of real and fake facial images. The results demonstrated significant improvements in both precision and recall rates across all models with the integration of Grad-CAM. Specifically, the enhanced ResNet50V2 model achieved a precision of 87\\% for fake images and 94\\% for real images, with recall rates of 94\\% for fake images and 86\\% for real images, resulting in f1 scores of 90\\% for both classes. The ResNet101V2 and ResNet152V2 models with Grad-CAM also showed notable improvements, with the ResNet101V2 + Grad-CAM model reaching a precision of 87\\% for fake and 96\\% for real, and the ResNet152V2 + Grad-CAM model achieving a precision of 90\\% for fake and 92\\% for real, both with high recall and f1 scores, highlighting the precision and reliability of the method. Our approach not only addresses the challenge of detecting deepfakes with high accuracy but also balances the model complexity with computational efficiency. Despite some limitations, such as data set biases and occasional misclassifications, our method significantly advances digital media authentication and shows promising prospects for identity and security verification. Future work will focus on refining these models, emphasizing the importance of XAI, and exploring their application to broader image-classification challenges to strengthen defenses against the evolving threat of deepfake technology.', 'pages': '86-90', 'number': '', 'volume': '', 'year': '2024', 'title': 'Deepfake Face Images: Explainable Detection using Deep Neural Networks and Class Activation Mapping', 'booktitle': '2024 IEEE International Symposium on Consumer Technology (ISCT)', 'author': 'Sugiantoro, Bambang', 'ENTRYTYPE': 'inproceedings', 'ID': '10791156'}"
10932320,Credit Card Fraud Detection using Machine Learning Models and Increasing Explainability using Explainable AI Methods,"Gokhale, Varada and Naik, Anjali",Gokhale,10.1109/ICAET63349.2025.10932320,2025,2025 1st International Conference on AIML-Applications for Engineering \& Technology (ICAET),"Intentional deception and exploitation through credit card fraud remain prevalent in the banking industry, as fraudsters continue to obtain credit card details without authorization. Recent years have seen a significant increase in the number of credit card accounts and expenditure, accompanied by a noticeable rise in fraud instances. For fraud detection, the financial sector has employed various machine learning (ML) models and artificial intelligence (AI) techniques. This research follows a systematic methodology to identify and elucidate instances of credit card fraud. A Credit Card Fraud dataset was used. Machine learning models, including Explainable Boosting Machine (EBM), Decision Tree (DT), Random Forest (RF), XGBoost (XGB) and an Artificial Neural Network (ANN) were instantiated. Explainable AI models such as SHAP, LIME, and EBM were applied to the outputs of these ML models to enhance the explainability. The results indicated that the EBM and Decision Tree models achieved the highest accuracy at 96.35\%.",Radio frequency;Accuracy;Explainable AI;Artificial neural networks;Predictive models;Credit cards;Fraud;Decision trees;Stakeholders;Random forests;fraud;ML;XAI;explain;accuracy,"{'month': 'Jan', 'issn': '', 'doi': '10.1109/ICAET63349.2025.10932320', 'keywords': 'Radio frequency;Accuracy;Explainable AI;Artificial neural networks;Predictive models;Credit cards;Fraud;Decision trees;Stakeholders;Random forests;fraud;ML;XAI;explain;accuracy', 'abstract': 'Intentional deception and exploitation through credit card fraud remain prevalent in the banking industry, as fraudsters continue to obtain credit card details without authorization. Recent years have seen a significant increase in the number of credit card accounts and expenditure, accompanied by a noticeable rise in fraud instances. For fraud detection, the financial sector has employed various machine learning (ML) models and artificial intelligence (AI) techniques. This research follows a systematic methodology to identify and elucidate instances of credit card fraud. A Credit Card Fraud dataset was used. Machine learning models, including Explainable Boosting Machine (EBM), Decision Tree (DT), Random Forest (RF), XGBoost (XGB) and an Artificial Neural Network (ANN) were instantiated. Explainable AI models such as SHAP, LIME, and EBM were applied to the outputs of these ML models to enhance the explainability. The results indicated that the EBM and Decision Tree models achieved the highest accuracy at 96.35\\%.', 'pages': '1-8', 'number': '', 'volume': '', 'year': '2025', 'title': 'Credit Card Fraud Detection using Machine Learning Models and Increasing Explainability using Explainable AI Methods', 'booktitle': '2025 1st International Conference on AIML-Applications for Engineering \\& Technology (ICAET)', 'author': 'Gokhale, Varada and Naik, Anjali', 'ENTRYTYPE': 'inproceedings', 'ID': '10932320'}"
9990245,Towards Creating Exotic Remote Sensing Datasets using Image Generating AI,"Abduljawad, Mohamed and Alsalmani, Abdullah",Abduljawad,10.1109/ICECTA57148.2022.9990245,2022,2022 International Conference on Electrical and Computing Technologies and Applications (ICECTA),"Over the past few years, neural networks have been used more often to solve long lasting challenges. Remote sensing and data classification were some of the fields that have widely depended on this continuously developing technology. In this context, remote sensing data related to places with harsh conditions have been missing, especially the ones related to SAR imagery. Such conditions include deserts, glaciers, and icebergs, where lots of people have lost their lives in, due to the lack of efficient methods of searching and finding these people in such critical timing. Training AI models on similar scenarios to fasten the process can be beneficial, but the lack of data is an obstacle in the way of development such models. In this paper, we propose using image generating AI systems to generate remote sensing datasets that are difficult to collect using normal imagery, thus creating more efficient image classification systems that can be used in scenarios such as locating missing people. Several AI models are discussed in this paper: Dall-E 2, Stable Diffusion and Midjourney, where they are found to vary a lot in terms of the generated images, that could be because of the architecture of the model, and the data they trained on. The overall performance of the AI models is promising. Dall-E 2 performed the best in our tests, followed by Stable Diffusion, and finally Midjourney. This research could open the door to using such models in generating lots of datasets, which might solve crucial problems.",Training;Satellites;Neural networks;Computer architecture;Data models;Radar polarimetry;Timing;Generative Adversarial Networks;Diffusion Models;Remote Sensing;Datasets,"{'month': 'Nov', 'issn': '', 'doi': '10.1109/ICECTA57148.2022.9990245', 'keywords': 'Training;Satellites;Neural networks;Computer architecture;Data models;Radar polarimetry;Timing;Generative Adversarial Networks;Diffusion Models;Remote Sensing;Datasets', 'abstract': 'Over the past few years, neural networks have been used more often to solve long lasting challenges. Remote sensing and data classification were some of the fields that have widely depended on this continuously developing technology. In this context, remote sensing data related to places with harsh conditions have been missing, especially the ones related to SAR imagery. Such conditions include deserts, glaciers, and icebergs, where lots of people have lost their lives in, due to the lack of efficient methods of searching and finding these people in such critical timing. Training AI models on similar scenarios to fasten the process can be beneficial, but the lack of data is an obstacle in the way of development such models. In this paper, we propose using image generating AI systems to generate remote sensing datasets that are difficult to collect using normal imagery, thus creating more efficient image classification systems that can be used in scenarios such as locating missing people. Several AI models are discussed in this paper: Dall-E 2, Stable Diffusion and Midjourney, where they are found to vary a lot in terms of the generated images, that could be because of the architecture of the model, and the data they trained on. The overall performance of the AI models is promising. Dall-E 2 performed the best in our tests, followed by Stable Diffusion, and finally Midjourney. This research could open the door to using such models in generating lots of datasets, which might solve crucial problems.', 'pages': '84-88', 'number': '', 'volume': '', 'year': '2022', 'title': 'Towards Creating Exotic Remote Sensing Datasets using Image Generating AI', 'booktitle': '2022 International Conference on Electrical and Computing Technologies and Applications (ICECTA)', 'author': 'Abduljawad, Mohamed and Alsalmani, Abdullah', 'ENTRYTYPE': 'inproceedings', 'ID': '9990245'}"
9970645,Research on Cartoon Face Generation Based on CycleGAN Assisted with Facial Landmarks,"Ma, Keyi and Wang, Xiaohong",Ma,10.1109/ICCSI55536.2022.9970645,2022,2022 International Conference on Cyber-Physical Social Intelligence (ICCSI),"Turn real faces into cartoon faces is a topic of style transfer, and style transfer is a hot topic in the application of generative adversarial networks in image. CycleGAN is one of generative adversarial networks. It has obvious universal applicability, and has a good transformation effect on various types of style transfer. But to the facial style transfer, it only focuses on the transformation of the whole face, and it is not ideal for the transformation of the details of the facial features. How can this situation be improved? In this paper, we use facial landmarks to assist the transformation of facial features. In the beginning, we use stacked hourglass networks to detection and capture landmarks of real faces. And then, use them to assist cartoon faces generation. In view of the fact that the hourglass network has its own advantages in feature extraction, we use it to replace the generator structure of the original CycleGAN for transformation. And in order to avoid the Checkerboard Artifacts and ensure the quality of image generation, we use bilinear interpolation in the upsampling part of the generator to replace the deconvolution of the original generator and the nearest interpolation of the hourglass network. Experiments show that these practices have good results in optimizing conversion performance and improving image quality.",Image quality;Interpolation;Deconvolution;Image synthesis;Generative adversarial networks;Feature extraction;Generators;style transfer;CycleGAN;stacked hourglass networks facial landmark,"{'month': 'Nov', 'issn': '', 'doi': '10.1109/ICCSI55536.2022.9970645', 'keywords': 'Image quality;Interpolation;Deconvolution;Image synthesis;Generative adversarial networks;Feature extraction;Generators;style transfer;CycleGAN;stacked hourglass networks facial landmark', 'abstract': 'Turn real faces into cartoon faces is a topic of style transfer, and style transfer is a hot topic in the application of generative adversarial networks in image. CycleGAN is one of generative adversarial networks. It has obvious universal applicability, and has a good transformation effect on various types of style transfer. But to the facial style transfer, it only focuses on the transformation of the whole face, and it is not ideal for the transformation of the details of the facial features. How can this situation be improved? In this paper, we use facial landmarks to assist the transformation of facial features. In the beginning, we use stacked hourglass networks to detection and capture landmarks of real faces. And then, use them to assist cartoon faces generation. In view of the fact that the hourglass network has its own advantages in feature extraction, we use it to replace the generator structure of the original CycleGAN for transformation. And in order to avoid the Checkerboard Artifacts and ensure the quality of image generation, we use bilinear interpolation in the upsampling part of the generator to replace the deconvolution of the original generator and the nearest interpolation of the hourglass network. Experiments show that these practices have good results in optimizing conversion performance and improving image quality.', 'pages': '356-361', 'number': '', 'volume': '', 'year': '2022', 'title': 'Research on Cartoon Face Generation Based on CycleGAN Assisted with Facial Landmarks', 'booktitle': '2022 International Conference on Cyber-Physical Social Intelligence (ICCSI)', 'author': 'Ma, Keyi and Wang, Xiaohong', 'ENTRYTYPE': 'inproceedings', 'ID': '9970645'}"
10599257,An Explainable Deep Learning-Based Method for Schizophrenia Diagnosis Using Generative Data-Augmentation,"Saadatinia, Mehrshad and Salimi-Badr, Armin",Saadatinia,10.1109/ACCESS.2024.3428847,2024,IEEE Access,"Schizophrenia is an example of a rare mental disorder that is challenging to diagnose using conventional methods. Deep learning methods have been extensively employed to aid in the diagnosis of schizophrenia. However, their efficacy relies heavily on data quantity, and their black-box nature raises trust concerns, especially in medical diagnosis contexts. In this study, we leverage a deep learning-based method for the automatic diagnosis of schizophrenia using EEG brain recordings. This approach utilizes generative data augmentation, a powerful technique that enhances the accuracy of the diagnosis. Additionally, our study provides a framework to use when dealing with the challenge of limited training data for the diagnosis of other potential rare mental disorders. To enable the utilization of time-frequency features, spectrograms were extracted from the raw signals. After exploring several neural network architectural setups, a proper convolutional neural network (CNN) was used for the initial diagnosis. Subsequently, using Wasserstein GAN with Gradient Penalty (WGAN-GP) and Variational Autoencoder (VAE), two different synthetic datasets were generated in order to augment the initial dataset and address the over-fitting issue. The augmented dataset using VAE achieved a 3.0\% improvement in accuracy, reaching 99.0\%, and also demonstrated faster convergence. Finally, we addressed the lack of trust in black-box models using the Local Interpretable Model-agnostic Explanations (LIME) algorithm to determine the most important superpixels (frequencies) in the diagnosis process.",Mental disorders;Brain modeling;Electroencephalography;Accuracy;Convolutional neural networks;Feature extraction;Data models;Medical diagnosis;Data augmentation;Encoders;Medical diagnosis;schizophrenia;generative data augmentation;variational autoencoder;generative adversarial networks;explainable artificial intelligence (XAI),"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2024.3428847', 'keywords': 'Mental disorders;Brain modeling;Electroencephalography;Accuracy;Convolutional neural networks;Feature extraction;Data models;Medical diagnosis;Data augmentation;Encoders;Medical diagnosis;schizophrenia;generative data augmentation;variational autoencoder;generative adversarial networks;explainable artificial intelligence (XAI)', 'abstract': 'Schizophrenia is an example of a rare mental disorder that is challenging to diagnose using conventional methods. Deep learning methods have been extensively employed to aid in the diagnosis of schizophrenia. However, their efficacy relies heavily on data quantity, and their black-box nature raises trust concerns, especially in medical diagnosis contexts. In this study, we leverage a deep learning-based method for the automatic diagnosis of schizophrenia using EEG brain recordings. This approach utilizes generative data augmentation, a powerful technique that enhances the accuracy of the diagnosis. Additionally, our study provides a framework to use when dealing with the challenge of limited training data for the diagnosis of other potential rare mental disorders. To enable the utilization of time-frequency features, spectrograms were extracted from the raw signals. After exploring several neural network architectural setups, a proper convolutional neural network (CNN) was used for the initial diagnosis. Subsequently, using Wasserstein GAN with Gradient Penalty (WGAN-GP) and Variational Autoencoder (VAE), two different synthetic datasets were generated in order to augment the initial dataset and address the over-fitting issue. The augmented dataset using VAE achieved a 3.0\\% improvement in accuracy, reaching 99.0\\%, and also demonstrated faster convergence. Finally, we addressed the lack of trust in black-box models using the Local Interpretable Model-agnostic Explanations (LIME) algorithm to determine the most important superpixels (frequencies) in the diagnosis process.', 'pages': '98379-98392', 'number': '', 'volume': '12', 'year': '2024', 'title': 'An Explainable Deep Learning-Based Method for Schizophrenia Diagnosis Using Generative Data-Augmentation', 'journal': 'IEEE Access', 'author': 'Saadatinia, Mehrshad and Salimi-Badr, Armin', 'ENTRYTYPE': 'article', 'ID': '10599257'}"
9547887,A coil counting model based on full convolution regression neural networks,"Liang, Xixi and Zhao, Erdun and Li, Ting and Lin, Zhuocheng",Liang,,2021,"AIIPCC 2021; The Second International Conference on Artificial Intelligence, Information Processing and Cloud Computing","This paper studies the visual counting problem of the number of winding turns on the micro terminal. Its main task is to distinguish the counting of cross coils and standard coils. Relying on manual counting is not only inefficient but also has large error. In order to improve the work efficiency of product detection, deep learning technology is used to realize automatic counting. The common deep learning counting model is realized by convolution neural network classification, but it can only recognize the fixed number of turns. A full revolution region neural network (FCRN) model is then proposed to solve the winding problem on micro terminal. The model consists of three parts: (a) the Faster RCNN to detect coils from scene images; (b) a FCRN network including a full convolution network (FCN) followed by two ASPP pooling to extract the coil features, and two prediction branches, i.e. a semantic segmentation branch and a regression counting branch to predict the coil segmentation images and to predict the coil turns number, respectively; and, (c) the horizontal projection algorithm to analyse whether the prediction images represent cross coil. The experimental results show that, compared with the counting model directly using CNN classification, the FCRN counting model works well in the environment with fewer samples, and can identify cross coil at the same time of counting, which improves its adaptability and accuracy. Thus, it can improve the detection rate of products.",,"{'month': 'June', 'issn': '', 'doi': '', 'keywords': '', 'abstract': 'This paper studies the visual counting problem of the number of winding turns on the micro terminal. Its main task is to distinguish the counting of cross coils and standard coils. Relying on manual counting is not only inefficient but also has large error. In order to improve the work efficiency of product detection, deep learning technology is used to realize automatic counting. The common deep learning counting model is realized by convolution neural network classification, but it can only recognize the fixed number of turns. A full revolution region neural network (FCRN) model is then proposed to solve the winding problem on micro terminal. The model consists of three parts: (a) the Faster RCNN to detect coils from scene images; (b) a FCRN network including a full convolution network (FCN) followed by two ASPP pooling to extract the coil features, and two prediction branches, i.e. a semantic segmentation branch and a regression counting branch to predict the coil segmentation images and to predict the coil turns number, respectively; and, (c) the horizontal projection algorithm to analyse whether the prediction images represent cross coil. The experimental results show that, compared with the counting model directly using CNN classification, the FCRN counting model works well in the environment with fewer samples, and can identify cross coil at the same time of counting, which improves its adaptability and accuracy. Thus, it can improve the detection rate of products.', 'pages': '1-6', 'number': '', 'volume': '', 'year': '2021', 'title': 'A coil counting model based on full convolution regression neural networks', 'booktitle': 'AIIPCC 2021; The Second International Conference on Artificial Intelligence, Information Processing and Cloud Computing', 'author': 'Liang, Xixi and Zhao, Erdun and Li, Ting and Lin, Zhuocheng', 'ENTRYTYPE': 'inproceedings', 'ID': '9547887'}"
10812716,Enhancing Quality Control: A Study on AI and Human Performance in Flip Chip Defect Detection,"Cheamsiri, Wannisa and Jitpattanakul, Anuchit and Muneesawang, Paisarn and Wongpatikaseree, Konlakorn and Hnoohom, Narit",Cheamsiri,10.1109/ACCESS.2024.3521459,2024,IEEE Access,"This study introduces an advanced defect inspection model that utilizes object detection techniques to identify defects in Flip Chip cross-section images. The model serves as a valuable tool for failure analysis (FA) engineers working with Chip-on-Wafer (CoW) products by enhancing inspection precision and accuracy, save time and costs, reduce human error, and ensure reliability. The dataset, provided by an electronics manufacturing service provider in Thailand, and is divided into four categories: good bump, head-in-pillow (HIP) defect, non-wetting defect, and solder void defect. High-resolution images were captured with an Olympus BX53M microscope at $1000\times $ magnification, focusing on a 50-micrometer copper pillar (CP) bump diameter. To address dataset imbalances, this research applies image augmentation techniques and generative artificial intelligence (AI) to synthesize additional HIP defect images. The experimental setup involved seven image datasets used to train multiple object detection models, including YOLOv5, YOLOv6, YOLOv7, and YOLOv8, resulting in a total of 26 trained models. Results show that YOLOv5 and YOLOv8 required the shortest training time, clocking in at 0.86 hours (51 minutes 48 seconds), making them the most computationally efficient models. F1-score evaluations indicated that YOLOv5 achieved scores ranging from 0.948 to 0.981, outperforming the other models. Additionally, testing with a panel of five experts revealed that the model achieved higher accuracy and precision than experts with over 20 years of experience.",Flip-chip devices;Defect detection;Accuracy;Inspection;Cows;X-ray imaging;Hip;YOLO;Three-dimensional displays;Deep learning;Flip chip defect;cross section;generative artificial intelligence;deep learning;object detection;YOLO models,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2024.3521459', 'keywords': 'Flip-chip devices;Defect detection;Accuracy;Inspection;Cows;X-ray imaging;Hip;YOLO;Three-dimensional displays;Deep learning;Flip chip defect;cross section;generative artificial intelligence;deep learning;object detection;YOLO models', 'abstract': 'This study introduces an advanced defect inspection model that utilizes object detection techniques to identify defects in Flip Chip cross-section images. The model serves as a valuable tool for failure analysis (FA) engineers working with Chip-on-Wafer (CoW) products by enhancing inspection precision and accuracy, save time and costs, reduce human error, and ensure reliability. The dataset, provided by an electronics manufacturing service provider in Thailand, and is divided into four categories: good bump, head-in-pillow (HIP) defect, non-wetting defect, and solder void defect. High-resolution images were captured with an Olympus BX53M microscope at $1000\\times $ magnification, focusing on a 50-micrometer copper pillar (CP) bump diameter. To address dataset imbalances, this research applies image augmentation techniques and generative artificial intelligence (AI) to synthesize additional HIP defect images. The experimental setup involved seven image datasets used to train multiple object detection models, including YOLOv5, YOLOv6, YOLOv7, and YOLOv8, resulting in a total of 26 trained models. Results show that YOLOv5 and YOLOv8 required the shortest training time, clocking in at 0.86 hours (51 minutes 48 seconds), making them the most computationally efficient models. F1-score evaluations indicated that YOLOv5 achieved scores ranging from 0.948 to 0.981, outperforming the other models. Additionally, testing with a panel of five experts revealed that the model achieved higher accuracy and precision than experts with over 20 years of experience.', 'pages': '197840-197855', 'number': '', 'volume': '12', 'year': '2024', 'title': 'Enhancing Quality Control: A Study on AI and Human Performance in Flip Chip Defect Detection', 'journal': 'IEEE Access', 'author': 'Cheamsiri, Wannisa and Jitpattanakul, Anuchit and Muneesawang, Paisarn and Wongpatikaseree, Konlakorn and Hnoohom, Narit', 'ENTRYTYPE': 'article', 'ID': '10812716'}"
10880591,Generative Intelligence\&\#x2010;Based Federated Learning Model for Brain Tumor Classification in Smart Health,"Maiti, Niladri and Chawla, Riddhi and Quraishi, Aadam and Soni, Mukesh and Rusho, Maher Ali and Pande, Sagar Dhanraj",Maiti,10.1002/9781394280735.ch22,2025,Generative Artificial Intelligence for Biomedical and Smart Health Informatics,"Summary <p>This study presents a sophisticated ResNet\&\#x2010;10 convolutional neural network model that is specifically developed to address the classification difficulties of brain computed tomography (CT) images, particularly those associated with Alzheimer's disease (AD), brain lesions (including tumors), and normal aging in smart healthcare. The model employs a residual hybrid attention module (RHAM) to enhance the specificity of features, enabling it to effectively collect both spatial information and relevant content within brain tissue. These enhancements enhance the model's efficacy in both traditional categorization and brain tumor diagnosis through the utilization of associative learning and interpretable generative artificial intelligence (GAI). To streamline the intricacy of the design, a global media collecting layer is implemented, and a dropout mechanism is incorporated in the subsequent levels to prevent unnecessary installation. Throughout training, this model makes use of label smoothing entropy loss functions to enhance its capacity for generalization, even with a limited quantity of training samples. The advanced ResNet\&\#x2010;10 network model has been extensively tested and proven effective on brain CT scans, obtaining an incredible 97.47\% classification accuracy. The demonstration emphasized its potential application in broader domains such as GAI\&\#x2010;based collaborative learning and brain tumor detection.</p>",Brain modeling;Computed tomography;Lesions;Accuracy;Visualization;Feature extraction;Brain tumors;Analytical models;Visual systems;Stacking,"{'url': 'https://ieeexplore-ieee-org.crai.referencistas.com/document/10880591', 'isbn': '9781394280728', 'publisher': 'IEEE', 'issn': '', 'doi': '10.1002/9781394280735.ch22', 'keywords': 'Brain modeling;Computed tomography;Lesions;Accuracy;Visualization;Feature extraction;Brain tumors;Analytical models;Visual systems;Stacking', 'abstract': ""Summary <p>This study presents a sophisticated ResNet\\&\\#x2010;10 convolutional neural network model that is specifically developed to address the classification difficulties of brain computed tomography (CT) images, particularly those associated with Alzheimer's disease (AD), brain lesions (including tumors), and normal aging in smart healthcare. The model employs a residual hybrid attention module (RHAM) to enhance the specificity of features, enabling it to effectively collect both spatial information and relevant content within brain tissue. These enhancements enhance the model's efficacy in both traditional categorization and brain tumor diagnosis through the utilization of associative learning and interpretable generative artificial intelligence (GAI). To streamline the intricacy of the design, a global media collecting layer is implemented, and a dropout mechanism is incorporated in the subsequent levels to prevent unnecessary installation. Throughout training, this model makes use of label smoothing entropy loss functions to enhance its capacity for generalization, even with a limited quantity of training samples. The advanced ResNet\\&\\#x2010;10 network model has been extensively tested and proven effective on brain CT scans, obtaining an incredible 97.47\\% classification accuracy. The demonstration emphasized its potential application in broader domains such as GAI\\&\\#x2010;based collaborative learning and brain tumor detection.</p>"", 'pages': '435-453', 'number': '', 'volume': '', 'year': '2025', 'title': 'Generative Intelligence\\&\\#x2010;Based Federated Learning Model for Brain Tumor Classification in Smart Health', 'booktitle': 'Generative Artificial Intelligence for Biomedical and Smart Health Informatics', 'author': 'Maiti, Niladri and Chawla, Riddhi and Quraishi, Aadam and Soni, Mukesh and Rusho, Maher Ali and Pande, Sagar Dhanraj', 'ENTRYTYPE': 'inbook', 'ID': '10880591'}"
10414101,Design and Performance Analysis of an Anti-Malware System Based on Generative Adversarial Network Framework,"Khan, Faiza Babar and Durad, Muhammad Hanif and Khan, Asifullah and Khan, Farrukh Aslam and Rizwan, Muhammad and Ali, Aftab",Khan,10.1109/ACCESS.2024.3358454,2024,IEEE Access,"The cyber realm is overwhelmed with dynamic malware that promptly penetrates all defense mechanisms, operates unapprehended to the user, and covertly causes damage to sensitive data. The current generation of cyber users is being victimized by the interpolation of malware each day due to the pervasive progression of Internet connectivity. Malware is dispersed to infiltrate the security, privacy, and integrity of the system. Conventional malware detection systems do not have the potential to detect novel malware without the accessibility of their signatures, which gives rise to a high False Negative Rate (FNR). Previously, there were numerous attempts to address the issue of malware detection, but none of them effectively combined the capabilities of signature-based and machine learning-based detection engines. To address this issue, we have developed an integrated Anti-Malware System (AMS) architecture that incorporates both conventional signature-based detection and AI-based detection modules. Our approach employs a Generative Adversarial Network (GAN) based Malware Classifier Optimizer (MCOGAN) framework, which can optimize a malware classifier. This framework utilizes GANs to generate fabricated benign files that can be used to train external discriminators for optimization purposes. We describe our proposed framework and anti-malware system in detail to provide a better understanding of how a malware detection system works. We evaluate our approach using the Figshare dataset and state-of-the-art models as discriminators. Our results showcase enhanced malware detection performance, yielding a 10\% performance boost, thus affirming the efficacy of our approach compared to existing models.",Malware;Generative adversarial networks;Support vector machines;Machine learning;Generators;Terminology;Training;Performance evaluation;Anti-malware system;generative adversarial networks;malware sandboxes;malware;unpacker;performance,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2024.3358454', 'keywords': 'Malware;Generative adversarial networks;Support vector machines;Machine learning;Generators;Terminology;Training;Performance evaluation;Anti-malware system;generative adversarial networks;malware sandboxes;malware;unpacker;performance', 'abstract': 'The cyber realm is overwhelmed with dynamic malware that promptly penetrates all defense mechanisms, operates unapprehended to the user, and covertly causes damage to sensitive data. The current generation of cyber users is being victimized by the interpolation of malware each day due to the pervasive progression of Internet connectivity. Malware is dispersed to infiltrate the security, privacy, and integrity of the system. Conventional malware detection systems do not have the potential to detect novel malware without the accessibility of their signatures, which gives rise to a high False Negative Rate (FNR). Previously, there were numerous attempts to address the issue of malware detection, but none of them effectively combined the capabilities of signature-based and machine learning-based detection engines. To address this issue, we have developed an integrated Anti-Malware System (AMS) architecture that incorporates both conventional signature-based detection and AI-based detection modules. Our approach employs a Generative Adversarial Network (GAN) based Malware Classifier Optimizer (MCOGAN) framework, which can optimize a malware classifier. This framework utilizes GANs to generate fabricated benign files that can be used to train external discriminators for optimization purposes. We describe our proposed framework and anti-malware system in detail to provide a better understanding of how a malware detection system works. We evaluate our approach using the Figshare dataset and state-of-the-art models as discriminators. Our results showcase enhanced malware detection performance, yielding a 10\\% performance boost, thus affirming the efficacy of our approach compared to existing models.', 'pages': '27683-27708', 'number': '', 'volume': '12', 'year': '2024', 'title': 'Design and Performance Analysis of an Anti-Malware System Based on Generative Adversarial Network Framework', 'journal': 'IEEE Access', 'author': 'Khan, Faiza Babar and Durad, Muhammad Hanif and Khan, Asifullah and Khan, Farrukh Aslam and Rizwan, Muhammad and Ali, Aftab', 'ENTRYTYPE': 'article', 'ID': '10414101'}"
10544960,Deep Learning Classification Techniques on Detecting Diabetic Retinopathy Dataset,"Revathi, B. and Usharani, C. and Kezial Elizabeth, S. K. and P, Nagaraj and Nithya, D.",Revathi,10.1109/ICICT60155.2024.10544960,2024,2024 International Conference on Inventive Computation Technologies (ICICT),"Deep learning algorithms can summarize images to understand how to carry out necessary tasks. The purpose of this study is to compare several deep learning methods. Both experience-based and explanation-based learning are possible in deep learning. The most widely utilized algorithms, such as Convolutional Neural Networks (CNN), Multilayer Perceptron (MLP), Generative Adversarial Networks (GAN), Radial Basis Function Networks (RBFN), and Deep Belief Networks (DBN), and the Diabetic Retinopathy dataset is utilized in this study to evaluate the effectiveness of the algorithms. A comparative study of the classifiers reveals that CNN performs more accurately than the other approaches.",Deep learning;Diabetic retinopathy;Reviews;Radial basis function networks;Multilayer perceptrons;Generative adversarial networks;Classification algorithms;Diabetic Retinopathy;Deep learning;Preprocessing;Classification;Data Validation,"{'month': 'April', 'issn': '2767-7788', 'doi': '10.1109/ICICT60155.2024.10544960', 'keywords': 'Deep learning;Diabetic retinopathy;Reviews;Radial basis function networks;Multilayer perceptrons;Generative adversarial networks;Classification algorithms;Diabetic Retinopathy;Deep learning;Preprocessing;Classification;Data Validation', 'abstract': 'Deep learning algorithms can summarize images to understand how to carry out necessary tasks. The purpose of this study is to compare several deep learning methods. Both experience-based and explanation-based learning are possible in deep learning. The most widely utilized algorithms, such as Convolutional Neural Networks (CNN), Multilayer Perceptron (MLP), Generative Adversarial Networks (GAN), Radial Basis Function Networks (RBFN), and Deep Belief Networks (DBN), and the Diabetic Retinopathy dataset is utilized in this study to evaluate the effectiveness of the algorithms. A comparative study of the classifiers reveals that CNN performs more accurately than the other approaches.', 'pages': '663-668', 'number': '', 'volume': '', 'year': '2024', 'title': 'Deep Learning Classification Techniques on Detecting Diabetic Retinopathy Dataset', 'booktitle': '2024 International Conference on Inventive Computation Technologies (ICICT)', 'author': 'Revathi, B. and Usharani, C. and Kezial Elizabeth, S. K. and P, Nagaraj and Nithya, D.', 'ENTRYTYPE': 'inproceedings', 'ID': '10544960'}"
10730665,Lecturers' and Students' Perspectives on Using Chat-GPT in Academics for Creative Problem Solving: A Dilemma or an Opportunity for Improvement?,"Razali, Samirah",Razali,10.1109/AiDAS63860.2024.10730665,2024,2024 5th International Conference on Artificial Intelligence and Data Sciences (AiDAS),"Emerging intelligent tools such as Chat-GPT and other generative AI technologies have gained significant interest, especially in higher education. Despite the ease that Chat-GPT can provide for both lecturers and students, there are rising concerns related to critical and creative problem-solving and innovative skills. There are unclear perspectives among lecturers and students on these rising issues. This study explores the opinions of both lecturers and students regarding the use of Chat-GPT in higher education, its impacts on accuracy, ethical issues, and creative problem-solving and innovative skills. This study uses descriptive analysis, gathering responses from 100 people, including 50 lecturers and 50 students, to understand their knowledge of Chat-GPT, how often they use it, its effectiveness, the main concerns, and the opportunities Chat-GPT can provide. The results show a balanced view of Chat-GPT and strategies to address the potential hindrance of creative problem-solving skills. This research provides useful insights into current attitudes towards AI in education, helping to understand its benefits and challenges and provide strategies to reduce the over-reliance on Chat-GPT in academics.",Ethics;Accuracy;Generative AI;Education;Problem-solving;Chat-GPT;Education;Artificial Intelligence;Problem Solving;Ethical,"{'month': 'Sep.', 'issn': '', 'doi': '10.1109/AiDAS63860.2024.10730665', 'keywords': 'Ethics;Accuracy;Generative AI;Education;Problem-solving;Chat-GPT;Education;Artificial Intelligence;Problem Solving;Ethical', 'abstract': 'Emerging intelligent tools such as Chat-GPT and other generative AI technologies have gained significant interest, especially in higher education. Despite the ease that Chat-GPT can provide for both lecturers and students, there are rising concerns related to critical and creative problem-solving and innovative skills. There are unclear perspectives among lecturers and students on these rising issues. This study explores the opinions of both lecturers and students regarding the use of Chat-GPT in higher education, its impacts on accuracy, ethical issues, and creative problem-solving and innovative skills. This study uses descriptive analysis, gathering responses from 100 people, including 50 lecturers and 50 students, to understand their knowledge of Chat-GPT, how often they use it, its effectiveness, the main concerns, and the opportunities Chat-GPT can provide. The results show a balanced view of Chat-GPT and strategies to address the potential hindrance of creative problem-solving skills. This research provides useful insights into current attitudes towards AI in education, helping to understand its benefits and challenges and provide strategies to reduce the over-reliance on Chat-GPT in academics.', 'pages': '30-34', 'number': '', 'volume': '', 'year': '2024', 'title': ""Lecturers' and Students' Perspectives on Using Chat-GPT in Academics for Creative Problem Solving: A Dilemma or an Opportunity for Improvement?"", 'booktitle': '2024 5th International Conference on Artificial Intelligence and Data Sciences (AiDAS)', 'author': 'Razali, Samirah', 'ENTRYTYPE': 'inproceedings', 'ID': '10730665'}"
10725256,Generative AI-based Health Monitoring and Prediction of Electrical Submersible Pumps,"Paroha, Abhay Dutt",Paroha,10.1109/ICCCNT61001.2024.10725256,2024,2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT),"Algorithms, Analytics, Modeling, and Knowledge-Based Techniques are used to propose an innovative solution in this research paper to enhance the prognostic health monitoring (PHM) of electrical submersible pumps (ESPs) by incorporating Generative Artificial Intelligence (AI) methods. The method under consideration creates artifacts that mimic various conditions of ESP failure and degradation by training deep learning models such as Generative Adversarial Networks (GANs). Thanks to the use of synthetic data for constructing machine learning models, it becomes possible to predict the conditions in which the ESP function will start degrading and could potentially find the root cause before it becomes an issue. The data gathered from this experiment affirm that this method is most efficient in enhancing maintenance plans and reducing downtime to achieve optimum dependability and effectiveness of ESP operations. The proposed architecture of PHM involves the incorporation of Generative AI for enhancing the time of operation of the ESPs in industrial applications and presents significant opportunities for preventive maintenance.",Training;Generative AI;Predictive models;Maintenance;Security;Resource management;Prognostics and health management;Underwater vehicles;Monitoring;Synthetic data;Electric Submersible Pump;Generative AI;Predictive Maintenance;Fault Detection;Deep Learning,"{'month': 'June', 'issn': '2473-7674', 'doi': '10.1109/ICCCNT61001.2024.10725256', 'keywords': 'Training;Generative AI;Predictive models;Maintenance;Security;Resource management;Prognostics and health management;Underwater vehicles;Monitoring;Synthetic data;Electric Submersible Pump;Generative AI;Predictive Maintenance;Fault Detection;Deep Learning', 'abstract': 'Algorithms, Analytics, Modeling, and Knowledge-Based Techniques are used to propose an innovative solution in this research paper to enhance the prognostic health monitoring (PHM) of electrical submersible pumps (ESPs) by incorporating Generative Artificial Intelligence (AI) methods. The method under consideration creates artifacts that mimic various conditions of ESP failure and degradation by training deep learning models such as Generative Adversarial Networks (GANs). Thanks to the use of synthetic data for constructing machine learning models, it becomes possible to predict the conditions in which the ESP function will start degrading and could potentially find the root cause before it becomes an issue. The data gathered from this experiment affirm that this method is most efficient in enhancing maintenance plans and reducing downtime to achieve optimum dependability and effectiveness of ESP operations. The proposed architecture of PHM involves the incorporation of Generative AI for enhancing the time of operation of the ESPs in industrial applications and presents significant opportunities for preventive maintenance.', 'pages': '1-7', 'number': '', 'volume': '', 'year': '2024', 'title': 'Generative AI-based Health Monitoring and Prediction of Electrical Submersible Pumps', 'booktitle': '2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT)', 'author': 'Paroha, Abhay Dutt', 'ENTRYTYPE': 'inproceedings', 'ID': '10725256'}"
10916843,A Novel Hierarchical Generative Model for Semi-Supervised Semantic Segmentation of Biomedical Images,"Chai, Lu and Wang, Zidong and Shao, Yuheng and Liu, Qinyuan",Chai,10.1109/TETCI.2025.3540418,2025,IEEE Transactions on Emerging Topics in Computational Intelligence,"In biomedical vision research, a significant challenge is the limited availability of pixel-wise labeled data. Data augmentation has been identified as a solution to this issue through generating labeled dummy data. While enhancing model efficacy, semi-supervised learning methodologies have emerged as a promising alternative that allows models to train on a mix of limited labeled and larger unlabeled data sets, potentially marking a significant advancement in biomedical vision research. Drawing from the semi-supervised learning strategy, in this paper, a novel medical image segmentation model is presented that features a hierarchical architecture with an attention mechanism. This model disentangles the synthesis process of biomedical images by employing a tail two-branch generator for semantic mask synthesis, thereby excelling in handling medical images with imbalanced class characteristics. During inference, the k-means clustering algorithm processes feature maps from the generator by using the clustering outcome as the segmentation mask. Experimental results show that this approach preserves biomedical image details more accurately than synthesized semantic masks. Experiments on various datasets, including those for vestibular schwannoma, kidney, and skin cancer, demonstrate the proposed method's superiority over other generative-adversarial-network-based and semi-supervised segmentation methods in both distribution fitting and semantic segmentation performance.",Biomedical imaging;Lesions;Biological system modeling;Semantics;Generators;Feature extraction;Data models;Training;Semantic segmentation;Generative adversarial networks;Generative adversarial network;semi-supervised learning;hierarchical architecture;attention mechanism;biomedical image segmentation,"{'month': 'June', 'issn': '2471-285X', 'doi': '10.1109/TETCI.2025.3540418', 'keywords': 'Biomedical imaging;Lesions;Biological system modeling;Semantics;Generators;Feature extraction;Data models;Training;Semantic segmentation;Generative adversarial networks;Generative adversarial network;semi-supervised learning;hierarchical architecture;attention mechanism;biomedical image segmentation', 'abstract': ""In biomedical vision research, a significant challenge is the limited availability of pixel-wise labeled data. Data augmentation has been identified as a solution to this issue through generating labeled dummy data. While enhancing model efficacy, semi-supervised learning methodologies have emerged as a promising alternative that allows models to train on a mix of limited labeled and larger unlabeled data sets, potentially marking a significant advancement in biomedical vision research. Drawing from the semi-supervised learning strategy, in this paper, a novel medical image segmentation model is presented that features a hierarchical architecture with an attention mechanism. This model disentangles the synthesis process of biomedical images by employing a tail two-branch generator for semantic mask synthesis, thereby excelling in handling medical images with imbalanced class characteristics. During inference, the k-means clustering algorithm processes feature maps from the generator by using the clustering outcome as the segmentation mask. Experimental results show that this approach preserves biomedical image details more accurately than synthesized semantic masks. Experiments on various datasets, including those for vestibular schwannoma, kidney, and skin cancer, demonstrate the proposed method's superiority over other generative-adversarial-network-based and semi-supervised segmentation methods in both distribution fitting and semantic segmentation performance."", 'pages': '2219-2231', 'number': '3', 'volume': '9', 'year': '2025', 'title': 'A Novel Hierarchical Generative Model for Semi-Supervised Semantic Segmentation of Biomedical Images', 'journal': 'IEEE Transactions on Emerging Topics in Computational Intelligence', 'author': 'Chai, Lu and Wang, Zidong and Shao, Yuheng and Liu, Qinyuan', 'ENTRYTYPE': 'article', 'ID': '10916843'}"
11152855,Task-Oriented Connectivity for Networked Robotics with Generative AI and Semantic Communications,"Li, Peizheng and Aijaz, Adnan",Li,10.1109/INFOCOMWKSHPS65812.2025.11152855,2025,IEEE INFOCOM 2025 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS),"The convergence of robotics, advanced communication networks, and artificial intelligence (AI) holds the promise of transforming industries through fully automated and intelligent operations. In this work, we introduce a novel co-working frame-work for robots that unifies goal-oriented semantic communication (SemCom) with a Generative AI (GenAI)-agent under a semantic-aware network. SemCom prioritizes the exchange of meaningful information among robots and the network, thereby reducing overhead and latency. Meanwhile, the GenAI-agent leverages generative AI models to interpret high-level task instructions, allocate resources, and adapt to dynamic changes in both network and robotic environments. This agent-driven paradigm ushers in a new level of autonomy and intelligence, enabling complex tasks of networked robots to be conducted with minimal human intervention. We validate our approach through a multi-robot anomaly detection use-case simulation, where robots detect, compress, and transmit relevant information for classification. Simulation results confirm that SemCom significantly reduces data traffic while preserving critical semantic details, and the GenAI-agent ensures task coordination and network adaptation. This synergy provides a robust, efficient, and scalable solution for modern industrial environments.",Industries;Service robots;Generative AI;Robot kinematics;Simulation;Semantic communication;Dynamic scheduling;Real-time systems;Anomaly detection;Next generation networking;AI-native network;generative AI agent;net-worked robotics;semantic communications;variational autoen-coder;workflow,"{'month': 'May', 'issn': '2833-0587', 'doi': '10.1109/INFOCOMWKSHPS65812.2025.11152855', 'keywords': 'Industries;Service robots;Generative AI;Robot kinematics;Simulation;Semantic communication;Dynamic scheduling;Real-time systems;Anomaly detection;Next generation networking;AI-native network;generative AI agent;net-worked robotics;semantic communications;variational autoen-coder;workflow', 'abstract': 'The convergence of robotics, advanced communication networks, and artificial intelligence (AI) holds the promise of transforming industries through fully automated and intelligent operations. In this work, we introduce a novel co-working frame-work for robots that unifies goal-oriented semantic communication (SemCom) with a Generative AI (GenAI)-agent under a semantic-aware network. SemCom prioritizes the exchange of meaningful information among robots and the network, thereby reducing overhead and latency. Meanwhile, the GenAI-agent leverages generative AI models to interpret high-level task instructions, allocate resources, and adapt to dynamic changes in both network and robotic environments. This agent-driven paradigm ushers in a new level of autonomy and intelligence, enabling complex tasks of networked robots to be conducted with minimal human intervention. We validate our approach through a multi-robot anomaly detection use-case simulation, where robots detect, compress, and transmit relevant information for classification. Simulation results confirm that SemCom significantly reduces data traffic while preserving critical semantic details, and the GenAI-agent ensures task coordination and network adaptation. This synergy provides a robust, efficient, and scalable solution for modern industrial environments.', 'pages': '1-6', 'number': '', 'volume': '', 'year': '2025', 'title': 'Task-Oriented Connectivity for Networked Robotics with Generative AI and Semantic Communications', 'booktitle': 'IEEE INFOCOM 2025 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)', 'author': 'Li, Peizheng and Aijaz, Adnan', 'ENTRYTYPE': 'inproceedings', 'ID': '11152855'}"
10892813,Beyond Imagination: Leveraging Generative AI to Enhance Learning Through Story World Analogies,"Baradari, Dünya and Han, Harry and Xia, Julia and Strelecki, Carey Ann",Baradari,10.1109/FIE61694.2024.10892813,2024,2024 IEEE Frontiers in Education Conference (FIE),"This innovative practice full paper describes ConceptualTales, a conversational AI that explains STEM and social science concepts using analogies from popular story worlds and Socratic reasoning. The disconnect between conventional teaching strategies and student engagement is a persistent challenge in educational systems, particularly STEM fields. Traditional methods often fail to resonate with students, rendering the learning process monotonous and detached from their personal interests. Concurrently, students are enthusiastic about and dedicated to fictional worlds such as Marvel, Harry Potter, and Disney. This observation forms the basis for our innovative practice: integrating these beloved narratives into educational content through generative AI. ConceptualTales was tried with middle and high school students in the USA and China and received overwhelmingly positive feedback. Our system combines fiction-inspired learning, analogical reasoning, and Socratic questions, to make educational content personal and interesting to students.",Conversational artificial intelligence;Generative AI;Social sciences;Education;Rendering (computer graphics);Cognition;STEM;analogies;concepts;generative AI;story worlds;narratives;artificial intelligence in education (AIinEd),"{'month': 'Oct', 'issn': '2377-634X', 'doi': '10.1109/FIE61694.2024.10892813', 'keywords': 'Conversational artificial intelligence;Generative AI;Social sciences;Education;Rendering (computer graphics);Cognition;STEM;analogies;concepts;generative AI;story worlds;narratives;artificial intelligence in education (AIinEd)', 'abstract': 'This innovative practice full paper describes ConceptualTales, a conversational AI that explains STEM and social science concepts using analogies from popular story worlds and Socratic reasoning. The disconnect between conventional teaching strategies and student engagement is a persistent challenge in educational systems, particularly STEM fields. Traditional methods often fail to resonate with students, rendering the learning process monotonous and detached from their personal interests. Concurrently, students are enthusiastic about and dedicated to fictional worlds such as Marvel, Harry Potter, and Disney. This observation forms the basis for our innovative practice: integrating these beloved narratives into educational content through generative AI. ConceptualTales was tried with middle and high school students in the USA and China and received overwhelmingly positive feedback. Our system combines fiction-inspired learning, analogical reasoning, and Socratic questions, to make educational content personal and interesting to students.', 'pages': '1-8', 'number': '', 'volume': '', 'year': '2024', 'title': 'Beyond Imagination: Leveraging Generative AI to Enhance Learning Through Story World Analogies', 'booktitle': '2024 IEEE Frontiers in Education Conference (FIE)', 'author': 'Baradari, Dünya and Han, Harry and Xia, Julia and Strelecki, Carey Ann', 'ENTRYTYPE': 'inproceedings', 'ID': '10892813'}"
10409100,Multilingual Text Classification Based On Deep Learning Models,"Lin, Lanxin",Lin,10.1109/ITAIC58329.2023.10409100,2023,2023 IEEE 11th Joint International Information Technology and Artificial Intelligence Conference (ITAIC),"In an era characterized by global interconnectedness, the imperative for seamless communication across diverse languages is more pronounced than ever. The present research introduces a sophisticated pre-trained model, denoted as Multilingual BERT (mBERT), meticulously engineered to execute text classification with efficacy across 21 European languages. This model is pivotal in augmenting performance uniformity amongst a myriad of European language families. Significantly, mBERT showcases enhanced parallelization and a notable reduction in processing time for extensive documents, standing out in comparison to its contemporaries. In addition, a distilled model is cultivated under the tutelage of mBERT, aiming to delve into the intricacies of knowledge transfer between the two entities. The empirical results corroborate the superior proficiency of mBERT in the realm of language type classification, thereby furnishing a robust methodology for language categorization. Concurrently, the distilled model adeptly assimilates knowledge from its precursor, mBERT, thereby validating the efficacy of the proposed methodology.",Deep learning;Adaptation models;Text categorization;Europe;Task analysis;Information technology;Knowledge transfer;multilingual texts classification;large language models;model distillation;pre-trained models;knowledge transferring,"{'month': 'Dec', 'issn': '2693-2865', 'doi': '10.1109/ITAIC58329.2023.10409100', 'keywords': 'Deep learning;Adaptation models;Text categorization;Europe;Task analysis;Information technology;Knowledge transfer;multilingual texts classification;large language models;model distillation;pre-trained models;knowledge transferring', 'abstract': 'In an era characterized by global interconnectedness, the imperative for seamless communication across diverse languages is more pronounced than ever. The present research introduces a sophisticated pre-trained model, denoted as Multilingual BERT (mBERT), meticulously engineered to execute text classification with efficacy across 21 European languages. This model is pivotal in augmenting performance uniformity amongst a myriad of European language families. Significantly, mBERT showcases enhanced parallelization and a notable reduction in processing time for extensive documents, standing out in comparison to its contemporaries. In addition, a distilled model is cultivated under the tutelage of mBERT, aiming to delve into the intricacies of knowledge transfer between the two entities. The empirical results corroborate the superior proficiency of mBERT in the realm of language type classification, thereby furnishing a robust methodology for language categorization. Concurrently, the distilled model adeptly assimilates knowledge from its precursor, mBERT, thereby validating the efficacy of the proposed methodology.', 'pages': '1202-1205', 'number': '', 'volume': '11', 'year': '2023', 'title': 'Multilingual Text Classification Based On Deep Learning Models', 'booktitle': '2023 IEEE 11th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)', 'author': 'Lin, Lanxin', 'ENTRYTYPE': 'inproceedings', 'ID': '10409100'}"
9207181,Catastrophic forgetting and mode collapse in GANs,"Thanh-Tung, Hoang and Tran, Truyen",Thanh-Tung,10.1109/IJCNN48605.2020.9207181,2020,2020 International Joint Conference on Neural Networks (IJCNN),"In this paper, we show that Generative Adversarial Networks (GANs) suffer from catastrophic forgetting even when they are trained to approximate a single target distribution. We show that GAN training is a continual learning problem in which the sequence of changing model distributions is the sequence of tasks to the discriminator. The level of mismatch between tasks in the sequence determines the level of forgetting. Catastrophic forgetting is interrelated to mode collapse and can make the training of GANs non-convergent. We investigate the landscape of the discriminator's output in different variants of GANs and find that when a GAN converges to a good equilibrium, real training datapoints are wide local maxima of the discriminator. We empirically show the relationship between the sharpness of local maxima and mode collapse and generalization in GANs. We show how catastrophic forgetting prevents the discriminator from making real datapoints local maxima, and thus causes non-convergence. Finally, we study methods for preventing catastrophic forgetting in GANs.",Gallium nitride;Task analysis;Training;Generators;Convergence;Generative adversarial networks;Neural networks;GANs;generative;catastrophic forgetting;mode collapse,"{'month': 'July', 'issn': '2161-4407', 'doi': '10.1109/IJCNN48605.2020.9207181', 'keywords': 'Gallium nitride;Task analysis;Training;Generators;Convergence;Generative adversarial networks;Neural networks;GANs;generative;catastrophic forgetting;mode collapse', 'abstract': ""In this paper, we show that Generative Adversarial Networks (GANs) suffer from catastrophic forgetting even when they are trained to approximate a single target distribution. We show that GAN training is a continual learning problem in which the sequence of changing model distributions is the sequence of tasks to the discriminator. The level of mismatch between tasks in the sequence determines the level of forgetting. Catastrophic forgetting is interrelated to mode collapse and can make the training of GANs non-convergent. We investigate the landscape of the discriminator's output in different variants of GANs and find that when a GAN converges to a good equilibrium, real training datapoints are wide local maxima of the discriminator. We empirically show the relationship between the sharpness of local maxima and mode collapse and generalization in GANs. We show how catastrophic forgetting prevents the discriminator from making real datapoints local maxima, and thus causes non-convergence. Finally, we study methods for preventing catastrophic forgetting in GANs."", 'pages': '1-10', 'number': '', 'volume': '', 'year': '2020', 'title': 'Catastrophic forgetting and mode collapse in GANs', 'booktitle': '2020 International Joint Conference on Neural Networks (IJCNN)', 'author': 'Thanh-Tung, Hoang and Tran, Truyen', 'ENTRYTYPE': 'inproceedings', 'ID': '9207181'}"
10578670,An AI-Driven Approach for Enhancing Engagement and Conceptual Understanding in Physics Education,"Domenichini, Diana and Bucchiarone, Antonio and Chiarello, Filippo and Schiavo, Gianluca and Fantoni, Gualtiero",Domenichini,10.1109/EDUCON60312.2024.10578670,2024,2024 IEEE Global Engineering Education Conference (EDUCON),"This Work in Progress paper introduces the design of an innovative educational system that leverages Artificial Intelligence (AI) to address challenges in physics education. The primary objective is to create a system that dynamically adapts to the individual needs and preferences of students while maintaining user-friendliness for teachers, allowing them to tailor their teaching methods. The emphasis is on fostering motivation and engagement, achieved through the implementation of a gamified virtual environment and a strong focus on personalization. Our aim is to develop a system capable of autonomously generating learning activities and constructing effective learning paths, all under the supervision and interaction of teachers. The generation of learning activities is guided by educational taxonomies that delineate and categorize the cognitive processes involved in these activities. The proposed educational system seeks to address challenges identified by Physics Education Research (PER), which offers valuable insights into how individuals learn physics and provides strategies to enhance the overall quality of physics education. Our specific focus revolves around two crucial aspects: concentrating on the conceptual understanding of physics concepts and processes, and fostering knowledge integration and coherence across various physics topics. These aspects are deemed essential for cultivating enduring knowledge and facilitating practical applications in the field of physics.",Cognitive processes;Taxonomy;Virtual environments;Coherence;Physics education;Artificial intelligence;Educational System;Adaptive Learning;Gamification;Artificial Intelligence in Education (AIED);Generative AI;Conceptual Understanding;Physics Education,"{'month': 'May', 'issn': '2165-9567', 'doi': '10.1109/EDUCON60312.2024.10578670', 'keywords': 'Cognitive processes;Taxonomy;Virtual environments;Coherence;Physics education;Artificial intelligence;Educational System;Adaptive Learning;Gamification;Artificial Intelligence in Education (AIED);Generative AI;Conceptual Understanding;Physics Education', 'abstract': 'This Work in Progress paper introduces the design of an innovative educational system that leverages Artificial Intelligence (AI) to address challenges in physics education. The primary objective is to create a system that dynamically adapts to the individual needs and preferences of students while maintaining user-friendliness for teachers, allowing them to tailor their teaching methods. The emphasis is on fostering motivation and engagement, achieved through the implementation of a gamified virtual environment and a strong focus on personalization. Our aim is to develop a system capable of autonomously generating learning activities and constructing effective learning paths, all under the supervision and interaction of teachers. The generation of learning activities is guided by educational taxonomies that delineate and categorize the cognitive processes involved in these activities. The proposed educational system seeks to address challenges identified by Physics Education Research (PER), which offers valuable insights into how individuals learn physics and provides strategies to enhance the overall quality of physics education. Our specific focus revolves around two crucial aspects: concentrating on the conceptual understanding of physics concepts and processes, and fostering knowledge integration and coherence across various physics topics. These aspects are deemed essential for cultivating enduring knowledge and facilitating practical applications in the field of physics.', 'pages': '1-3', 'number': '', 'volume': '', 'year': '2024', 'title': 'An AI-Driven Approach for Enhancing Engagement and Conceptual Understanding in Physics Education', 'booktitle': '2024 IEEE Global Engineering Education Conference (EDUCON)', 'author': 'Domenichini, Diana and Bucchiarone, Antonio and Chiarello, Filippo and Schiavo, Gianluca and Fantoni, Gualtiero', 'ENTRYTYPE': 'inproceedings', 'ID': '10578670'}"
10220695,Deep Learning for Uneven Data in Industrial IoT Using a Distributed Bias-Aware Adversarial Network,"Gupta, Raj Kumar and N, Naveena and Rao, B. Srinivasa and RS, Rajasree and Sarkar, Swagata and Kumar, Kallakunta Ravi",Gupta,10.1109/ICIRCA57980.2023.10220695,2023,2023 5th International Conference on Inventive Research in Computing Applications (ICIRCA),"In minority class and noisy data situations, supervised learning performs more favorably for the majority class but cannot generalize testing data. Performance in the aforementioned use cases might be improved with the help of neural-based data augmentation approaches for generating new data and deep convolutional models for classification. GANs (generative adversarial networks) have recently demonstrated impressive advancements in picture generation. To address the restrictions imposed by the distribution bias problem between the produced data and the unique data, and to provide a more vigorous data augmentation, the presented distribution bias aware collaborative GAN (DGAN) model for unbalanced deep learning in industrial IoT. By including an auxiliary classifier in the foundational GAN model, a comprehensive data augmentation system may be built. In particular, a provisional source of energy with random labels is envisioned and trained combatively with the classification model to appropriately augment the amount of data specimens in minority classes, and a mass fraction system is newly envisioned between two distinct feature extraction, allowing the cooperative adversarial training among some of the power source, voltage divider, and classification algorithm. The next step is to develop an augmentation approach for smart anomaly detection in class imbalance, which, by adjusting for dispersion bias using the properly balanced data, might significantly improve classification precision. Experiment assessments using real-world unbalanced datasets show the superior recital of the suggested model in addressing the distribution biased issue for multi-class classification in class imbalance for commercial IoT applications, as compared to five baseline techniques.",Deep learning;Training;Collaboration;Voltage;Generative adversarial networks;Data augmentation;Data models;Generative Adversarial Networks;Anomaly Identification;Bias Aware Collaborative;Class Imbalance;Industrial IoT,"{'month': 'Aug', 'issn': '', 'doi': '10.1109/ICIRCA57980.2023.10220695', 'keywords': 'Deep learning;Training;Collaboration;Voltage;Generative adversarial networks;Data augmentation;Data models;Generative Adversarial Networks;Anomaly Identification;Bias Aware Collaborative;Class Imbalance;Industrial IoT', 'abstract': 'In minority class and noisy data situations, supervised learning performs more favorably for the majority class but cannot generalize testing data. Performance in the aforementioned use cases might be improved with the help of neural-based data augmentation approaches for generating new data and deep convolutional models for classification. GANs (generative adversarial networks) have recently demonstrated impressive advancements in picture generation. To address the restrictions imposed by the distribution bias problem between the produced data and the unique data, and to provide a more vigorous data augmentation, the presented distribution bias aware collaborative GAN (DGAN) model for unbalanced deep learning in industrial IoT. By including an auxiliary classifier in the foundational GAN model, a comprehensive data augmentation system may be built. In particular, a provisional source of energy with random labels is envisioned and trained combatively with the classification model to appropriately augment the amount of data specimens in minority classes, and a mass fraction system is newly envisioned between two distinct feature extraction, allowing the cooperative adversarial training among some of the power source, voltage divider, and classification algorithm. The next step is to develop an augmentation approach for smart anomaly detection in class imbalance, which, by adjusting for dispersion bias using the properly balanced data, might significantly improve classification precision. Experiment assessments using real-world unbalanced datasets show the superior recital of the suggested model in addressing the distribution biased issue for multi-class classification in class imbalance for commercial IoT applications, as compared to five baseline techniques.', 'pages': '1435-1440', 'number': '', 'volume': '', 'year': '2023', 'title': 'Deep Learning for Uneven Data in Industrial IoT Using a Distributed Bias-Aware Adversarial Network', 'booktitle': '2023 5th International Conference on Inventive Research in Computing Applications (ICIRCA)', 'author': 'Gupta, Raj Kumar and N, Naveena and Rao, B. Srinivasa and RS, Rajasree and Sarkar, Swagata and Kumar, Kallakunta Ravi', 'ENTRYTYPE': 'inproceedings', 'ID': '10220695'}"
11023070,Federated TSRN-Enabled GANs for Effective Cyber Attack Detection in Edge Computing,"Sharmin, Zeseya and Xiang, Yong and Uddin, Md Palash and Chen, Feifei and Zhang, Yushu and Tang, Jine",Sharmin,10.1109/TNSE.2025.3576322,2025,IEEE Transactions on Network Science and Engineering,"Edge Computing (EC) processes data locally at edge servers, reducing latency, conserving bandwidth, enhancing privacy, and managing large data volumes independently of central servers, making it critical for applications like autonomous vehicles and healthcare. However, EC is vulnerable to cyber attacks, as edge nodes can compromise edge servers and exploit communication between them. Traditional Machine Learning (ML) methods, which require centralizing communication (activity) data, are ineffective for privacy-sensitive EC scenarios. Federated Learning (FL) offers a distributed ML approach for Federated Cyber-Attack Detection (FedCAD), preserving data privacy by training a shared model locally at each edge server, with a central server aggregating updates to form a global model. However, FedCAD faces challenges with unlabeled and non-Independent and non-Identically Distributed (non-IID) activity data. Existing Generative Adversarial Networks (GANs) methods for synthetic data generation do not fully address these issues. To overcome these challenges, we propose FedTSRGNet, which integrates novel Temporal Sequential Recurrent Network (TSRN)-enabled GANs with FL to generate realistic sequential data by capturing complex temporal patterns in local datasets. Additionally, we introduce a reformulated loss function to optimize TSRN-enabled GAN training within the FL paradigm, improving convergence and ensuring the synthetic data closely mimics real data. Theoretical analysis and extensive experiments on two benchmark datasets demonstrate that FedTSRGNet outperforms four state-of-the-art methods by achieving up to 89\% accuracy for unlabeled non-IID and 95\% accuracy in labeled IID distributions, representing a significant improvement over comparative methods.",Servers;Cyberattack;Image edge detection;Training;Data models;Synthetic data;Data privacy;Generative adversarial networks;Federated learning;Distributed databases;Federated cyber-attack detection;Edge computing;Federated learning;Generative adversarial network;Temporal convolutional network,"{'month': '', 'issn': '2327-4697', 'doi': '10.1109/TNSE.2025.3576322', 'keywords': 'Servers;Cyberattack;Image edge detection;Training;Data models;Synthetic data;Data privacy;Generative adversarial networks;Federated learning;Distributed databases;Federated cyber-attack detection;Edge computing;Federated learning;Generative adversarial network;Temporal convolutional network', 'abstract': 'Edge Computing (EC) processes data locally at edge servers, reducing latency, conserving bandwidth, enhancing privacy, and managing large data volumes independently of central servers, making it critical for applications like autonomous vehicles and healthcare. However, EC is vulnerable to cyber attacks, as edge nodes can compromise edge servers and exploit communication between them. Traditional Machine Learning (ML) methods, which require centralizing communication (activity) data, are ineffective for privacy-sensitive EC scenarios. Federated Learning (FL) offers a distributed ML approach for Federated Cyber-Attack Detection (FedCAD), preserving data privacy by training a shared model locally at each edge server, with a central server aggregating updates to form a global model. However, FedCAD faces challenges with unlabeled and non-Independent and non-Identically Distributed (non-IID) activity data. Existing Generative Adversarial Networks (GANs) methods for synthetic data generation do not fully address these issues. To overcome these challenges, we propose FedTSRGNet, which integrates novel Temporal Sequential Recurrent Network (TSRN)-enabled GANs with FL to generate realistic sequential data by capturing complex temporal patterns in local datasets. Additionally, we introduce a reformulated loss function to optimize TSRN-enabled GAN training within the FL paradigm, improving convergence and ensuring the synthetic data closely mimics real data. Theoretical analysis and extensive experiments on two benchmark datasets demonstrate that FedTSRGNet outperforms four state-of-the-art methods by achieving up to 89\\% accuracy for unlabeled non-IID and 95\\% accuracy in labeled IID distributions, representing a significant improvement over comparative methods.', 'pages': '1-17', 'number': '', 'volume': '', 'year': '2025', 'title': 'Federated TSRN-Enabled GANs for Effective Cyber Attack Detection in Edge Computing', 'journal': 'IEEE Transactions on Network Science and Engineering', 'author': 'Sharmin, Zeseya and Xiang, Yong and Uddin, Md Palash and Chen, Feifei and Zhang, Yushu and Tang, Jine', 'ENTRYTYPE': 'article', 'ID': '11023070'}"
9985936,Research on Variational Autoencoders and its Interpretability Based on Gaussian Cloud Model,"Dai, Jin and Guo, Qiuyan and Zheng, Zhifang and Liu, Xiao",Dai,10.1109/ICBAIE56435.2022.9985936,2022,"2022 3rd International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE)","As one of the important generative models, Variational Autoencoders (VAEs) can complete the feature characterization and generation of data. However, their reconstruction effect on high-resolution images is often not high. The poor image quality is partly due to the limited space for hiding variables. In order to solve this problem, based on the optimization model GCMVAE based on VAEs, this paper improves the generation ability of the model and studies its interpretability. In this study, the interpretability research focuses on the interpretation of some models without semantic information in different stages of the model. Finally, through the analysis of the results, we can get the characteristics of data, the distribution characteristics of implicit variables and the different learning stages experienced in the process of data generation.",Representation learning;Cloud computing;Analytical models;Convolution;Semantics;Data visualization;Learning (artificial intelligence);VAEs;Generative Model;Gaussian Cloud Model;Interpretability,"{'month': 'July', 'issn': '', 'doi': '10.1109/ICBAIE56435.2022.9985936', 'keywords': 'Representation learning;Cloud computing;Analytical models;Convolution;Semantics;Data visualization;Learning (artificial intelligence);VAEs;Generative Model;Gaussian Cloud Model;Interpretability', 'abstract': 'As one of the important generative models, Variational Autoencoders (VAEs) can complete the feature characterization and generation of data. However, their reconstruction effect on high-resolution images is often not high. The poor image quality is partly due to the limited space for hiding variables. In order to solve this problem, based on the optimization model GCMVAE based on VAEs, this paper improves the generation ability of the model and studies its interpretability. In this study, the interpretability research focuses on the interpretation of some models without semantic information in different stages of the model. Finally, through the analysis of the results, we can get the characteristics of data, the distribution characteristics of implicit variables and the different learning stages experienced in the process of data generation.', 'pages': '119-122', 'number': '', 'volume': '', 'year': '2022', 'title': 'Research on Variational Autoencoders and its Interpretability Based on Gaussian Cloud Model', 'booktitle': '2022 3rd International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE)', 'author': 'Dai, Jin and Guo, Qiuyan and Zheng, Zhifang and Liu, Xiao', 'ENTRYTYPE': 'inproceedings', 'ID': '9985936'}"
11050821,Reducing Alert Fatigue via AI-Assisted Negotiation: A Case for Dependabot,"Kula, Raula Gaikovina",Kula,10.1109/BotSE67031.2025.00010,2025,2025 IEEE/ACM International Workshop on Bots in Software Engineering (BotSE),"The increasing complexity of software dependencies has led to the emergence of automated dependency management tools, such as Dependabot. However, these tools often overwhelm developers with a high volume of alerts and notifications, leading to alert fatigue. This paper presents a position on using Artificial Intelligence (AI) agents as dependency negotiators to reduce alert fatigue. We then examine specific use cases where AI agents can facilitate dependency negotiations, such as when working with external dependencies or managing complex, multi-component systems. Our findings highlight the need for more research on the design and evaluation of AI-driven dependency mediation mechanisms. With a focus on ensuring transparency, explainability, and human trustworthiness in these GitHub software projects, our goal is to reduce alert fatigue to an extent that maintainers no longer feel overwhelmed and welcome pull requests just like any other contribution into their projects",Generative AI;Conferences;Supply chains;Fatigue;Software;Complexity theory;Security;Mediation;Software engineering;Software development management;Generative AI;Security Supply Chain,"{'month': 'April', 'issn': '', 'doi': '10.1109/BotSE67031.2025.00010', 'keywords': 'Generative AI;Conferences;Supply chains;Fatigue;Software;Complexity theory;Security;Mediation;Software engineering;Software development management;Generative AI;Security Supply Chain', 'abstract': 'The increasing complexity of software dependencies has led to the emergence of automated dependency management tools, such as Dependabot. However, these tools often overwhelm developers with a high volume of alerts and notifications, leading to alert fatigue. This paper presents a position on using Artificial Intelligence (AI) agents as dependency negotiators to reduce alert fatigue. We then examine specific use cases where AI agents can facilitate dependency negotiations, such as when working with external dependencies or managing complex, multi-component systems. Our findings highlight the need for more research on the design and evaluation of AI-driven dependency mediation mechanisms. With a focus on ensuring transparency, explainability, and human trustworthiness in these GitHub software projects, our goal is to reduce alert fatigue to an extent that maintainers no longer feel overwhelmed and welcome pull requests just like any other contribution into their projects', 'pages': '11-12', 'number': '', 'volume': '', 'year': '2025', 'title': 'Reducing Alert Fatigue via AI-Assisted Negotiation: A Case for Dependabot', 'booktitle': '2025 IEEE/ACM International Workshop on Bots in Software Engineering (BotSE)', 'author': 'Kula, Raula Gaikovina', 'ENTRYTYPE': 'inproceedings', 'ID': '11050821'}"
11103568,Comparative Analysis of Docker and Python Runtimes for AWS Lambda in RAG-Based AI Solutions,"Vidaković, Luka and Stojanović, Dimitrije and Pavković, Bogdan and Četić, Nenad and Krunić, Momčilo",Vidaković,10.1109/ZINC65316.2025.11103568,2025,2025 IEEE Zooming Innovation in Consumer Technologies Conference (ZINC),"As the use of large language models (LLMs) continues to grow and the rapid growth of artificial intelligence (AI)-based applications accelerates, scalable, cost-effective, low-latency solutions are needed. Serverless computing on AWS Lambda has emerged as a key platform for deploying AI applications, yet selecting the optimal runtime environment remains a significant challenge for Retrieval-Augmented Generation (RAG)-based solutions. We provide a thorough comparison between Docker-based and Python-native runtimes, examining key performance metrics such as cold start latency, warm execution time, and build time. Our empirical findings show that the Python-native runtime achieves substantial improvements resulting in up to an 84\% reduction in execution times. We provide insights that equip developers with actionable information, leading to more efficient, scalable, and cost-effective serverless AI deployments.",Technological innovation;Runtime environment;Generative AI;Scalability;Large language models;Retrieval augmented generation;Serverless computing;Performance metrics;Low latency communication;Zinc;Cloud computing;Serverless computing;Retrieval-Augmented Generation;Scalability;Generative AI,"{'month': 'May', 'issn': '2995-2689', 'doi': '10.1109/ZINC65316.2025.11103568', 'keywords': 'Technological innovation;Runtime environment;Generative AI;Scalability;Large language models;Retrieval augmented generation;Serverless computing;Performance metrics;Low latency communication;Zinc;Cloud computing;Serverless computing;Retrieval-Augmented Generation;Scalability;Generative AI', 'abstract': 'As the use of large language models (LLMs) continues to grow and the rapid growth of artificial intelligence (AI)-based applications accelerates, scalable, cost-effective, low-latency solutions are needed. Serverless computing on AWS Lambda has emerged as a key platform for deploying AI applications, yet selecting the optimal runtime environment remains a significant challenge for Retrieval-Augmented Generation (RAG)-based solutions. We provide a thorough comparison between Docker-based and Python-native runtimes, examining key performance metrics such as cold start latency, warm execution time, and build time. Our empirical findings show that the Python-native runtime achieves substantial improvements resulting in up to an 84\\% reduction in execution times. We provide insights that equip developers with actionable information, leading to more efficient, scalable, and cost-effective serverless AI deployments.', 'pages': '1-5', 'number': '', 'volume': '', 'year': '2025', 'title': 'Comparative Analysis of Docker and Python Runtimes for AWS Lambda in RAG-Based AI Solutions', 'booktitle': '2025 IEEE Zooming Innovation in Consumer Technologies Conference (ZINC)', 'author': 'Vidaković, Luka and Stojanović, Dimitrije and Pavković, Bogdan and Četić, Nenad and Krunić, Momčilo', 'ENTRYTYPE': 'inproceedings', 'ID': '11103568'}"
9809820,Type-I Generative Adversarial Attack,"He, Shenghong and Wang, Ruxin and Liu, Tongliang and Yi, Chao and Jin, Xin and Liu, Renyang and Zhou, Wei",He,10.1109/TDSC.2022.3186918,2023,IEEE Transactions on Dependable and Secure Computing,"Deep neural networks are vulnerable to adversarial attacks either by examples with indistinguishable perturbations which produce incorrect predictions, or by examples with noticeable transformations that are still predicted as the original label. The latter case is known as the Type I attack which, however, has achieved limited attention in literature. We advocate that the vulnerability comes from the ambiguous distributions among different classes in the resultant feature space of the model, which is saying that the examples with different appearances may present similar features. Inspired by this, we propose a novel Type I attack method called generative adversarial attack (GAA). Specifically, GAA aims at exploiting the distribution mapping from the source domain of multiple classes to the target domain of a single class by using generative adversarial networks. A novel loss and a U-net architecture with latent modification are elaborated to ensure the stable transformation between the two domains. In this way, the generated adversarial examples have similar appearances with examples of the target domain, yet obtaining the original prediction by the model being attacked. Extensive experiments on multiple benchmarks demonstrate that the proposed method generates adversarial images that are more visually similar to the target images than the competitors, and the state-of-the-art performance is achieved.",Gallium arsenide;Perturbation methods;Generative adversarial networks;Transforms;Videos;Task analysis;Predictive models;Type I attack;resultant feature space;similar features;adversarial attack;generative adversarial network,"{'month': 'May', 'issn': '1941-0018', 'doi': '10.1109/TDSC.2022.3186918', 'keywords': 'Gallium arsenide;Perturbation methods;Generative adversarial networks;Transforms;Videos;Task analysis;Predictive models;Type I attack;resultant feature space;similar features;adversarial attack;generative adversarial network', 'abstract': 'Deep neural networks are vulnerable to adversarial attacks either by examples with indistinguishable perturbations which produce incorrect predictions, or by examples with noticeable transformations that are still predicted as the original label. The latter case is known as the Type I attack which, however, has achieved limited attention in literature. We advocate that the vulnerability comes from the ambiguous distributions among different classes in the resultant feature space of the model, which is saying that the examples with different appearances may present similar features. Inspired by this, we propose a novel Type I attack method called generative adversarial attack (GAA). Specifically, GAA aims at exploiting the distribution mapping from the source domain of multiple classes to the target domain of a single class by using generative adversarial networks. A novel loss and a U-net architecture with latent modification are elaborated to ensure the stable transformation between the two domains. In this way, the generated adversarial examples have similar appearances with examples of the target domain, yet obtaining the original prediction by the model being attacked. Extensive experiments on multiple benchmarks demonstrate that the proposed method generates adversarial images that are more visually similar to the target images than the competitors, and the state-of-the-art performance is achieved.', 'pages': '2593-2606', 'number': '3', 'volume': '20', 'year': '2023', 'title': 'Type-I Generative Adversarial Attack', 'journal': 'IEEE Transactions on Dependable and Secure Computing', 'author': 'He, Shenghong and Wang, Ruxin and Liu, Tongliang and Yi, Chao and Jin, Xin and Liu, Renyang and Zhou, Wei', 'ENTRYTYPE': 'article', 'ID': '9809820'}"
10363970,A Graph-Based Scene Encoder for Vehicle Trajectory Prediction Using the Diffusion Model,"Yao, Yueyang and Liu, Yahui and Dai, Xingyuan and Chen, Shichao and Lv, Yisheng",Yao,10.1109/CSIS-IAC60628.2023.10363970,2023,2023 International Annual Conference on Complex Systems and Intelligent Science (CSIS-IAC),"To boost autonomous driving safety even further, autonomous vehicles must be able to predict future trajectory states, allowing them to grasp their surroundings in real time, just like a human driver. In this paper, we conduct research on the defects of poor multimodal richness and unstable training of existing vehicle trajectory prediction models, and propose an improved model MID++. In order to better grasp the interaction between agents and road elements, we mainly introduce an GNN-based ALEncoder to replace the traditional CNN block. Meanwhile, an importance sampling strategy is exploited in the training process. According to the experimental results on Argoverse 2 dataset, our model enhances the accuracy and variety of trajectory predictions and significantly shortens the training time.",Training;Monte Carlo methods;Pedestrians;Roads;Predictive models;Real-time systems;Trajectory;Autonomous Driving;Trajectory Prediction;Diffusion Model,"{'month': 'Oct', 'issn': '', 'doi': '10.1109/CSIS-IAC60628.2023.10363970', 'keywords': 'Training;Monte Carlo methods;Pedestrians;Roads;Predictive models;Real-time systems;Trajectory;Autonomous Driving;Trajectory Prediction;Diffusion Model', 'abstract': 'To boost autonomous driving safety even further, autonomous vehicles must be able to predict future trajectory states, allowing them to grasp their surroundings in real time, just like a human driver. In this paper, we conduct research on the defects of poor multimodal richness and unstable training of existing vehicle trajectory prediction models, and propose an improved model MID++. In order to better grasp the interaction between agents and road elements, we mainly introduce an GNN-based ALEncoder to replace the traditional CNN block. Meanwhile, an importance sampling strategy is exploited in the training process. According to the experimental results on Argoverse 2 dataset, our model enhances the accuracy and variety of trajectory predictions and significantly shortens the training time.', 'pages': '981-986', 'number': '', 'volume': '', 'year': '2023', 'title': 'A Graph-Based Scene Encoder for Vehicle Trajectory Prediction Using the Diffusion Model', 'booktitle': '2023 International Annual Conference on Complex Systems and Intelligent Science (CSIS-IAC)', 'author': 'Yao, Yueyang and Liu, Yahui and Dai, Xingyuan and Chen, Shichao and Lv, Yisheng', 'ENTRYTYPE': 'inproceedings', 'ID': '10363970'}"
10511639,AI driven Process Diagnostic \& Control: Device Manufacturing,"Park, Jaeyong",Park,10.1109/EDTM58488.2024.10511639,2024,2024 8th IEEE Electron Devices Technology \& Manufacturing Conference (EDTM),"This paper explores the integration of Artificial Intelligence (AI) in process control and diagnostics in semiconductor manufacturing. It highlights current trends, including machine learning (ML) for data alignment and predictive maintenance, and anticipates future advancements in data sharing and standardization. This overview showcases AI’s transformative impact on equipment optimization and industry collaboration, underlining its role in shaping efficient, proactive manufacturing processes.",Industries;Manufacturing processes;Process control;Collaboration;Standardization;Semiconductor device manufacture;Market research;AI;ML;Generative AI;Manufacturing,"{'month': 'March', 'issn': '', 'doi': '10.1109/EDTM58488.2024.10511639', 'keywords': 'Industries;Manufacturing processes;Process control;Collaboration;Standardization;Semiconductor device manufacture;Market research;AI;ML;Generative AI;Manufacturing', 'abstract': 'This paper explores the integration of Artificial Intelligence (AI) in process control and diagnostics in semiconductor manufacturing. It highlights current trends, including machine learning (ML) for data alignment and predictive maintenance, and anticipates future advancements in data sharing and standardization. This overview showcases AI’s transformative impact on equipment optimization and industry collaboration, underlining its role in shaping efficient, proactive manufacturing processes.', 'pages': '1-3', 'number': '', 'volume': '', 'year': '2024', 'title': 'AI driven Process Diagnostic \\& Control: Device Manufacturing', 'booktitle': '2024 8th IEEE Electron Devices Technology \\& Manufacturing Conference (EDTM)', 'author': 'Park, Jaeyong', 'ENTRYTYPE': 'inproceedings', 'ID': '10511639'}"
10788858,Index,,,,2021,Artificial Intelligence for Data-Driven Medical Diagnosis,,Medical diagnostic imaging;Indexes;Feature extraction;Tumors;Support vector machines;Robot sensing systems;Peer-to-peer computing;Informatics;Generative adversarial networks;Artificial intelligence,"{'url': 'https://ieeexplore-ieee-org.crai.referencistas.com/document/10788858', 'isbn': '9783110668384', 'publisher': 'De Gruyter', 'issn': '', 'doi': '', 'keywords': 'Medical diagnostic imaging;Indexes;Feature extraction;Tumors;Support vector machines;Robot sensing systems;Peer-to-peer computing;Informatics;Generative adversarial networks;Artificial intelligence', 'abstract': '', 'pages': '305-310', 'number': '', 'volume': '', 'year': '2021', 'title': 'Index', 'booktitle': 'Artificial Intelligence for Data-Driven Medical Diagnosis', 'ENTRYTYPE': 'inbook', 'ID': '10788858'}"
10986983,Construction and Application Research of University Academic Early Warning System Based on AIGC Technology,"Gong, Lishu",Gong,10.1109/EESPE63401.2025.10986983,2025,"2025 IEEE International Conference on Electronics, Energy Systems and Power Engineering (EESPE)","Traditional university academic early warning systems often suffer from single data dimensions, rigid warning thresholds, and a lack of personalized intervention, making it difficult to accurately identify high-risk students and provide effective support[1]. This research innovatively proposes a construction method for a university academic early warning system based on Artificial Intelligence Generated Content (AIGC) technology [2]. This system integrates multi-dimensional data such as student academic performance, classroom behavior, online interactions, and psychological state. Leveraging AIGC technologies, including Natural Language Processing (NLP) and Generative Adversarial Networks (GANs), the system deeply mines students' potential learning behavior patterns and psychological states, generating more interpretable warning reports and personalized intervention suggestions. By constructing a multi-modal data fusion model and intelligent warning algorithms, the system can achieve early and accurate identification of students' academic risks. Simultaneously, personalized learning resources and tutoring suggestions generated using AIGC technology can more effectively help students overcome learning difficulties and improve their academic performance. This research aims to explore the application potential of AIGC technology in the field of university academic early warning and verify the advantages of this system in improving warning accuracy and intervention effectiveness, providing theoretical support and practical reference for building a more intelligent and humanized university academic support system.",Power engineering;Accuracy;Buildings;Psychology;Data integration;Alarm systems;Generative adversarial networks;Natural language processing;Data models;Artificial intelligence;Artificial Intelligence Generated Content (AIGC);Academic Early Warning;Higher Education;Multi-modal Data Fusion;Personalized Intervention;Natural Language Processing;Generative Adversarial Network,"{'month': 'March', 'issn': '', 'doi': '10.1109/EESPE63401.2025.10986983', 'keywords': 'Power engineering;Accuracy;Buildings;Psychology;Data integration;Alarm systems;Generative adversarial networks;Natural language processing;Data models;Artificial intelligence;Artificial Intelligence Generated Content (AIGC);Academic Early Warning;Higher Education;Multi-modal Data Fusion;Personalized Intervention;Natural Language Processing;Generative Adversarial Network', 'abstract': ""Traditional university academic early warning systems often suffer from single data dimensions, rigid warning thresholds, and a lack of personalized intervention, making it difficult to accurately identify high-risk students and provide effective support[1]. This research innovatively proposes a construction method for a university academic early warning system based on Artificial Intelligence Generated Content (AIGC) technology [2]. This system integrates multi-dimensional data such as student academic performance, classroom behavior, online interactions, and psychological state. Leveraging AIGC technologies, including Natural Language Processing (NLP) and Generative Adversarial Networks (GANs), the system deeply mines students' potential learning behavior patterns and psychological states, generating more interpretable warning reports and personalized intervention suggestions. By constructing a multi-modal data fusion model and intelligent warning algorithms, the system can achieve early and accurate identification of students' academic risks. Simultaneously, personalized learning resources and tutoring suggestions generated using AIGC technology can more effectively help students overcome learning difficulties and improve their academic performance. This research aims to explore the application potential of AIGC technology in the field of university academic early warning and verify the advantages of this system in improving warning accuracy and intervention effectiveness, providing theoretical support and practical reference for building a more intelligent and humanized university academic support system."", 'pages': '283-291', 'number': '', 'volume': '', 'year': '2025', 'title': 'Construction and Application Research of University Academic Early Warning System Based on AIGC Technology', 'booktitle': '2025 IEEE International Conference on Electronics, Energy Systems and Power Engineering (EESPE)', 'author': 'Gong, Lishu', 'ENTRYTYPE': 'inproceedings', 'ID': '10986983'}"
10952356,Adapting and Growing with AI in Education,"Shah, Priten",Shah,,2023,AI and the Future of Education: Teaching in the Age of Artificial Intelligence,"Summary <p>This chapter presents some closing thoughts on the concepts covered in the preceding chapters of this book. The book outlines how AI systems are making our dreams as a community come true. There are clear areas where the education system has shortcomings in serving our students, and this is a perfect opportunity to address those head\&\#x2010;on. The book also outlines how students will need the skills to use these technologies, protect themselves from the malicious actors who manipulate the technologies to harm others, and contribute meaningfully to society side\&\#x2010;by\&\#x2010;side as AI technologies take on greater shares of the workload. As AI handles more and more of the routine tasks that make up our day, there will be a shift in the roles of educators. The book provides some ways educators can continue adapting to these changes with the right resources and tools.</p>",Artificial intelligence;Education;Generative AI;Chatbots;Pandemics;Buildings;Urban areas;Technological innovation;Navigation;Monitoring,"{'url': 'https://ieeexplore-ieee-org.crai.referencistas.com/document/10952356', 'isbn': '9781394219261', 'publisher': 'Wiley', 'issn': '', 'doi': '', 'keywords': 'Artificial intelligence;Education;Generative AI;Chatbots;Pandemics;Buildings;Urban areas;Technological innovation;Navigation;Monitoring', 'abstract': 'Summary <p>This chapter presents some closing thoughts on the concepts covered in the preceding chapters of this book. The book outlines how AI systems are making our dreams as a community come true. There are clear areas where the education system has shortcomings in serving our students, and this is a perfect opportunity to address those head\\&\\#x2010;on. The book also outlines how students will need the skills to use these technologies, protect themselves from the malicious actors who manipulate the technologies to harm others, and contribute meaningfully to society side\\&\\#x2010;by\\&\\#x2010;side as AI technologies take on greater shares of the workload. As AI handles more and more of the routine tasks that make up our day, there will be a shift in the roles of educators. The book provides some ways educators can continue adapting to these changes with the right resources and tools.</p>', 'pages': '218-221', 'number': '', 'volume': '', 'year': '2023', 'title': 'Adapting and Growing with AI in Education', 'booktitle': 'AI and the Future of Education: Teaching in the Age of Artificial Intelligence', 'author': 'Shah, Priten', 'ENTRYTYPE': 'inbook', 'ID': '10952356'}"
11022004,Research on the Application and Model Building of Image Recognition Technology Based on CNN in English Teaching,"Sun, Dandan and Jia, Lixia and Zhou, Li and Shi, Yuxia",Sun,10.1109/ACAIT63902.2024.11022004,2024,2024 8th Asian Conference on Artificial Intelligence Technology (ACAIT),"The traditional English teaching model relies on basic teaching materials and cannot improve students' understanding and interest in English learning. Therefore, this study applies convolutional neural network image recognition technology to English teaching, utilizing convolutional neural networks to achieve image recognition technology and improve the effectiveness of English teaching. This study first builds a convolutional neural network image recognition model, and then optimizes the design and method research of the convolutional neural network model, and secondly, the convolutional neural network image recognition technology is added on the basis of English teaching. Finally, the stability of the algorithm is determined through data processing and experimental analysis. The experimental results show that the error value of the convolutional neural network algorithm is 0.91, and the highest fitness is 0.96. The error is lower than that of traditional image recognition algorithms, and the fitness is higher than that of traditional image recognition algorithms. This shows that the convolutional neural network algorithm is able to recognize and analyze images, and at the same time can optimize the traditional English teaching mode.",Adaptation models;Image recognition;Design methodology;Education;Buildings;Data processing;Stability analysis;Convolutional neural networks;Artificial intelligence;Convolutional Neural Networks;Image Recognition;English Language Teaching;Adaptability,"{'month': 'Nov', 'issn': '', 'doi': '10.1109/ACAIT63902.2024.11022004', 'keywords': 'Adaptation models;Image recognition;Design methodology;Education;Buildings;Data processing;Stability analysis;Convolutional neural networks;Artificial intelligence;Convolutional Neural Networks;Image Recognition;English Language Teaching;Adaptability', 'abstract': ""The traditional English teaching model relies on basic teaching materials and cannot improve students' understanding and interest in English learning. Therefore, this study applies convolutional neural network image recognition technology to English teaching, utilizing convolutional neural networks to achieve image recognition technology and improve the effectiveness of English teaching. This study first builds a convolutional neural network image recognition model, and then optimizes the design and method research of the convolutional neural network model, and secondly, the convolutional neural network image recognition technology is added on the basis of English teaching. Finally, the stability of the algorithm is determined through data processing and experimental analysis. The experimental results show that the error value of the convolutional neural network algorithm is 0.91, and the highest fitness is 0.96. The error is lower than that of traditional image recognition algorithms, and the fitness is higher than that of traditional image recognition algorithms. This shows that the convolutional neural network algorithm is able to recognize and analyze images, and at the same time can optimize the traditional English teaching mode."", 'pages': '1552-1559', 'number': '', 'volume': '', 'year': '2024', 'title': 'Research on the Application and Model Building of Image Recognition Technology Based on CNN in English Teaching', 'booktitle': '2024 8th Asian Conference on Artificial Intelligence Technology (ACAIT)', 'author': 'Sun, Dandan and Jia, Lixia and Zhou, Li and Shi, Yuxia', 'ENTRYTYPE': 'inproceedings', 'ID': '11022004'}"
10913495,Review of 3D Scene Reconstruction: From Traditional Methods to Advanced Deep Learning Models,"Belkaid, Maryem and Alaoui, El Arbi Abdellaoui and Merras, Mostafa and Berrajaa, Achraf and Akkad, Nabil El",Belkaid,10.1109/ESAI62891.2024.10913495,2024,2024 3rd International Conference on Embedded Systems and Artificial Intelligence (ESAI),"3D scene reconstruction represents a pivotal domain within computer vision, involving a diverse array of techniques ranging from classical geometry-driven approaches to modern deep learning-based models. This review article offers an extensive summary of cutting-edge methods for 3D reconstruction, including techniques such as Structure-from-Motion (SfM) and Multi-View Stereo (MVS), as well as cutting-edge deep learning techniques including CNNs, GANs,Variational Autoencoders (VAEs) and Neural Radiance Fields (NeRF). We analyze the strengths and limitations of each approach, particularly in terms of accuracy, efficiency, generalization, and their ability to handle complex scenes. Furthermore, we delve into the key challenges faced in 3D scene reconstruction, including the trade-offs between computational efficiency and model accuracy, the generalization to diverse environments, and the integration of multi-modal data sources. Special attention is given to NeRF, a breakthrough in the field, discussing its current capabilities and potential areas for improvement in future research. This review aims to serve as a resource for researchers and practitioners by summarizing the current landscape of 3D reconstruction technologies and identifying promising directions for future exploration.",Solid modeling;Technological innovation;Three-dimensional displays;Accuracy;Reviews;Computational modeling;Autoencoders;Virtual reality;Neural radiance field;Artificial intelligence;3D Reconstruction Scenes;Artificial Intelligence;Deep Learning;Neural Radiance Fields;Generative Adversarial Networks;Variational Autoencoders;Convolutional Neural Networks,"{'month': 'Dec', 'issn': '', 'doi': '10.1109/ESAI62891.2024.10913495', 'keywords': 'Solid modeling;Technological innovation;Three-dimensional displays;Accuracy;Reviews;Computational modeling;Autoencoders;Virtual reality;Neural radiance field;Artificial intelligence;3D Reconstruction Scenes;Artificial Intelligence;Deep Learning;Neural Radiance Fields;Generative Adversarial Networks;Variational Autoencoders;Convolutional Neural Networks', 'abstract': '3D scene reconstruction represents a pivotal domain within computer vision, involving a diverse array of techniques ranging from classical geometry-driven approaches to modern deep learning-based models. This review article offers an extensive summary of cutting-edge methods for 3D reconstruction, including techniques such as Structure-from-Motion (SfM) and Multi-View Stereo (MVS), as well as cutting-edge deep learning techniques including CNNs, GANs,Variational Autoencoders (VAEs) and Neural Radiance Fields (NeRF). We analyze the strengths and limitations of each approach, particularly in terms of accuracy, efficiency, generalization, and their ability to handle complex scenes. Furthermore, we delve into the key challenges faced in 3D scene reconstruction, including the trade-offs between computational efficiency and model accuracy, the generalization to diverse environments, and the integration of multi-modal data sources. Special attention is given to NeRF, a breakthrough in the field, discussing its current capabilities and potential areas for improvement in future research. This review aims to serve as a resource for researchers and practitioners by summarizing the current landscape of 3D reconstruction technologies and identifying promising directions for future exploration.', 'pages': '1-11', 'number': '', 'volume': '', 'year': '2024', 'title': 'Review of 3D Scene Reconstruction: From Traditional Methods to Advanced Deep Learning Models', 'booktitle': '2024 3rd International Conference on Embedded Systems and Artificial Intelligence (ESAI)', 'author': 'Belkaid, Maryem and Alaoui, El Arbi Abdellaoui and Merras, Mostafa and Berrajaa, Achraf and Akkad, Nabil El', 'ENTRYTYPE': 'inproceedings', 'ID': '10913495'}"
9643213,Semantic-based Bidirectional Adversarial Neural Topic Model,"Ma, Zulong and Lu, Jiamin and Feng, Jun and Zhang, Yunfei and Wu, Wei",Ma,10.1109/ICTAI52525.2021.00061,2021,2021 IEEE 33rd International Conference on Tools with Artificial Intelligence (ICTAI),"The topic models can effectively reduce the dimensionality of document representation. In recent years, Generative Adversarial Networks (GANs) have begun to be applied to topic models, but most researches do not fully consider the documents’ context information, leading to the fact that the generated results of the encoder networks cannot represent the real data distribution. To this end, we propose a novel topic model named Semantic-based Bidirectional Adversarial Neural Topic Model (SNTM), which introduces semantic information into Bidirectional Generative Adversarial Networks (BiGAN) by adding the word embedding and BiLSTM-Attention mechanism. This improvement makes the encoder network in our model generate a distribution closer to the real document-topic distribution. Furthermore, we also propose a topic difference evaluation indicator, which more comprehensively evaluates the quality of the generated topics. The experimental results on different datasets show that SNTM performs better than the baseline topic model.",Conferences;Semantics;Generative adversarial networks;Data models;Artificial intelligence;Context modeling;topic model;topic mining;generative adversarial network;semantic information;neural network,"{'month': 'Nov', 'issn': '2375-0197', 'doi': '10.1109/ICTAI52525.2021.00061', 'keywords': 'Conferences;Semantics;Generative adversarial networks;Data models;Artificial intelligence;Context modeling;topic model;topic mining;generative adversarial network;semantic information;neural network', 'abstract': 'The topic models can effectively reduce the dimensionality of document representation. In recent years, Generative Adversarial Networks (GANs) have begun to be applied to topic models, but most researches do not fully consider the documents’ context information, leading to the fact that the generated results of the encoder networks cannot represent the real data distribution. To this end, we propose a novel topic model named Semantic-based Bidirectional Adversarial Neural Topic Model (SNTM), which introduces semantic information into Bidirectional Generative Adversarial Networks (BiGAN) by adding the word embedding and BiLSTM-Attention mechanism. This improvement makes the encoder network in our model generate a distribution closer to the real document-topic distribution. Furthermore, we also propose a topic difference evaluation indicator, which more comprehensively evaluates the quality of the generated topics. The experimental results on different datasets show that SNTM performs better than the baseline topic model.', 'pages': '376-380', 'number': '', 'volume': '', 'year': '2021', 'title': 'Semantic-based Bidirectional Adversarial Neural Topic Model', 'booktitle': '2021 IEEE 33rd International Conference on Tools with Artificial Intelligence (ICTAI)', 'author': 'Ma, Zulong and Lu, Jiamin and Feng, Jun and Zhang, Yunfei and Wu, Wei', 'ENTRYTYPE': 'inproceedings', 'ID': '9643213'}"
11137839,Research on the Application of Multi-Scale Feature Fusion GAN Framework in Vintage Photo Restoration and Colorization,"Jia, Yihan",Jia,10.1109/ICAITA67588.2025.11137839,2025,2025 7th International Conference on Artificial Intelligence Technologies and Applications (ICAITA),"Vintage photographs preserve precious historical memories but often suffer from damage and fading due to time erosion. This paper proposes a multi-scale feature fusion model based on Generative Adversarial Networks (GAN) for intelligent restoration and colorization of old photos. The model captures detailed information at different levels of old photos through a multi-scale feature extraction module, designs a fusion strategy to organically integrate local and global features, and enhances the authenticity of restored images via generative adversarial mechanisms. Experimental results show that compared with traditional methods, the model significantly improves in indicators such as Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM), enabling better restoration of the original appearance of old photos. This study provides an effective solution for the digital protection and restoration of vintage photographs.",Fading channels;PSNR;Generative adversarial networks;Feature extraction;Image restoration;Indexes;Protection;Artificial intelligence;Erosion;Generative Adversarial Networks (GAN);old photo restoration;colorization;multi-scale feature fusion,"{'month': 'June', 'issn': '', 'doi': '10.1109/ICAITA67588.2025.11137839', 'keywords': 'Fading channels;PSNR;Generative adversarial networks;Feature extraction;Image restoration;Indexes;Protection;Artificial intelligence;Erosion;Generative Adversarial Networks (GAN);old photo restoration;colorization;multi-scale feature fusion', 'abstract': 'Vintage photographs preserve precious historical memories but often suffer from damage and fading due to time erosion. This paper proposes a multi-scale feature fusion model based on Generative Adversarial Networks (GAN) for intelligent restoration and colorization of old photos. The model captures detailed information at different levels of old photos through a multi-scale feature extraction module, designs a fusion strategy to organically integrate local and global features, and enhances the authenticity of restored images via generative adversarial mechanisms. Experimental results show that compared with traditional methods, the model significantly improves in indicators such as Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM), enabling better restoration of the original appearance of old photos. This study provides an effective solution for the digital protection and restoration of vintage photographs.', 'pages': '500-505', 'number': '', 'volume': '', 'year': '2025', 'title': 'Research on the Application of Multi-Scale Feature Fusion GAN Framework in Vintage Photo Restoration and Colorization', 'booktitle': '2025 7th International Conference on Artificial Intelligence Technologies and Applications (ICAITA)', 'author': 'Jia, Yihan', 'ENTRYTYPE': 'inproceedings', 'ID': '11137839'}"
10356488,Enhancing Automatic Speech Recognition Quality with a Second-Stage Speech Enhancement Generative Adversarial Network,"Nossier, Soha A. and Wall, Julie and Moniri, Mansour and Glackin, Cornelius and Cannings, Nigel",Nossier,10.1109/ICTAI59109.2023.00087,2023,2023 IEEE 35th International Conference on Tools with Artificial Intelligence (ICTAI),"Speech enhancement is an essential preprocessing stage for automatic speech recognition in noisy conditions; however, the distortion caused by the denoising process may lead to degradation in automatic speech recognition performance. This paper presents a deep learning-based speech enhancement architecture to overcome this issue by applying a second-stage network that deals with distortion noise. Moreover, a signal-to-noise ratio binary classifier is implemented to activate the speech enhancement network for intrusive noise environments only, which improves the overall performance. The proposed architecture outperforms powerful models in the literature, as it improves a challenging noisy speech test set by 0.8 and 5.9\% improvement in the quality and intelligibility scores, respectively. Furthermore, the architecture improves the performance of automatic speech recognition with a 13.8\% reduction in the word error rate at 0dB signal-to-noise ratio. Finally, the second-stage network was proven to improve the performance of first-stage speech enhancement models, not previously seen in the training process.",Training;Degradation;Error analysis;Noise reduction;Speech enhancement;Distortion;Generative adversarial networks;Automatic speech recognition;deep learning;generative adversarial network;speech distortion;speech enhancement,"{'month': 'Nov', 'issn': '2375-0197', 'doi': '10.1109/ICTAI59109.2023.00087', 'keywords': 'Training;Degradation;Error analysis;Noise reduction;Speech enhancement;Distortion;Generative adversarial networks;Automatic speech recognition;deep learning;generative adversarial network;speech distortion;speech enhancement', 'abstract': 'Speech enhancement is an essential preprocessing stage for automatic speech recognition in noisy conditions; however, the distortion caused by the denoising process may lead to degradation in automatic speech recognition performance. This paper presents a deep learning-based speech enhancement architecture to overcome this issue by applying a second-stage network that deals with distortion noise. Moreover, a signal-to-noise ratio binary classifier is implemented to activate the speech enhancement network for intrusive noise environments only, which improves the overall performance. The proposed architecture outperforms powerful models in the literature, as it improves a challenging noisy speech test set by 0.8 and 5.9\\% improvement in the quality and intelligibility scores, respectively. Furthermore, the architecture improves the performance of automatic speech recognition with a 13.8\\% reduction in the word error rate at 0dB signal-to-noise ratio. Finally, the second-stage network was proven to improve the performance of first-stage speech enhancement models, not previously seen in the training process.', 'pages': '546-552', 'number': '', 'volume': '', 'year': '2023', 'title': 'Enhancing Automatic Speech Recognition Quality with a Second-Stage Speech Enhancement Generative Adversarial Network', 'booktitle': '2023 IEEE 35th International Conference on Tools with Artificial Intelligence (ICTAI)', 'author': 'Nossier, Soha A. and Wall, Julie and Moniri, Mansour and Glackin, Cornelius and Cannings, Nigel', 'ENTRYTYPE': 'inproceedings', 'ID': '10356488'}"
10708135,AI Pair Programming Acceptance: A Value-Based Approach with AHP Analysis,"Çaldağ, Murat Tahir",Çaldağ,10.1109/CoDIT62066.2024.10708135,2024,"2024 10th International Conference on Control, Decision and Information Technologies (CoDIT)","The emergence of Artificial Intelligence (AI) tools is transforming every aspect of life with new opportunities and risks. An impact of AI tools can be seen in AI pair programming which is defined as a generative and creative support tool with real-time interaction. The goal of this study is to explore the AI pair programming acceptance. To identify, describe, categorize, and rank the factors affecting the acceptance of AI pairs a literature review, a research model proposal based on an extension of the Value-based Adoption Model (VAM) framework, and an Analytic Hierarchy Process (AHP) analysis is conducted. The proposed model consists of six main factors and twenty-two sub-factors which are validated with an AHP analysis including eleven experts’ judgments. The findings presented the most essential factors as productivity, code accuracy, complexity, personal development, and innovativeness. The least significant factors were inspiration, motivation, intellectual property violation, AI interaction, and trust. This study provides insight to AI tool developers and producers in the context of programming on the key factors to consider for success.",Productivity;Analytical models;Codes;Accuracy;Intellectual property;Programming;Real-time systems;Complexity theory;Artificial intelligence;Software development management,"{'month': 'July', 'issn': '2576-3555', 'doi': '10.1109/CoDIT62066.2024.10708135', 'keywords': 'Productivity;Analytical models;Codes;Accuracy;Intellectual property;Programming;Real-time systems;Complexity theory;Artificial intelligence;Software development management', 'abstract': 'The emergence of Artificial Intelligence (AI) tools is transforming every aspect of life with new opportunities and risks. An impact of AI tools can be seen in AI pair programming which is defined as a generative and creative support tool with real-time interaction. The goal of this study is to explore the AI pair programming acceptance. To identify, describe, categorize, and rank the factors affecting the acceptance of AI pairs a literature review, a research model proposal based on an extension of the Value-based Adoption Model (VAM) framework, and an Analytic Hierarchy Process (AHP) analysis is conducted. The proposed model consists of six main factors and twenty-two sub-factors which are validated with an AHP analysis including eleven experts’ judgments. The findings presented the most essential factors as productivity, code accuracy, complexity, personal development, and innovativeness. The least significant factors were inspiration, motivation, intellectual property violation, AI interaction, and trust. This study provides insight to AI tool developers and producers in the context of programming on the key factors to consider for success.', 'pages': '556-561', 'number': '', 'volume': '', 'year': '2024', 'title': 'AI Pair Programming Acceptance: A Value-Based Approach with AHP Analysis', 'booktitle': '2024 10th International Conference on Control, Decision and Information Technologies (CoDIT)', 'author': 'Çaldağ, Murat Tahir', 'ENTRYTYPE': 'inproceedings', 'ID': '10708135'}"
11107113,"Denoising Probabilistic Diffusion Models, ResNet, and Class Activation Maps for Healthcare Imaging","Cosentino, Cristian and Defilippo, Annamaria and Guzzi, Pietro Hiram and Liò, Pietro and Iuliano, Antonella",Cosentino,10.1109/MetroLivEnv64961.2025.11107113,2025,2025 IEEE International Workshop on Metrology for Living Environment (MetroLivEnv),"Artificial Intelligence (AI) has the potential to enhance clinical practice by leveraging the vast amount of clinical data available today. In particular, biomedical imaging analysis plays a crucial role in healthcare, generating extensive datasets that can be used to study complex diseases, track their progression, and even predict their onset. This study focuses on an approach based on Denoising Diffusion Probabilistic Models (DDPMs), a type of generative model that utilizes a parameterized Markov chain and variational inference to generate synthetic samples that closely resemble real data. The model was trained on pneumonia images, producing high-quality synthetic samples to assess the performance of DDPMs by separately analyzing class 0 (healthy) and class 1 (pneumonia) cases. Furthermore, a ResNet-based convolutional neural network was employed for classification tasks to evaluate the effectiveness of DDPMs in generating synthetic images. The classification performance was compared using both real and synthetic images. Additionally, Class Activation Maps (CAM) were applied to provide visual explanations of the classification results, thereby enhancing the interpretability of our findings.",Visualization;Pneumonia;Noise reduction;Medical services;Diffusion models;Data models;Artificial intelligence;Synthetic data;Videos;Residual neural networks;DDPM;Healthcare images;Synthetic data;ResNet;CAM,"{'month': 'June', 'issn': '', 'doi': '10.1109/MetroLivEnv64961.2025.11107113', 'keywords': 'Visualization;Pneumonia;Noise reduction;Medical services;Diffusion models;Data models;Artificial intelligence;Synthetic data;Videos;Residual neural networks;DDPM;Healthcare images;Synthetic data;ResNet;CAM', 'abstract': 'Artificial Intelligence (AI) has the potential to enhance clinical practice by leveraging the vast amount of clinical data available today. In particular, biomedical imaging analysis plays a crucial role in healthcare, generating extensive datasets that can be used to study complex diseases, track their progression, and even predict their onset. This study focuses on an approach based on Denoising Diffusion Probabilistic Models (DDPMs), a type of generative model that utilizes a parameterized Markov chain and variational inference to generate synthetic samples that closely resemble real data. The model was trained on pneumonia images, producing high-quality synthetic samples to assess the performance of DDPMs by separately analyzing class 0 (healthy) and class 1 (pneumonia) cases. Furthermore, a ResNet-based convolutional neural network was employed for classification tasks to evaluate the effectiveness of DDPMs in generating synthetic images. The classification performance was compared using both real and synthetic images. Additionally, Class Activation Maps (CAM) were applied to provide visual explanations of the classification results, thereby enhancing the interpretability of our findings.', 'pages': '515-520', 'number': '', 'volume': '', 'year': '2025', 'title': 'Denoising Probabilistic Diffusion Models, ResNet, and Class Activation Maps for Healthcare Imaging', 'booktitle': '2025 IEEE International Workshop on Metrology for Living Environment (MetroLivEnv)', 'author': 'Cosentino, Cristian and Defilippo, Annamaria and Guzzi, Pietro Hiram and Liò, Pietro and Iuliano, Antonella', 'ENTRYTYPE': 'inproceedings', 'ID': '11107113'}"
9157564,A U-Net Based Discriminator for Generative Adversarial Networks,"Schönfeld, Edgar and Schiele, Bernt and Khoreva, Anna",Schönfeld,10.1109/CVPR42600.2020.00823,2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"Among the major remaining challenges for generative adversarial networks (GANs) is the capacity to synthesize globally and locally coherent images with object shapes and textures indistinguishable from real images. To target this issue we propose an alternative U-Net based discriminator architecture, borrowing the insights from the segmentation literature. The proposed U-Net based architecture allows to provide detailed per-pixel feedback to the generator while maintaining the global coherence of synthesized images, by providing the global image feedback as well. Empowered by the per-pixel response of the discriminator, we further propose a per-pixel consistency regularization technique based on the CutMix data augmentation, encouraging the U-Net discriminator to focus more on semantic and structural changes between real and fake images. This improves the U-Net discriminator training, further enhancing the quality of generated samples. The novel discriminator improves over the state of the art in terms of the standard distribution and image quality metrics, enabling the generator to synthesize images with varying structure, appearance and levels of detail, maintaining global and local realism. Compared to the BigGAN baseline, we achieve an average improvement of 2.7 FID points across FFHQ, CelebA, and the proposed COCO-Animals dataset.",Generators;Gallium nitride;Decoding;Training;Generative adversarial networks;Image segmentation;Computer architecture,"{'month': 'June', 'issn': '2575-7075', 'doi': '10.1109/CVPR42600.2020.00823', 'keywords': 'Generators;Gallium nitride;Decoding;Training;Generative adversarial networks;Image segmentation;Computer architecture', 'abstract': 'Among the major remaining challenges for generative adversarial networks (GANs) is the capacity to synthesize globally and locally coherent images with object shapes and textures indistinguishable from real images. To target this issue we propose an alternative U-Net based discriminator architecture, borrowing the insights from the segmentation literature. The proposed U-Net based architecture allows to provide detailed per-pixel feedback to the generator while maintaining the global coherence of synthesized images, by providing the global image feedback as well. Empowered by the per-pixel response of the discriminator, we further propose a per-pixel consistency regularization technique based on the CutMix data augmentation, encouraging the U-Net discriminator to focus more on semantic and structural changes between real and fake images. This improves the U-Net discriminator training, further enhancing the quality of generated samples. The novel discriminator improves over the state of the art in terms of the standard distribution and image quality metrics, enabling the generator to synthesize images with varying structure, appearance and levels of detail, maintaining global and local realism. Compared to the BigGAN baseline, we achieve an average improvement of 2.7 FID points across FFHQ, CelebA, and the proposed COCO-Animals dataset.', 'pages': '8204-8213', 'number': '', 'volume': '', 'year': '2020', 'title': 'A U-Net Based Discriminator for Generative Adversarial Networks', 'booktitle': '2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)', 'author': 'Schönfeld, Edgar and Schiele, Bernt and Khoreva, Anna', 'ENTRYTYPE': 'inproceedings', 'ID': '9157564'}"
10614268,7 At Least Lawyers Have Plenty of Work,"Ojanperä, Tero",Ojanperä,,2024,AI Revolution: Mastering AI for Personal and Organizational Growth,"""The AI Revolution"" is a practical guide to using new AI tools, such as ChatGPT, DALLE and Midjourney. Learn how to multiply your productivity by guiding or prompting AI in various ways. The book also introduces Microsoft Copilot, Google Bard, and Adobe Photoshop Generative Fill, among other new applications. ChatGPT reached a hundred million users in just two months after its release, faster than any other application before. This marked the advent of the generative AI era. Generative AI models generate text, images, music, videos, and even 3D models in ways previously thought impossible for machines. The book explains in an understandable manner how these AI models work. The book provides examples of how AI increases productivity, which professions are changing or disappearing, and how job markets will evolve in the coming years. With this book, you'll learn to recognize the opportunities and risks AI offers. Understand what this change demands from individuals and companies and what strategic skills are required. The book also covers legal questions caused by generative AI, like copyrights, data protection, and AI regulation. It also ponders societal impacts. AI produces content, thus influencing language, culture, and even worldviews. Therefore, it's crucial to understand by whom and how AI is trained. The AI revolution started by ChatGPT is just the beginning. This handbook is for you if you want to keep up with the rapid development of AI.",,"{'url': 'https://ieeexplore-ieee-org.crai.referencistas.com/document/10614268', 'isbn': '9788770042314', 'publisher': 'River Publishers', 'issn': '', 'doi': '', 'keywords': '', 'abstract': '""The AI Revolution"" is a practical guide to using new AI tools, such as ChatGPT, DALLE and Midjourney. Learn how to multiply your productivity by guiding or prompting AI in various ways. The book also introduces Microsoft Copilot, Google Bard, and Adobe Photoshop Generative Fill, among other new applications. ChatGPT reached a hundred million users in just two months after its release, faster than any other application before. This marked the advent of the generative AI era. Generative AI models generate text, images, music, videos, and even 3D models in ways previously thought impossible for machines. The book explains in an understandable manner how these AI models work. The book provides examples of how AI increases productivity, which professions are changing or disappearing, and how job markets will evolve in the coming years. With this book, you\'ll learn to recognize the opportunities and risks AI offers. Understand what this change demands from individuals and companies and what strategic skills are required. The book also covers legal questions caused by generative AI, like copyrights, data protection, and AI regulation. It also ponders societal impacts. AI produces content, thus influencing language, culture, and even worldviews. Therefore, it\'s crucial to understand by whom and how AI is trained. The AI revolution started by ChatGPT is just the beginning. This handbook is for you if you want to keep up with the rapid development of AI.', 'pages': '107-124', 'number': '', 'volume': '', 'year': '2024', 'title': '7 At Least Lawyers Have Plenty of Work', 'booktitle': 'AI Revolution: Mastering AI for Personal and Organizational Growth', 'author': 'Ojanperä, Tero', 'ENTRYTYPE': 'inbook', 'ID': '10614268'}"
10707879,Cultural Relevance Index: Measuring Cultural Relevance in AI-Generated Images,"ELsharif, Wala and Agus, Marco and Alzubaidi, Mahmoud and She, James",ELsharif,10.1109/MIPR62202.2024.00071,2024,2024 IEEE 7th International Conference on Multimedia Information Processing and Retrieval (MIPR),"This paper introduces the Cultural Relevance Index (CRI), a metric designed to evaluate and quantify the cultural relevance of AI-generated images. Leveraging the detection capabilities of GPT technology and our proposed mathematical formulas aligned with human perception, CRI assesses the extent to which the content of an image is relevant to a particular culture such as Arabic, a capability not commonly found in many AI technologies. Through rigorous validation, including comparison with human judgments and a mathematical baseline, with focus on Arabic culture, CRI demonstrates a compelling similarity (around 95\%) with human judgement, outperforming a quantified baseline metric (over 28\%). CRI also succeeds to matches human judgment in comparing cultural images 100\% of the time.",Measurement;Information processing;Cultural differences;Indexes;Artificial intelligence;Cultural Relevance;AI-generated images;Human assessment;Arabic culture,"{'month': 'Aug', 'issn': '2770-4319', 'doi': '10.1109/MIPR62202.2024.00071', 'keywords': 'Measurement;Information processing;Cultural differences;Indexes;Artificial intelligence;Cultural Relevance;AI-generated images;Human assessment;Arabic culture', 'abstract': 'This paper introduces the Cultural Relevance Index (CRI), a metric designed to evaluate and quantify the cultural relevance of AI-generated images. Leveraging the detection capabilities of GPT technology and our proposed mathematical formulas aligned with human perception, CRI assesses the extent to which the content of an image is relevant to a particular culture such as Arabic, a capability not commonly found in many AI technologies. Through rigorous validation, including comparison with human judgments and a mathematical baseline, with focus on Arabic culture, CRI demonstrates a compelling similarity (around 95\\%) with human judgement, outperforming a quantified baseline metric (over 28\\%). CRI also succeeds to matches human judgment in comparing cultural images 100\\% of the time.', 'pages': '410-416', 'number': '', 'volume': '', 'year': '2024', 'title': 'Cultural Relevance Index: Measuring Cultural Relevance in AI-Generated Images', 'booktitle': '2024 IEEE 7th International Conference on Multimedia Information Processing and Retrieval (MIPR)', 'author': 'ELsharif, Wala and Agus, Marco and Alzubaidi, Mahmoud and She, James', 'ENTRYTYPE': 'inproceedings', 'ID': '10707879'}"
9967399,Cognitive AI for Mitigation of Misinformation in Online Social Networks,"V, Indu and Thampi, Sabu M.",V,10.1109/MITP.2022.3168790,2022,IT Professional,"Misinformation propagation in social networks has emerged as a crucial problem that needs to be attended with prime importance. Despite the existence of several fact-checking mechanisms and misinformation detection tools, users of social media platforms continue to be the victims of misinformation propagation. This is because human cognition is a strong factor that drives users in consuming and spreading misinformation. This article highlights the significance of cognitive psychology in misinformation propagation analysis and summarizes the challenges faced by current misinformation detection mechanisms. The study shows that there is an immediate requirement for efficient mechanisms combining AI and cognitive psychology that can support humans in making judgements regarding the information appearing on social networks. A cognitive AI framework is proposed that can augment humans’ capability in assessing the veracity of the information online and reinforce positive information sharing behavior in individuals thereby reducing the spread of misinformation.",Social networking (online);Psychology;Information sharing;Feature extraction;Cognition;Fake news;Artificial intelligence,"{'month': 'Sep.', 'issn': '1941-045X', 'doi': '10.1109/MITP.2022.3168790', 'keywords': 'Social networking (online);Psychology;Information sharing;Feature extraction;Cognition;Fake news;Artificial intelligence', 'abstract': 'Misinformation propagation in social networks has emerged as a crucial problem that needs to be attended with prime importance. Despite the existence of several fact-checking mechanisms and misinformation detection tools, users of social media platforms continue to be the victims of misinformation propagation. This is because human cognition is a strong factor that drives users in consuming and spreading misinformation. This article highlights the significance of cognitive psychology in misinformation propagation analysis and summarizes the challenges faced by current misinformation detection mechanisms. The study shows that there is an immediate requirement for efficient mechanisms combining AI and cognitive psychology that can support humans in making judgements regarding the information appearing on social networks. A cognitive AI framework is proposed that can augment humans’ capability in assessing the veracity of the information online and reinforce positive information sharing behavior in individuals thereby reducing the spread of misinformation.', 'pages': '37-45', 'number': '5', 'volume': '24', 'year': '2022', 'title': 'Cognitive AI for Mitigation of Misinformation in Online Social Networks', 'journal': 'IT Professional', 'author': 'V, Indu and Thampi, Sabu M.', 'ENTRYTYPE': 'article', 'ID': '9967399'}"
11098641,6G AI-Driven Air Interface — Hexa-X-II View,"Farhadi, Hamed and Banerjee, Bitan and Berkvens, Rafael and Bhat, Nabeel Nisar and Bodji, Emmanuelle and Dampahalage, Dilin and Eldeeb, Eslam and Famaey, Jeroen and Fettweis, Gerhard P. and Jeong, Jaeseong and Korpi, Dani and Kumar, Siddhartha and Lebrun, Yann and Legouable, Rodolphe and Mateos-Ramos, José Miguel and Mahmood, Nurul Huda and Moghaddam, Mohammad Hossein and Nimr, Ahmad and Rajatheva, Nandana and Rajapaksha, Nuwanthika and Stavridis, Athanasios and Svensson, Tommy and Yu, Han and Wilhelmsson, Leif and Wymeersch, Henk",Farhadi,10.1109/MCOM.001.2400394,2025,IEEE Communications Magazine,"This article presents the European 6G Flagship project Hexa-X-II's view on 6G AI-driven air interface. It outlines motivations for AI in the physical layer, selected applications of AI for communication and sensing, their achieved performance, and the challenges to be addressed. The article also provides an overview of the relevant standardization activities.",Artificial intelligence;6G mobile communication;Hardware;Parity check codes;Communication channels;Receivers;Estimation;Decoding;Codes;Classification tree analysis,"{'month': '', 'issn': '1558-1896', 'doi': '10.1109/MCOM.001.2400394', 'keywords': 'Artificial intelligence;6G mobile communication;Hardware;Parity check codes;Communication channels;Receivers;Estimation;Decoding;Codes;Classification tree analysis', 'abstract': ""This article presents the European 6G Flagship project Hexa-X-II's view on 6G AI-driven air interface. It outlines motivations for AI in the physical layer, selected applications of AI for communication and sensing, their achieved performance, and the challenges to be addressed. The article also provides an overview of the relevant standardization activities."", 'pages': '1-9', 'number': '', 'volume': '', 'year': '2025', 'title': '6G AI-Driven Air Interface — Hexa-X-II View', 'journal': 'IEEE Communications Magazine', 'author': 'Farhadi, Hamed and Banerjee, Bitan and Berkvens, Rafael and Bhat, Nabeel Nisar and Bodji, Emmanuelle and Dampahalage, Dilin and Eldeeb, Eslam and Famaey, Jeroen and Fettweis, Gerhard P. and Jeong, Jaeseong and Korpi, Dani and Kumar, Siddhartha and Lebrun, Yann and Legouable, Rodolphe and Mateos-Ramos, José Miguel and Mahmood, Nurul Huda and Moghaddam, Mohammad Hossein and Nimr, Ahmad and Rajatheva, Nandana and Rajapaksha, Nuwanthika and Stavridis, Athanasios and Svensson, Tommy and Yu, Han and Wilhelmsson, Leif and Wymeersch, Henk', 'ENTRYTYPE': 'article', 'ID': '11098641'}"
11019764,Bran AI: An Integrated Approach to Heart Attack Risk Prediction Using Retinal Fundus Image,"M, Hariharasuthan and S, Mohanraj and K, Shamridha and Madhavi, S.",M,10.1109/ICCCT63501.2025.11019764,2025,2025 International Conference on Computing and Communication Technologies (ICCCT),"This study introduces Bran AI, an advanced framework for heart attack prediction, using retinal fundus images integrated with clinical data to assess cardiovascular risk. The system's core employs a neural network-based segmentation model for detailed image analysis, enabling the extraction of critical biomarkers. Bran AI combines this with BERT for medical metadata processing, capturing relevant factors such as age, sex, and diabetes status to enhance prediction accuracy. The model architecture integrates dense layers and attention mechanisms to optimize prediction, while explainability tools such as SHAP and Grad-CAM clarify the model's decision-making process for clinical applicability. Recent studies have demonstrated the efficacy of retinal image analysis in cardiovascular risk prediction, underscoring its potential as a diagnostic tool. Bran AI achieves a high predictive performance with 92.4\% accuracy, demonstrated by AUC-ROC and F1-score metrics. This innovative approach positions Bran AI as a valuable tool for healthcare professionals in cardiovascular diagnostics, fostering early detection and personalized management of heart attack risks.",Accuracy;Image analysis;Biological system modeling;Medical services;Cardiac arrest;Predictive models;Biomarkers;Retina;Artificial intelligence;Standards;BERT;Retinal Fundus Imaging;Biomarker;SHAP,"{'month': 'April', 'issn': '2995-3197', 'doi': '10.1109/ICCCT63501.2025.11019764', 'keywords': 'Accuracy;Image analysis;Biological system modeling;Medical services;Cardiac arrest;Predictive models;Biomarkers;Retina;Artificial intelligence;Standards;BERT;Retinal Fundus Imaging;Biomarker;SHAP', 'abstract': ""This study introduces Bran AI, an advanced framework for heart attack prediction, using retinal fundus images integrated with clinical data to assess cardiovascular risk. The system's core employs a neural network-based segmentation model for detailed image analysis, enabling the extraction of critical biomarkers. Bran AI combines this with BERT for medical metadata processing, capturing relevant factors such as age, sex, and diabetes status to enhance prediction accuracy. The model architecture integrates dense layers and attention mechanisms to optimize prediction, while explainability tools such as SHAP and Grad-CAM clarify the model's decision-making process for clinical applicability. Recent studies have demonstrated the efficacy of retinal image analysis in cardiovascular risk prediction, underscoring its potential as a diagnostic tool. Bran AI achieves a high predictive performance with 92.4\\% accuracy, demonstrated by AUC-ROC and F1-score metrics. This innovative approach positions Bran AI as a valuable tool for healthcare professionals in cardiovascular diagnostics, fostering early detection and personalized management of heart attack risks."", 'pages': '1-6', 'number': '', 'volume': '', 'year': '2025', 'title': 'Bran AI: An Integrated Approach to Heart Attack Risk Prediction Using Retinal Fundus Image', 'booktitle': '2025 International Conference on Computing and Communication Technologies (ICCCT)', 'author': 'M, Hariharasuthan and S, Mohanraj and K, Shamridha and Madhavi, S.', 'ENTRYTYPE': 'inproceedings', 'ID': '11019764'}"
10546022,Designing Explainable Defenses Against Sophisticated Adversarial Attacks,"Lakhanpal, Sorabh and Devi, KSKN Venkata Ramana and K, Aravinda and Jain, Sachindra Kumar and Adnan, Myasar Mundher and Kumar, Ashwani",Lakhanpal,10.1109/CSNT60213.2024.10546022,2024,2024 IEEE 13th International Conference on Communication Systems and Network Technologies (CSNT),"The requirement for strong defenses against complex adversarial assaults is increasing fast in the constantly evolving AI ecosystem. Considering this need, we put up the Explain Defend Net architecture, a unique adversarial defensive mechanism. This framework utilizes state-of-the-art methods to improve the robustness, openness, and flexibility of models. To protect the model from external interference, the Robust Feature Recalibrator (RFR) selectively adjusts the calibration of input features. The Explain Intercept Layer (EIL) offers transparency by offering interpretable insights into the decision-making process, enhancing human comprehension. The model can adapt to new forms of adversarial attack because of the dynamic adaptability guaranteed by Adaptive Reinforce Guard (ARG). With its comprehensive defensive strategy, Explain Defend Net is designed to outperform more conventional approaches. The suggested framework is put through rigorous testing, and the results indicate that it outperforms six established approaches in a wide range of categories. The findings show that Explain Defend Net regularly outperforms conventional techniques, proving its efficacy in protecting AI systems from malicious actors. Explain Defend Net is state-of-the-art in the field of adversarial defense because of its novel mix of recalibration, interpretability, and adaptive reinforcement.",Adaptation models;Biological system modeling;Decision making;Interference;Robustness;Security;Artificial intelligence;Adversarial attacks;AI security;Adaptive defense;Deep learning;Explainability;Human-in-the-loop;Interpretability;Machine learning;Robustness;Selective recalibration;Sophisticated,"{'month': 'April', 'issn': '2473-5655', 'doi': '10.1109/CSNT60213.2024.10546022', 'keywords': 'Adaptation models;Biological system modeling;Decision making;Interference;Robustness;Security;Artificial intelligence;Adversarial attacks;AI security;Adaptive defense;Deep learning;Explainability;Human-in-the-loop;Interpretability;Machine learning;Robustness;Selective recalibration;Sophisticated', 'abstract': 'The requirement for strong defenses against complex adversarial assaults is increasing fast in the constantly evolving AI ecosystem. Considering this need, we put up the Explain Defend Net architecture, a unique adversarial defensive mechanism. This framework utilizes state-of-the-art methods to improve the robustness, openness, and flexibility of models. To protect the model from external interference, the Robust Feature Recalibrator (RFR) selectively adjusts the calibration of input features. The Explain Intercept Layer (EIL) offers transparency by offering interpretable insights into the decision-making process, enhancing human comprehension. The model can adapt to new forms of adversarial attack because of the dynamic adaptability guaranteed by Adaptive Reinforce Guard (ARG). With its comprehensive defensive strategy, Explain Defend Net is designed to outperform more conventional approaches. The suggested framework is put through rigorous testing, and the results indicate that it outperforms six established approaches in a wide range of categories. The findings show that Explain Defend Net regularly outperforms conventional techniques, proving its efficacy in protecting AI systems from malicious actors. Explain Defend Net is state-of-the-art in the field of adversarial defense because of its novel mix of recalibration, interpretability, and adaptive reinforcement.', 'pages': '280-285', 'number': '', 'volume': '', 'year': '2024', 'title': 'Designing Explainable Defenses Against Sophisticated Adversarial Attacks', 'booktitle': '2024 IEEE 13th International Conference on Communication Systems and Network Technologies (CSNT)', 'author': 'Lakhanpal, Sorabh and Devi, KSKN Venkata Ramana and K, Aravinda and Jain, Sachindra Kumar and Adnan, Myasar Mundher and Kumar, Ashwani', 'ENTRYTYPE': 'inproceedings', 'ID': '10546022'}"
9106415,"Visual Perception Enabled Industry Intelligence: State of the Art, Challenges and Prospects","Yang, Jiachen and Wang, Chenguang and Jiang, Bin and Song, Houbing and Meng, Qinggang",Yang,10.1109/TII.2020.2998818,2021,IEEE Transactions on Industrial Informatics,"Visual perception refers to the process of organizing, identifying, and interpreting visual information in environmental awareness and understanding. With the rapid progress of multimedia acquisition technology, research on visual perception has been a hot topic in the academical field and industrial applications. Especially after the introduction of artificial intelligence theory, intelligent visual perception has been widely used to promote the development of industrial production towards intelligence. In this article, we review the previous research and application of visual perception in different industrial fields such as product surface defect detection, intelligent agricultural production, intelligent driving, image synthesis, and event reconstruction. The applications basically cover most of the intelligent visual perception processing technologies. Through this survey, it will provide a comprehensive reference for research on this direction. Finally, this article also summarizes the current challenges of visual perception and predicts its future development trends.",Visual perception;Fabrics;Production;Surface treatment;Informatics;Machine vision;Surface morphology;Artificial intelligence;industrial application;visual perception,"{'month': 'March', 'issn': '1941-0050', 'doi': '10.1109/TII.2020.2998818', 'keywords': 'Visual perception;Fabrics;Production;Surface treatment;Informatics;Machine vision;Surface morphology;Artificial intelligence;industrial application;visual perception', 'abstract': 'Visual perception refers to the process of organizing, identifying, and interpreting visual information in environmental awareness and understanding. With the rapid progress of multimedia acquisition technology, research on visual perception has been a hot topic in the academical field and industrial applications. Especially after the introduction of artificial intelligence theory, intelligent visual perception has been widely used to promote the development of industrial production towards intelligence. In this article, we review the previous research and application of visual perception in different industrial fields such as product surface defect detection, intelligent agricultural production, intelligent driving, image synthesis, and event reconstruction. The applications basically cover most of the intelligent visual perception processing technologies. Through this survey, it will provide a comprehensive reference for research on this direction. Finally, this article also summarizes the current challenges of visual perception and predicts its future development trends.', 'pages': '2204-2219', 'number': '3', 'volume': '17', 'year': '2021', 'title': 'Visual Perception Enabled Industry Intelligence: State of the Art, Challenges and Prospects', 'journal': 'IEEE Transactions on Industrial Informatics', 'author': 'Yang, Jiachen and Wang, Chenguang and Jiang, Bin and Song, Houbing and Meng, Qinggang', 'ENTRYTYPE': 'article', 'ID': '9106415'}"
8932813,BALD-VAE: Generative Active Learning based on the Uncertainties of Both Labeled and Unlabeled Data,"Lee, Sun-Kyung and Kim, Jong-Hwan",Lee,10.1109/RITAPP.2019.8932813,2019,2019 7th International Conference on Robot Intelligence Technology and Applications (RiTA),The following topics are dealt with: learning (artificial intelligence); mobile robots; robot vision; object detection; path planning; cameras; image classification; emotion recognition; SLAM (robots); feature extraction.,Uncertainty;Data models;Measurement uncertainty;Bayes methods;Entropy;Generative adversarial networks;Labeling,"{'month': 'Nov', 'issn': '', 'doi': '10.1109/RITAPP.2019.8932813', 'keywords': 'Uncertainty;Data models;Measurement uncertainty;Bayes methods;Entropy;Generative adversarial networks;Labeling', 'abstract': 'The following topics are dealt with: learning (artificial intelligence); mobile robots; robot vision; object detection; path planning; cameras; image classification; emotion recognition; SLAM (robots); feature extraction.', 'pages': '6-11', 'number': '', 'volume': '', 'year': '2019', 'title': 'BALD-VAE: Generative Active Learning based on the Uncertainties of Both Labeled and Unlabeled Data', 'booktitle': '2019 7th International Conference on Robot Intelligence Technology and Applications (RiTA)', 'author': 'Lee, Sun-Kyung and Kim, Jong-Hwan', 'ENTRYTYPE': 'inproceedings', 'ID': '8932813'}"
10953204,Augmented Computational Design,"Nourian, Pirouz and Azadi, Shervin and Uijtendaal, Roy and Bai, Nan",Nourian,10.1002/9781394172092.ch1,2024,"Artificial Intelligence in Performance-Driven Design: Theories, Methods, and Tools","Summary <p>This chapter presents methodological reflections on the necessity and utility of artificial intelligence (AI) in generative design. Specifically, the chapter discusses how generative design processes can be augmented by AI to deliver in terms of a few outcomes of interest or performance indicators while dealing with hundreds or thousands of small decisions. The core of the performance\&\#x2010;based generative design paradigm is about making statistical or simulation\&\#x2010;driven associations between these choices and their consequences for mapping and navigating such a complex decision space. This chapter will discuss promising directions in AI for augmenting decision\&\#x2010;making processes in architectural design for mapping and navigating complex design spaces.</p>",Artificial intelligence;Mathematical models;History;Decision making;Data models;Probabilistic logic;Predictive models;Physics;Navigation;Manifolds,"{'url': 'https://ieeexplore-ieee-org.crai.referencistas.com/document/10953204', 'isbn': '9781394172085', 'publisher': 'Wiley', 'issn': '', 'doi': '10.1002/9781394172092.ch1', 'keywords': 'Artificial intelligence;Mathematical models;History;Decision making;Data models;Probabilistic logic;Predictive models;Physics;Navigation;Manifolds', 'abstract': 'Summary <p>This chapter presents methodological reflections on the necessity and utility of artificial intelligence (AI) in generative design. Specifically, the chapter discusses how generative design processes can be augmented by AI to deliver in terms of a few outcomes of interest or performance indicators while dealing with hundreds or thousands of small decisions. The core of the performance\\&\\#x2010;based generative design paradigm is about making statistical or simulation\\&\\#x2010;driven associations between these choices and their consequences for mapping and navigating such a complex decision space. This chapter will discuss promising directions in AI for augmenting decision\\&\\#x2010;making processes in architectural design for mapping and navigating complex design spaces.</p>', 'pages': '1-30', 'number': '', 'volume': '', 'year': '2024', 'title': 'Augmented Computational Design', 'booktitle': 'Artificial Intelligence in Performance-Driven Design: Theories, Methods, and Tools', 'author': 'Nourian, Pirouz and Azadi, Shervin and Uijtendaal, Roy and Bai, Nan', 'ENTRYTYPE': 'inbook', 'ID': '10953204'}"
8476290,SCGAN: Disentangled Representation Learning by Adding Similarity Constraint on Generative Adversarial Nets,"Li, Xiaoqiang and Chen, Liangbo and Wang, Lu and Wu, Pin and Tong, Weiqin",Li,10.1109/ACCESS.2018.2872695,2019,IEEE Access,"We proposed a novel generative adversarial net called similarity constraint generative adversarial network (SCGAN), which is capable of learning the disentangled representation in a completely unsupervised manner. Inspired by the smoothness assumption and our assumption on the content and the representation of images, we design an effective similarity constraint. SCGAN can disentangle interpretable representations by adding this similarity constraint between conditions and synthetic images. In fact, similarity constraint works as a tutor to instruct generator network to comprehend the difference of representations based on conditions. SCGAN successfully distinguishes different representations on a number of datasets. Specifically, SCGAN captures digit type on MNIST, clothing type on Fashion-MNIST, lighting on SVHN, and object size on CIFAR10. On the CelebA dataset, SCGAN captures more semantic representations, e.g., poses, emotions, and hair styles. Experiments show that SCGAN is comparable with InfoGAN (another generative adversarial net disentangles interpretable representations on these datasets unsupervisedly) on disentangled representation learning. Code is available at https://github.com/gauss-clb/SCGAN.",Gallium nitride;Generative adversarial networks;Generators;Mutual information;Machine learning;Face;Clamps;Generative adversarial nets;representation learning;unsupervised learning,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2018.2872695', 'keywords': 'Gallium nitride;Generative adversarial networks;Generators;Mutual information;Machine learning;Face;Clamps;Generative adversarial nets;representation learning;unsupervised learning', 'abstract': 'We proposed a novel generative adversarial net called similarity constraint generative adversarial network (SCGAN), which is capable of learning the disentangled representation in a completely unsupervised manner. Inspired by the smoothness assumption and our assumption on the content and the representation of images, we design an effective similarity constraint. SCGAN can disentangle interpretable representations by adding this similarity constraint between conditions and synthetic images. In fact, similarity constraint works as a tutor to instruct generator network to comprehend the difference of representations based on conditions. SCGAN successfully distinguishes different representations on a number of datasets. Specifically, SCGAN captures digit type on MNIST, clothing type on Fashion-MNIST, lighting on SVHN, and object size on CIFAR10. On the CelebA dataset, SCGAN captures more semantic representations, e.g., poses, emotions, and hair styles. Experiments show that SCGAN is comparable with InfoGAN (another generative adversarial net disentangles interpretable representations on these datasets unsupervisedly) on disentangled representation learning. Code is available at https://github.com/gauss-clb/SCGAN.', 'pages': '147928-147938', 'number': '', 'volume': '7', 'year': '2019', 'title': 'SCGAN: Disentangled Representation Learning by Adding Similarity Constraint on Generative Adversarial Nets', 'journal': 'IEEE Access', 'author': 'Li, Xiaoqiang and Chen, Liangbo and Wang, Lu and Wu, Pin and Tong, Weiqin', 'ENTRYTYPE': 'article', 'ID': '8476290'}"
10113627,Stabilizing and Improving Training of Generative Adversarial Networks Through Identity Blocks and Modified Loss Function,"Fathallah, Mohamed and Sakr, Mohamed and Eletriby, Sherif",Fathallah,10.1109/ACCESS.2023.3272032,2023,IEEE Access,"Generative adversarial networks (GANs) are a powerful tool for synthesizing realistic images, but they can be difficult to train and are prone to instability and mode collapse. This paper proposes a new model called Identity Generative Adversarial Network (IGAN) that addresses these issues. This model is based on three modifications to the baseline deep convolutional generative adversarial network (DCGAN). The first change is to add a non-linear identity block to the architecture. This will make it easier for the model to fit complex data types and cut down on the time it takes to train. The second change is to smooth out the standard GAN loss function by using a modified loss function and label smoothing. The third and final change is to use minibatch training to let the model use other examples from the same minibatch as side information to improve the quality and variety of generated images. These changes help to stabilize the training process and improve the model’s performance. The performance of the GAN models is compared using the inception score (IS) and the Fréchet inception distance (FID), which are widely used metrics for evaluating the quality and diversity of generated images. The effectiveness of our approach was tested by comparing an IGAN model with other GAN models on the CelebA and stacked MNIST datasets. Results show that IGAN outperforms all the other models, achieving an IS of 13.95 and an FID of 43.71 after traning for 200 epochs. In addition to demonstrating the improvement in the performance of the IGAN, the instabilities, diversity, and fidelity of the models were investigated. The results showed that the IGAN was able to converge to a distribution of the real data more quickly. Furthermore, the experiments revealed that IGAN is capable of producing more stable and high-quality images. This suggests that IGAN is a promising approach for improving the training and performance of GANs and may have a range of applications in image synthesis and other areas.",Training;Generators;Generative adversarial networks;Smoothing methods;Data models;Standards;Optimization;Generative adversarial network;deep learning;mode collapse;label smoothing;identity block,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2023.3272032', 'keywords': 'Training;Generators;Generative adversarial networks;Smoothing methods;Data models;Standards;Optimization;Generative adversarial network;deep learning;mode collapse;label smoothing;identity block', 'abstract': 'Generative adversarial networks (GANs) are a powerful tool for synthesizing realistic images, but they can be difficult to train and are prone to instability and mode collapse. This paper proposes a new model called Identity Generative Adversarial Network (IGAN) that addresses these issues. This model is based on three modifications to the baseline deep convolutional generative adversarial network (DCGAN). The first change is to add a non-linear identity block to the architecture. This will make it easier for the model to fit complex data types and cut down on the time it takes to train. The second change is to smooth out the standard GAN loss function by using a modified loss function and label smoothing. The third and final change is to use minibatch training to let the model use other examples from the same minibatch as side information to improve the quality and variety of generated images. These changes help to stabilize the training process and improve the model’s performance. The performance of the GAN models is compared using the inception score (IS) and the Fréchet inception distance (FID), which are widely used metrics for evaluating the quality and diversity of generated images. The effectiveness of our approach was tested by comparing an IGAN model with other GAN models on the CelebA and stacked MNIST datasets. Results show that IGAN outperforms all the other models, achieving an IS of 13.95 and an FID of 43.71 after traning for 200 epochs. In addition to demonstrating the improvement in the performance of the IGAN, the instabilities, diversity, and fidelity of the models were investigated. The results showed that the IGAN was able to converge to a distribution of the real data more quickly. Furthermore, the experiments revealed that IGAN is capable of producing more stable and high-quality images. This suggests that IGAN is a promising approach for improving the training and performance of GANs and may have a range of applications in image synthesis and other areas.', 'pages': '43276-43285', 'number': '', 'volume': '11', 'year': '2023', 'title': 'Stabilizing and Improving Training of Generative Adversarial Networks Through Identity Blocks and Modified Loss Function', 'journal': 'IEEE Access', 'author': 'Fathallah, Mohamed and Sakr, Mohamed and Eletriby, Sherif', 'ENTRYTYPE': 'article', 'ID': '10113627'}"
11098884,CRGAN: A Context-Aware Clothing Design and Recommendation System for Young Sri Lankan Females Using Generative Adversarial Networks,"Pathirana, Nethmi and Imtiaz, Azma and Saheel, Shakir and Karunanayaka, Kasun and David Cheok, Adrian",Pathirana,10.1109/ACCESS.2025.3593498,2025,IEEE Access,"The fashion industry is constantly undergoing notable transformations fueled by advancements in technology, particularly AI and ML. This shift is driven by the desire for personalized clothing recommendations, especially among young females, who seek tailored suggestions based on their preferences, climate, and style. While traditional recommendation systems rely on existing databases to suggest predefined outfit options, this research takes a novel approach by integrating generative modeling techniques to create unique outfit designs based on contextual factors. The proposed system leverages a Conditional Generative Adversarial Network (GAN) model, trained on three predefined parameters–attire type, temperature conditions, and the user’s skin tone. This innovative integration of context-aware features marks a significant advancement over prior research, which has largely neglected these combined parameters, particularly in the context of cultural demographics. Targeting young Sri Lankan females, this study fills a critical gap in generative clothing systems by addressing the unique cultural and environmental needs of this population. Experiment results on a labeled dataset demonstrate the model’s ability to generate accurate and personalized outputs based on up to three input parameters. Results are verified through quantitative model evaluations and user studies, underscoring the system’s potential to redefine personalized fashion recommendations.",Training;Generative adversarial networks;Data models;Image synthesis;Clothing;Cultural differences;Adaptation models;Recommender systems;Meteorology;Deep learning;Context-aware recommendation;deep learning;generative adversarial networks;image generation,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2025.3593498', 'keywords': 'Training;Generative adversarial networks;Data models;Image synthesis;Clothing;Cultural differences;Adaptation models;Recommender systems;Meteorology;Deep learning;Context-aware recommendation;deep learning;generative adversarial networks;image generation', 'abstract': 'The fashion industry is constantly undergoing notable transformations fueled by advancements in technology, particularly AI and ML. This shift is driven by the desire for personalized clothing recommendations, especially among young females, who seek tailored suggestions based on their preferences, climate, and style. While traditional recommendation systems rely on existing databases to suggest predefined outfit options, this research takes a novel approach by integrating generative modeling techniques to create unique outfit designs based on contextual factors. The proposed system leverages a Conditional Generative Adversarial Network (GAN) model, trained on three predefined parameters–attire type, temperature conditions, and the user’s skin tone. This innovative integration of context-aware features marks a significant advancement over prior research, which has largely neglected these combined parameters, particularly in the context of cultural demographics. Targeting young Sri Lankan females, this study fills a critical gap in generative clothing systems by addressing the unique cultural and environmental needs of this population. Experiment results on a labeled dataset demonstrate the model’s ability to generate accurate and personalized outputs based on up to three input parameters. Results are verified through quantitative model evaluations and user studies, underscoring the system’s potential to redefine personalized fashion recommendations.', 'pages': '135776-135790', 'number': '', 'volume': '13', 'year': '2025', 'title': 'CRGAN: A Context-Aware Clothing Design and Recommendation System for Young Sri Lankan Females Using Generative Adversarial Networks', 'journal': 'IEEE Access', 'author': 'Pathirana, Nethmi and Imtiaz, Azma and Saheel, Shakir and Karunanayaka, Kasun and David Cheok, Adrian', 'ENTRYTYPE': 'article', 'ID': '11098884'}"
10025749,PTcomp: Post-Training Compression Technique for Generative Adversarial Networks,"Tantawy, Dina and Zahran, Mohamed and Wassal, Amr G.",Tantawy,10.1109/ACCESS.2023.3239786,2023,IEEE Access,"In a time of virtual spaces, the usage of generative adversarial networks is inevitable. Generative adversarial networks (GANs) are generative deep-learning models that can generate realistic data. GANs have been used in many applications like text-to-image, image-to-image, image synthesis, speech synthesis, etc. Its power lies in the diversity and novelty of the generated data. Despite their advantages, GANs are resource-hungry. GANs’ output resolution and high correlation make it more challenging to compress and fit on edge-devices storage and power budget. Hence, traditional compression techniques are not the best fit to use with GANs. Additionally, GANs training instability adds another dimension of difficulty. Therefore, compression techniques that require retraining are challenging for GANs. In this paper, we developed a weight clustering technique to compress GANs without the need for retraining, hence the name post-training compression technique (PTcomp). We also proposed a clustered-based pruning which adds more savings. Experiments on Cyclegan, Deep convolution gan (DCGAN), and Stargan using several datasets show the superiority of our technique against traditional post-training quantization. Our technique provides a 4x to 8x compression ratio with comparable quality to original models and 14\% fewer mac operations due to pruning.",Generative adversarial networks;Generators;Quantization (signal);Training;Mathematical models;Clustering algorithms;Tensors;Compression;deep-learning;generative adversarial networks;post-training;clustering;pruning,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2023.3239786', 'keywords': 'Generative adversarial networks;Generators;Quantization (signal);Training;Mathematical models;Clustering algorithms;Tensors;Compression;deep-learning;generative adversarial networks;post-training;clustering;pruning', 'abstract': 'In a time of virtual spaces, the usage of generative adversarial networks is inevitable. Generative adversarial networks (GANs) are generative deep-learning models that can generate realistic data. GANs have been used in many applications like text-to-image, image-to-image, image synthesis, speech synthesis, etc. Its power lies in the diversity and novelty of the generated data. Despite their advantages, GANs are resource-hungry. GANs’ output resolution and high correlation make it more challenging to compress and fit on edge-devices storage and power budget. Hence, traditional compression techniques are not the best fit to use with GANs. Additionally, GANs training instability adds another dimension of difficulty. Therefore, compression techniques that require retraining are challenging for GANs. In this paper, we developed a weight clustering technique to compress GANs without the need for retraining, hence the name post-training compression technique (PTcomp). We also proposed a clustered-based pruning which adds more savings. Experiments on Cyclegan, Deep convolution gan (DCGAN), and Stargan using several datasets show the superiority of our technique against traditional post-training quantization. Our technique provides a 4x to 8x compression ratio with comparable quality to original models and 14\\% fewer mac operations due to pruning.', 'pages': '9763-9774', 'number': '', 'volume': '11', 'year': '2023', 'title': 'PTcomp: Post-Training Compression Technique for Generative Adversarial Networks', 'journal': 'IEEE Access', 'author': 'Tantawy, Dina and Zahran, Mohamed and Wassal, Amr G.', 'ENTRYTYPE': 'article', 'ID': '10025749'}"
10381195,Exploring the Factors and Influence Mechanisms of User Loyalty for Generative AI,"Zeng, Ang and Cheng, Xusen and Wang, Yajie",Zeng,10.1109/DSC59305.2023.00063,2023,2023 8th International Conference on Data Science in Cyberspace (DSC),"This article explores the antecedent variables and mechanisms of influence for user satisfaction with generative AI. The variables and models were extracted through a literature review, combining the technology acceptance model, self-determination theory, and anthropomorphism and perceptual intelligence as two domain-specific influences on generative AI. Data was collected by distributing a questionnaire to conduct an empirical study. The empirical results showed that perceived usefulness, perceived intelligence, and anthropomorphism have a significant positive impact on user satisfaction, while the mechanisms that influence loyalty are through the individual's internal perception rather than external pressure. The paper proposes a user loyalty model for generative AI that extends loyalty research to the emerging field of generative AI and provides guidance for the rapid iterative development of generative AI.",Technology acceptance model;Bibliographies;Cyberspace;Data science;Data models;Iterative methods;Data mining;artificial intelligence;generative AI;loyalty;TAM;self-determination theory,"{'month': 'Aug', 'issn': '', 'doi': '10.1109/DSC59305.2023.00063', 'keywords': 'Technology acceptance model;Bibliographies;Cyberspace;Data science;Data models;Iterative methods;Data mining;artificial intelligence;generative AI;loyalty;TAM;self-determination theory', 'abstract': ""This article explores the antecedent variables and mechanisms of influence for user satisfaction with generative AI. The variables and models were extracted through a literature review, combining the technology acceptance model, self-determination theory, and anthropomorphism and perceptual intelligence as two domain-specific influences on generative AI. Data was collected by distributing a questionnaire to conduct an empirical study. The empirical results showed that perceived usefulness, perceived intelligence, and anthropomorphism have a significant positive impact on user satisfaction, while the mechanisms that influence loyalty are through the individual's internal perception rather than external pressure. The paper proposes a user loyalty model for generative AI that extends loyalty research to the emerging field of generative AI and provides guidance for the rapid iterative development of generative AI."", 'pages': '397-400', 'number': '', 'volume': '', 'year': '2023', 'title': 'Exploring the Factors and Influence Mechanisms of User Loyalty for Generative AI', 'booktitle': '2023 8th International Conference on Data Science in Cyberspace (DSC)', 'author': 'Zeng, Ang and Cheng, Xusen and Wang, Yajie', 'ENTRYTYPE': 'inproceedings', 'ID': '10381195'}"
10356776,MFECLIP: CLIP With Mapping-Fusion Embedding for Text-Guided Image Editing,"Wu, Fei and Ma, Yongheng and Jin, Hao and Jing, Xiao-Yuan and Jiang, Guo-Ping",Wu,10.1109/LSP.2023.3342649,2024,IEEE Signal Processing Letters,"Recently, generative adversarial networks (GAN) have made remarkable progress, particularly with the advent of Contrastive Language-Image Pretraining (CLIP), which take image and text into a joint latent space, bridging the gap between these two modalities. Several impressive text-guided image editing methods based on GANs and CLIP have emerged. However, in these studies, most of them simply minimize the distance between the target image embedding and text embedding in the CLIP space, and take this objective as network's optimization goal, overlooking the real distance between them may be large. This may result in inability to accurately guide the editing process according to the text prompts and the changes in text-irrelevant attributes. To mitigate this issue, we propose a novel approach named CLIP with Mapping-Fusion Embedding (MFECLIP) for text-guided image editing, which comprises two components: the MFE Block and MFE Loss. Through the MFE Block, we obtain Mapping-Fusion Embedding (MFE), which can further eliminate the modality gap, and it can serve as a superior guide for editing process instead of the original text embedding. Based on contrastive learning, the MFE Loss is designed to achieve accurate alignment between the target image and text prompt. We have conducted extensive experiments on real datasets, CUB and Oxford, demonstrating the favorable performance of the proposed method.",Semantics;Generative adversarial networks;Training;Task analysis;Flowering plants;Birds;Telecommunications;Text-guided image editing;GAN;CLIP,"{'month': '', 'issn': '1558-2361', 'doi': '10.1109/LSP.2023.3342649', 'keywords': 'Semantics;Generative adversarial networks;Training;Task analysis;Flowering plants;Birds;Telecommunications;Text-guided image editing;GAN;CLIP', 'abstract': ""Recently, generative adversarial networks (GAN) have made remarkable progress, particularly with the advent of Contrastive Language-Image Pretraining (CLIP), which take image and text into a joint latent space, bridging the gap between these two modalities. Several impressive text-guided image editing methods based on GANs and CLIP have emerged. However, in these studies, most of them simply minimize the distance between the target image embedding and text embedding in the CLIP space, and take this objective as network's optimization goal, overlooking the real distance between them may be large. This may result in inability to accurately guide the editing process according to the text prompts and the changes in text-irrelevant attributes. To mitigate this issue, we propose a novel approach named CLIP with Mapping-Fusion Embedding (MFECLIP) for text-guided image editing, which comprises two components: the MFE Block and MFE Loss. Through the MFE Block, we obtain Mapping-Fusion Embedding (MFE), which can further eliminate the modality gap, and it can serve as a superior guide for editing process instead of the original text embedding. Based on contrastive learning, the MFE Loss is designed to achieve accurate alignment between the target image and text prompt. We have conducted extensive experiments on real datasets, CUB and Oxford, demonstrating the favorable performance of the proposed method."", 'pages': '116-120', 'number': '', 'volume': '31', 'year': '2024', 'title': 'MFECLIP: CLIP With Mapping-Fusion Embedding for Text-Guided Image Editing', 'journal': 'IEEE Signal Processing Letters', 'author': 'Wu, Fei and Ma, Yongheng and Jin, Hao and Jing, Xiao-Yuan and Jiang, Guo-Ping', 'ENTRYTYPE': 'article', 'ID': '10356776'}"
10642894,TADGAN-Based Anomaly Detection for PS-InSAR Deformation,"Deng, Zhichao and Xiong, Siting and Zhang, Bochen and Li, Qingquan",Deng,10.1109/IGARSS53475.2024.10642894,2024,IGARSS 2024 - 2024 IEEE International Geoscience and Remote Sensing Symposium,"Interferometric Synthetic Aperture Radar (InSAR) has become a widely used and efficient tool for monitoring large-scale, long-term land subsidence. Most applications, especially those related to risky assessment, utilize only the mean deformation rate derived through a linear fit of the InSAR-derived time series deformations without considering the full-time sequence. In this way, the embedded information, such as the onset, duration, and patterns of abnormal deformations, is ignored. This information, however, should not be compromised as it is critical for assessing the risky deformations and recognizing the causal factors. Due to the large data volume, processing the full-time-series information derived by InSAR analysis can be a challenge. In this study, we propose to use the Time-series Anomaly Detection Generative Adversarial Network (TadGAN) to deal with the time series InSAR deformation and recognize the onset and duration of abnormal epochs. The proposed method has been tested with InSAR-derived results over the Hong Kong airport region. It performs better than conventional risk assessment methods, such as the one based on Mean Absolute Deviation (MAD) outlier detection.",Training;Deformation;Time series analysis;Geoscience and remote sensing;Generative adversarial networks;Airports;Risk management;PS-InSAR;GAN;anomaly detection;time series deformation,"{'month': 'July', 'issn': '2153-7003', 'doi': '10.1109/IGARSS53475.2024.10642894', 'keywords': 'Training;Deformation;Time series analysis;Geoscience and remote sensing;Generative adversarial networks;Airports;Risk management;PS-InSAR;GAN;anomaly detection;time series deformation', 'abstract': 'Interferometric Synthetic Aperture Radar (InSAR) has become a widely used and efficient tool for monitoring large-scale, long-term land subsidence. Most applications, especially those related to risky assessment, utilize only the mean deformation rate derived through a linear fit of the InSAR-derived time series deformations without considering the full-time sequence. In this way, the embedded information, such as the onset, duration, and patterns of abnormal deformations, is ignored. This information, however, should not be compromised as it is critical for assessing the risky deformations and recognizing the causal factors. Due to the large data volume, processing the full-time-series information derived by InSAR analysis can be a challenge. In this study, we propose to use the Time-series Anomaly Detection Generative Adversarial Network (TadGAN) to deal with the time series InSAR deformation and recognize the onset and duration of abnormal epochs. The proposed method has been tested with InSAR-derived results over the Hong Kong airport region. It performs better than conventional risk assessment methods, such as the one based on Mean Absolute Deviation (MAD) outlier detection.', 'pages': '7420-7423', 'number': '', 'volume': '', 'year': '2024', 'title': 'TADGAN-Based Anomaly Detection for PS-InSAR Deformation', 'booktitle': 'IGARSS 2024 - 2024 IEEE International Geoscience and Remote Sensing Symposium', 'author': 'Deng, Zhichao and Xiong, Siting and Zhang, Bochen and Li, Qingquan', 'ENTRYTYPE': 'inproceedings', 'ID': '10642894'}"
10379614,VHetNets for AI and AI for VHetNets: An Anomaly Detection Case Study for Ubiquitous IoT,"Wang, Weili and Abbasi, Omid and Yanikomeroglu, Halim and Liang, Chengchao and Tang, Lun and Chen, Qianbin",Wang,10.1109/MNET.2023.3349309,2024,IEEE Network,"Vertical heterogeneous networks (VHetNets) and artificial intelligence (AI) play critical roles in 6G and beyond networks. This article presents an AI-native VHetNets architecture to enable the synergy of VHetNets and AI, thereby supporting varieties of AI services while facilitating the intelligent network management. Anomaly detection stands as an essential AI service across various applications in Internet of Things (IoT), including intrusion detection, state monitoring, analysis of device activities, and security supervision. In this article, we first discuss the possibilities of VHetNets used for distributed AI model training to provide the anomaly detection service for ubiquitous IoT, i.e., VHetNets for AI. After that, we study the application of AI approaches in helping implement the intelligent network management functionalities for VHetNets, i.e., AI for VHetNets, whose aim is to facilitate the efficient implementation of the anomaly detection service. Finally, a case study is presented to demonstrate the efficiency and effectiveness of the proposed AInative VHetNets-enabled anomaly detection framework.",Artificial intelligence;Internet of Things;Sensors;Data models;Autonomous aerial vehicles;Anomaly detection;Training;Heterogeneous networks;Ubiquitous computing,"{'month': 'Nov', 'issn': '1558-156X', 'doi': '10.1109/MNET.2023.3349309', 'keywords': 'Artificial intelligence;Internet of Things;Sensors;Data models;Autonomous aerial vehicles;Anomaly detection;Training;Heterogeneous networks;Ubiquitous computing', 'abstract': 'Vertical heterogeneous networks (VHetNets) and artificial intelligence (AI) play critical roles in 6G and beyond networks. This article presents an AI-native VHetNets architecture to enable the synergy of VHetNets and AI, thereby supporting varieties of AI services while facilitating the intelligent network management. Anomaly detection stands as an essential AI service across various applications in Internet of Things (IoT), including intrusion detection, state monitoring, analysis of device activities, and security supervision. In this article, we first discuss the possibilities of VHetNets used for distributed AI model training to provide the anomaly detection service for ubiquitous IoT, i.e., VHetNets for AI. After that, we study the application of AI approaches in helping implement the intelligent network management functionalities for VHetNets, i.e., AI for VHetNets, whose aim is to facilitate the efficient implementation of the anomaly detection service. Finally, a case study is presented to demonstrate the efficiency and effectiveness of the proposed AInative VHetNets-enabled anomaly detection framework.', 'pages': '170-177', 'number': '6', 'volume': '38', 'year': '2024', 'title': 'VHetNets for AI and AI for VHetNets: An Anomaly Detection Case Study for Ubiquitous IoT', 'journal': 'IEEE Network', 'author': 'Wang, Weili and Abbasi, Omid and Yanikomeroglu, Halim and Liang, Chengchao and Tang, Lun and Chen, Qianbin', 'ENTRYTYPE': 'article', 'ID': '10379614'}"
10789125,An overview of the technological performance of deep learning in modern medicine,"Monteiro, Ana Carolina Borges and França, Reinaldo Padilha and Arthur, Rangel and Iano, Yuzo",Monteiro,,2021,Deep Learning for Personalized Healthcare Services,": Artificial intelligence is a technology that uses several layers of data and information, encompassing algorithms, machine and also deep learning models, pattern matching, and even cognitive computing, learning to digitally assimilate data, gain insights on diagnoses, the variability of medical treatment, as also patient outcome with machine learning (ML). ML programming models are trained on data sets before being implemented, with properties to create their own rules or questions. It automatically and gradually improves their efficiency, accuracy, and precision with the number of experiments in which these models (algorithms) are trained. Deep learning (DL) replicates the basic structure of biological neurons, using complex algorithms, allowing predictive digital health and also allowing logical and complex structures to be established, without the need for human supervision. In other words, it is a digital architecture to study, understand, and learn how to interpret information related to the object of study that helps to more accurately identify a tumor, for example, considering that they do not need to be an expert in the field to perform this type of identification through technology. In the same sense as more pertinent diagnostic recommendations, are some examples of the application of this type of technology that can be widely used in various health segments. Emphasizing that it will give the doctor and the patient more accurate diagnoses, faster diagnoses, more assertive treatments, better-calculated risks, possibility of detecting infections, abnormalities, and diseases in a matter of seconds, the use of deep learning in health is very well accepted for reducing the margin of human error. Therefore, this manuscript aims to provide an updated overview of DL, concerning its essential concepts and in the field of medicine, as also its relationships in frameworks with meaningful intelligent properties, enabling cost-effective personalized digital services as a primary goal in healthcare in the current modern scenario.",Medical diagnostic imaging;Artificial intelligence;Medical services;Image recognition;Digital images;Deep learning;Data models;Computational modeling;Biological system modeling;Drugs,"{'url': 'https://ieeexplore-ieee-org.crai.referencistas.com/document/10789125', 'isbn': '9783110708172', 'publisher': 'De Gruyter', 'issn': '', 'doi': '', 'keywords': 'Medical diagnostic imaging;Artificial intelligence;Medical services;Image recognition;Digital images;Deep learning;Data models;Computational modeling;Biological system modeling;Drugs', 'abstract': ': Artificial intelligence is a technology that uses several layers of data and information, encompassing algorithms, machine and also deep learning models, pattern matching, and even cognitive computing, learning to digitally assimilate data, gain insights on diagnoses, the variability of medical treatment, as also patient outcome with machine learning (ML). ML programming models are trained on data sets before being implemented, with properties to create their own rules or questions. It automatically and gradually improves their efficiency, accuracy, and precision with the number of experiments in which these models (algorithms) are trained. Deep learning (DL) replicates the basic structure of biological neurons, using complex algorithms, allowing predictive digital health and also allowing logical and complex structures to be established, without the need for human supervision. In other words, it is a digital architecture to study, understand, and learn how to interpret information related to the object of study that helps to more accurately identify a tumor, for example, considering that they do not need to be an expert in the field to perform this type of identification through technology. In the same sense as more pertinent diagnostic recommendations, are some examples of the application of this type of technology that can be widely used in various health segments. Emphasizing that it will give the doctor and the patient more accurate diagnoses, faster diagnoses, more assertive treatments, better-calculated risks, possibility of detecting infections, abnormalities, and diseases in a matter of seconds, the use of deep learning in health is very well accepted for reducing the margin of human error. Therefore, this manuscript aims to provide an updated overview of DL, concerning its essential concepts and in the field of medicine, as also its relationships in frameworks with meaningful intelligent properties, enabling cost-effective personalized digital services as a primary goal in healthcare in the current modern scenario.', 'pages': '225-244', 'number': '', 'volume': '', 'year': '2021', 'title': 'An overview of the technological performance of deep learning in modern medicine', 'booktitle': 'Deep Learning for Personalized Healthcare Services', 'author': 'Monteiro, Ana Carolina Borges and França, Reinaldo Padilha and Arthur, Rangel and Iano, Yuzo', 'ENTRYTYPE': 'inbook', 'ID': '10789125'}"
11042202,Adversarial Multi-feature Fusion for Sentiment Analysis,"Lv, Wenlong and Wang, Yipeng and Tang, Fei and Tang, Yuwen",Lv,10.1109/ISEAE64934.2025.11042202,2025,"2025 7th International Conference on Information Science, Electrical and Automation Engineering (ISEAE)","Existing text sentiment classification methods suffer from several limitations: they struggle to capture global semantic information, fail to fully utilize positional and relational features of words, and exhibit inadequate handling of long-range semantic dependencies or emotional transitions. Moreover, the suboptimal fusion of features often leads to unsatisfactory classification accuracy. To address these issues, this paper proposes a multichannel sentiment analysis model. First, BERT is employed for data preprocessing. Next, Graph Convolutional Networks (GCN) are utilized to extract part-of-speech and syntactic features, while Convolutional Neural Networks (CNN) capture positional and local semantic features. Additionally, Bidirectional Simple Recurrent Units (Bi-SRU) are applied to model contextual dependencies, enabling deep exploration of textual features. Finally, a Generative Adversarial Network (GAN) is introduced to fuse these features, enhancing their complementarity. Ablation experiments validate the effectiveness of each module, and comparative results on public datasets demonstrate that the proposed model outperforms existing baseline methods, showcasing its superior performance.",Training;Sentiment analysis;Analytical models;Accuracy;Semantics;Syntactics;Feature extraction;Generative adversarial networks;Encoding;Context modeling;Deep Learning;Local Feature Extraction;Multi-Channel Sentiment Analysis Model;Feature Fusion,"{'month': 'April', 'issn': '', 'doi': '10.1109/ISEAE64934.2025.11042202', 'keywords': 'Training;Sentiment analysis;Analytical models;Accuracy;Semantics;Syntactics;Feature extraction;Generative adversarial networks;Encoding;Context modeling;Deep Learning;Local Feature Extraction;Multi-Channel Sentiment Analysis Model;Feature Fusion', 'abstract': 'Existing text sentiment classification methods suffer from several limitations: they struggle to capture global semantic information, fail to fully utilize positional and relational features of words, and exhibit inadequate handling of long-range semantic dependencies or emotional transitions. Moreover, the suboptimal fusion of features often leads to unsatisfactory classification accuracy. To address these issues, this paper proposes a multichannel sentiment analysis model. First, BERT is employed for data preprocessing. Next, Graph Convolutional Networks (GCN) are utilized to extract part-of-speech and syntactic features, while Convolutional Neural Networks (CNN) capture positional and local semantic features. Additionally, Bidirectional Simple Recurrent Units (Bi-SRU) are applied to model contextual dependencies, enabling deep exploration of textual features. Finally, a Generative Adversarial Network (GAN) is introduced to fuse these features, enhancing their complementarity. Ablation experiments validate the effectiveness of each module, and comparative results on public datasets demonstrate that the proposed model outperforms existing baseline methods, showcasing its superior performance.', 'pages': '806-810', 'number': '', 'volume': '', 'year': '2025', 'title': 'Adversarial Multi-feature Fusion for Sentiment Analysis', 'booktitle': '2025 7th International Conference on Information Science, Electrical and Automation Engineering (ISEAE)', 'author': 'Lv, Wenlong and Wang, Yipeng and Tang, Fei and Tang, Yuwen', 'ENTRYTYPE': 'inproceedings', 'ID': '11042202'}"
10813561,Exploring Equilibrium Strategies in Network Games With Generative AI,"Yang, Yaoqi and Du, Hongyang and Sun, Geng and Xiong, Zehui and Niyato, Dusit and Han, Zhu",Yang,10.1109/MNET.2024.3521887,2025,IEEE Network,"Game theory offers a powerful framework for analyzing strategic interactions among decision-makers, providing tools to model, analyze, and predict their behavior. However, implementing game theory can be challenging due to difficulties in deriving solutions, understanding interactions, and ensuring optimal performance. Traditional non-AI and discriminative AI approaches have made valuable contributions but struggle with limitations in handling large-scale games and dynamic scenarios. In this context, generative AI emerges as a promising solution because of its superior data analysis and generation capabilities. This paper comprehensively summarizes the challenges, solutions, and outlooks of combining generative AI with game theory. We start with reviewing the limitations of traditional non-AI and discriminative AI approaches in employing game theory, and then highlight the necessity and advantages of integrating generative AI. Next, we explore the applications of generative AI in various stages of the game theory lifecycle, including model formulation, solution derivation, and strategy improvement. Additionally, from game theory viewpoint, we propose a generative AI-enabled framework for optimizing machine learning model performance against false data injection attacks, supported by a case study to demonstrate its effectiveness. Finally, we outline future research directions for generative AI-enabled game theory, paving the way for its further advancements and development.",Game theory;Artificial intelligence;Generative AI;Predictive models;Digital twins;Data models;Analytical models;Standards;Adaptation models;Generative AI;game theory;game theoretical model formulation;equilibrium solution derivation,"{'month': 'Sep.', 'issn': '1558-156X', 'doi': '10.1109/MNET.2024.3521887', 'keywords': 'Game theory;Artificial intelligence;Generative AI;Predictive models;Digital twins;Data models;Analytical models;Standards;Adaptation models;Generative AI;game theory;game theoretical model formulation;equilibrium solution derivation', 'abstract': 'Game theory offers a powerful framework for analyzing strategic interactions among decision-makers, providing tools to model, analyze, and predict their behavior. However, implementing game theory can be challenging due to difficulties in deriving solutions, understanding interactions, and ensuring optimal performance. Traditional non-AI and discriminative AI approaches have made valuable contributions but struggle with limitations in handling large-scale games and dynamic scenarios. In this context, generative AI emerges as a promising solution because of its superior data analysis and generation capabilities. This paper comprehensively summarizes the challenges, solutions, and outlooks of combining generative AI with game theory. We start with reviewing the limitations of traditional non-AI and discriminative AI approaches in employing game theory, and then highlight the necessity and advantages of integrating generative AI. Next, we explore the applications of generative AI in various stages of the game theory lifecycle, including model formulation, solution derivation, and strategy improvement. Additionally, from game theory viewpoint, we propose a generative AI-enabled framework for optimizing machine learning model performance against false data injection attacks, supported by a case study to demonstrate its effectiveness. Finally, we outline future research directions for generative AI-enabled game theory, paving the way for its further advancements and development.', 'pages': '191-200', 'number': '5', 'volume': '39', 'year': '2025', 'title': 'Exploring Equilibrium Strategies in Network Games With Generative AI', 'journal': 'IEEE Network', 'author': 'Yang, Yaoqi and Du, Hongyang and Sun, Geng and Xiong, Zehui and Niyato, Dusit and Han, Zhu', 'ENTRYTYPE': 'article', 'ID': '10813561'}"
10332091,An Efficient Wave Propagation Prediction Model Generated by CycleGAN with Ray Tracing Data,"Zheng, Yangboyin and Li, Shidong and Su, Ting and Chi, Kuo and Yang, Yongqin and Xing, Wenqing",Zheng,10.1109/PRAI59366.2023.10332091,2023,2023 IEEE 6th International Conference on Pattern Recognition and Artificial Intelligence (PRAI),"The ray tracing technique is an application of optical ray technology in the field of electromagnetic computing, which can accurately consider various propagation paths of electromagnetic waves with high computational accuracy but large computational volume. In this paper, we generate an accurate and efficient prediction model for urban radio propagation using Cycle Generative Adversarial Network (CycleGAN). The color information of red, green and blue (RGB) channels of the planar images of the urban area is used to characterize the power of radio wave propagation, CycleGAN is used to learn this information to achieve the prediction effect. Compared with the traditional modeling methods, the method proposed in this paper can greatly reduce the time required for prediction, and the structural similarity between the prediction and the actual simulation results can reach 0.95. It takes 10.85s to calculate the same number of city area images using Winprop software, while the proposed method only needs 3.08s, which saves a lot of time.",Image color analysis;Propagation;Simulation;Urban areas;Electromagnetic scattering;Green products;Predictive models;wave propagation;ray tracing;cycle generative adversarial network (CycleGAN),"{'month': 'Aug', 'issn': '', 'doi': '10.1109/PRAI59366.2023.10332091', 'keywords': 'Image color analysis;Propagation;Simulation;Urban areas;Electromagnetic scattering;Green products;Predictive models;wave propagation;ray tracing;cycle generative adversarial network (CycleGAN)', 'abstract': 'The ray tracing technique is an application of optical ray technology in the field of electromagnetic computing, which can accurately consider various propagation paths of electromagnetic waves with high computational accuracy but large computational volume. In this paper, we generate an accurate and efficient prediction model for urban radio propagation using Cycle Generative Adversarial Network (CycleGAN). The color information of red, green and blue (RGB) channels of the planar images of the urban area is used to characterize the power of radio wave propagation, CycleGAN is used to learn this information to achieve the prediction effect. Compared with the traditional modeling methods, the method proposed in this paper can greatly reduce the time required for prediction, and the structural similarity between the prediction and the actual simulation results can reach 0.95. It takes 10.85s to calculate the same number of city area images using Winprop software, while the proposed method only needs 3.08s, which saves a lot of time.', 'pages': '1130-1135', 'number': '', 'volume': '', 'year': '2023', 'title': 'An Efficient Wave Propagation Prediction Model Generated by CycleGAN with Ray Tracing Data', 'booktitle': '2023 IEEE 6th International Conference on Pattern Recognition and Artificial Intelligence (PRAI)', 'author': 'Zheng, Yangboyin and Li, Shidong and Su, Ting and Chi, Kuo and Yang, Yongqin and Xing, Wenqing', 'ENTRYTYPE': 'inproceedings', 'ID': '10332091'}"
8462579,Towards Conditional Adversarial Training for Predicting Emotions from Speech,"Han, Jing and Zhang, Zixing and Ren, Zhao and Ringeval, Fabien and Schuller, Björn",Han,10.1109/ICASSP.2018.8462579,2018,"2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","Motivated by the encouraging results recently obtained by generative adversarial networks in various image processing tasks, we propose a conditional adversarial training framework to predict dimensional representations of emotion, i. e., arousal and valence, from speech signals. The framework consists of two networks, trained in an adversarial manner: The first network tries to predict emotion from acoustic features, while the second network aims at distinguishing between the predictions provided by the first network and the emotion labels from the database using the acoustic features as conditional information. We evaluate the performance of the proposed conditional adversarial training framework on the widely used emotion database RECOLA. Experimental results show that the proposed training strategy outperforms the conventional training method, and is comparable with, or even superior to other recently reported approaches, including deep and end-to-end learning.",Training;Artificial neural networks;Gallium nitride;Generators;Generative adversarial networks;Feature extraction;Predictive models;Emotion recognition;conditional adversarial training;generative adversarial network,"{'month': 'April', 'issn': '2379-190X', 'doi': '10.1109/ICASSP.2018.8462579', 'keywords': 'Training;Artificial neural networks;Gallium nitride;Generators;Generative adversarial networks;Feature extraction;Predictive models;Emotion recognition;conditional adversarial training;generative adversarial network', 'abstract': 'Motivated by the encouraging results recently obtained by generative adversarial networks in various image processing tasks, we propose a conditional adversarial training framework to predict dimensional representations of emotion, i. e., arousal and valence, from speech signals. The framework consists of two networks, trained in an adversarial manner: The first network tries to predict emotion from acoustic features, while the second network aims at distinguishing between the predictions provided by the first network and the emotion labels from the database using the acoustic features as conditional information. We evaluate the performance of the proposed conditional adversarial training framework on the widely used emotion database RECOLA. Experimental results show that the proposed training strategy outperforms the conventional training method, and is comparable with, or even superior to other recently reported approaches, including deep and end-to-end learning.', 'pages': '6822-6826', 'number': '', 'volume': '', 'year': '2018', 'title': 'Towards Conditional Adversarial Training for Predicting Emotions from Speech', 'booktitle': '2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)', 'author': 'Han, Jing and Zhang, Zixing and Ren, Zhao and Ringeval, Fabien and Schuller, Björn', 'ENTRYTYPE': 'inproceedings', 'ID': '8462579'}"
10008363,Comparative Analysis of Deep Fake Detection Techniques,"Alanazi, Fatimah",Alanazi,10.1109/CICN56167.2022.10008363,2022,2022 14th International Conference on Computational Intelligence and Communication Networks (CICN),"Deep learning and artificial intelligence are important knowledge areas that have provided solutions allowing the successful resolution of complex problems. Some of these problems include, but are not limited to, human-level control, data analytics and other digitisation challenges. One of the offshoots of deep learning is a concept termed ‘deepfake’, which can be described as the imposition of video of a face image from a source to video of the face image of a target individual in order to make the targeted person appear to express the content of the source video [2]. It is important to establish the fact that deepfakes have been used for malicious purposes, becoming a threat to national security, privacy, democracy, and society at large. It is, therefore, fundamental to review the science behind the method, and the available detection techniques to curtail this digital innovation, so as to reduce its level of threat; that is the focus of this paper.",Deep learning;Deepfakes;Technological innovation;Privacy;Image resolution;Recurrent neural networks;Fingerprint recognition;deepfakes;artificial intelligence;deep learning;autoencoders;forensics;GAN;generative adversarial networks,"{'month': 'Dec', 'issn': '2472-7555', 'doi': '10.1109/CICN56167.2022.10008363', 'keywords': 'Deep learning;Deepfakes;Technological innovation;Privacy;Image resolution;Recurrent neural networks;Fingerprint recognition;deepfakes;artificial intelligence;deep learning;autoencoders;forensics;GAN;generative adversarial networks', 'abstract': 'Deep learning and artificial intelligence are important knowledge areas that have provided solutions allowing the successful resolution of complex problems. Some of these problems include, but are not limited to, human-level control, data analytics and other digitisation challenges. One of the offshoots of deep learning is a concept termed ‘deepfake’, which can be described as the imposition of video of a face image from a source to video of the face image of a target individual in order to make the targeted person appear to express the content of the source video [2]. It is important to establish the fact that deepfakes have been used for malicious purposes, becoming a threat to national security, privacy, democracy, and society at large. It is, therefore, fundamental to review the science behind the method, and the available detection techniques to curtail this digital innovation, so as to reduce its level of threat; that is the focus of this paper.', 'pages': '119-124', 'number': '', 'volume': '', 'year': '2022', 'title': 'Comparative Analysis of Deep Fake Detection Techniques', 'booktitle': '2022 14th International Conference on Computational Intelligence and Communication Networks (CICN)', 'author': 'Alanazi, Fatimah', 'ENTRYTYPE': 'inproceedings', 'ID': '10008363'}"
10999131,Service Life Modeling of Pavement with Ensemble Learning,"Azodinia, Mohammadreza and Mudabbir, Mohamed and Ardabili, Sina and Varkonyi-Koczy, Annamaria and Iskakov, Kazizat and Mosavi, Amir",Azodinia,10.1109/ICCC64928.2025.10999131,2025,2025 IEEE 12th International Conference on Computational Cybernetics and Cyber-Medical Systems (ICCC),"Random Forest (RF) is an ensemble learning which creates multiple decision trees and combines their outputs for creating models with less over-fitting. In this study, we apply RF to model the remaining service life (RSL) of rural pavements, a critical factor for developing optimal maintenance strategies and ensuring long-lasting infrastructure. We utilize key variables such as asphalt concrete thickness, base thickness, and surface temperature, along with data from Falling Weight Deflectometer (FWD) measurements. RF demonstrated performance in predicting RSL with consistent accuracy across a variety of conditions. The ensemble nature of RF allows it to effectively manage complex interactions among variables and handle the inherent variability in pavement performance data which makes it well-suited for rural road networks, where environmental and material differences are significant. While some sensitivity to parameter adjustments was noted, the robustness and reliability of RF highlights its potential to be a transformative tool in rural pavement management.",Radio frequency;Temperature measurement;Temperature distribution;Asphalt;Accuracy;Roads;Telecommunication traffic;Ensemble learning;Random forests;Testing;ensemble learning;ensemble;random forest;artificial intelligence;machine learning;remaining service life;service life;pavement;big data;data mining;engineering;mathematics;deep learning;civil engineering;data science;generative AI;applied artificial intelligence;applied mathematics;applied informatics;information systems;soft computing;geoscience;earth science;industry 4.0;society 5.0;AI;XAI;simulation;sustainable development;sustainable development goals;neural networks;pavement engineering;asphalt pavement;concrete pavement;pavement durability;traffic load;surface roughness;asphalt mix;concrete mix;bitumen;aggregates;subgrade;base course;reinforced pavement;flexible pavement;rigid pavement;pavement distress;cracking;rutting;skid resistance;drainage performance;load-bearing capacity;structural integrity;pavement degradation;thermal expansion;ground-penetrating radar;radargram analysis;non-destructive testing;pavement condition index;road surface profiling;geotechnical investigation;structural health monitoring;machine learning for pavement defects;predictive maintenance,"{'month': 'April', 'issn': '2689-7768', 'doi': '10.1109/ICCC64928.2025.10999131', 'keywords': 'Radio frequency;Temperature measurement;Temperature distribution;Asphalt;Accuracy;Roads;Telecommunication traffic;Ensemble learning;Random forests;Testing;ensemble learning;ensemble;random forest;artificial intelligence;machine learning;remaining service life;service life;pavement;big data;data mining;engineering;mathematics;deep learning;civil engineering;data science;generative AI;applied artificial intelligence;applied mathematics;applied informatics;information systems;soft computing;geoscience;earth science;industry 4.0;society 5.0;AI;XAI;simulation;sustainable development;sustainable development goals;neural networks;pavement engineering;asphalt pavement;concrete pavement;pavement durability;traffic load;surface roughness;asphalt mix;concrete mix;bitumen;aggregates;subgrade;base course;reinforced pavement;flexible pavement;rigid pavement;pavement distress;cracking;rutting;skid resistance;drainage performance;load-bearing capacity;structural integrity;pavement degradation;thermal expansion;ground-penetrating radar;radargram analysis;non-destructive testing;pavement condition index;road surface profiling;geotechnical investigation;structural health monitoring;machine learning for pavement defects;predictive maintenance', 'abstract': 'Random Forest (RF) is an ensemble learning which creates multiple decision trees and combines their outputs for creating models with less over-fitting. In this study, we apply RF to model the remaining service life (RSL) of rural pavements, a critical factor for developing optimal maintenance strategies and ensuring long-lasting infrastructure. We utilize key variables such as asphalt concrete thickness, base thickness, and surface temperature, along with data from Falling Weight Deflectometer (FWD) measurements. RF demonstrated performance in predicting RSL with consistent accuracy across a variety of conditions. The ensemble nature of RF allows it to effectively manage complex interactions among variables and handle the inherent variability in pavement performance data which makes it well-suited for rural road networks, where environmental and material differences are significant. While some sensitivity to parameter adjustments was noted, the robustness and reliability of RF highlights its potential to be a transformative tool in rural pavement management.', 'pages': '000167-000174', 'number': '', 'volume': '', 'year': '2025', 'title': 'Service Life Modeling of Pavement with Ensemble Learning', 'booktitle': '2025 IEEE 12th International Conference on Computational Cybernetics and Cyber-Medical Systems (ICCC)', 'author': 'Azodinia, Mohammadreza and Mudabbir, Mohamed and Ardabili, Sina and Varkonyi-Koczy, Annamaria and Iskakov, Kazizat and Mosavi, Amir', 'ENTRYTYPE': 'inproceedings', 'ID': '10999131'}"
11142690,Research on Fast Calculation Method for Relay Protection Setting Mismatch Points of Lines Based on Deep Learning,"Gong, Yongzhi and Yang, Zhongyan and Luo, Zhengya and Zhang, Zequ and Zhang, Jijun",Gong,10.1109/ACCESS.2025.3603252,2025,IEEE Access,"In complex ring networks, a significant amount of time is typically required to determine the coordination relationships of backup protection settings during online adjustment, which affects the efficiency of the online setting process. To address this, the paper introduces a data-driven approach by applying artificial intelligence technology—specifically, for the first time—into the field of online setting. A fast computation scheme for online backup protection setting based on Generative Adversarial Networks (GANs) is proposed. Firstly, a conditional GAN based on the Wasserstein distance is constructed, using system operating modes as conditional labels. The coordination pairs of backup protection settings are transformed into matrix indices, forming a backup protection coordination matrix, which serves as real sample data for GAN training. The generator and discriminator networks of the GAN are primarily composed of Convolutional Neural Networks (CNNs), and batch normalization is applied to the outputs of the neural networks to improve training stability. Next, the specific implementation of the GAN-based online setting scheme for backup protection is presented. Parallel computing techniques are employed to accelerate the computation of both the CNN and the setting values, further enhancing the efficiency of the online adjustment process. Finally, the IEEE 39-bus system is used as an example to compute backup protection coordination matrices under various operating conditions, thus constructing the dataset required for GAN training and validating the proposed scheme. Simulation results show that the proposed method can learn the complex coordination relationships among various backup protection settings from real sample data via GANs. It can generate the corresponding coordination matrix according to different operating modes, thereby achieving fast computation for online backup protection setting.",Protection;Generative adversarial networks;Generators;Training;Noise;Real-time systems;Vectors;Protective relaying;Computational efficiency;Artificial intelligence;Backup protection;online setting;generative adversarial network;operating mode;protection coordination pair;data-driven,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2025.3603252', 'keywords': 'Protection;Generative adversarial networks;Generators;Training;Noise;Real-time systems;Vectors;Protective relaying;Computational efficiency;Artificial intelligence;Backup protection;online setting;generative adversarial network;operating mode;protection coordination pair;data-driven', 'abstract': 'In complex ring networks, a significant amount of time is typically required to determine the coordination relationships of backup protection settings during online adjustment, which affects the efficiency of the online setting process. To address this, the paper introduces a data-driven approach by applying artificial intelligence technology—specifically, for the first time—into the field of online setting. A fast computation scheme for online backup protection setting based on Generative Adversarial Networks (GANs) is proposed. Firstly, a conditional GAN based on the Wasserstein distance is constructed, using system operating modes as conditional labels. The coordination pairs of backup protection settings are transformed into matrix indices, forming a backup protection coordination matrix, which serves as real sample data for GAN training. The generator and discriminator networks of the GAN are primarily composed of Convolutional Neural Networks (CNNs), and batch normalization is applied to the outputs of the neural networks to improve training stability. Next, the specific implementation of the GAN-based online setting scheme for backup protection is presented. Parallel computing techniques are employed to accelerate the computation of both the CNN and the setting values, further enhancing the efficiency of the online adjustment process. Finally, the IEEE 39-bus system is used as an example to compute backup protection coordination matrices under various operating conditions, thus constructing the dataset required for GAN training and validating the proposed scheme. Simulation results show that the proposed method can learn the complex coordination relationships among various backup protection settings from real sample data via GANs. It can generate the corresponding coordination matrix according to different operating modes, thereby achieving fast computation for online backup protection setting.', 'pages': '151714-151725', 'number': '', 'volume': '13', 'year': '2025', 'title': 'Research on Fast Calculation Method for Relay Protection Setting Mismatch Points of Lines Based on Deep Learning', 'journal': 'IEEE Access', 'author': 'Gong, Yongzhi and Yang, Zhongyan and Luo, Zhengya and Zhang, Zequ and Zhang, Jijun', 'ENTRYTYPE': 'article', 'ID': '11142690'}"
10952520,"The Future of Society, Work, and AI","Haq, Rashed",Haq,,2020,Enterprise Artificial Intelligence Transformation,"Summary <p>This chapter looks at the future of society and work, and how near\&\#x2010;future developments within artificial intelligence (AI) \&\#x2013; that is, developments prior to the arrival of artificial general intelligence \&\#x2013; will impact both the advances we expect and the challenges we will face. In the years to come, AI will show up in many applications, from robotics to productivity enhancements to the emerging technology of quantum computing. In robotics, companies such as Intuitive Surgical are already developing robot\&\#x2010;assisted technologies, tools, and services for surgical operations. Machines will interact with one another using AI algorithms and make decisions about the production chain without the need for human intervention. Even during the time of the industrial revolution, Karl Marx, in the chapter on machinery and modern industry from his book Capital.</p>",Artificial intelligence;Robots;Medical services;Faces;Business;Surveillance;Safety;Regulation;Internet;Employment,"{'url': 'https://ieeexplore-ieee-org.crai.referencistas.com/document/10952520', 'isbn': '9781119665861', 'publisher': 'Wiley', 'issn': '', 'doi': '', 'keywords': 'Artificial intelligence;Robots;Medical services;Faces;Business;Surveillance;Safety;Regulation;Internet;Employment', 'abstract': 'Summary <p>This chapter looks at the future of society and work, and how near\\&\\#x2010;future developments within artificial intelligence (AI) \\&\\#x2013; that is, developments prior to the arrival of artificial general intelligence \\&\\#x2013; will impact both the advances we expect and the challenges we will face. In the years to come, AI will show up in many applications, from robotics to productivity enhancements to the emerging technology of quantum computing. In robotics, companies such as Intuitive Surgical are already developing robot\\&\\#x2010;assisted technologies, tools, and services for surgical operations. Machines will interact with one another using AI algorithms and make decisions about the production chain without the need for human intervention. Even during the time of the industrial revolution, Karl Marx, in the chapter on machinery and modern industry from his book Capital.</p>', 'pages': '289-311', 'number': '', 'volume': '', 'year': '2020', 'title': 'The Future of Society, Work, and AI', 'booktitle': 'Enterprise Artificial Intelligence Transformation', 'author': 'Haq, Rashed', 'ENTRYTYPE': 'inbook', 'ID': '10952520'}"
10400632,Design of an intelligent reminder system based on EEG monitoring,"Zhang, Y. and Wu, J. and Yi, H. and Deng, W. and Li, Q.",Zhang,10.1049/icp.2023.2914,2023,5th International Conference on Artificial Intelligence and Advanced Manufacturing (AIAM 2023),"EEG waves reflect the synchronized firing of neurons in the brain, and different waveform patterns are associated with different cognitive states, such as attention, relaxation, or sleep. In this paper, we design an intelligent reminder system based on EEG monitoring. The process of system involves several steps, including pre-processing, filtering, artifact removal, feature extraction, and classification. Preprocessing involves removing any artifacts or noise from the raw EEG signal. Filtering is used to remove unwanted frequencies or amplify desired frequencies. Artifact removal techniques include independent component analysis and template matching, which can identify and remove artifacts such as eye blinks or heartbeats. Feature extraction algorithms aim to extract significant information from the processed signal. Classification algorithms are then used to classify the extracted features into specific categories, such as cognitive states, diseases, or mental tasks. Commonly used classification algorithms include support vector machines, neural networks, and linear discriminant analysis. EEG collection can potentially have an effect on reminding fatigue driving, as it may increase drivers' awareness of their cognitive states and alert them to signs of fatigue. By monitoring the electrical activity of the brain, EEG can provide real-time feedback on changes in cognitive states, such as decreased attention or increased drowsiness.",,"{'month': 'Oct', 'issn': '', 'doi': '10.1049/icp.2023.2914', 'keywords': '', 'abstract': ""EEG waves reflect the synchronized firing of neurons in the brain, and different waveform patterns are associated with different cognitive states, such as attention, relaxation, or sleep. In this paper, we design an intelligent reminder system based on EEG monitoring. The process of system involves several steps, including pre-processing, filtering, artifact removal, feature extraction, and classification. Preprocessing involves removing any artifacts or noise from the raw EEG signal. Filtering is used to remove unwanted frequencies or amplify desired frequencies. Artifact removal techniques include independent component analysis and template matching, which can identify and remove artifacts such as eye blinks or heartbeats. Feature extraction algorithms aim to extract significant information from the processed signal. Classification algorithms are then used to classify the extracted features into specific categories, such as cognitive states, diseases, or mental tasks. Commonly used classification algorithms include support vector machines, neural networks, and linear discriminant analysis. EEG collection can potentially have an effect on reminding fatigue driving, as it may increase drivers' awareness of their cognitive states and alert them to signs of fatigue. By monitoring the electrical activity of the brain, EEG can provide real-time feedback on changes in cognitive states, such as decreased attention or increased drowsiness."", 'pages': '48-52', 'number': '', 'volume': '2023', 'year': '2023', 'title': 'Design of an intelligent reminder system based on EEG monitoring', 'booktitle': '5th International Conference on Artificial Intelligence and Advanced Manufacturing (AIAM 2023)', 'author': 'Zhang, Y. and Wu, J. and Yi, H. and Deng, W. and Li, Q.', 'ENTRYTYPE': 'inproceedings', 'ID': '10400632'}"
10696448,A Fine-Tuned MobileNetV3 Model for Real and Fake Image Classification,"Singh, Gurpreet and Guleria, Kalpna and Sharma, Shagun",Singh,10.1109/ICoICI62503.2024.10696448,2024,2024 Second International Conference on Intelligent Cyber Physical Systems and Internet of Things (ICoICI),"This study focuses on the utilization of the MobileNet model for the classification of artificial and real images, addressing the increasing demand for accurate image identification in the digital realm. Through the implementation of convolutional neural networks (CNNs), specifically optimized for mobile and embedded devices, we aim to distinguish between authentic and digitally manipulated images efficiently. After training the MobileNet model on a dataset comprising both real and artificial images, we evaluated its performance on a validation set. The results indicate a validation accuracy of 95.02\%, demonstrating the model's robustness in distinguishing between the two image categories. Additionally, the validation loss was found to be 0.1621, indicating minimal discrepancies between predicted and actual labels. Furthermore, the precision and recall metrics provide insights into the model's ability to identify real and artificial images accurately. With a validation precision of 0.9494 and a validation recall of 0.9511, the MobileNet model showcases high precision in correctly classifying both image types while minimizing false positives and negatives. Overall, these findings highlight the effectiveness of the MobileNet model in artificial image detection, offering a reliable solution for combating the proliferation of manipulated visuals in various domains, from social media to journalism and beyond.",Training;Deepfakes;Adaptation models;Visualization;Accuracy;Social networking (online);Computational modeling;Transfer learning;Topology;Image classification;Machine learning;Deep learning;Image processing;MobileNet Model;Artificial Images;Real Image,"{'month': 'Aug', 'issn': '', 'doi': '10.1109/ICoICI62503.2024.10696448', 'keywords': 'Training;Deepfakes;Adaptation models;Visualization;Accuracy;Social networking (online);Computational modeling;Transfer learning;Topology;Image classification;Machine learning;Deep learning;Image processing;MobileNet Model;Artificial Images;Real Image', 'abstract': ""This study focuses on the utilization of the MobileNet model for the classification of artificial and real images, addressing the increasing demand for accurate image identification in the digital realm. Through the implementation of convolutional neural networks (CNNs), specifically optimized for mobile and embedded devices, we aim to distinguish between authentic and digitally manipulated images efficiently. After training the MobileNet model on a dataset comprising both real and artificial images, we evaluated its performance on a validation set. The results indicate a validation accuracy of 95.02\\%, demonstrating the model's robustness in distinguishing between the two image categories. Additionally, the validation loss was found to be 0.1621, indicating minimal discrepancies between predicted and actual labels. Furthermore, the precision and recall metrics provide insights into the model's ability to identify real and artificial images accurately. With a validation precision of 0.9494 and a validation recall of 0.9511, the MobileNet model showcases high precision in correctly classifying both image types while minimizing false positives and negatives. Overall, these findings highlight the effectiveness of the MobileNet model in artificial image detection, offering a reliable solution for combating the proliferation of manipulated visuals in various domains, from social media to journalism and beyond."", 'pages': '1-5', 'number': '', 'volume': '', 'year': '2024', 'title': 'A Fine-Tuned MobileNetV3 Model for Real and Fake Image Classification', 'booktitle': '2024 Second International Conference on Intelligent Cyber Physical Systems and Internet of Things (ICoICI)', 'author': 'Singh, Gurpreet and Guleria, Kalpna and Sharma, Shagun', 'ENTRYTYPE': 'inproceedings', 'ID': '10696448'}"
11089795,"An Approach for Detecting Myocardial Infarction and Multi-Label Classification Using GAN, GNNs and DBN","Shanthi, A.S. and Nikhil, M. and M, Naveen Kumar and D, Sam Alwin Satyavasagan and D, Santhosh Kumar",Shanthi,10.1109/ICIRCA65293.2025.11089795,2025,2025 6th International Conference on Inventive Research in Computing Applications (ICIRCA),"Myocardial Infarction (MI), also referred to as heart attack is leading cause of death globally, commonly caused by issues in the coronary arteries and Left Ventricle (LV) changes. Early medical care is essential to minimize the chances of death from MI. Existing algorithms are effective at identifying most cases of MI but tend to fail to catch early signs of the disease. In most cases, detection of MI depends on algorithms such as DNN [4], CNN, RNN, and LSTM. We're investigating advanced technologies like GAN, DBN, and GNNs. Such novel technologies have the potential to enhance early detection and more accurate diagnosis resulting in improved patient outcomes and reduced mortality. With these novel tools, we seek to improve the accuracy and efficacy of MI detection and intervention.",Neurology;Accuracy;Recurrent neural networks;Mortality;Multi label classification;Medical services;Myocardium;Generative adversarial networks;Long short term memory;Medical diagnostic imaging;Myocardial Infarction;Cardiovascular disease;Deep Learning;Convolutional Neural Network;Recurrent Neural Network;Deep Neural Network;Long Short-Term Memory,"{'month': 'June', 'issn': '', 'doi': '10.1109/ICIRCA65293.2025.11089795', 'keywords': 'Neurology;Accuracy;Recurrent neural networks;Mortality;Multi label classification;Medical services;Myocardium;Generative adversarial networks;Long short term memory;Medical diagnostic imaging;Myocardial Infarction;Cardiovascular disease;Deep Learning;Convolutional Neural Network;Recurrent Neural Network;Deep Neural Network;Long Short-Term Memory', 'abstract': ""Myocardial Infarction (MI), also referred to as heart attack is leading cause of death globally, commonly caused by issues in the coronary arteries and Left Ventricle (LV) changes. Early medical care is essential to minimize the chances of death from MI. Existing algorithms are effective at identifying most cases of MI but tend to fail to catch early signs of the disease. In most cases, detection of MI depends on algorithms such as DNN [4], CNN, RNN, and LSTM. We're investigating advanced technologies like GAN, DBN, and GNNs. Such novel technologies have the potential to enhance early detection and more accurate diagnosis resulting in improved patient outcomes and reduced mortality. With these novel tools, we seek to improve the accuracy and efficacy of MI detection and intervention."", 'pages': '1811-1815', 'number': '', 'volume': '', 'year': '2025', 'title': 'An Approach for Detecting Myocardial Infarction and Multi-Label Classification Using GAN, GNNs and DBN', 'booktitle': '2025 6th International Conference on Inventive Research in Computing Applications (ICIRCA)', 'author': 'Shanthi, A.S. and Nikhil, M. and M, Naveen Kumar and D, Sam Alwin Satyavasagan and D, Santhosh Kumar', 'ENTRYTYPE': 'inproceedings', 'ID': '11089795'}"
8508269,Artificial Intelligence for Conversational Robo-Advisor,"Day, Min-Yuh and Lin, Jian-Ting and Chen, Yuan-Chih",Day,10.1109/ASONAM.2018.8508269,2018,2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM),"With the advent of the artificial intelligence (AI) era, the combination of AI with financial technology (FinTech) has become a development trend in the financial industry. However, deep learning (DL) on the application of automated financial management has been rarely investigated. Thus, this research focuses on the applications of FinTech and DL in asset allocation and aims to optimize investment portfolio. The best investment portfolio in index-based funds based on Taiwan's index-type security investment trust funds are the main investment targets. Time series models for DL, that is, long short-term memory, predict the increase of each investment target and find the best investment portfolio in combination with the relevant asset allocation theory. In this research, we use the Markowitz mean-variance and Black-Litterman models as our asset allocation models for robo-advisor. Results show that the Black-Litterman model has a better accumulated return performance than the Morkowitz model and outperforms other strategies. The Human-Computer Interaction (HCI) dialogue service adopts artificial intelligence markup language (AIML) and a generative model. The main contribution of this paper is that we have developed an integrated knowledge-based and generative-based models for AI conversational robo-advisor.",Resource management;Portfolios;Investment;Computational modeling;Neural networks;Knowledge based systems;Artificial Intelligence (AI);Conversational Commerce;Deep Learning;Financial Technology (FinTech);Robo-Advisor,"{'month': 'Aug', 'issn': '2473-991X', 'doi': '10.1109/ASONAM.2018.8508269', 'keywords': 'Resource management;Portfolios;Investment;Computational modeling;Neural networks;Knowledge based systems;Artificial Intelligence (AI);Conversational Commerce;Deep Learning;Financial Technology (FinTech);Robo-Advisor', 'abstract': ""With the advent of the artificial intelligence (AI) era, the combination of AI with financial technology (FinTech) has become a development trend in the financial industry. However, deep learning (DL) on the application of automated financial management has been rarely investigated. Thus, this research focuses on the applications of FinTech and DL in asset allocation and aims to optimize investment portfolio. The best investment portfolio in index-based funds based on Taiwan's index-type security investment trust funds are the main investment targets. Time series models for DL, that is, long short-term memory, predict the increase of each investment target and find the best investment portfolio in combination with the relevant asset allocation theory. In this research, we use the Markowitz mean-variance and Black-Litterman models as our asset allocation models for robo-advisor. Results show that the Black-Litterman model has a better accumulated return performance than the Morkowitz model and outperforms other strategies. The Human-Computer Interaction (HCI) dialogue service adopts artificial intelligence markup language (AIML) and a generative model. The main contribution of this paper is that we have developed an integrated knowledge-based and generative-based models for AI conversational robo-advisor."", 'pages': '1057-1064', 'number': '', 'volume': '', 'year': '2018', 'title': 'Artificial Intelligence for Conversational Robo-Advisor', 'booktitle': '2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)', 'author': 'Day, Min-Yuh and Lin, Jian-Ting and Chen, Yuan-Chih', 'ENTRYTYPE': 'inproceedings', 'ID': '8508269'}"
10594061,Automatic Face Image Restoration Based on an Improved Generative Adversarial Network,"Shi, Yuzhe",Shi,10.1109/ICETCI61221.2024.10594061,2024,"2024 IEEE 4th International Conference on Electronic Technology, Communication and Information (ICETCI)","Recent advance in computer network and camera technology, the volume of face image data on the internet is expanding rapidly. However, these images often suffer from hole-like missing or occlusions that can compromise their visual presentation. Identifying and reconstructing such occlusions is a key solution for these problems, which aims to detect all the missing and fill them with suitable pixels to complete the restoration task. The technology remains a challenge due to the difficulties of high computational complexity, varied shapes of obstructions and high requirement on the visual consistency across the image. In this paper, we propose a novel face image restoration method that builds on the traditional generative adversarial network (GAN) by incorporating a classifier. This addition enables the method to classify the images based on the different face features to be repaired and the classified images are repaired after entering the generative adversarial network of the corresponding category. Experimental results indicate that the proposed method can achieve great performance in many indexes and produce better results when compared with existing methods.",Visualization;Shape;Generative adversarial networks;Feature extraction;Generators;Skin;Image restoration;Face image restoration;Generative adversarial network;Image classifier;Face feature,"{'month': 'May', 'issn': '', 'doi': '10.1109/ICETCI61221.2024.10594061', 'keywords': 'Visualization;Shape;Generative adversarial networks;Feature extraction;Generators;Skin;Image restoration;Face image restoration;Generative adversarial network;Image classifier;Face feature', 'abstract': 'Recent advance in computer network and camera technology, the volume of face image data on the internet is expanding rapidly. However, these images often suffer from hole-like missing or occlusions that can compromise their visual presentation. Identifying and reconstructing such occlusions is a key solution for these problems, which aims to detect all the missing and fill them with suitable pixels to complete the restoration task. The technology remains a challenge due to the difficulties of high computational complexity, varied shapes of obstructions and high requirement on the visual consistency across the image. In this paper, we propose a novel face image restoration method that builds on the traditional generative adversarial network (GAN) by incorporating a classifier. This addition enables the method to classify the images based on the different face features to be repaired and the classified images are repaired after entering the generative adversarial network of the corresponding category. Experimental results indicate that the proposed method can achieve great performance in many indexes and produce better results when compared with existing methods.', 'pages': '358-365', 'number': '', 'volume': '', 'year': '2024', 'title': 'Automatic Face Image Restoration Based on an Improved Generative Adversarial Network', 'booktitle': '2024 IEEE 4th International Conference on Electronic Technology, Communication and Information (ICETCI)', 'author': 'Shi, Yuzhe', 'ENTRYTYPE': 'inproceedings', 'ID': '10594061'}"
10042182,Leveraging Adversarial Augmentation on Imbalance Data for Online Trading Fraud Detection,"Teng, Hu and Wang, Cheng and Yang, Qing and Chen, Xue and Li, Rui",Teng,10.1109/TCSS.2023.3240968,2024,IEEE Transactions on Computational Social Systems,"Nowadays, the emergence of online trading greatly facilitates people’s life. Meanwhile, online trading also brings hidden dangers, such as online fraudulent trading. To solve the issue, researchers have proposed many different detection models. However, in actual business scenarios, fraudulent transactions usually only account for a small portion of normal transactions, resulting in extremely imbalanced data. Besides, the concealment of fraud is reflected in that the fraudsters are imitating the normal transactions of users, posing a huge challenge for fraudulent transaction detection modeling. Inspired by generative adversarial networks (GANs), we propose a GAN-based framework to detect online banking fraud on extremely imbalanced data, called BalanceGAN. A fraud detection model is first pretrained using the data generated by the generator and then the model is fine-tuned using transfer learning on real-world datasets, by using this approach to address data imbalances. Compared with the conventional methods for solving imbalanced data, our BalanceGAN can avoid over-fitting of the model relatively, experiments on two real datasets show that our BalanceGAN has more than 10\% performance improvement in Precision and Recall.",Fraud;Data models;Transfer learning;Generators;Biological system modeling;Training data;Generative adversarial networks;Fraud detection;generative adversarial networks (GANs);imbalanced data,"{'month': 'April', 'issn': '2329-924X', 'doi': '10.1109/TCSS.2023.3240968', 'keywords': 'Fraud;Data models;Transfer learning;Generators;Biological system modeling;Training data;Generative adversarial networks;Fraud detection;generative adversarial networks (GANs);imbalanced data', 'abstract': 'Nowadays, the emergence of online trading greatly facilitates people’s life. Meanwhile, online trading also brings hidden dangers, such as online fraudulent trading. To solve the issue, researchers have proposed many different detection models. However, in actual business scenarios, fraudulent transactions usually only account for a small portion of normal transactions, resulting in extremely imbalanced data. Besides, the concealment of fraud is reflected in that the fraudsters are imitating the normal transactions of users, posing a huge challenge for fraudulent transaction detection modeling. Inspired by generative adversarial networks (GANs), we propose a GAN-based framework to detect online banking fraud on extremely imbalanced data, called BalanceGAN. A fraud detection model is first pretrained using the data generated by the generator and then the model is fine-tuned using transfer learning on real-world datasets, by using this approach to address data imbalances. Compared with the conventional methods for solving imbalanced data, our BalanceGAN can avoid over-fitting of the model relatively, experiments on two real datasets show that our BalanceGAN has more than 10\\% performance improvement in Precision and Recall.', 'pages': '1602-1614', 'number': '2', 'volume': '11', 'year': '2024', 'title': 'Leveraging Adversarial Augmentation on Imbalance Data for Online Trading Fraud Detection', 'journal': 'IEEE Transactions on Computational Social Systems', 'author': 'Teng, Hu and Wang, Cheng and Yang, Qing and Chen, Xue and Li, Rui', 'ENTRYTYPE': 'article', 'ID': '10042182'}"
9940286,View-Normalized and Subject-Independent Skeleton Generation for Action Recognition,"Pan, Qingzhe and Zhao, Zhifu and Xie, Xuemei and Li, Jianan and Cao, Yuhan and Shi, Guangming",Pan,10.1109/TCSVT.2022.3219864,2023,IEEE Transactions on Circuits and Systems for Video Technology,"Skeleton-based action recognition has attracted great interest in computer vision. For this task, a challenging problem concerns the large intraclass variances of skeleton data, which are mainly caused by diverse viewpoints and subjects, and greatly increase the difficulty of modeling actions through a network. To address the above problem, we propose a variance reduction (VaRe) framework for skeleton-based action recognition, which consists of a view-normalization generative adversarial network (VN-GAN), a subject-independent network (SINet) and a classification network. First, the VN-GAN is responsible for reducing view-induced intraclass variances. Specifically, this network, comprising a generator and a discriminator, is aimed at learning a mapping from a diverse-view skeleton distribution to a unified-view skeleton distribution in an unsupervised manner, thereby generating a view-normalized skeleton. Second, taking the view-normalized skeleton as input, the SINet focuses on reducing the influences of the personal habits of subjects on action recognition. To generate SI skeleton data, the SINet automatically adjusts the human pose according to the human kinematic structure under a classification loss constraint. Finally, without the interference of view- and subject-induced variances, the classification network can concentrate more on learning discriminative action features to predict classes. Furthermore, by combining the joint and bone modalities, the proposed framework achieves competitive performance on three benchmarks: NTU RGB+D, NTU-120 RGB+D and Northwestern-UCLA Multiview Action 3D.",Kinematics;Feature extraction;Data models;Spatiotemporal phenomena;Generative adversarial networks;Training;Skeleton-based action recognition;view-normalized;subject-independent;generative adversarial network;human kinematic structure,"{'month': 'Dec', 'issn': '1558-2205', 'doi': '10.1109/TCSVT.2022.3219864', 'keywords': 'Kinematics;Feature extraction;Data models;Spatiotemporal phenomena;Generative adversarial networks;Training;Skeleton-based action recognition;view-normalized;subject-independent;generative adversarial network;human kinematic structure', 'abstract': 'Skeleton-based action recognition has attracted great interest in computer vision. For this task, a challenging problem concerns the large intraclass variances of skeleton data, which are mainly caused by diverse viewpoints and subjects, and greatly increase the difficulty of modeling actions through a network. To address the above problem, we propose a variance reduction (VaRe) framework for skeleton-based action recognition, which consists of a view-normalization generative adversarial network (VN-GAN), a subject-independent network (SINet) and a classification network. First, the VN-GAN is responsible for reducing view-induced intraclass variances. Specifically, this network, comprising a generator and a discriminator, is aimed at learning a mapping from a diverse-view skeleton distribution to a unified-view skeleton distribution in an unsupervised manner, thereby generating a view-normalized skeleton. Second, taking the view-normalized skeleton as input, the SINet focuses on reducing the influences of the personal habits of subjects on action recognition. To generate SI skeleton data, the SINet automatically adjusts the human pose according to the human kinematic structure under a classification loss constraint. Finally, without the interference of view- and subject-induced variances, the classification network can concentrate more on learning discriminative action features to predict classes. Furthermore, by combining the joint and bone modalities, the proposed framework achieves competitive performance on three benchmarks: NTU RGB+D, NTU-120 RGB+D and Northwestern-UCLA Multiview Action 3D.', 'pages': '7398-7412', 'number': '12', 'volume': '33', 'year': '2023', 'title': 'View-Normalized and Subject-Independent Skeleton Generation for Action Recognition', 'journal': 'IEEE Transactions on Circuits and Systems for Video Technology', 'author': 'Pan, Qingzhe and Zhao, Zhifu and Xie, Xuemei and Li, Jianan and Cao, Yuhan and Shi, Guangming', 'ENTRYTYPE': 'article', 'ID': '9940286'}"
10643547,A Proposed Model for Distinguishing Between Human-Based and ChatGPT Content in Scientific Articles,"Mohamed, Toka A. and Khafgy, Mohamed H. and Elsedawy, Ahmed B. and Ismail, Ahmed S.",Mohamed,10.1109/ACCESS.2024.3448315,2024,IEEE Access,"This study introduces an innovative approach to address the growing challenge of detecting and distinguishing ChatGPT-generated content within scientific articles, particularly in the context of Learning Management Systems (LMS). Leveraging state-of-the-art large language models, including Robustly Optimized BERT Pretraining (RoBERTa), Text-to-Text Transfer Transformer (T5), and Generative Pre-trained Transformers (EleutherAI GPT-Neo-125M), our methodology focuses on the incorporation of the LMS concept into the research framework. To construct a comprehensive dataset representative of the diverse landscape of scientific abstracts, samples of the dataset are gathered from articles produced by human authors and those generated by ChatGPT within the LMS framework. The models (RoBERTa, T5, and EleutherAI GPT-Neo-125M) were subsequently trained on this unique dataset, showcasing their adaptability to the distinct characteristics of both human-generated and AI-generated content within the LMS context. The efficacy of our approach was rigorously evaluated using a range of metrics, resulting in an outstanding accuracy exceeding 99\%. This achievement underscores the robustness of our methodology in successfully discerning content generated by ChatGPT within the LMS and that authored by human contributors, thereby advancing the field of content differentiation in scientific discourse.",Chatbots;Data models;Accuracy;Training;Large language models;Transformers;Artificial intelligence;Content management;GPT-3.5;LLM;ChatGPT;T5;RoBERTa;EleutherAI GPT-Neo-125M;AI content;LMS,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2024.3448315', 'keywords': 'Chatbots;Data models;Accuracy;Training;Large language models;Transformers;Artificial intelligence;Content management;GPT-3.5;LLM;ChatGPT;T5;RoBERTa;EleutherAI GPT-Neo-125M;AI content;LMS', 'abstract': 'This study introduces an innovative approach to address the growing challenge of detecting and distinguishing ChatGPT-generated content within scientific articles, particularly in the context of Learning Management Systems (LMS). Leveraging state-of-the-art large language models, including Robustly Optimized BERT Pretraining (RoBERTa), Text-to-Text Transfer Transformer (T5), and Generative Pre-trained Transformers (EleutherAI GPT-Neo-125M), our methodology focuses on the incorporation of the LMS concept into the research framework. To construct a comprehensive dataset representative of the diverse landscape of scientific abstracts, samples of the dataset are gathered from articles produced by human authors and those generated by ChatGPT within the LMS framework. The models (RoBERTa, T5, and EleutherAI GPT-Neo-125M) were subsequently trained on this unique dataset, showcasing their adaptability to the distinct characteristics of both human-generated and AI-generated content within the LMS context. The efficacy of our approach was rigorously evaluated using a range of metrics, resulting in an outstanding accuracy exceeding 99\\%. This achievement underscores the robustness of our methodology in successfully discerning content generated by ChatGPT within the LMS and that authored by human contributors, thereby advancing the field of content differentiation in scientific discourse.', 'pages': '121251-121260', 'number': '', 'volume': '12', 'year': '2024', 'title': 'A Proposed Model for Distinguishing Between Human-Based and ChatGPT Content in Scientific Articles', 'journal': 'IEEE Access', 'author': 'Mohamed, Toka A. and Khafgy, Mohamed H. and Elsedawy, Ahmed B. and Ismail, Ahmed S.', 'ENTRYTYPE': 'article', 'ID': '10643547'}"
10816449,HVT-cGAN: Hybrid Vision Transformer cGAN for SAR-to-Optical Image Translation,"Zhao, Wenbo and Jiang, Nana and Liao, Xiaoxin and Zhu, Jubo",Zhao,10.1109/TGRS.2024.3523040,2025,IEEE Transactions on Geoscience and Remote Sensing,"Due to its capability for all-weather, all-time information acquisition, synthetic aperture radar (SAR) plays a vital role in the field of Earth observation. However, the specificity of the radar sensor and the complexity of electromagnetic scattering imaging physics result in SAR images lacking the intuitiveness of optical images, making them unsuitable for interpretation by nonexperts. A common approach to tackle this challenge is to use a conditional generative adversarial network (cGAN) to translate SAR images into optical images, thereby enhancing readability and assisting nonexperts in interpretation while filling the gaps in optical data due to acquisition constraints. Nevertheless, traditional cGAN-based methods are limited by inadequate global semantic information extraction and poor detail preservation, leading to translated images with incoherent texture and color, and blurred edge. To address these issues, we propose a hybrid vision transformer cGAN (HVT-cGAN) for SAR-to-optical image translation (S2OIT). In our proposed HVT-cGAN, the generator utilizes a convolutional stem for patch embedding and encoding. The parallel CNN branch and vision transformer (ViT) branch are employed for the extraction and mapping of local and global information, respectively. Moreover, we propose a novel attention-based feature fusion module, named the convolutional attention fusion module (CAFM), which can adaptively aggregate local and global information from parallel branches by learning both channel-wise and spatial-wise relations. Benefiting from these improvements, our method achieves superior performance in both qualitative and quantitative comparisons with other methods on the SEN1-2 dataset. In addition, the results of multiple ablation experiments validate the effectiveness of the proposed method.",Optical sensors;Optical imaging;Transformers;Generative adversarial networks;Translation;Radar polarimetry;Adaptive optics;Optical interferometry;Computer vision;Training;Generative adversarial network (GAN);hybrid vision transformer (ViT);image translation;synthetic aperture radar (SAR),"{'month': '', 'issn': '1558-0644', 'doi': '10.1109/TGRS.2024.3523040', 'keywords': 'Optical sensors;Optical imaging;Transformers;Generative adversarial networks;Translation;Radar polarimetry;Adaptive optics;Optical interferometry;Computer vision;Training;Generative adversarial network (GAN);hybrid vision transformer (ViT);image translation;synthetic aperture radar (SAR)', 'abstract': 'Due to its capability for all-weather, all-time information acquisition, synthetic aperture radar (SAR) plays a vital role in the field of Earth observation. However, the specificity of the radar sensor and the complexity of electromagnetic scattering imaging physics result in SAR images lacking the intuitiveness of optical images, making them unsuitable for interpretation by nonexperts. A common approach to tackle this challenge is to use a conditional generative adversarial network (cGAN) to translate SAR images into optical images, thereby enhancing readability and assisting nonexperts in interpretation while filling the gaps in optical data due to acquisition constraints. Nevertheless, traditional cGAN-based methods are limited by inadequate global semantic information extraction and poor detail preservation, leading to translated images with incoherent texture and color, and blurred edge. To address these issues, we propose a hybrid vision transformer cGAN (HVT-cGAN) for SAR-to-optical image translation (S2OIT). In our proposed HVT-cGAN, the generator utilizes a convolutional stem for patch embedding and encoding. The parallel CNN branch and vision transformer (ViT) branch are employed for the extraction and mapping of local and global information, respectively. Moreover, we propose a novel attention-based feature fusion module, named the convolutional attention fusion module (CAFM), which can adaptively aggregate local and global information from parallel branches by learning both channel-wise and spatial-wise relations. Benefiting from these improvements, our method achieves superior performance in both qualitative and quantitative comparisons with other methods on the SEN1-2 dataset. In addition, the results of multiple ablation experiments validate the effectiveness of the proposed method.', 'pages': '1-17', 'number': '', 'volume': '63', 'year': '2025', 'title': 'HVT-cGAN: Hybrid Vision Transformer cGAN for SAR-to-Optical Image Translation', 'journal': 'IEEE Transactions on Geoscience and Remote Sensing', 'author': 'Zhao, Wenbo and Jiang, Nana and Liao, Xiaoxin and Zhu, Jubo', 'ENTRYTYPE': 'article', 'ID': '10816449'}"
10690548,Optimizing Resource Utilization in Generative AI Ensembles for Edge Computing,"Rao, N.Venkatesvara and Swaminathan, Subbiah and Kanimozhi, K.V. and Manikandan, S.P. and Seethalakshmi, K.",Rao,10.1109/ICAIT61638.2024.10690548,2024,2024 Second International Conference on Advances in Information Technology (ICAIT),"Edge computing in this period makes it difficult to deploy, AI ensemble on resource limited devices together flow the computation complexity and the system performs. The methods state below presents a comprehensive approach to the optimizing envelope of resources in generative AI sheaves for edge computing while characteristic adaptability, privacy and efficiency are minded. With improvement of model compression, hardware acceleration, and model optimization, computational cost can be reduced with high quality generated objects. Privately-oriented algorithms like federated learning and differential privacy provide entity safety of the data in the distributed and collaborative edge environments. Intelligent distribution resources management policies perform such actions instantaneously on-demand and automatically, hence improving performance. The experiments prove that the methodology is worthy to trusted adopting as it facilitates calculational efficiency, generates high quality results and also protects the information of the users. The ongoing evolution and sharing of new technologies including generative AI enhance edge intelligence and stimulate innovative breakthroughs in next generation devices while influencing society.",Performance evaluation;Adaptation models;Quantum computing;Generative AI;Federated learning;Computational modeling;Scalability;Resource management;Optimization;Edge computing;Generative AI;Resource utilization optimization;Edge computing;Compression techniques;Privacy-preserving mechanisms,"{'month': 'July', 'issn': '', 'doi': '10.1109/ICAIT61638.2024.10690548', 'keywords': 'Performance evaluation;Adaptation models;Quantum computing;Generative AI;Federated learning;Computational modeling;Scalability;Resource management;Optimization;Edge computing;Generative AI;Resource utilization optimization;Edge computing;Compression techniques;Privacy-preserving mechanisms', 'abstract': 'Edge computing in this period makes it difficult to deploy, AI ensemble on resource limited devices together flow the computation complexity and the system performs. The methods state below presents a comprehensive approach to the optimizing envelope of resources in generative AI sheaves for edge computing while characteristic adaptability, privacy and efficiency are minded. With improvement of model compression, hardware acceleration, and model optimization, computational cost can be reduced with high quality generated objects. Privately-oriented algorithms like federated learning and differential privacy provide entity safety of the data in the distributed and collaborative edge environments. Intelligent distribution resources management policies perform such actions instantaneously on-demand and automatically, hence improving performance. The experiments prove that the methodology is worthy to trusted adopting as it facilitates calculational efficiency, generates high quality results and also protects the information of the users. The ongoing evolution and sharing of new technologies including generative AI enhance edge intelligence and stimulate innovative breakthroughs in next generation devices while influencing society.', 'pages': '1-6', 'number': '', 'volume': '1', 'year': '2024', 'title': 'Optimizing Resource Utilization in Generative AI Ensembles for Edge Computing', 'booktitle': '2024 Second International Conference on Advances in Information Technology (ICAIT)', 'author': 'Rao, N.Venkatesvara and Swaminathan, Subbiah and Kanimozhi, K.V. and Manikandan, S.P. and Seethalakshmi, K.', 'ENTRYTYPE': 'inproceedings', 'ID': '10690548'}"
10910483,"The Ethics of Generative AI: Analyzing ChatGPT's Impact on Bias, Fairness, Privacy, and Accountability","Beg, Mohd Haider Raza and Mehndiratta, Vandana",Beg,10.1109/ICSES63760.2024.10910483,2024,"2024 International Conference on Innovative Computing, Intelligent Communication and Smart Electrical Systems (ICSES)","Pros and cons of Generative Artificial Intelligence (Intent), Many fields has its own use-case space for GAI. ChatGPT and systems like it give a sophisticated understanding of cognition along with content that is indistinguishable from humans, however these systems raise ethical issues in professional and social contexts. Challenges in the broad adoption of AI technologies including, accountability, fairness, bias and privacy arise as issues. These reforms will be critical as AI systems continue to be both developed and deployed, maintaining the crucial ethical standards and underpinning social trust. In the context of algorithmic biases, potential privacy violations, and an increased amount of risk due to automation in these critical decision areas; this paper provides a commentary regarding ethical concerns using generative AI. In addition, is the necessity of addressing these ethical challenges with tools like AI Ethics Impact Assessments, VADER Sentiment Analysis, TensorFlow's Indicators of Fairness, and continuous conversation.",Ethics;Privacy;Sentiment analysis;Automation;Generative AI;Oral communication;Chatbots;Cognition;Standards;GAI;Ethical Concern;Algorithm biases;bias response,"{'month': 'Dec', 'issn': '', 'doi': '10.1109/ICSES63760.2024.10910483', 'keywords': 'Ethics;Privacy;Sentiment analysis;Automation;Generative AI;Oral communication;Chatbots;Cognition;Standards;GAI;Ethical Concern;Algorithm biases;bias response', 'abstract': ""Pros and cons of Generative Artificial Intelligence (Intent), Many fields has its own use-case space for GAI. ChatGPT and systems like it give a sophisticated understanding of cognition along with content that is indistinguishable from humans, however these systems raise ethical issues in professional and social contexts. Challenges in the broad adoption of AI technologies including, accountability, fairness, bias and privacy arise as issues. These reforms will be critical as AI systems continue to be both developed and deployed, maintaining the crucial ethical standards and underpinning social trust. In the context of algorithmic biases, potential privacy violations, and an increased amount of risk due to automation in these critical decision areas; this paper provides a commentary regarding ethical concerns using generative AI. In addition, is the necessity of addressing these ethical challenges with tools like AI Ethics Impact Assessments, VADER Sentiment Analysis, TensorFlow's Indicators of Fairness, and continuous conversation."", 'pages': '1-7', 'number': '', 'volume': '', 'year': '2024', 'title': ""The Ethics of Generative AI: Analyzing ChatGPT's Impact on Bias, Fairness, Privacy, and Accountability"", 'booktitle': '2024 International Conference on Innovative Computing, Intelligent Communication and Smart Electrical Systems (ICSES)', 'author': 'Beg, Mohd Haider Raza and Mehndiratta, Vandana', 'ENTRYTYPE': 'inproceedings', 'ID': '10910483'}"
10508937,Performance Comparison and Visualization of AI-Generated-Image Detection Methods,"Park, Daeeol and Na, Hyunsik and Choi, Daeseon",Park,10.1109/ACCESS.2024.3394250,2024,IEEE Access,"Recent advancements in artificial intelligence (AI) have revolutionized the field of image generation. This has concurrently escalated social problems and concerns related to AI image generation, underscoring the necessity for an effective AI-generated-image detection method. Therefore, numerous methods for detecting AI-generated images have been developed, but there remains a need for research comparing the effectiveness of and visualizing these detection methods. In this study, we classify AI-generated-image detection methods by the image features they use and compare their generalization performance in detecting AI-generated images of different types. We selected five AI-generated-image detection methods for performance evaluation and selected vision transformer as an additional method for comparison. We use two types of training datasets, i.e., ProGAN and latent diffusion; combine existing AI-generated-image test datasets into a diverse test dataset; and divide them into three types of generative models, i.e., generative adversarial network (GAN), diffusion, and transformer, to evaluate the comprehensive performance of the detection methods. We also analyze their detection performance on images with data augmentation, considering scenarios that make it difficult to detect AI-generated images. Grad-CAM and t-SNE are used to visualize the detection area and data distribution of each detection method. As a result, we determine that artifact-feature-based detection performs well on GAN and real images, whereas image-encoder-feature-based detection performs well on diffusion and transformer images. In summary, our research analyzes the comparative detection performance of various AI-generated-image detection methods, identifies their limitations, and suggests directions for further research.",Artificial intelligence;Transformers;Generative adversarial networks;Training;Solid modeling;Generators;Feature extraction;Generative AI;AI-generated-image detection;synthetic-image detection;performance comparison;GAN;diffusion model;transformer;Grad-CAM;t-SNE,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2024.3394250', 'keywords': 'Artificial intelligence;Transformers;Generative adversarial networks;Training;Solid modeling;Generators;Feature extraction;Generative AI;AI-generated-image detection;synthetic-image detection;performance comparison;GAN;diffusion model;transformer;Grad-CAM;t-SNE', 'abstract': 'Recent advancements in artificial intelligence (AI) have revolutionized the field of image generation. This has concurrently escalated social problems and concerns related to AI image generation, underscoring the necessity for an effective AI-generated-image detection method. Therefore, numerous methods for detecting AI-generated images have been developed, but there remains a need for research comparing the effectiveness of and visualizing these detection methods. In this study, we classify AI-generated-image detection methods by the image features they use and compare their generalization performance in detecting AI-generated images of different types. We selected five AI-generated-image detection methods for performance evaluation and selected vision transformer as an additional method for comparison. We use two types of training datasets, i.e., ProGAN and latent diffusion; combine existing AI-generated-image test datasets into a diverse test dataset; and divide them into three types of generative models, i.e., generative adversarial network (GAN), diffusion, and transformer, to evaluate the comprehensive performance of the detection methods. We also analyze their detection performance on images with data augmentation, considering scenarios that make it difficult to detect AI-generated images. Grad-CAM and t-SNE are used to visualize the detection area and data distribution of each detection method. As a result, we determine that artifact-feature-based detection performs well on GAN and real images, whereas image-encoder-feature-based detection performs well on diffusion and transformer images. In summary, our research analyzes the comparative detection performance of various AI-generated-image detection methods, identifies their limitations, and suggests directions for further research.', 'pages': '62609-62627', 'number': '', 'volume': '12', 'year': '2024', 'title': 'Performance Comparison and Visualization of AI-Generated-Image Detection Methods', 'journal': 'IEEE Access', 'author': 'Park, Daeeol and Na, Hyunsik and Choi, Daeseon', 'ENTRYTYPE': 'article', 'ID': '10508937'}"
10433140,Enhancing Reasoning Ability in Semantic Communication Through Generative AI-Assisted Knowledge Construction,"Zhao, Fangzhou and Sun, Yao and Feng, Lei and Zhang, Lan and Zhao, Dezong",Zhao,10.1109/LCOMM.2024.3365158,2024,IEEE Communications Letters,"Semantic communication (SemCom), a pioneering paradigm that places emphasis on conveying the meaning of information, faces challenges in constructing background knowledge to drive precise reasoning of semantic coding models. Fortunately, the recent emergence of Generative Artificial Intelligence (GAI) technology is promising to create high-quality content that can be harnessed to assist knowledge construction in SemCom, enhancing the reasoning ability of semantic coding models. In this letter, we propose a GAI-assisted SemCom framework, named Gen-SC, where sufficient samples for training SemCom transceivers are generated using GAI as per user contextual information. In addition, to guide the GAI model in producing contextually relevant content, a discriminator is incorporated into Gen-SC to measure the disparity between generated samples and actual samples. The simulation results demonstrate that the Gen-SC achieves higher semantic accuracy, especially when the original training samples are insufficient, in contrast to traditional SemCom without knowledge enhancement.",Semantics;Training;Decoding;Mutual information;Knowledge engineering;Encoding;Transceivers;Semantic communication;generative artificial intelligence;background knowledge construction,"{'month': 'April', 'issn': '1558-2558', 'doi': '10.1109/LCOMM.2024.3365158', 'keywords': 'Semantics;Training;Decoding;Mutual information;Knowledge engineering;Encoding;Transceivers;Semantic communication;generative artificial intelligence;background knowledge construction', 'abstract': 'Semantic communication (SemCom), a pioneering paradigm that places emphasis on conveying the meaning of information, faces challenges in constructing background knowledge to drive precise reasoning of semantic coding models. Fortunately, the recent emergence of Generative Artificial Intelligence (GAI) technology is promising to create high-quality content that can be harnessed to assist knowledge construction in SemCom, enhancing the reasoning ability of semantic coding models. In this letter, we propose a GAI-assisted SemCom framework, named Gen-SC, where sufficient samples for training SemCom transceivers are generated using GAI as per user contextual information. In addition, to guide the GAI model in producing contextually relevant content, a discriminator is incorporated into Gen-SC to measure the disparity between generated samples and actual samples. The simulation results demonstrate that the Gen-SC achieves higher semantic accuracy, especially when the original training samples are insufficient, in contrast to traditional SemCom without knowledge enhancement.', 'pages': '832-836', 'number': '4', 'volume': '28', 'year': '2024', 'title': 'Enhancing Reasoning Ability in Semantic Communication Through Generative AI-Assisted Knowledge Construction', 'journal': 'IEEE Communications Letters', 'author': 'Zhao, Fangzhou and Sun, Yao and Feng, Lei and Zhang, Lan and Zhao, Dezong', 'ENTRYTYPE': 'article', 'ID': '10433140'}"
11152871,Generative AI-Driven Cross-Modal Semantic Communication System for Visual Transmission,"Fang, Zechuan and Sun, Mengying and Zhang, Yuantao and Wang, Haiming and Xu, Xiaodong",Fang,10.1109/INFOCOMWKSHPS65812.2025.11152871,2025,IEEE INFOCOM 2025 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS),"This paper proposes a generative artificial intelligence-driven cross-modal semantic communication framework (GC-SemCom) designed for efficient image transmission. Specifically, at the transmitter, a multi-modal large language model is utilized to extract compact semantic information, i.e. image difference captions between the original image and retrieved background knowledge. This semantic information, expressed in textual instruction, is then transmitted based on joint source-channel codec. At the receiver, the diffusion model, guided by both received semantic information and synchronized background knowledge, iteratively generates the recovered image through the reverse denoising process. Furthermore, the semantic knowledge base provides background knowledge for both transceivers, defined as the semantically similar image retrieved. Experimental results demonstrate that the proposed framework reduces the overall data size to just 0.06\% of the original data, while maintaining high-quality transmission even under substantial noise conditions.",Visualization;Transmitters;Simulation;Noise reduction;Noise;Receivers;Semantic communication;Transceivers;Data mining;Synchronization;generative artificial intelligence (GenAI);semantic communication (SemCom);cross-modal transmission,"{'month': 'May', 'issn': '2833-0587', 'doi': '10.1109/INFOCOMWKSHPS65812.2025.11152871', 'keywords': 'Visualization;Transmitters;Simulation;Noise reduction;Noise;Receivers;Semantic communication;Transceivers;Data mining;Synchronization;generative artificial intelligence (GenAI);semantic communication (SemCom);cross-modal transmission', 'abstract': 'This paper proposes a generative artificial intelligence-driven cross-modal semantic communication framework (GC-SemCom) designed for efficient image transmission. Specifically, at the transmitter, a multi-modal large language model is utilized to extract compact semantic information, i.e. image difference captions between the original image and retrieved background knowledge. This semantic information, expressed in textual instruction, is then transmitted based on joint source-channel codec. At the receiver, the diffusion model, guided by both received semantic information and synchronized background knowledge, iteratively generates the recovered image through the reverse denoising process. Furthermore, the semantic knowledge base provides background knowledge for both transceivers, defined as the semantically similar image retrieved. Experimental results demonstrate that the proposed framework reduces the overall data size to just 0.06\\% of the original data, while maintaining high-quality transmission even under substantial noise conditions.', 'pages': '1-2', 'number': '', 'volume': '', 'year': '2025', 'title': 'Generative AI-Driven Cross-Modal Semantic Communication System for Visual Transmission', 'booktitle': 'IEEE INFOCOM 2025 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)', 'author': 'Fang, Zechuan and Sun, Mengying and Zhang, Yuantao and Wang, Haiming and Xu, Xiaodong', 'ENTRYTYPE': 'inproceedings', 'ID': '11152871'}"
10448434,Subdivision Features-Guided Brain MRI Super-Resolution via Forward and Backward Propagation,"Hu, Jinbin and Sun, Xiaoxue and Bai, Xinhao and Qin, Yanding and Wang, Hongpeng and Han, Jianda",Hu,10.1109/ICASSP48485.2024.10448434,2024,"ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","There have been many models designed for super-resolution (SR) tasks. However, the universal model rarely takes into account the potential characteristics of magnetic resonance imaging (MRI). Further, for real-time MRI, SR is a critical technology to improve spatial resolution while maintaining temporal resolution. Trying not to change the existing well-established models, we propose a subdivision feature-guided enhancement module which can be attached to the arbitrary generative networks for the brain MRI SR tasks. The experimental results proved the effectiveness of our designed module.",Backpropagation;Magnetic resonance imaging;Superresolution;Signal processing;Brain modeling;Real-time systems;Acoustics;Subdivision features;brain MRI;super-resolution,"{'month': 'April', 'issn': '2379-190X', 'doi': '10.1109/ICASSP48485.2024.10448434', 'keywords': 'Backpropagation;Magnetic resonance imaging;Superresolution;Signal processing;Brain modeling;Real-time systems;Acoustics;Subdivision features;brain MRI;super-resolution', 'abstract': 'There have been many models designed for super-resolution (SR) tasks. However, the universal model rarely takes into account the potential characteristics of magnetic resonance imaging (MRI). Further, for real-time MRI, SR is a critical technology to improve spatial resolution while maintaining temporal resolution. Trying not to change the existing well-established models, we propose a subdivision feature-guided enhancement module which can be attached to the arbitrary generative networks for the brain MRI SR tasks. The experimental results proved the effectiveness of our designed module.', 'pages': '1666-1670', 'number': '', 'volume': '', 'year': '2024', 'title': 'Subdivision Features-Guided Brain MRI Super-Resolution via Forward and Backward Propagation', 'booktitle': 'ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)', 'author': 'Hu, Jinbin and Sun, Xiaoxue and Bai, Xinhao and Qin, Yanding and Wang, Hongpeng and Han, Jianda', 'ENTRYTYPE': 'inproceedings', 'ID': '10448434'}"
10086829,Tentative Program,,,10.1109/KST57286.2023.10086829,2023,2023 15th International Conference on Knowledge and Smart Technology (KST),,Computational intelligence;Image recognition;Face recognition;Real-time systems;Emotion recognition;Urban areas;Technological innovation,"{'month': 'Feb', 'issn': '2374-314X', 'doi': '10.1109/KST57286.2023.10086829', 'keywords': 'Computational intelligence;Image recognition;Face recognition;Real-time systems;Emotion recognition;Urban areas;Technological innovation', 'abstract': '', 'pages': 'i-iv', 'number': '', 'volume': '', 'year': '2023', 'title': 'Tentative Program', 'booktitle': '2023 15th International Conference on Knowledge and Smart Technology (KST)', 'ENTRYTYPE': 'inproceedings', 'ID': '10086829'}"
10614289,4 Productivity Leap \&\#x2013; Internet to the Power of 2,"Ojanperä, Tero",Ojanperä,,2024,AI Revolution: Mastering AI for Personal and Organizational Growth,"""The AI Revolution"" is a practical guide to using new AI tools, such as ChatGPT, DALLE and Midjourney. Learn how to multiply your productivity by guiding or prompting AI in various ways. The book also introduces Microsoft Copilot, Google Bard, and Adobe Photoshop Generative Fill, among other new applications. ChatGPT reached a hundred million users in just two months after its release, faster than any other application before. This marked the advent of the generative AI era. Generative AI models generate text, images, music, videos, and even 3D models in ways previously thought impossible for machines. The book explains in an understandable manner how these AI models work. The book provides examples of how AI increases productivity, which professions are changing or disappearing, and how job markets will evolve in the coming years. With this book, you'll learn to recognize the opportunities and risks AI offers. Understand what this change demands from individuals and companies and what strategic skills are required. The book also covers legal questions caused by generative AI, like copyrights, data protection, and AI regulation. It also ponders societal impacts. AI produces content, thus influencing language, culture, and even worldviews. Therefore, it's crucial to understand by whom and how AI is trained. The AI revolution started by ChatGPT is just the beginning. This handbook is for you if you want to keep up with the rapid development of AI.",,"{'url': 'https://ieeexplore-ieee-org.crai.referencistas.com/document/10614289', 'isbn': '9788770042314', 'publisher': 'River Publishers', 'issn': '', 'doi': '', 'keywords': '', 'abstract': '""The AI Revolution"" is a practical guide to using new AI tools, such as ChatGPT, DALLE and Midjourney. Learn how to multiply your productivity by guiding or prompting AI in various ways. The book also introduces Microsoft Copilot, Google Bard, and Adobe Photoshop Generative Fill, among other new applications. ChatGPT reached a hundred million users in just two months after its release, faster than any other application before. This marked the advent of the generative AI era. Generative AI models generate text, images, music, videos, and even 3D models in ways previously thought impossible for machines. The book explains in an understandable manner how these AI models work. The book provides examples of how AI increases productivity, which professions are changing or disappearing, and how job markets will evolve in the coming years. With this book, you\'ll learn to recognize the opportunities and risks AI offers. Understand what this change demands from individuals and companies and what strategic skills are required. The book also covers legal questions caused by generative AI, like copyrights, data protection, and AI regulation. It also ponders societal impacts. AI produces content, thus influencing language, culture, and even worldviews. Therefore, it\'s crucial to understand by whom and how AI is trained. The AI revolution started by ChatGPT is just the beginning. This handbook is for you if you want to keep up with the rapid development of AI.', 'pages': '83-90', 'number': '', 'volume': '', 'year': '2024', 'title': '4 Productivity Leap \\&\\#x2013; Internet to the Power of 2', 'booktitle': 'AI Revolution: Mastering AI for Personal and Organizational Growth', 'author': 'Ojanperä, Tero', 'ENTRYTYPE': 'inbook', 'ID': '10614289'}"
9087871,Predicting Students’ Performance With School and Family Tutoring Using Generative Adversarial Network-Based Deep Support Vector Machine,"Chui, Kwok Tai and Liu, Ryan Wen and Zhao, Mingbo and De Pablos, Patricia Ordóñez",Chui,10.1109/ACCESS.2020.2992869,2020,IEEE Access,"It has been witnessed that supportive learning has played a crucial role in educational quality enhancement. School and family tutoring offer personalized help and provide positive feedback on students' learning. Predicting students' performance is of much interest which reflects their understanding on the subjects. Particularly it is desired students to manage well in fundamental knowledge in order to build a strong foundation for post-secondary studies and career. In this paper, improved conditional generative adversarial network based deep support vector machine (ICGAN-DSVM) algorithm has been proposed to predict students' performance under supportive learning via school and family tutoring. Owning to the nature of the students' academic dataset is generally low sample size. ICGAN-DSVM offers dual benefits for the nature of low sample size in students' academic dataset in which ICGAN increases the data volume whereas DSVM enhances the prediction accuracy with deep learning architecture. Results with 10-fold cross-validation show that the proposed ICGAN-DSVM yields specificity, sensitivity and area under the receiver operating characteristic curve (AUC) of 0.968, 0.971 and 0.954 respectively. Results also suggest that incorporating both school and family tutoring into the prediction model could further improve the performance compared with only school tutoring and only family tutoring. To show the necessity of ICGAN and DSVM, comparison has been made between ICGAN and traditional conditional generative adversarial network (CGAN). Also, the proposed kernel design via heuristic based multiple kernel learning (MKL) is compared with typical kernels including linear, radial basis function (RBF), polynomial and sigmoid. The prediction of student's performance with and without GAN is presented which is followed by comparison with DSVM and with traditional SVM. The proposed ICGAN-DSVM outperforms related works by 8-29\% in terms of performance indicators specificity, sensitivity and AUC.",Support vector machines;Kernel;Education;Deep learning;Predictive models;Gallium nitride;Generative adversarial networks;Generative adversarial network;students’ academic performance;deep support vector machine;supportive learning,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2020.2992869', 'keywords': 'Support vector machines;Kernel;Education;Deep learning;Predictive models;Gallium nitride;Generative adversarial networks;Generative adversarial network;students’ academic performance;deep support vector machine;supportive learning', 'abstract': ""It has been witnessed that supportive learning has played a crucial role in educational quality enhancement. School and family tutoring offer personalized help and provide positive feedback on students' learning. Predicting students' performance is of much interest which reflects their understanding on the subjects. Particularly it is desired students to manage well in fundamental knowledge in order to build a strong foundation for post-secondary studies and career. In this paper, improved conditional generative adversarial network based deep support vector machine (ICGAN-DSVM) algorithm has been proposed to predict students' performance under supportive learning via school and family tutoring. Owning to the nature of the students' academic dataset is generally low sample size. ICGAN-DSVM offers dual benefits for the nature of low sample size in students' academic dataset in which ICGAN increases the data volume whereas DSVM enhances the prediction accuracy with deep learning architecture. Results with 10-fold cross-validation show that the proposed ICGAN-DSVM yields specificity, sensitivity and area under the receiver operating characteristic curve (AUC) of 0.968, 0.971 and 0.954 respectively. Results also suggest that incorporating both school and family tutoring into the prediction model could further improve the performance compared with only school tutoring and only family tutoring. To show the necessity of ICGAN and DSVM, comparison has been made between ICGAN and traditional conditional generative adversarial network (CGAN). Also, the proposed kernel design via heuristic based multiple kernel learning (MKL) is compared with typical kernels including linear, radial basis function (RBF), polynomial and sigmoid. The prediction of student's performance with and without GAN is presented which is followed by comparison with DSVM and with traditional SVM. The proposed ICGAN-DSVM outperforms related works by 8-29\\% in terms of performance indicators specificity, sensitivity and AUC."", 'pages': '86745-86752', 'number': '', 'volume': '8', 'year': '2020', 'title': 'Predicting Students’ Performance With School and Family Tutoring Using Generative Adversarial Network-Based Deep Support Vector Machine', 'journal': 'IEEE Access', 'author': 'Chui, Kwok Tai and Liu, Ryan Wen and Zhao, Mingbo and De Pablos, Patricia Ordóñez', 'ENTRYTYPE': 'article', 'ID': '9087871'}"
11166628,Comparing Generative Artificial Intelligence (GenAI) Models’ Capabilities in Solving Cryptographic Problems and Algorithms,"Almadhoob, Ali and Qarooni, Ahmed and Khadem, Hasan",Almadhoob,10.1109/ACDSA65407.2025.11166628,2025,"2025 International Conference on Artificial Intelligence, Computer, Data Sciences and Applications (ACDSA)","This study aims to evaluate the capabilities of four large language models (LLMs) that include ChatGPT 4o, Claude 3.5 Sonnet, Copilot, and Gemini 1.5 Flash in different cryptographic tasks and algorithms, mainly solving foundation mathematical problems and classical algorithms in cryptography, while also generating code and performing cryptoanalysis. Using a structured methodology, this study assesses the performance of these models in such tasks. The results of this research vary between models, as some are excelling in simpler tasks (e.g Caesar Cipher), but struggle when it comes to more complex algorithms (e.g AES, DES) and large number factorizations and computations.",Ciphers;Codes;Generative AI;Computational modeling;Large language models;Chatbots;Mathematical models;Data models;Cryptography;Language Models;Mathematical Concepts;Cryptographic Algorithms;Code Generation;Cryptanalysis,"{'month': 'Aug', 'issn': '', 'doi': '10.1109/ACDSA65407.2025.11166628', 'keywords': 'Ciphers;Codes;Generative AI;Computational modeling;Large language models;Chatbots;Mathematical models;Data models;Cryptography;Language Models;Mathematical Concepts;Cryptographic Algorithms;Code Generation;Cryptanalysis', 'abstract': 'This study aims to evaluate the capabilities of four large language models (LLMs) that include ChatGPT 4o, Claude 3.5 Sonnet, Copilot, and Gemini 1.5 Flash in different cryptographic tasks and algorithms, mainly solving foundation mathematical problems and classical algorithms in cryptography, while also generating code and performing cryptoanalysis. Using a structured methodology, this study assesses the performance of these models in such tasks. The results of this research vary between models, as some are excelling in simpler tasks (e.g Caesar Cipher), but struggle when it comes to more complex algorithms (e.g AES, DES) and large number factorizations and computations.', 'pages': '1-6', 'number': '', 'volume': '', 'year': '2025', 'title': 'Comparing Generative Artificial Intelligence (GenAI) Models’ Capabilities in Solving Cryptographic Problems and Algorithms', 'booktitle': '2025 International Conference on Artificial Intelligence, Computer, Data Sciences and Applications (ACDSA)', 'author': 'Almadhoob, Ali and Qarooni, Ahmed and Khadem, Hasan', 'ENTRYTYPE': 'inproceedings', 'ID': '11166628'}"
10488939,AI Driven IoT Healthcare Devices Security Vulnerability Management,"Bajpayi, Pragya and Sharma, Smita and Gaur, Madhu Sharma",Bajpayi,10.1109/ICDT61202.2024.10488939,2024,2024 2nd International Conference on Disruptive Technologies (ICDT),"In the modern healthcare ecosystem, emerging technologies, Internet of Things(IoT) medical devices playing major role for providing seamless and remote clinical care, disease diagnosis, patient monitoring and many more healthcare services. Wireless, seamless sensor devices and underlying infrastructure creates the IoT ecosystem. An IoT device identification and vital healthcare data collection are the most important where vulnerability management and security is a critical aspect of IoT security Against a number of vulnerabilities, attacks and cyber threats, a proactive approach needs to be builds for Internet of Things medical devices security resilience. In the current fast moving digital transformation, integrating Artificial Intelligence (AI)-driven safety measures into IoT enabled healthcare applications represents a significant advance in building a secure and trustworthy healthcare system. The AI system can identify ordinary patterns of behavior and swiftly recognize any deviations suggestive of a possible security risk or unauthorized access through the continuous analysis of data flows from networked sensor devices. In order to enhance the security of IoT medical devices, we explore security vulnerabilities in IoT enabled healthcare applications and propose an Artificial Intelligence (AI) driven approach for IoT Device level vulnerabilities management life cycle for known and unknown vulnerability detection and protection based on zero tolerance and zero-trust model. The aim of this work is to strengthen the healthcare data vulnerability management strategy through zero-trust based strategy and AI-driven automated IoT devices security vulnerabilities management and risk assessment for an application on managed or unmanaged clinical and diagnosis operational services.",Wireless communication;Wireless sensor networks;Medical devices;Ecosystems;Medical services;Zero Trust;Internet of Things;Artificial Intelligence;Healthcare;Internet of things (IoT);Medical device;Security;Vulnerabilities,"{'month': 'March', 'issn': '', 'doi': '10.1109/ICDT61202.2024.10488939', 'keywords': 'Wireless communication;Wireless sensor networks;Medical devices;Ecosystems;Medical services;Zero Trust;Internet of Things;Artificial Intelligence;Healthcare;Internet of things (IoT);Medical device;Security;Vulnerabilities', 'abstract': 'In the modern healthcare ecosystem, emerging technologies, Internet of Things(IoT) medical devices playing major role for providing seamless and remote clinical care, disease diagnosis, patient monitoring and many more healthcare services. Wireless, seamless sensor devices and underlying infrastructure creates the IoT ecosystem. An IoT device identification and vital healthcare data collection are the most important where vulnerability management and security is a critical aspect of IoT security Against a number of vulnerabilities, attacks and cyber threats, a proactive approach needs to be builds for Internet of Things medical devices security resilience. In the current fast moving digital transformation, integrating Artificial Intelligence (AI)-driven safety measures into IoT enabled healthcare applications represents a significant advance in building a secure and trustworthy healthcare system. The AI system can identify ordinary patterns of behavior and swiftly recognize any deviations suggestive of a possible security risk or unauthorized access through the continuous analysis of data flows from networked sensor devices. In order to enhance the security of IoT medical devices, we explore security vulnerabilities in IoT enabled healthcare applications and propose an Artificial Intelligence (AI) driven approach for IoT Device level vulnerabilities management life cycle for known and unknown vulnerability detection and protection based on zero tolerance and zero-trust model. The aim of this work is to strengthen the healthcare data vulnerability management strategy through zero-trust based strategy and AI-driven automated IoT devices security vulnerabilities management and risk assessment for an application on managed or unmanaged clinical and diagnosis operational services.', 'pages': '366-373', 'number': '', 'volume': '', 'year': '2024', 'title': 'AI Driven IoT Healthcare Devices Security Vulnerability Management', 'booktitle': '2024 2nd International Conference on Disruptive Technologies (ICDT)', 'author': 'Bajpayi, Pragya and Sharma, Smita and Gaur, Madhu Sharma', 'ENTRYTYPE': 'inproceedings', 'ID': '10488939'}"
10511483,Exploring the Chat GPT's Impact and Prospects for Business Research Purposes,"Kurnianingrum, Dian and Bin Jumri, Isma Addi and Ratnapuri, Chyntia Ika and Karmagatri, Mulyani and Kartawinata, Budi Rustandi",Kurnianingrum,10.1109/INOCON60754.2024.10511483,2024,2024 3rd International Conference for Innovation in Technology (INOCON),"This research provides a detailed analysis of publications related to the Chat GPT topic using data from the Scopus database. The main purpose of this research is to find research gaps within ChatGPT research, especially in business. This research content is divided into four key areas: publication and citation structure, past research topics, author co-citation analysis, and keyword co-occurrence analysis. The research finds that Chat GPT is actively explored in medicine, social science, computer science, and engineering. Most past research is not directly related to business, but it can support business continuity. The research notes a balanced contribution from authors and identifies ""Artificial Intelligence"" as a leading keyword. Countries like the United States, China, and India are major contributors. Using Latent Dirichlet Allocation (LDA), the study categorizes previous work into five areas: remote learning, AI-based customer support, Learning Management Systems, academic research roles, and AI data handling. The co-citation analysis shows that authors, specifically Wu J. and Radford A., have significantly impacted the Chat GPT publication. For keyword co-occurrence, ""Artificial Intelligence"" and ""ChatGPT"" are keywords that have a strong link. The keywords cluster are divided into technology (red) and human-centric (green) topics.",Computer science;Learning management systems;Distance learning;Databases;Data handling;Social sciences;Chatbots;Chat GPT;Artificial Intelligence;Scopus Database;Publication Structure;Academic Research,"{'month': 'March', 'issn': '', 'doi': '10.1109/INOCON60754.2024.10511483', 'keywords': 'Computer science;Learning management systems;Distance learning;Databases;Data handling;Social sciences;Chatbots;Chat GPT;Artificial Intelligence;Scopus Database;Publication Structure;Academic Research', 'abstract': 'This research provides a detailed analysis of publications related to the Chat GPT topic using data from the Scopus database. The main purpose of this research is to find research gaps within ChatGPT research, especially in business. This research content is divided into four key areas: publication and citation structure, past research topics, author co-citation analysis, and keyword co-occurrence analysis. The research finds that Chat GPT is actively explored in medicine, social science, computer science, and engineering. Most past research is not directly related to business, but it can support business continuity. The research notes a balanced contribution from authors and identifies ""Artificial Intelligence"" as a leading keyword. Countries like the United States, China, and India are major contributors. Using Latent Dirichlet Allocation (LDA), the study categorizes previous work into five areas: remote learning, AI-based customer support, Learning Management Systems, academic research roles, and AI data handling. The co-citation analysis shows that authors, specifically Wu J. and Radford A., have significantly impacted the Chat GPT publication. For keyword co-occurrence, ""Artificial Intelligence"" and ""ChatGPT"" are keywords that have a strong link. The keywords cluster are divided into technology (red) and human-centric (green) topics.', 'pages': '1-6', 'number': '', 'volume': '', 'year': '2024', 'title': ""Exploring the Chat GPT's Impact and Prospects for Business Research Purposes"", 'booktitle': '2024 3rd International Conference for Innovation in Technology (INOCON)', 'author': 'Kurnianingrum, Dian and Bin Jumri, Isma Addi and Ratnapuri, Chyntia Ika and Karmagatri, Mulyani and Kartawinata, Budi Rustandi', 'ENTRYTYPE': 'inproceedings', 'ID': '10511483'}"
10296890,Ensembled Deep Convolutional Generative Adversarial Network for Grading Imbalanced Diabetic Retinopathy Recognition,"Naz, Huma and Nijhawan, Rahul and Ahuja, Neelu Jyothi and Al-Otaibi, Shaha and Saba, Tanzila and Bahaj, Saeed Ali and Rehman, Amjad",Naz,10.1109/ACCESS.2023.3327900,2023,IEEE Access,"Diabetic Retinopathy (DR) is one of the leading causes of blindness and vision loss worldwide. According to the International Diabetes Federation (IDF), approximately one-third of individuals with diabetes, equivalent to 32.2\%, are affected by some form of DR. Due to the uneven data distribution, intra-class variance, and a dearth of ophthalmologists, DR diagnosis is considered challenging. In recent years, Convolutional Neural Networks (CNN) and supervised learning techniques have been potentially useful in computer vision applications. However, unsupervised CNN has received less attention. Moreover, it is more manageable to use synthetic images for model training with the recent advancements in graphics. Therefore, the proposed method combines the actual and augmented views using the Deep Convolutional Generative Adversarial Network (DCGAN) algorithm. The generated views are implemented to balance the minority class in the imbalanced dataset. Furthermore, a novel ensemble convolutional neural network algorithm named Different View Ensemble (DVE) that merges the weighted average prediction of CNN, CNN-i, and CNN+i algorithms has been proposed. The proposed algorithm is evaluated on the DDR and EyePACS datasets, and its performance is compared with K-Means, Fuzzy C-Means (FCM), and Autoencoder-based Deep Embedded Clustering Techniques (DEC). The results demonstrate the superiority of the proposed algorithm, achieving an accuracy rate of 97.4\%, specificity of 99.6\%, and sensitivity of 92.3\%. The promising results underscore the potential impact of this methodology in enhancing the accuracy and reliability of automated diagnostic systems in the field of ophthalmology. Notably, the evaluation considers imbalanced data and a DCGAN-balanced dataset, where the proposed approach exhibits even better performance with balanced classes.",Generative adversarial networks;Prediction algorithms;Convolutional neural networks;Training;Data models;Predictive models;Generators;Diabetic retinopathy;Diabetic retinopathy detection;imbalance data;ensembled GAN;healthcare;health risks,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2023.3327900', 'keywords': 'Generative adversarial networks;Prediction algorithms;Convolutional neural networks;Training;Data models;Predictive models;Generators;Diabetic retinopathy;Diabetic retinopathy detection;imbalance data;ensembled GAN;healthcare;health risks', 'abstract': 'Diabetic Retinopathy (DR) is one of the leading causes of blindness and vision loss worldwide. According to the International Diabetes Federation (IDF), approximately one-third of individuals with diabetes, equivalent to 32.2\\%, are affected by some form of DR. Due to the uneven data distribution, intra-class variance, and a dearth of ophthalmologists, DR diagnosis is considered challenging. In recent years, Convolutional Neural Networks (CNN) and supervised learning techniques have been potentially useful in computer vision applications. However, unsupervised CNN has received less attention. Moreover, it is more manageable to use synthetic images for model training with the recent advancements in graphics. Therefore, the proposed method combines the actual and augmented views using the Deep Convolutional Generative Adversarial Network (DCGAN) algorithm. The generated views are implemented to balance the minority class in the imbalanced dataset. Furthermore, a novel ensemble convolutional neural network algorithm named Different View Ensemble (DVE) that merges the weighted average prediction of CNN, CNN-i, and CNN+i algorithms has been proposed. The proposed algorithm is evaluated on the DDR and EyePACS datasets, and its performance is compared with K-Means, Fuzzy C-Means (FCM), and Autoencoder-based Deep Embedded Clustering Techniques (DEC). The results demonstrate the superiority of the proposed algorithm, achieving an accuracy rate of 97.4\\%, specificity of 99.6\\%, and sensitivity of 92.3\\%. The promising results underscore the potential impact of this methodology in enhancing the accuracy and reliability of automated diagnostic systems in the field of ophthalmology. Notably, the evaluation considers imbalanced data and a DCGAN-balanced dataset, where the proposed approach exhibits even better performance with balanced classes.', 'pages': '120554-120568', 'number': '', 'volume': '11', 'year': '2023', 'title': 'Ensembled Deep Convolutional Generative Adversarial Network for Grading Imbalanced Diabetic Retinopathy Recognition', 'journal': 'IEEE Access', 'author': 'Naz, Huma and Nijhawan, Rahul and Ahuja, Neelu Jyothi and Al-Otaibi, Shaha and Saba, Tanzila and Bahaj, Saeed Ali and Rehman, Amjad', 'ENTRYTYPE': 'article', 'ID': '10296890'}"
9696138,Face Generation using DCGAN for Low Computing Resources,"Liu, Weichen and Gu, Yuxuan and Zhang, Kenan",Liu,10.1109/ICBASE53849.2021.00076,2021,2021 2nd International Conference on Big Data \& Artificial Intelligence \& Software Engineering (ICBASE),"Image generation is the task of generating brand new images from existing datasets. With the increasing development of the Generative Adversarial Network (GAN), it is now widely used in the field of image generation. Research based on its time cost and the convergence of its training process has been widely concerned. However, existing techniques for network convergence during the training process usually require high computing resources. This paper has used Deep Convolutional Generative Adversarial Network (DCGAN) on the CelebA dataset for experimental evaluation: The number of training epochs and the algorithm’s parameters is adjusted to achieve a balance between the quality of generation results and computing resources used. This is assumed to be convenient for future studies under the condition of limited computing resources. Besides, by adjusting the parameters of the optimization algorithm, the convergence of GAN under the conditions of a specific optimization algorithm is studied.",Training;Costs;Image synthesis;Focusing;Generative adversarial networks;Generators;Task analysis;component;Image generation;DCGAN;Adam;Deep learning,"{'month': 'Sep.', 'issn': '', 'doi': '10.1109/ICBASE53849.2021.00076', 'keywords': 'Training;Costs;Image synthesis;Focusing;Generative adversarial networks;Generators;Task analysis;component;Image generation;DCGAN;Adam;Deep learning', 'abstract': 'Image generation is the task of generating brand new images from existing datasets. With the increasing development of the Generative Adversarial Network (GAN), it is now widely used in the field of image generation. Research based on its time cost and the convergence of its training process has been widely concerned. However, existing techniques for network convergence during the training process usually require high computing resources. This paper has used Deep Convolutional Generative Adversarial Network (DCGAN) on the CelebA dataset for experimental evaluation: The number of training epochs and the algorithm’s parameters is adjusted to achieve a balance between the quality of generation results and computing resources used. This is assumed to be convenient for future studies under the condition of limited computing resources. Besides, by adjusting the parameters of the optimization algorithm, the convergence of GAN under the conditions of a specific optimization algorithm is studied.', 'pages': '377-382', 'number': '', 'volume': '', 'year': '2021', 'title': 'Face Generation using DCGAN for Low Computing Resources', 'booktitle': '2021 2nd International Conference on Big Data \\& Artificial Intelligence \\& Software Engineering (ICBASE)', 'author': 'Liu, Weichen and Gu, Yuxuan and Zhang, Kenan', 'ENTRYTYPE': 'inproceedings', 'ID': '9696138'}"
11091361,Numerical Model-Aided Intelligent Diagnosis Method to Detect Faults in Induction Motors,"Chen, Jiheng and Fang, Bo and Dianov, Anton and Xiang, Jiawei",Chen,10.1109/JSEN.2025.3586913,2025,IEEE Sensors Journal,"Faults in induction motors will reduce the operational efficiency and stability of mechanical systems. In recent years, artificial intelligence (AI) models have become an important technology for diagnosing induction motor faults. These AI models can only accurately identify faults when trained on a complete dataset. However, it is difficult to obtain a complete dataset in practical engineering scenarios, which will worsen the performance of AI models. To solve the problem, a numerical model-aided intelligent diagnosis method is proposed. First, a numerical model of the health induction motor is established, and the model is updated using measured health signals. Subsequently, various types of fault mathematical models are injected into the healthy numerical model to obtain simulated fault samples. Finally, the simulated and/or measured signals are used to train the convolutional neural network (CNN). The experimental results show that the model has achieved satisfactory results to construct a relatively complete dataset for engineering applications.",Circuit faults;Stator windings;Rotors;Induction motors;Numerical models;Integrated circuit modeling;Circuits;Artificial intelligence;Motors;Mathematical models;Convolutional neural network (CNN);fault detection;fault sample generation;induction motor;numerical model,"{'month': 'Sep.', 'issn': '1558-1748', 'doi': '10.1109/JSEN.2025.3586913', 'keywords': 'Circuit faults;Stator windings;Rotors;Induction motors;Numerical models;Integrated circuit modeling;Circuits;Artificial intelligence;Motors;Mathematical models;Convolutional neural network (CNN);fault detection;fault sample generation;induction motor;numerical model', 'abstract': 'Faults in induction motors will reduce the operational efficiency and stability of mechanical systems. In recent years, artificial intelligence (AI) models have become an important technology for diagnosing induction motor faults. These AI models can only accurately identify faults when trained on a complete dataset. However, it is difficult to obtain a complete dataset in practical engineering scenarios, which will worsen the performance of AI models. To solve the problem, a numerical model-aided intelligent diagnosis method is proposed. First, a numerical model of the health induction motor is established, and the model is updated using measured health signals. Subsequently, various types of fault mathematical models are injected into the healthy numerical model to obtain simulated fault samples. Finally, the simulated and/or measured signals are used to train the convolutional neural network (CNN). The experimental results show that the model has achieved satisfactory results to construct a relatively complete dataset for engineering applications.', 'pages': '32194-32208', 'number': '17', 'volume': '25', 'year': '2025', 'title': 'Numerical Model-Aided Intelligent Diagnosis Method to Detect Faults in Induction Motors', 'journal': 'IEEE Sensors Journal', 'author': 'Chen, Jiheng and Fang, Bo and Dianov, Anton and Xiang, Jiawei', 'ENTRYTYPE': 'article', 'ID': '11091361'}"
11050535,Multimodal Generative AI for Story Point Estimation,"Islam, Mohammad Rubyet and Sandborn, Peter",Islam,10.1109/CAI64502.2025.00120,2025,2025 IEEE Conference on Artificial Intelligence (CAI),"This research explores Multimodal Generative AI for Agile story point estimation, integrating text, images, and categorical data with BERT, CNN, and XGBoost to enhance project planning. While effective for simpler estimates, challenges with complex cases due to data imbalance highlight the need for tailored strategies. Findings underscore multimodal AI's impact on Agile decision-making and suggest future improvements in data handling and model robustness.",Automation;Accuracy;Generative AI;Data handling;Decision making;Estimation;Agile project management;Robustness;Data models;Planning;multimodal;NLP;story point estimation;agile framework;fibonacci sequences,"{'month': 'May', 'issn': '', 'doi': '10.1109/CAI64502.2025.00120', 'keywords': 'Automation;Accuracy;Generative AI;Data handling;Decision making;Estimation;Agile project management;Robustness;Data models;Planning;multimodal;NLP;story point estimation;agile framework;fibonacci sequences', 'abstract': ""This research explores Multimodal Generative AI for Agile story point estimation, integrating text, images, and categorical data with BERT, CNN, and XGBoost to enhance project planning. While effective for simpler estimates, challenges with complex cases due to data imbalance highlight the need for tailored strategies. Findings underscore multimodal AI's impact on Agile decision-making and suggest future improvements in data handling and model robustness."", 'pages': '659-662', 'number': '', 'volume': '', 'year': '2025', 'title': 'Multimodal Generative AI for Story Point Estimation', 'booktitle': '2025 IEEE Conference on Artificial Intelligence (CAI)', 'author': 'Islam, Mohammad Rubyet and Sandborn, Peter', 'ENTRYTYPE': 'inproceedings', 'ID': '11050535'}"
10674524,"Taiwanese Hokkien in AI: Challenges, Approaches, and Language Modeling","Sheu, Jeng-Shin and Ahmad, Aftab and Xu, Shun-Yi",Sheu,10.1109/ICCE-Taiwan62264.2024.10674524,2024,2024 International Conference on Consumer Electronics - Taiwan (ICCE-Taiwan),"In the evolving landscape of artificial intelligence (AI) and natural language processing (NLP), major advancements have been made, particularly with powerful pre-trained language models. However, these models often overlook region-specific languages like Taiwanese Hokkien. This paper addresses the challenges in integrating Taiwanese Hokkien into NLP and proposes solutions. We employ deep neural networks to develop a translation model that unifies diverse Taiwanese Hokkien texts into a standardized script. Leveraging this unified text, we train pre-trained language and generative models tailored for Taiwanese Hokkien. These models are pivotal in facilitating the integration of Taiwanese Hokkien into the AI landscape, enabling exploration of its various aspects in NLP. Our experiments demonstrate the successful acquisition of a universal language representation by the pre-trained language model and the impressive generation of fluent Taiwanese Hokkien text by the generative model.",Pipelines;Artificial neural networks;Writing;Transformers;Natural language processing;Artificial intelligence;Consumer electronics;Taiwanese Hokkien;natural language processing;Transformer architecture;BERT;GPT,"{'month': 'July', 'issn': '2575-8284', 'doi': '10.1109/ICCE-Taiwan62264.2024.10674524', 'keywords': 'Pipelines;Artificial neural networks;Writing;Transformers;Natural language processing;Artificial intelligence;Consumer electronics;Taiwanese Hokkien;natural language processing;Transformer architecture;BERT;GPT', 'abstract': 'In the evolving landscape of artificial intelligence (AI) and natural language processing (NLP), major advancements have been made, particularly with powerful pre-trained language models. However, these models often overlook region-specific languages like Taiwanese Hokkien. This paper addresses the challenges in integrating Taiwanese Hokkien into NLP and proposes solutions. We employ deep neural networks to develop a translation model that unifies diverse Taiwanese Hokkien texts into a standardized script. Leveraging this unified text, we train pre-trained language and generative models tailored for Taiwanese Hokkien. These models are pivotal in facilitating the integration of Taiwanese Hokkien into the AI landscape, enabling exploration of its various aspects in NLP. Our experiments demonstrate the successful acquisition of a universal language representation by the pre-trained language model and the impressive generation of fluent Taiwanese Hokkien text by the generative model.', 'pages': '195-196', 'number': '', 'volume': '', 'year': '2024', 'title': 'Taiwanese Hokkien in AI: Challenges, Approaches, and Language Modeling', 'booktitle': '2024 International Conference on Consumer Electronics - Taiwan (ICCE-Taiwan)', 'author': 'Sheu, Jeng-Shin and Ahmad, Aftab and Xu, Shun-Yi', 'ENTRYTYPE': 'inproceedings', 'ID': '10674524'}"
9339895,Data Augmented Deep Behavioral Cloning for Urban Traffic Control Operations Under a Parallel Learning Framework,"Li, Xiaoshuang and Ye, Peijun and Jin, Junchen and Zhu, Fenghua and Wang, Fei-Yue",Li,10.1109/TITS.2020.3048151,2022,IEEE Transactions on Intelligent Transportation Systems,"It is indispensable for professional traffic signal engineers to perform manual operations of traffic signal control (TSC) to mitigate traffic congestion, especially with complicated scenarios. However, such a task is time-consuming, and the level of congestion mitigation heavily relies on individual expertise in engineering practice. Therefore, it is cost-effective to learn traffic engineers’ knowledge to enhance the problem-solving skills for a large-scale urban traffic network. In this paper, a data augmented deep behavioral cloning (DADBC) method is proposed to imitate the problem-solving skills of traffic engineers. The method is under a conceptual framework, parallel learning (PL) framework, that incorporates machine learning techniques for solving decision-making problems in complex systems. The DADBC method enhances a hybrid demonstration by exploiting a generative adversarial network (GAN) and then uses the deep behavioral cloning (DBC) model to learn traffic engineers’ control schemes. According to the validation results using the real manipulation data from Hangzhou, China, our method can imitate complex human behaviors in intervening traffic signal control operations to improve traffic efficiency in urban areas.",Generative adversarial networks;Gallium nitride;Cloning;Data models;Task analysis;Complex systems;Knowledge engineering;Intelligent traffic signal operations;parallel learning;deep behavioral cloning;generative adversarial networks,"{'month': 'June', 'issn': '1558-0016', 'doi': '10.1109/TITS.2020.3048151', 'keywords': 'Generative adversarial networks;Gallium nitride;Cloning;Data models;Task analysis;Complex systems;Knowledge engineering;Intelligent traffic signal operations;parallel learning;deep behavioral cloning;generative adversarial networks', 'abstract': 'It is indispensable for professional traffic signal engineers to perform manual operations of traffic signal control (TSC) to mitigate traffic congestion, especially with complicated scenarios. However, such a task is time-consuming, and the level of congestion mitigation heavily relies on individual expertise in engineering practice. Therefore, it is cost-effective to learn traffic engineers’ knowledge to enhance the problem-solving skills for a large-scale urban traffic network. In this paper, a data augmented deep behavioral cloning (DADBC) method is proposed to imitate the problem-solving skills of traffic engineers. The method is under a conceptual framework, parallel learning (PL) framework, that incorporates machine learning techniques for solving decision-making problems in complex systems. The DADBC method enhances a hybrid demonstration by exploiting a generative adversarial network (GAN) and then uses the deep behavioral cloning (DBC) model to learn traffic engineers’ control schemes. According to the validation results using the real manipulation data from Hangzhou, China, our method can imitate complex human behaviors in intervening traffic signal control operations to improve traffic efficiency in urban areas.', 'pages': '5128-5137', 'number': '6', 'volume': '23', 'year': '2022', 'title': 'Data Augmented Deep Behavioral Cloning for Urban Traffic Control Operations Under a Parallel Learning Framework', 'journal': 'IEEE Transactions on Intelligent Transportation Systems', 'author': 'Li, Xiaoshuang and Ye, Peijun and Jin, Junchen and Zhu, Fenghua and Wang, Fei-Yue', 'ENTRYTYPE': 'article', 'ID': '9339895'}"
10210519,Generating Counterfactual Instances for Explainable Class-Imbalance Learning,"Chen, Zhi and Duan, Jiang and Kang, Li and Xu, Hongyan and Chen, Rui and Qiu, Guoping",Chen,10.1109/TKDE.2023.3302847,2024,IEEE Transactions on Knowledge and Data Engineering,"Existing class imbalance learning paradigms focus on lifting the importance of minority instance, aiming to improve the model in terms of certain evaluation metrics (e.g., AUC and $F\_{1}$F1-measure). One drawback of these methods is that they lack enough transparency, hence, cannot be fully trusted in vital domains. To this end, this paper deal with the class imbalance learning task with counterfactual instances. Given an instance and a classifier, a counterfactual is a fake instance which, while having smallest distance to the original instance, is classified as a different class by the classifier. Therefore, the most important features for a classifier can be identified by inspecting the difference between an instance and its counterfactual. To utilize counterfactuals, a novel Explainable Generative Adversarial Network (EXGAN) is proposed. EXGAN has a unique “two generators versus multiple discriminators” architecture where the generators are used to generate effective counterfactuals and discriminators are trained for the class imbalance learning task. In addition to the architecture, an innovative ensemble loss function ensuring each discriminator complementing each other is designed to overcome the class imbalance issue. Extensive experiments prove that the counterfactuals generated by EXGAN can be used to produce effective local explanation and provide significant better class imbalance learning ability than existing competitors.",Generators;Classification algorithms;Task analysis;Standards;Ensemble learning;Costs;Computational modeling;Class imbalance learning;counterfactual;explainable machine learning;explainable generative adversarial network;ensemble learning,"{'month': 'March', 'issn': '1558-2191', 'doi': '10.1109/TKDE.2023.3302847', 'keywords': 'Generators;Classification algorithms;Task analysis;Standards;Ensemble learning;Costs;Computational modeling;Class imbalance learning;counterfactual;explainable machine learning;explainable generative adversarial network;ensemble learning', 'abstract': 'Existing class imbalance learning paradigms focus on lifting the importance of minority instance, aiming to improve the model in terms of certain evaluation metrics (e.g., AUC and $F\\_{1}$F1-measure). One drawback of these methods is that they lack enough transparency, hence, cannot be fully trusted in vital domains. To this end, this paper deal with the class imbalance learning task with counterfactual instances. Given an instance and a classifier, a counterfactual is a fake instance which, while having smallest distance to the original instance, is classified as a different class by the classifier. Therefore, the most important features for a classifier can be identified by inspecting the difference between an instance and its counterfactual. To utilize counterfactuals, a novel Explainable Generative Adversarial Network (EXGAN) is proposed. EXGAN has a unique “two generators versus multiple discriminators” architecture where the generators are used to generate effective counterfactuals and discriminators are trained for the class imbalance learning task. In addition to the architecture, an innovative ensemble loss function ensuring each discriminator complementing each other is designed to overcome the class imbalance issue. Extensive experiments prove that the counterfactuals generated by EXGAN can be used to produce effective local explanation and provide significant better class imbalance learning ability than existing competitors.', 'pages': '1130-1144', 'number': '3', 'volume': '36', 'year': '2024', 'title': 'Generating Counterfactual Instances for Explainable Class-Imbalance Learning', 'journal': 'IEEE Transactions on Knowledge and Data Engineering', 'author': 'Chen, Zhi and Duan, Jiang and Kang, Li and Xu, Hongyan and Chen, Rui and Qiu, Guoping', 'ENTRYTYPE': 'article', 'ID': '10210519'}"
10848615,MAGICIAN: Malware classification Approach through Generation Image using a Conditional and wassersteIn generative Adversarial Network variants,"Galantucci, Stefano and Iannacone, Andrea and Pirlo, Giuseppe and Sarcinella, Lucia and Stamerra, Alessandro",Galantucci,10.1109/ICAIC63015.2025.10848615,2025,2025 IEEE 4th International Conference on AI in Cybersecurity (ICAIC),"One of the main challenges of cybersecurity is the detection and classification of malware to prevent damage to systems by both companies and private users. Identifying the specific type of malware is critical to performing targeted actions. This study proposes a classification approach that generates synthetic images of malware using Conditional Generative Adversarial Networks (cGAN) and Wasserstein Generative Adversarial Networks (WGAN). Using the Malimg dataset, consisting of 25 malware classes, the ResNet50 model shows an overall accuracy of $\mathbf{9 1. 4 \\%}$ and an $\mathbf{F 1}$-score of $\mathbf{9 0. 8 \\%}$ for synthetic images generated with WGAN. Resizing and resampling were employed as preprocessing strategies to obtain images of size $48 \times 48$; resampling has been shown to be more effective. Thus, the proposed methodology allows malware to be classified quickly and efficiently, and, on the other hand, unbalanced datasets can be enriched to aid classification performance.",Hands;Accuracy;Companies;Generative adversarial networks;Malware;Computer security;Artificial intelligence;Residual neural networks;MalImg;cGAN;WGAN;ResNet50;Malware images;Resizing;Resampling,"{'month': 'Feb', 'issn': '', 'doi': '10.1109/ICAIC63015.2025.10848615', 'keywords': 'Hands;Accuracy;Companies;Generative adversarial networks;Malware;Computer security;Artificial intelligence;Residual neural networks;MalImg;cGAN;WGAN;ResNet50;Malware images;Resizing;Resampling', 'abstract': 'One of the main challenges of cybersecurity is the detection and classification of malware to prevent damage to systems by both companies and private users. Identifying the specific type of malware is critical to performing targeted actions. This study proposes a classification approach that generates synthetic images of malware using Conditional Generative Adversarial Networks (cGAN) and Wasserstein Generative Adversarial Networks (WGAN). Using the Malimg dataset, consisting of 25 malware classes, the ResNet50 model shows an overall accuracy of $\\mathbf{9 1. 4 \\\\%}$ and an $\\mathbf{F 1}$-score of $\\mathbf{9 0. 8 \\\\%}$ for synthetic images generated with WGAN. Resizing and resampling were employed as preprocessing strategies to obtain images of size $48 \\times 48$; resampling has been shown to be more effective. Thus, the proposed methodology allows malware to be classified quickly and efficiently, and, on the other hand, unbalanced datasets can be enriched to aid classification performance.', 'pages': '1-7', 'number': '', 'volume': '', 'year': '2025', 'title': 'MAGICIAN: Malware classification Approach through Generation Image using a Conditional and wassersteIn generative Adversarial Network variants', 'booktitle': '2025 IEEE 4th International Conference on AI in Cybersecurity (ICAIC)', 'author': 'Galantucci, Stefano and Iannacone, Andrea and Pirlo, Giuseppe and Sarcinella, Lucia and Stamerra, Alessandro', 'ENTRYTYPE': 'inproceedings', 'ID': '10848615'}"
10673889,Designer-GenAI Co-Creation: Exploring the Potential Impact of Micro-Mobility Tool Design Recommendations on Elderly Mobility and Welfare,"Lin, Jia-Xian and Chen, Chun-Ching",Lin,10.1109/ICCE-Taiwan62264.2024.10673889,2024,2024 International Conference on Consumer Electronics - Taiwan (ICCE-Taiwan),"As age progresses, mobility limitations can hinder the social participation of the elderly, underscoring the importance of enhancing mobility for societal development. This study emphasizes the critical role of mobility in enhancing life satisfaction and facilitating successful aging among the elderly and explores the potential of micro-mobility tools to improve the quality of life and independence. Through workshops that foster collaborative ideation with designers and generative artificial intelligence, we have developed solutions tailored for micro-mobility and offer recommendations for future micro-mobility planning, particularly focusing on the integration of hardware and software details. This research integrates design expertise and artificial intelligence technology, advocating for the continuous innovation of micro-mobility tools to support the active participation of the elderly in society.",Technological innovation;Electric potential;Generative AI;Conferences;Focusing;Software;Hardware;Micro-mobility;generative AI;older people;design recommendations;brainstorming,"{'month': 'July', 'issn': '2575-8284', 'doi': '10.1109/ICCE-Taiwan62264.2024.10673889', 'keywords': 'Technological innovation;Electric potential;Generative AI;Conferences;Focusing;Software;Hardware;Micro-mobility;generative AI;older people;design recommendations;brainstorming', 'abstract': 'As age progresses, mobility limitations can hinder the social participation of the elderly, underscoring the importance of enhancing mobility for societal development. This study emphasizes the critical role of mobility in enhancing life satisfaction and facilitating successful aging among the elderly and explores the potential of micro-mobility tools to improve the quality of life and independence. Through workshops that foster collaborative ideation with designers and generative artificial intelligence, we have developed solutions tailored for micro-mobility and offer recommendations for future micro-mobility planning, particularly focusing on the integration of hardware and software details. This research integrates design expertise and artificial intelligence technology, advocating for the continuous innovation of micro-mobility tools to support the active participation of the elderly in society.', 'pages': '153-154', 'number': '', 'volume': '', 'year': '2024', 'title': 'Designer-GenAI Co-Creation: Exploring the Potential Impact of Micro-Mobility Tool Design Recommendations on Elderly Mobility and Welfare', 'booktitle': '2024 International Conference on Consumer Electronics - Taiwan (ICCE-Taiwan)', 'author': 'Lin, Jia-Xian and Chen, Chun-Ching', 'ENTRYTYPE': 'inproceedings', 'ID': '10673889'}"
10221755,"A Survey on ChatGPT: AI–Generated Contents, Challenges, and Solutions","Wang, Yuntao and Pan, Yanghe and Yan, Miao and Su, Zhou and Luan, Tom H.",Wang,10.1109/OJCS.2023.3300321,2023,IEEE Open Journal of the Computer Society,"With the widespread use of large artificial intelligence (AI) models such as ChatGPT, AI-generated content (AIGC) has garnered increasing attention and is leading a paradigm shift in content creation and knowledge representation. AIGC uses generative large AI algorithms to assist or replace humans in creating massive, high-quality, and human-like content at a faster pace and lower cost, based on user-provided prompts. Despite the recent significant progress in AIGC, security, privacy, ethical, and legal challenges still need to be addressed. This paper presents an in-depth survey of working principles, security and privacy threats, state-of-the-art solutions, and future challenges of the AIGC paradigm. Specifically, we first explore the enabling technologies, general architecture of AIGC, and discuss its working modes and key characteristics. Then, we investigate the taxonomy of security and privacy threats to AIGC and highlight the ethical and societal implications of GPT and AIGC technologies. Furthermore, we review the state-of-the-art AIGC watermarking approaches for regulatable AIGC paradigms regarding the AIGC model and its produced content. Finally, we identify future challenges and open research directions related to AIGC.",Artificial intelligence;Chatbots;Surveys;Security;Computational modeling;Privacy;Training;AIGC;generative AI;ChatGPT;security;and privacy,"{'month': '', 'issn': '2644-1268', 'doi': '10.1109/OJCS.2023.3300321', 'keywords': 'Artificial intelligence;Chatbots;Surveys;Security;Computational modeling;Privacy;Training;AIGC;generative AI;ChatGPT;security;and privacy', 'abstract': 'With the widespread use of large artificial intelligence (AI) models such as ChatGPT, AI-generated content (AIGC) has garnered increasing attention and is leading a paradigm shift in content creation and knowledge representation. AIGC uses generative large AI algorithms to assist or replace humans in creating massive, high-quality, and human-like content at a faster pace and lower cost, based on user-provided prompts. Despite the recent significant progress in AIGC, security, privacy, ethical, and legal challenges still need to be addressed. This paper presents an in-depth survey of working principles, security and privacy threats, state-of-the-art solutions, and future challenges of the AIGC paradigm. Specifically, we first explore the enabling technologies, general architecture of AIGC, and discuss its working modes and key characteristics. Then, we investigate the taxonomy of security and privacy threats to AIGC and highlight the ethical and societal implications of GPT and AIGC technologies. Furthermore, we review the state-of-the-art AIGC watermarking approaches for regulatable AIGC paradigms regarding the AIGC model and its produced content. Finally, we identify future challenges and open research directions related to AIGC.', 'pages': '280-302', 'number': '', 'volume': '4', 'year': '2023', 'title': 'A Survey on ChatGPT: AI–Generated Contents, Challenges, and Solutions', 'journal': 'IEEE Open Journal of the Computer Society', 'author': 'Wang, Yuntao and Pan, Yanghe and Yan, Miao and Su, Zhou and Luan, Tom H.', 'ENTRYTYPE': 'article', 'ID': '10221755'}"
10942898,Optimizing Industry 5.0 Machine Learning-Based Applications via Synthetic Data Generation,"Colombi, Lorenzo and Brina, Matteo and Vespa, Michela and Tabanelli, Filippo and Dahdal, Simon and Bellodi, Elena and Venanzi, Riccardo and Tortonesi, Mauro and Vignoli, Massimiliano and Stefanelli, Cesare",Colombi,10.1109/CAMAD62243.2024.10942898,2024,2024 IEEE 29th International Workshop on Computer Aided Modeling and Design of Communication Links and Networks (CAMAD),"Machine Learning (ML) innovations are revolutionizing industrial processes by improving productivity and competitiveness, particularly by creating predictive maintenance applications and Artificial Intelligence (AI) powered services. In the Industry 5.0 paradigm, which strives to reach Zero Defect and Zero Waste Manufacturing, the role of ML is crucial for optimizing these processes. However, developing ML solutions for production-ready industrial settings presents various challenges, such as data scarcity and dataset imbalance, which are further amplified when dealing with tabular data. To overcome such issues we investigate the use of various Deep Generative Models (DGMs) to generate synthetic data that closely mimics real-world conditions. Despite extensive studies of DGMs in the literature, there is still limited knowledge of their practical suitability in Industry 5.0 environments. Our research includes a detailed evaluation of several DGMs, such as Generative Adversarial Networks, Variational Autoencoders, and Diffusion Models, implemented in the gearbox assembly and testing line in the Bonfiglioli EVO plant. Based on our results, the evaluated DGMs demonstrate significant potential in generating high-quality synthetic data, that allows training a high-performance classifier to distinguish faulty gearboxes from well-functioning ones.",Training;Technological innovation;Computational modeling;Autoencoders;Generative adversarial networks;Diffusion models;Data models;Fifth Industrial Revolution;Synthetic data;Testing;Industry 5.0;Machine Learning;Deep Generative Model;Generative Adversarial Network;Diffusion Model;Variational Autoencoder,"{'month': 'Oct', 'issn': '2378-4873', 'doi': '10.1109/CAMAD62243.2024.10942898', 'keywords': 'Training;Technological innovation;Computational modeling;Autoencoders;Generative adversarial networks;Diffusion models;Data models;Fifth Industrial Revolution;Synthetic data;Testing;Industry 5.0;Machine Learning;Deep Generative Model;Generative Adversarial Network;Diffusion Model;Variational Autoencoder', 'abstract': 'Machine Learning (ML) innovations are revolutionizing industrial processes by improving productivity and competitiveness, particularly by creating predictive maintenance applications and Artificial Intelligence (AI) powered services. In the Industry 5.0 paradigm, which strives to reach Zero Defect and Zero Waste Manufacturing, the role of ML is crucial for optimizing these processes. However, developing ML solutions for production-ready industrial settings presents various challenges, such as data scarcity and dataset imbalance, which are further amplified when dealing with tabular data. To overcome such issues we investigate the use of various Deep Generative Models (DGMs) to generate synthetic data that closely mimics real-world conditions. Despite extensive studies of DGMs in the literature, there is still limited knowledge of their practical suitability in Industry 5.0 environments. Our research includes a detailed evaluation of several DGMs, such as Generative Adversarial Networks, Variational Autoencoders, and Diffusion Models, implemented in the gearbox assembly and testing line in the Bonfiglioli EVO plant. Based on our results, the evaluated DGMs demonstrate significant potential in generating high-quality synthetic data, that allows training a high-performance classifier to distinguish faulty gearboxes from well-functioning ones.', 'pages': '1-6', 'number': '', 'volume': '', 'year': '2024', 'title': 'Optimizing Industry 5.0 Machine Learning-Based Applications via Synthetic Data Generation', 'booktitle': '2024 IEEE 29th International Workshop on Computer Aided Modeling and Design of Communication Links and Networks (CAMAD)', 'author': 'Colombi, Lorenzo and Brina, Matteo and Vespa, Michela and Tabanelli, Filippo and Dahdal, Simon and Bellodi, Elena and Venanzi, Riccardo and Tortonesi, Mauro and Vignoli, Massimiliano and Stefanelli, Cesare', 'ENTRYTYPE': 'inproceedings', 'ID': '10942898'}"
10542413,RTRGAN: Ridge Texture Rendering-Based Generative Adversarial Fingerprint Attack for Portable Consumer Electronics Devices,"Yuan, Chengsheng and Cui, Baojie and Zhou, Zhili and Liu, Yuming and Yang, Yimin and Jonathan Wu, Q. M.",Yuan,10.1109/TCE.2024.3407713,2024,IEEE Transactions on Consumer Electronics,"Deep Fake Fingerprint Detection (DFFD) technique is ubiquitously deployed in automatic fingerprint identification systems (AFIS). Portable consumer electronics devices (PCED), such as smartphones, smart tablets, and PCs, also utilize fingerprint as a authentication method of AFIS. In recent years, the emergence of adversarial examples reveals the vulnerability of DNNs and weakens the credibility of PCED. Existing adversarial examples generally adopt perturbation addition, which lack robustness in the face of defensive measures such as adversarial training. Moreover, the perturbations used to deceive DFFD are easily perceived by human eyes. To address the above challenges, this paper proposes a novel generative adversarial fingerprint attack method for PCED. Firstly, to address the issue of poor robustness against defense strategies, this paper proposes a ridge texture rendering based generative adversarial network (RTRGAN) to perform robust generative adversarial fingerprint attack. Subsequently, to further enhance the visual quality of adversarial fingerprints, the realistic ridge texture is assigned by comparing the feature similarity between real fingerprints and adversarial fingerprints. Finally, this paper designs a joint optimization loss function, including the discriminator loss and the adversarial loss, to balance the attack robustness and visual fidelity. Extensive experiments demonstrate that compared with traditional perturbation-based methods, the proposed scheme significantly improves the attack success rate, has remarkable robustness, and generates more realistic adversarial fingerprints to human perception.",Fingerprint recognition;Perturbation methods;Training;Consumer electronics;Robustness;Task analysis;Optimization;Portable consumer electronics devices;adversarial fingerprints attacks;deep fake fingerprint detection,"{'month': 'Aug', 'issn': '1558-4127', 'doi': '10.1109/TCE.2024.3407713', 'keywords': 'Fingerprint recognition;Perturbation methods;Training;Consumer electronics;Robustness;Task analysis;Optimization;Portable consumer electronics devices;adversarial fingerprints attacks;deep fake fingerprint detection', 'abstract': 'Deep Fake Fingerprint Detection (DFFD) technique is ubiquitously deployed in automatic fingerprint identification systems (AFIS). Portable consumer electronics devices (PCED), such as smartphones, smart tablets, and PCs, also utilize fingerprint as a authentication method of AFIS. In recent years, the emergence of adversarial examples reveals the vulnerability of DNNs and weakens the credibility of PCED. Existing adversarial examples generally adopt perturbation addition, which lack robustness in the face of defensive measures such as adversarial training. Moreover, the perturbations used to deceive DFFD are easily perceived by human eyes. To address the above challenges, this paper proposes a novel generative adversarial fingerprint attack method for PCED. Firstly, to address the issue of poor robustness against defense strategies, this paper proposes a ridge texture rendering based generative adversarial network (RTRGAN) to perform robust generative adversarial fingerprint attack. Subsequently, to further enhance the visual quality of adversarial fingerprints, the realistic ridge texture is assigned by comparing the feature similarity between real fingerprints and adversarial fingerprints. Finally, this paper designs a joint optimization loss function, including the discriminator loss and the adversarial loss, to balance the attack robustness and visual fidelity. Extensive experiments demonstrate that compared with traditional perturbation-based methods, the proposed scheme significantly improves the attack success rate, has remarkable robustness, and generates more realistic adversarial fingerprints to human perception.', 'pages': '5471-5482', 'number': '3', 'volume': '70', 'year': '2024', 'title': 'RTRGAN: Ridge Texture Rendering-Based Generative Adversarial Fingerprint Attack for Portable Consumer Electronics Devices', 'journal': 'IEEE Transactions on Consumer Electronics', 'author': 'Yuan, Chengsheng and Cui, Baojie and Zhou, Zhili and Liu, Yuming and Yang, Yimin and Jonathan Wu, Q. M.', 'ENTRYTYPE': 'article', 'ID': '10542413'}"
8953992,Out-Of-Distribution Detection for Generalized Zero-Shot Action Recognition,"Mandal, Devraj and Narayan, Sanath and Dwivedi, Sai Kumar and Gupta, Vikram and Ahmed, Shuaib and Khan, Fahad Shahbaz and Shao, Ling",Mandal,10.1109/CVPR.2019.01022,2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"Generalized zero-shot action recognition is a challenging problem, where the task is to recognize new action categories that are unavailable during the training stage, in addition to the seen action categories. Existing approaches suffer from the inherent bias of the learned classifier towards the seen action categories. As a consequence, unseen category samples are incorrectly classified as belonging to one of the seen action categories. In this paper, we set out to tackle this issue by arguing for a separate treatment of seen and unseen action categories in generalized zero-shot action recognition. We introduce an out-of-distribution detector that determines whether the video features belong to a seen or unseen action category. To train our out-of-distribution detector, video features for unseen action categories are synthesized using generative adversarial networks trained on seen action category features. To the best of our knowledge, we are the first to propose an out-of-distribution detector based GZSL framework for action recognition in videos. Experiments are performed on three action recognition datasets: Olympic Sports, HMDB51 and UCF101. For generalized zero-shot action recognition, our proposed approach outperforms the baseline with absolute gains (in classification accuracy) of 7.0\%, 3.4\%, and 4.9\%, respectively, on these datasets.",Training;Computer vision;Accuracy;Zero shot learning;Detectors;Feature extraction;Generative adversarial networks;Pattern recognition;Videos;Sports;Action Recognition;Recognition: Detection;Categorization;Retrieval,"{'month': 'June', 'issn': '2575-7075', 'doi': '10.1109/CVPR.2019.01022', 'keywords': 'Training;Computer vision;Accuracy;Zero shot learning;Detectors;Feature extraction;Generative adversarial networks;Pattern recognition;Videos;Sports;Action Recognition;Recognition: Detection;Categorization;Retrieval', 'abstract': 'Generalized zero-shot action recognition is a challenging problem, where the task is to recognize new action categories that are unavailable during the training stage, in addition to the seen action categories. Existing approaches suffer from the inherent bias of the learned classifier towards the seen action categories. As a consequence, unseen category samples are incorrectly classified as belonging to one of the seen action categories. In this paper, we set out to tackle this issue by arguing for a separate treatment of seen and unseen action categories in generalized zero-shot action recognition. We introduce an out-of-distribution detector that determines whether the video features belong to a seen or unseen action category. To train our out-of-distribution detector, video features for unseen action categories are synthesized using generative adversarial networks trained on seen action category features. To the best of our knowledge, we are the first to propose an out-of-distribution detector based GZSL framework for action recognition in videos. Experiments are performed on three action recognition datasets: Olympic Sports, HMDB51 and UCF101. For generalized zero-shot action recognition, our proposed approach outperforms the baseline with absolute gains (in classification accuracy) of 7.0\\%, 3.4\\%, and 4.9\\%, respectively, on these datasets.', 'pages': '9977-9985', 'number': '', 'volume': '', 'year': '2019', 'title': 'Out-Of-Distribution Detection for Generalized Zero-Shot Action Recognition', 'booktitle': '2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)', 'author': 'Mandal, Devraj and Narayan, Sanath and Dwivedi, Sai Kumar and Gupta, Vikram and Ahmed, Shuaib and Khan, Fahad Shahbaz and Shao, Ling', 'ENTRYTYPE': 'inproceedings', 'ID': '8953992'}"
10030467,One-Shot Synthesis of Images and Segmentation Masks,"Sushko, Vadim and Zhang, Dan and Gall, Juergen and Khoreva, Anna",Sushko,10.1109/WACV56688.2023.00622,2023,2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),"Joint synthesis of images and segmentation masks with generative adversarial networks (GANs) is promising to reduce the effort needed for collecting image data with pixel-wise annotations. However, to learn high-fidelity image-mask synthesis, existing GAN approaches first need a pre-training phase requiring large amounts of image data, which limits their utilization in restricted image domains. In this work, we take a step to reduce this limitation, introducing the task of one-shot image-mask synthesis. We aim to generate diverse images and their segmentation masks given only a single labelled example, and assuming, contrary to previous models, no access to any pre-training data. To this end, inspired by the recent architectural developments of single-image GANs, we introduce our OSMIS model which enables the synthesis of segmentation masks that are precisely aligned to the generated images in the one-shot regime. Besides achieving the high fidelity of generated masks, OSMIS outperforms state-of-the-art single-image GAN models in image synthesis quality and diversity. In addition, despite not using any additional data, OSMIS demonstrates an impressive ability to serve as a source of useful data augmentation for one-shot segmentation applications, providing performance gains that are complementary to standard data augmentation techniques. Code is available at https://github.com/boschresearch/one-shot-synthesis.",Training;Image segmentation;Computer vision;Codes;Image synthesis;Performance gain;Generative adversarial networks;Algorithms: Computational photography;image and video synthesis;Machine learning architectures;formulations;and algorithms (including transfer;low-shot;semi-;self-;and un-supervised learning),"{'month': 'Jan', 'issn': '2642-9381', 'doi': '10.1109/WACV56688.2023.00622', 'keywords': 'Training;Image segmentation;Computer vision;Codes;Image synthesis;Performance gain;Generative adversarial networks;Algorithms: Computational photography;image and video synthesis;Machine learning architectures;formulations;and algorithms (including transfer;low-shot;semi-;self-;and un-supervised learning)', 'abstract': 'Joint synthesis of images and segmentation masks with generative adversarial networks (GANs) is promising to reduce the effort needed for collecting image data with pixel-wise annotations. However, to learn high-fidelity image-mask synthesis, existing GAN approaches first need a pre-training phase requiring large amounts of image data, which limits their utilization in restricted image domains. In this work, we take a step to reduce this limitation, introducing the task of one-shot image-mask synthesis. We aim to generate diverse images and their segmentation masks given only a single labelled example, and assuming, contrary to previous models, no access to any pre-training data. To this end, inspired by the recent architectural developments of single-image GANs, we introduce our OSMIS model which enables the synthesis of segmentation masks that are precisely aligned to the generated images in the one-shot regime. Besides achieving the high fidelity of generated masks, OSMIS outperforms state-of-the-art single-image GAN models in image synthesis quality and diversity. In addition, despite not using any additional data, OSMIS demonstrates an impressive ability to serve as a source of useful data augmentation for one-shot segmentation applications, providing performance gains that are complementary to standard data augmentation techniques. Code is available at https://github.com/boschresearch/one-shot-synthesis.', 'pages': '6274-6283', 'number': '', 'volume': '', 'year': '2023', 'title': 'One-Shot Synthesis of Images and Segmentation Masks', 'booktitle': '2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)', 'author': 'Sushko, Vadim and Zhang, Dan and Gall, Juergen and Khoreva, Anna', 'ENTRYTYPE': 'inproceedings', 'ID': '10030467'}"
7948855,Security and Privacy Challenges in the Internet of Things [Security and Privacy Matters],"Lee, Jong-Hyouk and Kim, Hyoungshick",Lee,10.1109/MCE.2017.2685019,2017,IEEE Consumer Electronics Magazine,"This is the first installment of the new ""Security and Privacy Matters"" column in IEEE Consumer Electronics Magazine. Security and privacy are always at the heart of everything that happens in the Consumer Electronics (CE) industry. This column aims to provide insight on various aspects of security and privacy in the CE industry.",Computer security;Privacy;Automobiles;Cloud computing;Artificial intelligence;Servers;Wireless communication,"{'month': 'July', 'issn': '2162-2256', 'doi': '10.1109/MCE.2017.2685019', 'keywords': 'Computer security;Privacy;Automobiles;Cloud computing;Artificial intelligence;Servers;Wireless communication', 'abstract': 'This is the first installment of the new ""Security and Privacy Matters"" column in IEEE Consumer Electronics Magazine. Security and privacy are always at the heart of everything that happens in the Consumer Electronics (CE) industry. This column aims to provide insight on various aspects of security and privacy in the CE industry.', 'pages': '134-136', 'number': '3', 'volume': '6', 'year': '2017', 'title': 'Security and Privacy Challenges in the Internet of Things [Security and Privacy Matters]', 'journal': 'IEEE Consumer Electronics Magazine', 'author': 'Lee, Jong-Hyouk and Kim, Hyoungshick', 'ENTRYTYPE': 'article', 'ID': '7948855'}"
10951560,Revolutionizing Drug Discovery,"Sayal, Anu and Jha, Janhvi and Chaithra, N. and Gangodkar, Atharv Rajesh and Shaziya Banu, S.",Sayal,10.1002/9781394234196.ch7,2024,Artificial Intelligence and Machine Learning in Drug Design and Development,"Summary <p>Historically, drug discovery was dominated by relentless scientific experiments and repetitive laboratory procedures. However, with the introduction of computational technologies and multidimensional data, this process has undergone significant transformation. This chapter emphasizes the pivotal role of AI, ML, DL, NLP, and robotics in contemporary drug development. AI, with its evolving intelligence, amplifies decision processes when supported by comprehensive data. The focus remains on the capabilities of ML, DL, and NLP in the pharmaceutical industry\&\#x2014;from accurate drug interaction predictions to the formulation of specialized treatment methods. Robotics has emerged as a vital tool, streamlining the management and distribution of medications. By leveraging AI methodologies such as random forest, SVM, and others, it is feasible to predict drug outcomes, identify new pharmaceutical benefits, and foresee any adverse side effects. It is notable how AI is the cornerstone for innovations including personalized medications, digital drug analysis, original drug formulation, and data\&\#x2010;driven predictions. While these technological breakthroughs signify a monumental evolution in drug discovery, there exist challenges like data gaps, unclear models, and ethical considerations. This chapter provides a comprehensive overview of the present drug discovery techniques, outlines prevalent challenges, and suggests potential solutions.</p>",Drugs;Deep learning;Natural language processing;Drug discovery;Random forests;Medical services;Machine learning algorithms;Internet;Hardware;Genetic algorithms,"{'url': 'https://ieeexplore-ieee-org.crai.referencistas.com/document/10951560', 'isbn': '9781394234189', 'publisher': 'Wiley', 'issn': '', 'doi': '10.1002/9781394234196.ch7', 'keywords': 'Drugs;Deep learning;Natural language processing;Drug discovery;Random forests;Medical services;Machine learning algorithms;Internet;Hardware;Genetic algorithms', 'abstract': 'Summary <p>Historically, drug discovery was dominated by relentless scientific experiments and repetitive laboratory procedures. However, with the introduction of computational technologies and multidimensional data, this process has undergone significant transformation. This chapter emphasizes the pivotal role of AI, ML, DL, NLP, and robotics in contemporary drug development. AI, with its evolving intelligence, amplifies decision processes when supported by comprehensive data. The focus remains on the capabilities of ML, DL, and NLP in the pharmaceutical industry\\&\\#x2014;from accurate drug interaction predictions to the formulation of specialized treatment methods. Robotics has emerged as a vital tool, streamlining the management and distribution of medications. By leveraging AI methodologies such as random forest, SVM, and others, it is feasible to predict drug outcomes, identify new pharmaceutical benefits, and foresee any adverse side effects. It is notable how AI is the cornerstone for innovations including personalized medications, digital drug analysis, original drug formulation, and data\\&\\#x2010;driven predictions. While these technological breakthroughs signify a monumental evolution in drug discovery, there exist challenges like data gaps, unclear models, and ethical considerations. This chapter provides a comprehensive overview of the present drug discovery techniques, outlines prevalent challenges, and suggests potential solutions.</p>', 'pages': '189-221', 'number': '', 'volume': '', 'year': '2024', 'title': 'Revolutionizing Drug Discovery', 'booktitle': 'Artificial Intelligence and Machine Learning in Drug Design and Development', 'author': 'Sayal, Anu and Jha, Janhvi and Chaithra, N. and Gangodkar, Atharv Rajesh and Shaziya Banu, S.', 'ENTRYTYPE': 'inbook', 'ID': '10951560'}"
8537810,CIC 2018 Tutorials,,,10.1109/CIC.2018.00-47,2018,2018 IEEE 4th International Conference on Collaboration and Internet Computing (CIC),Provides an abstract for each of the tutorial presentations and may include a brief professional biography of each presenter. The complete presentations were not made available for publication as part of the conference proceedings.,Cloud computing;Artificial intelligence;Internet of Things;Tutorials;Distributed databases;Safety,"{'month': 'Oct', 'issn': '', 'doi': '10.1109/CIC.2018.00-47', 'keywords': 'Cloud computing;Artificial intelligence;Internet of Things;Tutorials;Distributed databases;Safety', 'abstract': 'Provides an abstract for each of the tutorial presentations and may include a brief professional biography of each presenter. The complete presentations were not made available for publication as part of the conference proceedings.', 'pages': '25-33', 'number': '', 'volume': '', 'year': '2018', 'title': 'CIC 2018 Tutorials', 'booktitle': '2018 IEEE 4th International Conference on Collaboration and Internet Computing (CIC)', 'ENTRYTYPE': 'inproceedings', 'ID': '8537810'}"
9883242,A Transformer-Based Cross-Modal Image-Text Retrieval Method using Feature Decoupling and Reconstruction,"Zhang, Huan and Sun, Yingzhi and Liao, Yu and Xu, SiYuan and Yang, Rui and Wang, Shuang and Hou, Biao and Jiao, Licheng",Zhang,10.1109/IGARSS46834.2022.9883242,2022,IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium,"With the increasing application of remote sensing technology, the task of cross-modal retrieval of remote sensing images (CMRRS) has gradually attracted widespread attention. Ex-isting methods often completely map the features of different modalities to a shared space and do not decouple between the modal-invariant information and modal-heterogeneous in-formation, which leads to redundant information in feature mapping and usually gets sub-optimal retrieval performance. This paper proposes a Transformer-based CMRRS method using feature decoupling and reconstruction (TBFDR) to solve this problem. TBFDR achieves state-of-the-art performance in remote sensing image-text retrieval task on Sydney-Captions dataset.",Measurement;Reconstruction algorithms;Transformers;Adversarial machine learning;Sensors;Task analysis;Remote sensing;cross-modal retrieval;remote sensing;feature decoupling;adversarial learning;transformer,"{'month': 'July', 'issn': '2153-7003', 'doi': '10.1109/IGARSS46834.2022.9883242', 'keywords': 'Measurement;Reconstruction algorithms;Transformers;Adversarial machine learning;Sensors;Task analysis;Remote sensing;cross-modal retrieval;remote sensing;feature decoupling;adversarial learning;transformer', 'abstract': 'With the increasing application of remote sensing technology, the task of cross-modal retrieval of remote sensing images (CMRRS) has gradually attracted widespread attention. Ex-isting methods often completely map the features of different modalities to a shared space and do not decouple between the modal-invariant information and modal-heterogeneous in-formation, which leads to redundant information in feature mapping and usually gets sub-optimal retrieval performance. This paper proposes a Transformer-based CMRRS method using feature decoupling and reconstruction (TBFDR) to solve this problem. TBFDR achieves state-of-the-art performance in remote sensing image-text retrieval task on Sydney-Captions dataset.', 'pages': '1796-1799', 'number': '', 'volume': '', 'year': '2022', 'title': 'A Transformer-Based Cross-Modal Image-Text Retrieval Method using Feature Decoupling and Reconstruction', 'booktitle': 'IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium', 'author': 'Zhang, Huan and Sun, Yingzhi and Liao, Yu and Xu, SiYuan and Yang, Rui and Wang, Shuang and Hou, Biao and Jiao, Licheng', 'ENTRYTYPE': 'inproceedings', 'ID': '9883242'}"
10222528,ADFA: Attention-Augmented Differentiable Top-K Feature Adaptation for Unsupervised Medical Anomaly Detection,"Huang, Yiming and Liu, Guole and Luo, Yaoru and Yang, Ge",Huang,10.1109/ICIP49359.2023.10222528,2023,2023 IEEE International Conference on Image Processing (ICIP),"The scarcity of annotated data, particularly for rare diseases, limits the variability of training data and the range of detectable lesions, presenting a significant challenge for supervised anomaly detection in medical imaging. To solve this problem, we propose a novel unsupervised method for medical image anomaly detection: Attention-Augmented Differentiable top-k Feature Adaptation (ADFA). The method utilizes Wide-ResNet50-2 (WR50) network pre-trained on ImageNet to extract initial feature representations. To reduce the channel dimensionality while preserving relevant channel information, we employ an attention-augmented patch descriptor on the extracted features. We then apply differentiable top-k feature adaptation to train the patch descriptor, mapping the extracted feature representations to a new vector space, enabling effective detection of anomalies. Experiments show that ADFA outperforms state-of-the-art (SOTA) methods on multiple challenging medical image datasets, confirming its effectiveness in medical anomaly detection.",Location awareness;Image analysis;Training data;Feature extraction;Lesions;Anomaly detection;Biomedical imaging;anomaly detection;medical image;unsupervised learning;attention mechanism;feature adaptation,"{'month': 'Oct', 'issn': '', 'doi': '10.1109/ICIP49359.2023.10222528', 'keywords': 'Location awareness;Image analysis;Training data;Feature extraction;Lesions;Anomaly detection;Biomedical imaging;anomaly detection;medical image;unsupervised learning;attention mechanism;feature adaptation', 'abstract': 'The scarcity of annotated data, particularly for rare diseases, limits the variability of training data and the range of detectable lesions, presenting a significant challenge for supervised anomaly detection in medical imaging. To solve this problem, we propose a novel unsupervised method for medical image anomaly detection: Attention-Augmented Differentiable top-k Feature Adaptation (ADFA). The method utilizes Wide-ResNet50-2 (WR50) network pre-trained on ImageNet to extract initial feature representations. To reduce the channel dimensionality while preserving relevant channel information, we employ an attention-augmented patch descriptor on the extracted features. We then apply differentiable top-k feature adaptation to train the patch descriptor, mapping the extracted feature representations to a new vector space, enabling effective detection of anomalies. Experiments show that ADFA outperforms state-of-the-art (SOTA) methods on multiple challenging medical image datasets, confirming its effectiveness in medical anomaly detection.', 'pages': '206-210', 'number': '', 'volume': '', 'year': '2023', 'title': 'ADFA: Attention-Augmented Differentiable Top-K Feature Adaptation for Unsupervised Medical Anomaly Detection', 'booktitle': '2023 IEEE International Conference on Image Processing (ICIP)', 'author': 'Huang, Yiming and Liu, Guole and Luo, Yaoru and Yang, Ge', 'ENTRYTYPE': 'inproceedings', 'ID': '10222528'}"
9554533,Cross-Modal Feature Fusion Retrieval for Remote Sensing Image-Voice Retrieval,"Yang, Rui and Gu, Yu and Liao, Yu and Zhang, Huan and Sun, Yingzhi and Wang, Shuang and Hou, Biao and Jiao, Licheng and Zhang, He",Yang,10.1109/IGARSS47720.2021.9554533,2021,2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS,"With the increasing popularity of remote sensing technology applications, some emergency scenarios require rapid retrieval of remote sensing images, such as earthquake rescue, etc. Due to the high efficiency of voice input, researchers have focused on cross-modal remote sensing image-voice retrieval methods. However, these methods have two major drawbacks: speech input lacks discrimination and the intra-modal semantic information is under used. To address these drawbacks, we propose a novel cross-modal feature fusion retrieval model. Our model provides a more optimized cross-modal common feature space than previous models and thus optimizes the retrieval performance. First, our model adds the extra textual keyword information to the audio feature for remote sensing image retrieval. Second, it introduces inter-modality adversarial learning and intra-modality semantic discrimination into the remote sensing image-voice retrieval task. We conducted experiments on two datasets modified from the UCM-Captions dataset and the Remote Sensing Image Caption Dataset. The experimental results show that our model outperforms state-of-the-art models in this task.",Visualization;Semantics;Image retrieval;Earthquakes;Adversarial machine learning;Sensors;Task analysis;cross-modal retrieval;remote sensing;feature fusion;adversarial learning,"{'month': 'July', 'issn': '2153-7003', 'doi': '10.1109/IGARSS47720.2021.9554533', 'keywords': 'Visualization;Semantics;Image retrieval;Earthquakes;Adversarial machine learning;Sensors;Task analysis;cross-modal retrieval;remote sensing;feature fusion;adversarial learning', 'abstract': 'With the increasing popularity of remote sensing technology applications, some emergency scenarios require rapid retrieval of remote sensing images, such as earthquake rescue, etc. Due to the high efficiency of voice input, researchers have focused on cross-modal remote sensing image-voice retrieval methods. However, these methods have two major drawbacks: speech input lacks discrimination and the intra-modal semantic information is under used. To address these drawbacks, we propose a novel cross-modal feature fusion retrieval model. Our model provides a more optimized cross-modal common feature space than previous models and thus optimizes the retrieval performance. First, our model adds the extra textual keyword information to the audio feature for remote sensing image retrieval. Second, it introduces inter-modality adversarial learning and intra-modality semantic discrimination into the remote sensing image-voice retrieval task. We conducted experiments on two datasets modified from the UCM-Captions dataset and the Remote Sensing Image Caption Dataset. The experimental results show that our model outperforms state-of-the-art models in this task.', 'pages': '2855-2858', 'number': '', 'volume': '', 'year': '2021', 'title': 'Cross-Modal Feature Fusion Retrieval for Remote Sensing Image-Voice Retrieval', 'booktitle': '2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS', 'author': 'Yang, Rui and Gu, Yu and Liao, Yu and Zhang, Huan and Sun, Yingzhi and Wang, Shuang and Hou, Biao and Jiao, Licheng and Zhang, He', 'ENTRYTYPE': 'inproceedings', 'ID': '9554533'}"
10981777,Mobility Management With AI,"Jahanmanesh, Pooyan and Farhadi, Amirfarhad and Zamanifar, Azadeh",Jahanmanesh,10.1109/ACCESS.2025.3566337,2025,IEEE Access,"Today, considering the complexity of different types of networks worldwide and the variety of different pathways for mobility, researchers are interested in research and analysis in mobility management, one of the critical aspects of network management. Mobility management is used for many cases, such as urban management, location-based services (LBS), collision avoidance and tourism industry improvement. On the other hand, the dramatic advances in artificial intelligence (AI) have attracted much attention to the use of this tool in network management, using which they can quickly predict the following route or destination of users in the network in different scenarios. However, there are many challenges in this field, and ignoring them reduces the accuracy of the models. In this paper, We introduce the methods that researchers have presented and mention the advantages and disadvantages of each. The articles used include reputable journals from 2020 to 2025. Mobility management is done by focusing on four topics human mobility, vehicle, aircraft and ship. After introducing the approaches, we categorized and compared these models with related articles. Then, we showed the important items used in the articles, such as datasets, in the diagram and introduced the most used and famous ones. Also, commonly used metrics were reviewed. We highlight aspects that can help improve the performance of models. By considering these aspects, researchers can introduce more practical models that achieve lower errors using metrics such as Root-mean-square deviation (RMSE).",Artificial intelligence;Predictive models;Hidden Markov models;Data models;Measurement;Marine vehicles;Aircraft;Accuracy;Trajectory;Generative adversarial networks;Mobility management;artificial intelligence;mobility prediction;network management;deep learning;machine learning,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2025.3566337', 'keywords': 'Artificial intelligence;Predictive models;Hidden Markov models;Data models;Measurement;Marine vehicles;Aircraft;Accuracy;Trajectory;Generative adversarial networks;Mobility management;artificial intelligence;mobility prediction;network management;deep learning;machine learning', 'abstract': 'Today, considering the complexity of different types of networks worldwide and the variety of different pathways for mobility, researchers are interested in research and analysis in mobility management, one of the critical aspects of network management. Mobility management is used for many cases, such as urban management, location-based services (LBS), collision avoidance and tourism industry improvement. On the other hand, the dramatic advances in artificial intelligence (AI) have attracted much attention to the use of this tool in network management, using which they can quickly predict the following route or destination of users in the network in different scenarios. However, there are many challenges in this field, and ignoring them reduces the accuracy of the models. In this paper, We introduce the methods that researchers have presented and mention the advantages and disadvantages of each. The articles used include reputable journals from 2020 to 2025. Mobility management is done by focusing on four topics human mobility, vehicle, aircraft and ship. After introducing the approaches, we categorized and compared these models with related articles. Then, we showed the important items used in the articles, such as datasets, in the diagram and introduced the most used and famous ones. Also, commonly used metrics were reviewed. We highlight aspects that can help improve the performance of models. By considering these aspects, researchers can introduce more practical models that achieve lower errors using metrics such as Root-mean-square deviation (RMSE).', 'pages': '83022-83060', 'number': '', 'volume': '13', 'year': '2025', 'title': 'Mobility Management With AI', 'journal': 'IEEE Access', 'author': 'Jahanmanesh, Pooyan and Farhadi, Amirfarhad and Zamanifar, Azadeh', 'ENTRYTYPE': 'article', 'ID': '10981777'}"
10980939,MR-to-CT Translation Using Frequency-Separated Diffusion Models,"Gong, Chaohui and Huang, Zisheng and Wu, Zhiying and Zhao, Mingyang and Xu, Fan and Bai, Xuexue and Feng, Ming and Granados, Alejandro and Meng, Gaofeng and Lei, Zhen and Liu, Hongbin",Gong,10.1109/ISBI60581.2025.10980939,2025,2025 IEEE 22nd International Symposium on Biomedical Imaging (ISBI),"Diffusion models have shown great potential in generating high-quality images for magnetic resonance (MR)-to-computed tomography (CT) translation. However, existing diffusion models have limitations in preserving skeletal structures, especially for unpaired datasets. The preservation of skeletal structural details is crucial to avoid disease misidentification and to achieve accurate medical diagnosis and treatment planning. In this paper, a frequency-separated diffusion model (FSDM) is proposed to maintain skeletal structural details, whereby a frequency-separated module can effectively separate the frequency components of medical images in the Fourier domain. In the proposed FSDM, a frequency conversion module is incorporated to convert the MR images into frequency-specific outputs. It leverages the frequency-specific information to guide a subsequent diffusion model for MR-to-CT translation. The results on a public brain dataset demonstrate that our method outperforms the state-of-the-art methods in metrics of Structural Similarity Index Measure (SSIM) and Peak Signal-to-Noise Ratio (PSNR). Our experiments show that FSDM can generate high-quality CT images while preserving skeletal structural details. Our code will be publicly available.",Translation;PSNR;Frequency-domain analysis;Computed tomography;Diffusion models;Frequency conversion;Planning;Medical diagnosis;Reliability;Medical diagnostic imaging;MR-to-CT Translation;Frequency separated diffusion model;Patch contrastive alignment,"{'month': 'April', 'issn': '1945-8452', 'doi': '10.1109/ISBI60581.2025.10980939', 'keywords': 'Translation;PSNR;Frequency-domain analysis;Computed tomography;Diffusion models;Frequency conversion;Planning;Medical diagnosis;Reliability;Medical diagnostic imaging;MR-to-CT Translation;Frequency separated diffusion model;Patch contrastive alignment', 'abstract': 'Diffusion models have shown great potential in generating high-quality images for magnetic resonance (MR)-to-computed tomography (CT) translation. However, existing diffusion models have limitations in preserving skeletal structures, especially for unpaired datasets. The preservation of skeletal structural details is crucial to avoid disease misidentification and to achieve accurate medical diagnosis and treatment planning. In this paper, a frequency-separated diffusion model (FSDM) is proposed to maintain skeletal structural details, whereby a frequency-separated module can effectively separate the frequency components of medical images in the Fourier domain. In the proposed FSDM, a frequency conversion module is incorporated to convert the MR images into frequency-specific outputs. It leverages the frequency-specific information to guide a subsequent diffusion model for MR-to-CT translation. The results on a public brain dataset demonstrate that our method outperforms the state-of-the-art methods in metrics of Structural Similarity Index Measure (SSIM) and Peak Signal-to-Noise Ratio (PSNR). Our experiments show that FSDM can generate high-quality CT images while preserving skeletal structural details. Our code will be publicly available.', 'pages': '1-5', 'number': '', 'volume': '', 'year': '2025', 'title': 'MR-to-CT Translation Using Frequency-Separated Diffusion Models', 'booktitle': '2025 IEEE 22nd International Symposium on Biomedical Imaging (ISBI)', 'author': 'Gong, Chaohui and Huang, Zisheng and Wu, Zhiying and Zhao, Mingyang and Xu, Fan and Bai, Xuexue and Feng, Ming and Granados, Alejandro and Meng, Gaofeng and Lei, Zhen and Liu, Hongbin', 'ENTRYTYPE': 'inproceedings', 'ID': '10980939'}"
9970668,Research on Underwater Image Enhancement Algorithm Based on SRGAN,"Zhang, Zhiming and Jin, Lina and Gao, Tianzhu",Zhang,10.1109/ICCSI55536.2022.9970668,2022,2022 International Conference on Cyber-Physical Social Intelligence (ICCSI),"Due to the limitation of the special underwater imaging environment, underwater images usually have problems such as low contrast, blurred texture features, color distortion and so on. Based on the typical problem of underwater images, this paper improves the network structure and loss function on the basis of the original SRGAN network model, and achieves good results. The generative network reduces the convolutional layers and removes the normalization layer (BN layer), reducing resource consumption. The loss function introduces L1 content loss and VGG19 perceptual loss to improve the stability of training. The experimental results show that the improved SRGAN network model effectively solves the color distortion and blurring of underwater images, and has a good enhancement effect on underwater images.",Training;Image quality;Image color analysis;Superresolution;Imaging;Distortion;Stability analysis;deep learning;super resolution generative adversarial(SRGAN);underwater image enhancement,"{'month': 'Nov', 'issn': '', 'doi': '10.1109/ICCSI55536.2022.9970668', 'keywords': 'Training;Image quality;Image color analysis;Superresolution;Imaging;Distortion;Stability analysis;deep learning;super resolution generative adversarial(SRGAN);underwater image enhancement', 'abstract': 'Due to the limitation of the special underwater imaging environment, underwater images usually have problems such as low contrast, blurred texture features, color distortion and so on. Based on the typical problem of underwater images, this paper improves the network structure and loss function on the basis of the original SRGAN network model, and achieves good results. The generative network reduces the convolutional layers and removes the normalization layer (BN layer), reducing resource consumption. The loss function introduces L1 content loss and VGG19 perceptual loss to improve the stability of training. The experimental results show that the improved SRGAN network model effectively solves the color distortion and blurring of underwater images, and has a good enhancement effect on underwater images.', 'pages': '374-379', 'number': '', 'volume': '', 'year': '2022', 'title': 'Research on Underwater Image Enhancement Algorithm Based on SRGAN', 'booktitle': '2022 International Conference on Cyber-Physical Social Intelligence (ICCSI)', 'author': 'Zhang, Zhiming and Jin, Lina and Gao, Tianzhu', 'ENTRYTYPE': 'inproceedings', 'ID': '9970668'}"
10797604,Navigating Data Abundance: Generative Conversational AI Agents in Information Analysis,"Sahu, Barnali and Das, Sanjog and Pattnaik, Ayush and Puri, Abhishek and Mohanty, Kasmik",Sahu,10.1109/ODICON62106.2024.10797604,2024,"2024 3rd Odisha International Conference on Electrical Power Engineering, Communication and Computing Technology (ODICON)","In response to the escalating challenges posed by information overload, particularly in today's data-rich environment, we propose the development of a sophisticated Generative Conversational AI Agent (GCAIA) tailored specifically for information retrieval tasks. This innovative system integrates cutting-edge techniques in natural language processing, machine learning, and generative modeling to provide users with an intuitive platform for accessing, analyzing, and retrieving information across diverse domains. Key features include robust natural language understanding facilitating seamless interaction, efficient retrieval mechanisms enabling quick access to relevant data, contextual adaptation for delivering personalized responses, generative capabilities fostering natural and engaging conversations, multimodal integration supporting various data formats, and utilization of knowledge graphs for enhanced organization and semantic understanding. Through rigorous research, development, and evaluation processes, our goal is to demonstrate the effectiveness, reliability, and scalability of the GCAIA in significantly improving information retrieval processes across diverse domains. Ultimately, our aim is to empower users to navigate the ever-expanding landscape of knowledge and data effectively, thus mitigating the challenges posed by information overload in the digital age.",Power engineering;Conversational artificial intelligence;Navigation;Scalability;Semantics;Organizations;Oral communication;Information retrieval;Natural language processing;Reliability;Generative Conversational AI;Information retrieval;Machine Learning,"{'month': 'Nov', 'issn': '', 'doi': '10.1109/ODICON62106.2024.10797604', 'keywords': 'Power engineering;Conversational artificial intelligence;Navigation;Scalability;Semantics;Organizations;Oral communication;Information retrieval;Natural language processing;Reliability;Generative Conversational AI;Information retrieval;Machine Learning', 'abstract': ""In response to the escalating challenges posed by information overload, particularly in today's data-rich environment, we propose the development of a sophisticated Generative Conversational AI Agent (GCAIA) tailored specifically for information retrieval tasks. This innovative system integrates cutting-edge techniques in natural language processing, machine learning, and generative modeling to provide users with an intuitive platform for accessing, analyzing, and retrieving information across diverse domains. Key features include robust natural language understanding facilitating seamless interaction, efficient retrieval mechanisms enabling quick access to relevant data, contextual adaptation for delivering personalized responses, generative capabilities fostering natural and engaging conversations, multimodal integration supporting various data formats, and utilization of knowledge graphs for enhanced organization and semantic understanding. Through rigorous research, development, and evaluation processes, our goal is to demonstrate the effectiveness, reliability, and scalability of the GCAIA in significantly improving information retrieval processes across diverse domains. Ultimately, our aim is to empower users to navigate the ever-expanding landscape of knowledge and data effectively, thus mitigating the challenges posed by information overload in the digital age."", 'pages': '1-5', 'number': '', 'volume': '', 'year': '2024', 'title': 'Navigating Data Abundance: Generative Conversational AI Agents in Information Analysis', 'booktitle': '2024 3rd Odisha International Conference on Electrical Power Engineering, Communication and Computing Technology (ODICON)', 'author': 'Sahu, Barnali and Das, Sanjog and Pattnaik, Ayush and Puri, Abhishek and Mohanty, Kasmik', 'ENTRYTYPE': 'inproceedings', 'ID': '10797604'}"
10466226,Art Generation AI Model for Low-End Devices,"Verma, Seema and Arora, Vasudha and Perumal, Thinagaran",Verma,10.1109/ICAICCIT60255.2023.10466226,2023,"2023 International Conference on Advances in Computation, Communication and Information Technology (ICAICCIT)","Art is a powerful medium of emotional expression, using elements like color, sound, and form. It reflects the artist's personality and the era they lived in, telling captivating stories. AI has advanced in generating and manipulating high-quality images using text prompts. Generative Adversarial Networks (GANs) revolutionized image generation, leading to various models like Open-Edit, GLIDE, DALL-E, and VQGAN. These models have shown impressive capabilities, producing creative outputs. However, accessibility remains a challenge due to the computational resources required.This work aims to enable art generation on low-end devices, making AI-driven art more accessible. One major challenge identified in the models is the high memory demand during image creation. Here, the chosen method, VQGAN-Clip, allows for image creation and manipulation solely through human-written text prompts. To address the memory constraint, the sliding window method is used in conjunction with VQGAN-Clip. This approach effectively reduces the VRAM requirement from 8 GB to 2 GB for generating images of the same size.",Art;Image synthesis;Image color analysis;Computational modeling;Memory management;Speech recognition;Generative adversarial networks;GAN;Sliding Window;Performance,"{'month': 'Nov', 'issn': '', 'doi': '10.1109/ICAICCIT60255.2023.10466226', 'keywords': 'Art;Image synthesis;Image color analysis;Computational modeling;Memory management;Speech recognition;Generative adversarial networks;GAN;Sliding Window;Performance', 'abstract': ""Art is a powerful medium of emotional expression, using elements like color, sound, and form. It reflects the artist's personality and the era they lived in, telling captivating stories. AI has advanced in generating and manipulating high-quality images using text prompts. Generative Adversarial Networks (GANs) revolutionized image generation, leading to various models like Open-Edit, GLIDE, DALL-E, and VQGAN. These models have shown impressive capabilities, producing creative outputs. However, accessibility remains a challenge due to the computational resources required.This work aims to enable art generation on low-end devices, making AI-driven art more accessible. One major challenge identified in the models is the high memory demand during image creation. Here, the chosen method, VQGAN-Clip, allows for image creation and manipulation solely through human-written text prompts. To address the memory constraint, the sliding window method is used in conjunction with VQGAN-Clip. This approach effectively reduces the VRAM requirement from 8 GB to 2 GB for generating images of the same size."", 'pages': '30-35', 'number': '', 'volume': '', 'year': '2023', 'title': 'Art Generation AI Model for Low-End Devices', 'booktitle': '2023 International Conference on Advances in Computation, Communication and Information Technology (ICAICCIT)', 'author': 'Verma, Seema and Arora, Vasudha and Perumal, Thinagaran', 'ENTRYTYPE': 'inproceedings', 'ID': '10466226'}"
11017600,Comprehensive Style Transfer for Facial Images Using Enhanced Feature Attribution in Generative Adversarial Nets,"Yoo, Yongseon and Kim, Seonggyu and Lee, Jong-Min",Yoo,10.1109/ACCESS.2025.3574729,2025,IEEE Access,"Image-to-image translation is a fundamental task in computer vision that transforms images between domains while preserving essential content. Although adaptive instance normalization (AdaIN) is widely used for style transfer, its reliance on simple statistical measures (mean and variance) may limit its ability to capture complex style characteristics. We propose a novel framework that enhances style transfer by combining AdaIN with Gram matrices, leveraging the complementary strengths of both approaches. Our method introduces two key innovations for enhanced feature attribution: 1) dual Gram matrix-based loss functions (G1 and G2), which operate at different stages of the generation process to capture richer style information by establishing deeper correlations between feature maps, and 2) a balanced training objective that integrates perceptual loss with cycle-consistency loss to maintain content fidelity during style transfer. This comprehensive feature attribution mechanism enables our model to decompose and reassign stylistic elements across domains more precisely. Through ablation studies, we demonstrate that each component of our framework contributes to performance improvements, with the complete model achieving the best results on both the CelebA-HQ and FFHQ datasets. Our comprehensive evaluation, using distribution similarity metrics, classification-based assessments, and visual comparisons, demonstrates that our approach effectively captures and transfers complex style characteristics while preserving content integrity, outperforming state-of-the-art models. Specifically, our model achieves superior Fréchet Inception Distance (FID) scores (19.88 vs. 24.22) and recognition accuracy (0.966 vs. 0.941) compared to StarGAN v2, confirming the performance gains introduced by our enhanced feature attribution strategy.",Translation;Generators;Visualization;Feature extraction;Numerical models;Correlation;Training;Standards;Convolutional neural networks;Transforms;Image-to-image translation;style transfer;gram matrix;generative adversarial networks (GANs);style application;style evaluation,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2025.3574729', 'keywords': 'Translation;Generators;Visualization;Feature extraction;Numerical models;Correlation;Training;Standards;Convolutional neural networks;Transforms;Image-to-image translation;style transfer;gram matrix;generative adversarial networks (GANs);style application;style evaluation', 'abstract': 'Image-to-image translation is a fundamental task in computer vision that transforms images between domains while preserving essential content. Although adaptive instance normalization (AdaIN) is widely used for style transfer, its reliance on simple statistical measures (mean and variance) may limit its ability to capture complex style characteristics. We propose a novel framework that enhances style transfer by combining AdaIN with Gram matrices, leveraging the complementary strengths of both approaches. Our method introduces two key innovations for enhanced feature attribution: 1) dual Gram matrix-based loss functions (G1 and G2), which operate at different stages of the generation process to capture richer style information by establishing deeper correlations between feature maps, and 2) a balanced training objective that integrates perceptual loss with cycle-consistency loss to maintain content fidelity during style transfer. This comprehensive feature attribution mechanism enables our model to decompose and reassign stylistic elements across domains more precisely. Through ablation studies, we demonstrate that each component of our framework contributes to performance improvements, with the complete model achieving the best results on both the CelebA-HQ and FFHQ datasets. Our comprehensive evaluation, using distribution similarity metrics, classification-based assessments, and visual comparisons, demonstrates that our approach effectively captures and transfers complex style characteristics while preserving content integrity, outperforming state-of-the-art models. Specifically, our model achieves superior Fréchet Inception Distance (FID) scores (19.88 vs. 24.22) and recognition accuracy (0.966 vs. 0.941) compared to StarGAN v2, confirming the performance gains introduced by our enhanced feature attribution strategy.', 'pages': '99145-99159', 'number': '', 'volume': '13', 'year': '2025', 'title': 'Comprehensive Style Transfer for Facial Images Using Enhanced Feature Attribution in Generative Adversarial Nets', 'journal': 'IEEE Access', 'author': 'Yoo, Yongseon and Kim, Seonggyu and Lee, Jong-Min', 'ENTRYTYPE': 'article', 'ID': '11017600'}"
10806626,Multiview Deep Learning-Based Molecule Design and Structural Optimization Accelerates Inhibitor Discover,"Pang, Chao and Wang, Yu and Jiang, Yi and Wang, Ruheng and Yao, Xiaojun and Zou, Quan and Zeng, Xiangxiang and Su, Ran and Wei, Leyi",Pang,10.1109/TNNLS.2024.3506619,2025,IEEE Transactions on Neural Networks and Learning Systems,"In this work, we propose MEDICO, a multiview deep generative model for molecule generation, structural optimization, and the SARS-CoV-2 inhibitor discovery. To the best of our knowledge, MEDICO is the first-of-this-kind graph generative model that can generate molecular graphs similar to the structure of targeted molecules, with a multiview representation learning framework to sufficiently and adaptively learn comprehensive structural semantics from targeted molecular topology and geometry. We show that our MEDICO significantly outperforms the state-of-the-art methods in generating valid, novel, and unique molecules under benchmarking comparisons, particularly achieving $\tilde {8}5 \\%$ improvement compared with the state-of-the-art methods in terms of validity. Importantly, we showcase that the multiview deep learning model enables us to generate not only the molecules structurally similar to the targeted molecules but also the molecules with desired chemical properties. Moreover, case study results on targeted molecule generation for the SARS-CoV-2 main protease (Mpro) show that we successfully generate new small molecules with desired drug-like properties for the Mpro by integrating molecular docking into our model as a chemical priori, potentially accelerating the de novo design of COVID-19 drugs. Furthermore, we apply MEDICO to the structural optimization of three well-known Mpro inhibitors (N3, 11a, and GC376) and achieve $\tilde {8}8 \\%$ improvement compared with the origin inhibitors in their binding affinity to Mpro, demonstrating the application value of our model for the development of therapeutics for SARS-CoV-2 infection.",Geometry;Chemicals;Three-dimensional displays;Topology;Drugs;Atoms;Computational modeling;Tensors;Optimization;Inhibitors;Generative model;machine learning;multiview learning,"{'month': 'Aug', 'issn': '2162-2388', 'doi': '10.1109/TNNLS.2024.3506619', 'keywords': 'Geometry;Chemicals;Three-dimensional displays;Topology;Drugs;Atoms;Computational modeling;Tensors;Optimization;Inhibitors;Generative model;machine learning;multiview learning', 'abstract': 'In this work, we propose MEDICO, a multiview deep generative model for molecule generation, structural optimization, and the SARS-CoV-2 inhibitor discovery. To the best of our knowledge, MEDICO is the first-of-this-kind graph generative model that can generate molecular graphs similar to the structure of targeted molecules, with a multiview representation learning framework to sufficiently and adaptively learn comprehensive structural semantics from targeted molecular topology and geometry. We show that our MEDICO significantly outperforms the state-of-the-art methods in generating valid, novel, and unique molecules under benchmarking comparisons, particularly achieving $\\tilde {8}5 \\\\%$ improvement compared with the state-of-the-art methods in terms of validity. Importantly, we showcase that the multiview deep learning model enables us to generate not only the molecules structurally similar to the targeted molecules but also the molecules with desired chemical properties. Moreover, case study results on targeted molecule generation for the SARS-CoV-2 main protease (Mpro) show that we successfully generate new small molecules with desired drug-like properties for the Mpro by integrating molecular docking into our model as a chemical priori, potentially accelerating the de novo design of COVID-19 drugs. Furthermore, we apply MEDICO to the structural optimization of three well-known Mpro inhibitors (N3, 11a, and GC376) and achieve $\\tilde {8}8 \\\\%$ improvement compared with the origin inhibitors in their binding affinity to Mpro, demonstrating the application value of our model for the development of therapeutics for SARS-CoV-2 infection.', 'pages': '14022-14036', 'number': '8', 'volume': '36', 'year': '2025', 'title': 'Multiview Deep Learning-Based Molecule Design and Structural Optimization Accelerates Inhibitor Discover', 'journal': 'IEEE Transactions on Neural Networks and Learning Systems', 'author': 'Pang, Chao and Wang, Yu and Jiang, Yi and Wang, Ruheng and Yao, Xiaojun and Zou, Quan and Zeng, Xiangxiang and Su, Ran and Wei, Leyi', 'ENTRYTYPE': 'article', 'ID': '10806626'}"
9929762,Semi-supervised Malicious Traffic Detection with Improved Wasserstein Generative Adversarial Network with Gradient Penalty,"Wang, Jiafeng and Liu, Ming and Yin, Xiaokang and Zhao, Yuhao and Liu, Shengli",Wang,10.1109/IAEAC54830.2022.9929762,2022,"2022 IEEE 6th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC )","With the development of artificial intelligence, malicious traffic detection technology based on deep learning has become mainstream with its powerful detection performance. Most existing deep learning-based detection methods require sufficient labeled data to train classifiers. But much labeled traffic is difficult to obtain in practical applications. To solve this problem, we propose and implement a semi-supervised malicious traffic detection method based on improved Wasserstein Generative Adversarial Network with Gradient Penalized (WGAN-GP), denoted as SEMI-WGAN-GP. First, we construct a pseudo- feature map (PFM) for each stream in the dataset using the time-series properties of consecutive packets in a given stream. Second, we fix the generator and only train the discriminator on a few labeled PFMs, which obtain a discriminator that can distinguish malicious from benign traffic. Finally, the generator and discriminator are trained unsupervisedly in the adversarial setting, which allows the discriminator to improve detection performance by generator-generated PFMs. Experiments on the publicly available UNSW-NB15 dataset demonstrate that SEMI-WGAN-GP can achieve 90.53\% accuracy using a few labeled samples (20\% of the samples in the dataset are marked), exceeding the 79.92\% and 84.94\% of fully supervised multilayer perceptron network (MLP) and 2- dimensional convolutional neural network (2DCNN). In addition, SEMI-WGAN-GP also achieves better detection performance than SEMI-DCGAN by generating better samples.",Training;Deep learning;Automation;Neural networks;Multilayer perceptrons;Generative adversarial networks;Generators;traffic detection;semi-supervised learning;generative adversarial networks;malicious traffic,"{'month': 'Oct', 'issn': '2689-6621', 'doi': '10.1109/IAEAC54830.2022.9929762', 'keywords': 'Training;Deep learning;Automation;Neural networks;Multilayer perceptrons;Generative adversarial networks;Generators;traffic detection;semi-supervised learning;generative adversarial networks;malicious traffic', 'abstract': 'With the development of artificial intelligence, malicious traffic detection technology based on deep learning has become mainstream with its powerful detection performance. Most existing deep learning-based detection methods require sufficient labeled data to train classifiers. But much labeled traffic is difficult to obtain in practical applications. To solve this problem, we propose and implement a semi-supervised malicious traffic detection method based on improved Wasserstein Generative Adversarial Network with Gradient Penalized (WGAN-GP), denoted as SEMI-WGAN-GP. First, we construct a pseudo- feature map (PFM) for each stream in the dataset using the time-series properties of consecutive packets in a given stream. Second, we fix the generator and only train the discriminator on a few labeled PFMs, which obtain a discriminator that can distinguish malicious from benign traffic. Finally, the generator and discriminator are trained unsupervisedly in the adversarial setting, which allows the discriminator to improve detection performance by generator-generated PFMs. Experiments on the publicly available UNSW-NB15 dataset demonstrate that SEMI-WGAN-GP can achieve 90.53\\% accuracy using a few labeled samples (20\\% of the samples in the dataset are marked), exceeding the 79.92\\% and 84.94\\% of fully supervised multilayer perceptron network (MLP) and 2- dimensional convolutional neural network (2DCNN). In addition, SEMI-WGAN-GP also achieves better detection performance than SEMI-DCGAN by generating better samples.', 'pages': '1916-1922', 'number': '', 'volume': '', 'year': '2022', 'title': 'Semi-supervised Malicious Traffic Detection with Improved Wasserstein Generative Adversarial Network with Gradient Penalty', 'booktitle': '2022 IEEE 6th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC )', 'author': 'Wang, Jiafeng and Liu, Ming and Yin, Xiaokang and Zhao, Yuhao and Liu, Shengli', 'ENTRYTYPE': 'inproceedings', 'ID': '9929762'}"
10827504,Generative AI-Powered Aerial Access Networks: Recent Studies and Future Outlook,"Nguyen, Tri-Hai and Nguyen, Truong Khang and Quoc Bao, Vo Nguyen and Park, Heejae and Park, Laihyuk",Nguyen,10.1109/ICTC62082.2024.10827504,2024,2024 15th International Conference on Information and Communication Technology Convergence (ICTC),"Aerial access networks (AANs), comprising un-manned aerial vehicles (UAV), high-altitude platforms (HAPs), and low-earth orbit (LEO) satellites, are rapidly emerging as a critical component of next-generation communication systems. Their dynamic nature, heterogeneous composition, and vast coverage area pose significant challenges for network management and optimization. Generative artificial intelligence (GenAI), renowned for its ability to generate new content, offers a promising solution to these challenges. In this paper, we explore the applications of GenAI in AANs in recent studies. We also identify key challenges and outline promising research directions.",Satellites;Reviews;Generative AI;Low earth orbit satellites;Orbits;Information and communication technology;Security;Vehicle dynamics;Optimization;Next generation networking;aerial access network;generative AI;recent studies;future outlook,"{'month': 'Oct', 'issn': '2162-1241', 'doi': '10.1109/ICTC62082.2024.10827504', 'keywords': 'Satellites;Reviews;Generative AI;Low earth orbit satellites;Orbits;Information and communication technology;Security;Vehicle dynamics;Optimization;Next generation networking;aerial access network;generative AI;recent studies;future outlook', 'abstract': 'Aerial access networks (AANs), comprising un-manned aerial vehicles (UAV), high-altitude platforms (HAPs), and low-earth orbit (LEO) satellites, are rapidly emerging as a critical component of next-generation communication systems. Their dynamic nature, heterogeneous composition, and vast coverage area pose significant challenges for network management and optimization. Generative artificial intelligence (GenAI), renowned for its ability to generate new content, offers a promising solution to these challenges. In this paper, we explore the applications of GenAI in AANs in recent studies. We also identify key challenges and outline promising research directions.', 'pages': '529-533', 'number': '', 'volume': '', 'year': '2024', 'title': 'Generative AI-Powered Aerial Access Networks: Recent Studies and Future Outlook', 'booktitle': '2024 15th International Conference on Information and Communication Technology Convergence (ICTC)', 'author': 'Nguyen, Tri-Hai and Nguyen, Truong Khang and Quoc Bao, Vo Nguyen and Park, Heejae and Park, Laihyuk', 'ENTRYTYPE': 'inproceedings', 'ID': '10827504'}"
9426423,Integrated Generative Model for Industrial Anomaly Detection via Bidirectional LSTM and Attention Mechanism,"Kong, Fanhui and Li, Jianqiang and Jiang, Bin and Wang, Huihui and Song, Houbing",Kong,10.1109/TII.2021.3078192,2023,IEEE Transactions on Industrial Informatics,"For emerging industrial Internet of Things (IIoT), intelligent anomaly detection is a key step to build smart industry. Especially, explosive time-series data pose enormous challenges to the information mining and processing for modern industry. How to identify and detect the multidimensional industrial time-series anomaly is an important issue. However, most of the existing studies fail to handle with large amounts of unlabeled data, thus generating the undesirable results. In this article, we propose a novel integrated deep generative model, which is built by generative adversarial networks based on bidirectional long short-term memory and attention mechanism (AMBi-GAN). The structure for the generator and the discriminator is the bidirectional long short-term memory with attention mechanism, which can capture time-series dependence. Reconstruction loss and generation loss test the input of sample training space and random latent space. Experimental results show that the detection performance of our proposed AMBi-GAN has the potential to improve the detection accuracy of industrial multidimensional time-series anomaly toward IIoT in the era of artificial intelligence.",Time series analysis;Anomaly detection;Hidden Markov models;Generative adversarial networks;Generators;Training;Data models;Anomaly detection;attention mechanism;bidirectional long short-term memory (LSTM);industrial time series;integrated generative model,"{'month': 'Jan', 'issn': '1941-0050', 'doi': '10.1109/TII.2021.3078192', 'keywords': 'Time series analysis;Anomaly detection;Hidden Markov models;Generative adversarial networks;Generators;Training;Data models;Anomaly detection;attention mechanism;bidirectional long short-term memory (LSTM);industrial time series;integrated generative model', 'abstract': 'For emerging industrial Internet of Things (IIoT), intelligent anomaly detection is a key step to build smart industry. Especially, explosive time-series data pose enormous challenges to the information mining and processing for modern industry. How to identify and detect the multidimensional industrial time-series anomaly is an important issue. However, most of the existing studies fail to handle with large amounts of unlabeled data, thus generating the undesirable results. In this article, we propose a novel integrated deep generative model, which is built by generative adversarial networks based on bidirectional long short-term memory and attention mechanism (AMBi-GAN). The structure for the generator and the discriminator is the bidirectional long short-term memory with attention mechanism, which can capture time-series dependence. Reconstruction loss and generation loss test the input of sample training space and random latent space. Experimental results show that the detection performance of our proposed AMBi-GAN has the potential to improve the detection accuracy of industrial multidimensional time-series anomaly toward IIoT in the era of artificial intelligence.', 'pages': '541-550', 'number': '1', 'volume': '19', 'year': '2023', 'title': 'Integrated Generative Model for Industrial Anomaly Detection via Bidirectional LSTM and Attention Mechanism', 'journal': 'IEEE Transactions on Industrial Informatics', 'author': 'Kong, Fanhui and Li, Jianqiang and Jiang, Bin and Wang, Huihui and Song, Houbing', 'ENTRYTYPE': 'article', 'ID': '9426423'}"
10853439,Using generative artificial intelligence in text generation,"Huang, David",Huang,10.1049/icp.2024.4493,2024,2nd International Conference on Mechatronic Automation and Electrical Engineering (ICMAEE 2024),"One of the most impactful uses of Artificial Intelligence lies within the domain of text generation. Leveraging discoveries made from Natural Language Processing, Deep Learning, and Large Language Models, text generation has rapidly integrated into modern life. It provides tools that allow anyone on the web to summarize materials, check grammar, and more. To navigate the progress generative text has undergone in the last couple of decades, this work examines a comprehensive review of research that addresses the fundamental components in building an effective model, ranging from the initial developments in algorithmic modules to the advent of neural networks. Then there is a further focuses on the challenges associated with such developments primarily in how the user interacts with the text, the responsiveness of the text, and the grammatical discrepancies between human \& machine content. Accordingly, different tools and models are used to varying degrees of success in tackling these issues.",,"{'month': 'Nov', 'issn': '', 'doi': '10.1049/icp.2024.4493', 'keywords': '', 'abstract': 'One of the most impactful uses of Artificial Intelligence lies within the domain of text generation. Leveraging discoveries made from Natural Language Processing, Deep Learning, and Large Language Models, text generation has rapidly integrated into modern life. It provides tools that allow anyone on the web to summarize materials, check grammar, and more. To navigate the progress generative text has undergone in the last couple of decades, this work examines a comprehensive review of research that addresses the fundamental components in building an effective model, ranging from the initial developments in algorithmic modules to the advent of neural networks. Then there is a further focuses on the challenges associated with such developments primarily in how the user interacts with the text, the responsiveness of the text, and the grammatical discrepancies between human \\& machine content. Accordingly, different tools and models are used to varying degrees of success in tackling these issues.', 'pages': '264-267', 'number': '', 'volume': '2024', 'year': '2024', 'title': 'Using generative artificial intelligence in text generation', 'booktitle': '2nd International Conference on Mechatronic Automation and Electrical Engineering (ICMAEE 2024)', 'author': 'Huang, David', 'ENTRYTYPE': 'inproceedings', 'ID': '10853439'}"
9515223,Generative Adversarial Networks Using Neural Architecture Search for Semantic Image Segmentation,"Ganepola, Vayangi Vishmi Vishara and Wirasingha, Torin",Ganepola,10.1109/BDAI52447.2021.9515223,2021,2021 IEEE 4th International Conference on Big Data and Artificial Intelligence (BDAI),"Semantic image segmentation is a crucial task in various fields that use computer-vision based applications. Generative Adversarial Networks (GANs) are attracting widespread interest in the data science community for their prowess in image feature recognition due to their adversarial nature of training. Neural Architecture Search (NAS) is known as the process of obtaining a neural architectural schema that performs the best for a particular task. NAS has been applied in GANs, and it achieved striking success compared to human-designed architectures in conditional and unconditional image generation and GAN-compression. Our research was inspired by the success of NAS applied in GANs. This paper proposes a novel approach for NAS in GANs for semantic image segmentation. After extensive research on related works, the architecture of the Pix2Pix GAN variant was selected for the proposed approach. The architecture of the Pix2Pix GAN consists of a U-Net as the Generator and a PatchGAN classifier as the Discriminator. The NAS component is searched for U-Net architectures using PASCAL VOC 2012 dataset. The NAS component is adapted from using the NAS-Unet research proposed by Weng et al. in 2019. The NAS searched architecture was used as the Generator of the proposed GAN by transferring the searched architecture from the PASCAL VOC 2012 dataset to the Cityscapes dataset. To determine the success of the proposed approach, quantitative analysis was performed with Mean Pixel Accuracy (MPA) and mean Intersection over Union (mIoU) metrics. Several experiments were done on the Cityscape validation set and achieved 81.73 MPA and 71.91 mIoU. The proposed approach outperformed several NAS in semantic segmentation approaches and GANs in semantic segmentation approaches. This study is a preliminary attempt to apply NAS for semantic segmentation using GANs. Further, this research has raised many possible areas in need of further investigation.",Training;Measurement;Image segmentation;Statistical analysis;Image synthesis;Semantics;Computer architecture;semantic segmentation;generative adversarial networks;neural architecture search,"{'month': 'July', 'issn': '', 'doi': '10.1109/BDAI52447.2021.9515223', 'keywords': 'Training;Measurement;Image segmentation;Statistical analysis;Image synthesis;Semantics;Computer architecture;semantic segmentation;generative adversarial networks;neural architecture search', 'abstract': 'Semantic image segmentation is a crucial task in various fields that use computer-vision based applications. Generative Adversarial Networks (GANs) are attracting widespread interest in the data science community for their prowess in image feature recognition due to their adversarial nature of training. Neural Architecture Search (NAS) is known as the process of obtaining a neural architectural schema that performs the best for a particular task. NAS has been applied in GANs, and it achieved striking success compared to human-designed architectures in conditional and unconditional image generation and GAN-compression. Our research was inspired by the success of NAS applied in GANs. This paper proposes a novel approach for NAS in GANs for semantic image segmentation. After extensive research on related works, the architecture of the Pix2Pix GAN variant was selected for the proposed approach. The architecture of the Pix2Pix GAN consists of a U-Net as the Generator and a PatchGAN classifier as the Discriminator. The NAS component is searched for U-Net architectures using PASCAL VOC 2012 dataset. The NAS component is adapted from using the NAS-Unet research proposed by Weng et al. in 2019. The NAS searched architecture was used as the Generator of the proposed GAN by transferring the searched architecture from the PASCAL VOC 2012 dataset to the Cityscapes dataset. To determine the success of the proposed approach, quantitative analysis was performed with Mean Pixel Accuracy (MPA) and mean Intersection over Union (mIoU) metrics. Several experiments were done on the Cityscape validation set and achieved 81.73 MPA and 71.91 mIoU. The proposed approach outperformed several NAS in semantic segmentation approaches and GANs in semantic segmentation approaches. This study is a preliminary attempt to apply NAS for semantic segmentation using GANs. Further, this research has raised many possible areas in need of further investigation.', 'pages': '236-241', 'number': '', 'volume': '', 'year': '2021', 'title': 'Generative Adversarial Networks Using Neural Architecture Search for Semantic Image Segmentation', 'booktitle': '2021 IEEE 4th International Conference on Big Data and Artificial Intelligence (BDAI)', 'author': 'Ganepola, Vayangi Vishmi Vishara and Wirasingha, Torin', 'ENTRYTYPE': 'inproceedings', 'ID': '9515223'}"
10570229,GainNet: Coordinates the Odd Couple of Generative AI and 6G Networks,"Chen, Ning and Yang, Jie and Cheng, Zhipeng and Fan, Xuwei and Liu, Zhang and Huang, Bangzhen and Zhao, Yifeng and Huang, Lianfen and Du, Xiaojiang and Guizani, Mohsen",Chen,10.1109/MNET.2024.3418671,2024,IEEE Network,"The rapid expansion of AI-generated content (AIGC) reflects the iteration from assistive AI towards generative AI (GAI). Meanwhile, the 6G networks will also evolve from the Internet-of-Everything to the Internet-of-Intelligence. However, they seem to be an odd couple, due to the contradiction of data and resources. To achieve a better-coordinated interplay between GAI and 6G, the GAI-native Networks (GainNet), a GAI-oriented collaborative cloud-edge-end intelligence framework, is proposed in this article. By deeply integrating GAI with 6G network design, GainNet realizes the positive closed-loop knowledge flow and sustainable-evolution GAI model optimization. On this basis, the GAI-oriented generic Resource Orchestration Mechanism with Integrated Sensing, Communication, and Computing (GaiRomISCC) is proposed to guarantee the efficient operation of GainNet. Two simple case studies demonstrate the effectiveness and robustness of the proposed schemes. Finally, we envision the key challenges and future directions concerning the interplay between GAI models and 6G networks.",6G mobile communication;Computational modeling;Data models;Artificial intelligence;Knowledge engineering;Sensors;Optimization;Cloud computing;Resource management;6G;generative AI;collaborative cloud-edge-end intelligence;resource orchestration;integrated sensing;communication;computing,"{'month': 'Sep.', 'issn': '1558-156X', 'doi': '10.1109/MNET.2024.3418671', 'keywords': '6G mobile communication;Computational modeling;Data models;Artificial intelligence;Knowledge engineering;Sensors;Optimization;Cloud computing;Resource management;6G;generative AI;collaborative cloud-edge-end intelligence;resource orchestration;integrated sensing;communication;computing', 'abstract': 'The rapid expansion of AI-generated content (AIGC) reflects the iteration from assistive AI towards generative AI (GAI). Meanwhile, the 6G networks will also evolve from the Internet-of-Everything to the Internet-of-Intelligence. However, they seem to be an odd couple, due to the contradiction of data and resources. To achieve a better-coordinated interplay between GAI and 6G, the GAI-native Networks (GainNet), a GAI-oriented collaborative cloud-edge-end intelligence framework, is proposed in this article. By deeply integrating GAI with 6G network design, GainNet realizes the positive closed-loop knowledge flow and sustainable-evolution GAI model optimization. On this basis, the GAI-oriented generic Resource Orchestration Mechanism with Integrated Sensing, Communication, and Computing (GaiRomISCC) is proposed to guarantee the efficient operation of GainNet. Two simple case studies demonstrate the effectiveness and robustness of the proposed schemes. Finally, we envision the key challenges and future directions concerning the interplay between GAI models and 6G networks.', 'pages': '56-65', 'number': '5', 'volume': '38', 'year': '2024', 'title': 'GainNet: Coordinates the Odd Couple of Generative AI and 6G Networks', 'journal': 'IEEE Network', 'author': 'Chen, Ning and Yang, Jie and Cheng, Zhipeng and Fan, Xuwei and Liu, Zhang and Huang, Bangzhen and Zhao, Yifeng and Huang, Lianfen and Du, Xiaojiang and Guizani, Mohsen', 'ENTRYTYPE': 'article', 'ID': '10570229'}"
9045713,Hierarchical Attention Model for Acquiring Relationships Among Sentences,"Teranishi, Hiroki and Okada, Makoto and Mori, Naoki",Teranishi,10.1109/iSAI-NLP48611.2019.9045713,2019,2019 14th International Joint Symposium on Artificial Intelligence and Natural Language Processing (iSAI-NLP),"In this paper, we propose a hierarchical attention model for summarization. Normally, sentences have relations among another sentence and it is important to consider these relations in summarizing. Our proposed model can make each sentence vectors from document composed of multi sentences and get relations among sentences from these vectors by the incorporated operation. As an operation of taking relations, we use self-attention and gated convolutional neural network. It has been reported that these operations can get dependencies among words, and self-attention is particularly powerful. Therefore we adopted these operations expecting the same work in sentences. We conducted an experiment of title generation by using Japanese news articles. We evaluated the performance of our proposed model by Rouge and visualized the relations among sentences.",Heating systems;Attention mechanisms;Data visualization;Logic gates;Vectors;Natural language processing;Convolutional neural networks;Artificial intelligence;nueral network;attention mechanism;generative summarization,"{'month': 'Oct', 'issn': '', 'doi': '10.1109/iSAI-NLP48611.2019.9045713', 'keywords': 'Heating systems;Attention mechanisms;Data visualization;Logic gates;Vectors;Natural language processing;Convolutional neural networks;Artificial intelligence;nueral network;attention mechanism;generative summarization', 'abstract': 'In this paper, we propose a hierarchical attention model for summarization. Normally, sentences have relations among another sentence and it is important to consider these relations in summarizing. Our proposed model can make each sentence vectors from document composed of multi sentences and get relations among sentences from these vectors by the incorporated operation. As an operation of taking relations, we use self-attention and gated convolutional neural network. It has been reported that these operations can get dependencies among words, and self-attention is particularly powerful. Therefore we adopted these operations expecting the same work in sentences. We conducted an experiment of title generation by using Japanese news articles. We evaluated the performance of our proposed model by Rouge and visualized the relations among sentences.', 'pages': '1-6', 'number': '', 'volume': '', 'year': '2019', 'title': 'Hierarchical Attention Model for Acquiring Relationships Among Sentences', 'booktitle': '2019 14th International Joint Symposium on Artificial Intelligence and Natural Language Processing (iSAI-NLP)', 'author': 'Teranishi, Hiroki and Okada, Makoto and Mori, Naoki', 'ENTRYTYPE': 'inproceedings', 'ID': '9045713'}"
9724833,Research Progress in the Field of Image Completion,"Li, Quanfeng and Hu, Lingxi and Shang, Qiqi and Wang, Yawen and Jiang, Linhua and Long, Wei",Li,10.1109/AIAM54119.2021.00086,2021,2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture (AIAM),"Image completion technology is a challenging research direction in the field of image restoration. The traditional image completion technology mainly fills the missing areas with missing values based on the information of the unmissed areas of the image. Traditional image completion can well complement images with a small missing area and relatively simple texture structure, but it does not work well for images with large missing areas or complex texture structures. With the continuous development of deep learning, the performance of image restoration has been significantly improved. The image completion method based on deep learning can learn the high-level features of the image, so that the result of the completion is more realistic. This article reviews the image completion technology, introduces the basic principles of typical methods and compares their advantages and disadvantages. Finally, we analyze the future research directions in this field and put forward prospects.",Deep learning;Image restoration;Artificial intelligence;image completion;deep learning;generative adversarial network,"{'month': 'Oct', 'issn': '', 'doi': '10.1109/AIAM54119.2021.00086', 'keywords': 'Deep learning;Image restoration;Artificial intelligence;image completion;deep learning;generative adversarial network', 'abstract': 'Image completion technology is a challenging research direction in the field of image restoration. The traditional image completion technology mainly fills the missing areas with missing values based on the information of the unmissed areas of the image. Traditional image completion can well complement images with a small missing area and relatively simple texture structure, but it does not work well for images with large missing areas or complex texture structures. With the continuous development of deep learning, the performance of image restoration has been significantly improved. The image completion method based on deep learning can learn the high-level features of the image, so that the result of the completion is more realistic. This article reviews the image completion technology, introduces the basic principles of typical methods and compares their advantages and disadvantages. Finally, we analyze the future research directions in this field and put forward prospects.', 'pages': '398-402', 'number': '', 'volume': '', 'year': '2021', 'title': 'Research Progress in the Field of Image Completion', 'booktitle': '2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture (AIAM)', 'author': 'Li, Quanfeng and Hu, Lingxi and Shang, Qiqi and Wang, Yawen and Jiang, Linhua and Long, Wei', 'ENTRYTYPE': 'inproceedings', 'ID': '9724833'}"
9643214,User-Guided Image Inpatinting with Transformer,"Qiu, Jingjun and Gao, Yan",Qiu,10.1109/ICTAI52525.2021.00174,2021,2021 IEEE 33rd International Conference on Tools with Artificial Intelligence (ICTAI),"Deep learning has brought unprecedented progress to image inpainting. Most of the image inpainting methods with guidance mainly use the line as guiding information, which is still a huge challenge for ordinary users. Thus, we propose a novel method for user-guided image inpainting, this method can achieve interactive object extraction in the guided image and add the target object to the original image reasonably. Our model splits this task into three parts: interactive mask extractor, guided feature extractor and inpainting network. The interactive mask extractor can interactively extract the target object from the guided image. Then, we adopt the guided feature extractor to extract the features of the target object, Finally, the inpainting network generates the fine-grained images by fusing the features from the original and guided image. The experimental results prove that our model has a higher inpainting quality than the existing state-of-the-art approaches.",Deep learning;Fuses;Conferences;Learning (artificial intelligence);Feature extraction;Transformers;Data mining;Inpainting;Generative model;Deep learning,"{'month': 'Nov', 'issn': '2375-0197', 'doi': '10.1109/ICTAI52525.2021.00174', 'keywords': 'Deep learning;Fuses;Conferences;Learning (artificial intelligence);Feature extraction;Transformers;Data mining;Inpainting;Generative model;Deep learning', 'abstract': 'Deep learning has brought unprecedented progress to image inpainting. Most of the image inpainting methods with guidance mainly use the line as guiding information, which is still a huge challenge for ordinary users. Thus, we propose a novel method for user-guided image inpainting, this method can achieve interactive object extraction in the guided image and add the target object to the original image reasonably. Our model splits this task into three parts: interactive mask extractor, guided feature extractor and inpainting network. The interactive mask extractor can interactively extract the target object from the guided image. Then, we adopt the guided feature extractor to extract the features of the target object, Finally, the inpainting network generates the fine-grained images by fusing the features from the original and guided image. The experimental results prove that our model has a higher inpainting quality than the existing state-of-the-art approaches.', 'pages': '1099-1104', 'number': '', 'volume': '', 'year': '2021', 'title': 'User-Guided Image Inpatinting with Transformer', 'booktitle': '2021 IEEE 33rd International Conference on Tools with Artificial Intelligence (ICTAI)', 'author': 'Qiu, Jingjun and Gao, Yan', 'ENTRYTYPE': 'inproceedings', 'ID': '9643214'}"
10277883,The Super-Resolution Reconstruction Based on Domain Adaptation Model,"Sun, Jifeng and Zhao, Shuai and Lin, Yibin",Sun,10.1109/AICIT59054.2023.10277883,2023,2023 2nd International Conference on Artificial Intelligence and Computer Information Technology (AICIT),A super-resolution reconstruction method based on domain adaptation model is proposed in this paper. The experimental result on the super-resolution reconstruction shows the effectiveness of the proposed scheme.,Degradation;Adaptation models;Computational modeling;Superresolution;Reconstruction algorithms;Information technology;Artificial intelligence;super-resolution reconstruction;domain adaptation model;degraded generative network;transformer,"{'month': 'Sep.', 'issn': '', 'doi': '10.1109/AICIT59054.2023.10277883', 'keywords': 'Degradation;Adaptation models;Computational modeling;Superresolution;Reconstruction algorithms;Information technology;Artificial intelligence;super-resolution reconstruction;domain adaptation model;degraded generative network;transformer', 'abstract': 'A super-resolution reconstruction method based on domain adaptation model is proposed in this paper. The experimental result on the super-resolution reconstruction shows the effectiveness of the proposed scheme.', 'pages': '1-5', 'number': '', 'volume': '', 'year': '2023', 'title': 'The Super-Resolution Reconstruction Based on Domain Adaptation Model', 'booktitle': '2023 2nd International Conference on Artificial Intelligence and Computer Information Technology (AICIT)', 'author': 'Sun, Jifeng and Zhao, Shuai and Lin, Yibin', 'ENTRYTYPE': 'inproceedings', 'ID': '10277883'}"
11050667,Physically Interpretable Representation and Controlled Generation for Turbulence Data,"Fan, Tiffany and Cutforth, Murray and D'Elia, Marta and Cortiella, Alexandre and Doostan, Alireza and Darve, Eric",Fan,10.1109/CAI64502.2025.00188,2025,2025 IEEE Conference on Artificial Intelligence (CAI),"Computational Fluid Dynamics (CFD) is central to fluid mechanics, offering precise simulations of fluid behavior through partial differential equations (PDEs). Traditional CFD methods, such as those based on finite difference and finite volume schemes, are resource-consuming, especially for high-fidelity simulations of complex flows. Understanding such datasets presents unique challenges due to their high dimensionality, inherent stochasticity, and limited data availability.",Dimensionality reduction;Solid modeling;Translation;Computational fluid dynamics;Computational modeling;Soft sensors;Partial differential equations;Learning (artificial intelligence);Mathematical models;Finite difference methods;dimension reduction;generative modeling;interpretability;unsupervised learning,"{'month': 'May', 'issn': '', 'doi': '10.1109/CAI64502.2025.00188', 'keywords': 'Dimensionality reduction;Solid modeling;Translation;Computational fluid dynamics;Computational modeling;Soft sensors;Partial differential equations;Learning (artificial intelligence);Mathematical models;Finite difference methods;dimension reduction;generative modeling;interpretability;unsupervised learning', 'abstract': 'Computational Fluid Dynamics (CFD) is central to fluid mechanics, offering precise simulations of fluid behavior through partial differential equations (PDEs). Traditional CFD methods, such as those based on finite difference and finite volume schemes, are resource-consuming, especially for high-fidelity simulations of complex flows. Understanding such datasets presents unique challenges due to their high dimensionality, inherent stochasticity, and limited data availability.', 'pages': '1084-1085', 'number': '', 'volume': '', 'year': '2025', 'title': 'Physically Interpretable Representation and Controlled Generation for Turbulence Data', 'booktitle': '2025 IEEE Conference on Artificial Intelligence (CAI)', 'author': ""Fan, Tiffany and Cutforth, Murray and D'Elia, Marta and Cortiella, Alexandre and Doostan, Alireza and Darve, Eric"", 'ENTRYTYPE': 'inproceedings', 'ID': '11050667'}"
9216082,User-Guided Chinese Painting Completion–A Generative Adversarial Network Approach,"Xue, Jieting and Guo, Jingtao and Liu, Yi",Xue,10.1109/ACCESS.2020.3029084,2020,IEEE Access,"Image completion models based on deep neural networks have been a research hot spot in computer vision. However, most of the previous methods focus on natural images, such as faces and landscapes. In this paper, we propose a novel image completion model for a special set of artificial ancient Chinese paintings to address this limitation. Specifically, we integrate three complements: the Wasserstein Generative Adversarial Networks (WGAN), Perceptual loss, and Mean Squared Error (MSE) to train the model robustly. We propose a unique generator which can not only pay more attention to complete the details of ancient Chinese paintings but also can provide the synthesized lines to help artists to analyze paintings conveniently. Additionally, we also allow a user to supply a structure hint to guide our model to complete Chinese paintings according to his/her preference. Extensive experiments firmly demonstrate the effectiveness of our approach to complete ancient Chinese paintings and remove abnormal color blocks from them.",Painting;Generative adversarial networks;Generators;Gallium nitride;Decoding;Computational modeling;Neural networks;Deep learning;Generative adversarial network;Image completion,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2020.3029084', 'keywords': 'Painting;Generative adversarial networks;Generators;Gallium nitride;Decoding;Computational modeling;Neural networks;Deep learning;Generative adversarial network;Image completion', 'abstract': 'Image completion models based on deep neural networks have been a research hot spot in computer vision. However, most of the previous methods focus on natural images, such as faces and landscapes. In this paper, we propose a novel image completion model for a special set of artificial ancient Chinese paintings to address this limitation. Specifically, we integrate three complements: the Wasserstein Generative Adversarial Networks (WGAN), Perceptual loss, and Mean Squared Error (MSE) to train the model robustly. We propose a unique generator which can not only pay more attention to complete the details of ancient Chinese paintings but also can provide the synthesized lines to help artists to analyze paintings conveniently. Additionally, we also allow a user to supply a structure hint to guide our model to complete Chinese paintings according to his/her preference. Extensive experiments firmly demonstrate the effectiveness of our approach to complete ancient Chinese paintings and remove abnormal color blocks from them.', 'pages': '187431-187440', 'number': '', 'volume': '8', 'year': '2020', 'title': 'User-Guided Chinese Painting Completion–A Generative Adversarial Network Approach', 'journal': 'IEEE Access', 'author': 'Xue, Jieting and Guo, Jingtao and Liu, Yi', 'ENTRYTYPE': 'article', 'ID': '9216082'}"
10803329,Revolutionizing Digital Health with Generative AI: User Experiences and Healthcare Performance,"Deng, Kainan and Liu, Xiang and Xu, Dongming and Sengupta, Avijit",Deng,10.1109/MedAI62885.2024.00055,2024,2024 IEEE International Conference on Medical Artificial Intelligence (MedAI),"Generative artificial intelligence (GenAl) is revolutionizing digital health by enhancing diagnostic accuracy, predicting health risks, and reducing wait times and costs. This study adopts an inductive approach to analyze user experiences with GenAl through social media comments. We construct a conceptual framework and find that users, including physicians and patients/general users, have varying needs that need to be met using different GenAl types. Our conceptual framework also reveals the interactions among the characteristics of GenAl, users, and users' tasks. This research advances the field of Information Systems by bridging the gap between technology and user behavior in digital health. We also offer insights for future research to explore the influence of GenAl on users' behavior efficiently and how to effectively integrate GenAl tools into their clinical workflows, enhancing patient care and operational efficiency.",Costs;Accuracy;Generative AI;Social networking (online);Electronic healthcare;Medical diagnostic imaging;Information systems;generative AI;healthcare performance;user experiences;generative AI adoption;digital health,"{'month': 'Nov', 'issn': '', 'doi': '10.1109/MedAI62885.2024.00055', 'keywords': 'Costs;Accuracy;Generative AI;Social networking (online);Electronic healthcare;Medical diagnostic imaging;Information systems;generative AI;healthcare performance;user experiences;generative AI adoption;digital health', 'abstract': ""Generative artificial intelligence (GenAl) is revolutionizing digital health by enhancing diagnostic accuracy, predicting health risks, and reducing wait times and costs. This study adopts an inductive approach to analyze user experiences with GenAl through social media comments. We construct a conceptual framework and find that users, including physicians and patients/general users, have varying needs that need to be met using different GenAl types. Our conceptual framework also reveals the interactions among the characteristics of GenAl, users, and users' tasks. This research advances the field of Information Systems by bridging the gap between technology and user behavior in digital health. We also offer insights for future research to explore the influence of GenAl on users' behavior efficiently and how to effectively integrate GenAl tools into their clinical workflows, enhancing patient care and operational efficiency."", 'pages': '366-371', 'number': '', 'volume': '', 'year': '2024', 'title': 'Revolutionizing Digital Health with Generative AI: User Experiences and Healthcare Performance', 'booktitle': '2024 IEEE International Conference on Medical Artificial Intelligence (MedAI)', 'author': 'Deng, Kainan and Liu, Xiang and Xu, Dongming and Sengupta, Avijit', 'ENTRYTYPE': 'inproceedings', 'ID': '10803329'}"
9619455,A method for generating images of abnormal combustion state in MSWI process based on DCGAN,"Guo, Haitao and Tang, Jian and Zhang, Hao and Wang, Dandan",Guo,10.1109/IAI53119.2021.9619455,2021,2021 3rd International Conference on Industrial Artificial Intelligence (IAI),"This article is to provide qualified images of abnormal combustion state for the research of machine vision in municipal solid waste incineration (MSWI) process. Owing to the scarcity of the images of abnormal combustion state and the high cost of labeling, it is difficult to obtain sufficient images of abnormal combustion state. Aim at the problem, this paper proposes a method for generating images of abnormal combustion state based on a deep convolutional generative adversarial network (DCGAN). First, the real image data of abnormal combustion state is preprocessed. Second, the abnormal combustion state image generation generates false combustion images. Third, the real images and the generated images are fed into the discrimination network. The loss values are used to train the discrimination and generation. Finally, whether to update the parameters of the generation and discrimination network is determined by the error and epoch. The qualified generated abnormal combustion state images are obtained after the epoch setting met. The evaluation result of the generated image quality based on the Fréchet Inception Distance (FID) shows that DCGAN can realize the generation of abnormal combustion state images.",Waste management;Image quality;Waste materials;Image synthesis;Incineration;Machine vision;Generative adversarial networks;Municipal solid waste incineration (MSWI);Abnormal combustion state;Image generation;deep convolutional generative adversarial network (DCGAN),"{'month': 'Nov', 'issn': '', 'doi': '10.1109/IAI53119.2021.9619455', 'keywords': 'Waste management;Image quality;Waste materials;Image synthesis;Incineration;Machine vision;Generative adversarial networks;Municipal solid waste incineration (MSWI);Abnormal combustion state;Image generation;deep convolutional generative adversarial network (DCGAN)', 'abstract': 'This article is to provide qualified images of abnormal combustion state for the research of machine vision in municipal solid waste incineration (MSWI) process. Owing to the scarcity of the images of abnormal combustion state and the high cost of labeling, it is difficult to obtain sufficient images of abnormal combustion state. Aim at the problem, this paper proposes a method for generating images of abnormal combustion state based on a deep convolutional generative adversarial network (DCGAN). First, the real image data of abnormal combustion state is preprocessed. Second, the abnormal combustion state image generation generates false combustion images. Third, the real images and the generated images are fed into the discrimination network. The loss values are used to train the discrimination and generation. Finally, whether to update the parameters of the generation and discrimination network is determined by the error and epoch. The qualified generated abnormal combustion state images are obtained after the epoch setting met. The evaluation result of the generated image quality based on the Fréchet Inception Distance (FID) shows that DCGAN can realize the generation of abnormal combustion state images.', 'pages': '1-6', 'number': '', 'volume': '', 'year': '2021', 'title': 'A method for generating images of abnormal combustion state in MSWI process based on DCGAN', 'booktitle': '2021 3rd International Conference on Industrial Artificial Intelligence (IAI)', 'author': 'Guo, Haitao and Tang, Jian and Zhang, Hao and Wang, Dandan', 'ENTRYTYPE': 'inproceedings', 'ID': '9619455'}"
11167619,Impact of AI-Generated Tourism Ads on Consumer Intent in Indonesian Market Using PLS-SEM,"Christian, Michael and Pardede, Ratlan and Dewantara, Yudhiet Fajar and Nan, Guan and Geng, Bi and Irlandra, Frendy",Christian,10.1109/ICCIT65724.2025.11167619,2025,2025 4th International Conference on Creative Communication and Innovative Technology (ICCIT),"The integration of artificial intelligence (AI) in digital marketing has revolutionized consumer engagement, particularly through AI-generated advertisements. Despite growing applications, the effectiveness of these ads in influencing consumer behavior remains an area of ongoing exploration. This study investigates how AI-generated tourism advertisements affect the intention of Indonesian consumers to visit China. A quantitative experiment with 118 participants was conducted, using a structured questionnaire for data collection. The analysis applied Partial Least Squares Structural Equation Modeling (PLS-SEM) using SmartPLS software to examine the relationships between AI content characteristics and consumer intent. Results indicate that novelty and perceived eeriness of AI-generated advertisements significantly influence consumer travel intentions. Additionally, the synthesis of artificial elements in the ads impacts perceived eeriness, which indirectly affects travel intent. These findings suggest that while AI-generated advertisements can enhance novelty, addressing perceptions of eeriness is crucial for optimizing consumer engagement. This research contributes to the understanding of AI-driven digital marketing and provides valuable insights for tourism marketers aiming to improve the effectiveness of AI-based campaigns, while also supporting the Uncanny Valley theory in marketing applications.",Ethics;Data privacy;Generative AI;Data collection;Motion pictures;Mathematical models;Software;Advertising;Creativity;Guidelines;Artificial Intelligence;AI-Generated Tourism Advertisements;Perceived Eeriness;Intention to Visit,"{'month': 'Aug', 'issn': '', 'doi': '10.1109/ICCIT65724.2025.11167619', 'keywords': 'Ethics;Data privacy;Generative AI;Data collection;Motion pictures;Mathematical models;Software;Advertising;Creativity;Guidelines;Artificial Intelligence;AI-Generated Tourism Advertisements;Perceived Eeriness;Intention to Visit', 'abstract': 'The integration of artificial intelligence (AI) in digital marketing has revolutionized consumer engagement, particularly through AI-generated advertisements. Despite growing applications, the effectiveness of these ads in influencing consumer behavior remains an area of ongoing exploration. This study investigates how AI-generated tourism advertisements affect the intention of Indonesian consumers to visit China. A quantitative experiment with 118 participants was conducted, using a structured questionnaire for data collection. The analysis applied Partial Least Squares Structural Equation Modeling (PLS-SEM) using SmartPLS software to examine the relationships between AI content characteristics and consumer intent. Results indicate that novelty and perceived eeriness of AI-generated advertisements significantly influence consumer travel intentions. Additionally, the synthesis of artificial elements in the ads impacts perceived eeriness, which indirectly affects travel intent. These findings suggest that while AI-generated advertisements can enhance novelty, addressing perceptions of eeriness is crucial for optimizing consumer engagement. This research contributes to the understanding of AI-driven digital marketing and provides valuable insights for tourism marketers aiming to improve the effectiveness of AI-based campaigns, while also supporting the Uncanny Valley theory in marketing applications.', 'pages': '1-7', 'number': '', 'volume': '', 'year': '2025', 'title': 'Impact of AI-Generated Tourism Ads on Consumer Intent in Indonesian Market Using PLS-SEM', 'booktitle': '2025 4th International Conference on Creative Communication and Innovative Technology (ICCIT)', 'author': 'Christian, Michael and Pardede, Ratlan and Dewantara, Yudhiet Fajar and Nan, Guan and Geng, Bi and Irlandra, Frendy', 'ENTRYTYPE': 'inproceedings', 'ID': '11167619'}"
10570407,Locally Conditioned GANs: Self-Supervised Local Patch Representation Learning for Conditional Generation,"Kim, Dongseob and Shim, Hyunjung",Kim,10.1109/ACCESS.2024.3418884,2024,IEEE Access,"Existing conditional generation models using generative adversarial networks (GANs) suffer from two common limitations: 1) they heavily rely on supervision, or 2) their performance is favorable to the scenario of creating only small changes. This study aims to address both issues by introducing new locally conditioned generative adversarial networks (LCGAN). Inspired by self-supervised representation learning, we devise intuitive learning signals and training tactics to learn the local patch encoding for developing the locally controllable latent space of GANs. Powered by local patch encoding with our novel loss design, the proposed model successfully performs locally conditioned image generation while covering various attributes. Utilizing LCGAN, ordinary users can easily design an image by browsing its patch-level appearance from various patch examples, even including out-of-domain examples. Besides, LCGAN, with latent optimization, offers high-quality results in local editing. Experimental evaluations verify that our model is effective in both conditional generation and local editing in achieving both image quality and fidelity. Our method is the most preferred by 55.78\% of user study participants, and it achieved Fréchet inception distance scores of 16.24 and 15.01 on the FFHQ and AFHQ-cat datasets, respectively. Especially, a comprehensive user study supports that: 1) trade-off between quality and fidelity exists in existing methods and 2) our model is the first to alleviate their trade-off relationships, showing the potential in practical image editing applications.",Training;Image synthesis;Generative adversarial networks;Semantics;Vectors;Representation learning;Image reconstruction;Condition monitoring;Generative adversarial network;conditional generation;image composition,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2024.3418884', 'keywords': 'Training;Image synthesis;Generative adversarial networks;Semantics;Vectors;Representation learning;Image reconstruction;Condition monitoring;Generative adversarial network;conditional generation;image composition', 'abstract': 'Existing conditional generation models using generative adversarial networks (GANs) suffer from two common limitations: 1) they heavily rely on supervision, or 2) their performance is favorable to the scenario of creating only small changes. This study aims to address both issues by introducing new locally conditioned generative adversarial networks (LCGAN). Inspired by self-supervised representation learning, we devise intuitive learning signals and training tactics to learn the local patch encoding for developing the locally controllable latent space of GANs. Powered by local patch encoding with our novel loss design, the proposed model successfully performs locally conditioned image generation while covering various attributes. Utilizing LCGAN, ordinary users can easily design an image by browsing its patch-level appearance from various patch examples, even including out-of-domain examples. Besides, LCGAN, with latent optimization, offers high-quality results in local editing. Experimental evaluations verify that our model is effective in both conditional generation and local editing in achieving both image quality and fidelity. Our method is the most preferred by 55.78\\% of user study participants, and it achieved Fréchet inception distance scores of 16.24 and 15.01 on the FFHQ and AFHQ-cat datasets, respectively. Especially, a comprehensive user study supports that: 1) trade-off between quality and fidelity exists in existing methods and 2) our model is the first to alleviate their trade-off relationships, showing the potential in practical image editing applications.', 'pages': '134115-134132', 'number': '', 'volume': '12', 'year': '2024', 'title': 'Locally Conditioned GANs: Self-Supervised Local Patch Representation Learning for Conditional Generation', 'journal': 'IEEE Access', 'author': 'Kim, Dongseob and Shim, Hyunjung', 'ENTRYTYPE': 'article', 'ID': '10570407'}"
10916662,Generative-Diffusion-Model-Based Deep-Learning Framework for Remaining Useful Life Prediction,"Ha, Sangjun and Sung, Mingyu and Saeed, Faisal and Yun, Sangseok and Kim, Il-Min and Kang, Jae-Mo",Ha,10.1109/JIOT.2025.3549038,2025,IEEE Internet of Things Journal,"In this letter, we propose a novel and high-performing deep learning framework for remaining useful life (RUL) prediction, called RUL-Diff, by leveraging a generative diffusion model. It is composed of two modules that are connected in tandem: 1) a feature extractor corresponding to the encoder part of our customized U-Net and 2) a RUL predictor constructed by a multilayer perceptron. We further devise an effective two-stage training methodology for the proposed RUL-Diff, in which the feature extractor is initially pretrained for high-quality feature learning, and then, is retrained jointly with the RUL predictor for accurate RUL prediction. Extensive experimental results on NASA Commercial Modular Aero-Propulsion System Simulation (C-MAPSS) datasets demonstrate the superiority and effectiveness of the proposed scheme.",Feature extraction;Training;Data mining;Representation learning;Network architecture;Internet of Things;Diffusion models;Convolutional neural networks;Time series analysis;Deep learning;Deep learning (DL);generative diffusion model;Internet-of-Things (IoT);remaining useful life (RUL) prediction,"{'month': 'June', 'issn': '2327-4662', 'doi': '10.1109/JIOT.2025.3549038', 'keywords': 'Feature extraction;Training;Data mining;Representation learning;Network architecture;Internet of Things;Diffusion models;Convolutional neural networks;Time series analysis;Deep learning;Deep learning (DL);generative diffusion model;Internet-of-Things (IoT);remaining useful life (RUL) prediction', 'abstract': 'In this letter, we propose a novel and high-performing deep learning framework for remaining useful life (RUL) prediction, called RUL-Diff, by leveraging a generative diffusion model. It is composed of two modules that are connected in tandem: 1) a feature extractor corresponding to the encoder part of our customized U-Net and 2) a RUL predictor constructed by a multilayer perceptron. We further devise an effective two-stage training methodology for the proposed RUL-Diff, in which the feature extractor is initially pretrained for high-quality feature learning, and then, is retrained jointly with the RUL predictor for accurate RUL prediction. Extensive experimental results on NASA Commercial Modular Aero-Propulsion System Simulation (C-MAPSS) datasets demonstrate the superiority and effectiveness of the proposed scheme.', 'pages': '18431-18434', 'number': '11', 'volume': '12', 'year': '2025', 'title': 'Generative-Diffusion-Model-Based Deep-Learning Framework for Remaining Useful Life Prediction', 'journal': 'IEEE Internet of Things Journal', 'author': 'Ha, Sangjun and Sung, Mingyu and Saeed, Faisal and Yun, Sangseok and Kim, Il-Min and Kang, Jae-Mo', 'ENTRYTYPE': 'article', 'ID': '10916662'}"
10889691,A2GP-SF: Enhancing Few-shot Class Incremental Learning via Attribute Generative Prompting and Adaptive Sharpness Flattening,"Chen, Zhiming and Wang, Desen and Fu, Sisi and Wen, Congcong and Lin, Hui and Chen, Bingzhi",Chen,10.1109/ICASSP49660.2025.10889691,2025,"ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","Few-shot Class Incremental Learning (FSCIL) aims to incrementally learn new classes with limited examples while retaining knowledge of previously learned classes. Recent advancements in prompt tuning for large pre-trained models have shown promise in FSCIL. However, current FSCIL methods still suffer from challenges like insufficient plasticity and limited generalization. To tackle these challenges, we propose a novel prompt tuning-based framework named A2GP-SF, which integrates attribute generative prompting (AGP) and adaptive sharpness flattening (ASF). The proposed AGP paradigm dynamically generates attribute-aware prompts for each instance, facilitating better semantics learning and enhancing plasticity. Additionally, the ASF mechanism aims to mitigate overfitting by applying adaptive perturbations to flatten sharpness, with these perturbations adjusted based on gradient norm changes, thereby enhancing the model’s robustness and generalization. Extensive experiments on multiple benchmark datasets consistently demonstrate the superiority of our proposed A2GP-SF framework.",Adaptation models;Incremental learning;Perturbation methods;Semantics;Signal processing;Robustness;Power capacitors;Speech processing;Tuning;Overfitting;FSCIL;Insufficient Plasticity;Limited Generalization;Generative Prompting;Sharpness Flattening,"{'month': 'April', 'issn': '2379-190X', 'doi': '10.1109/ICASSP49660.2025.10889691', 'keywords': 'Adaptation models;Incremental learning;Perturbation methods;Semantics;Signal processing;Robustness;Power capacitors;Speech processing;Tuning;Overfitting;FSCIL;Insufficient Plasticity;Limited Generalization;Generative Prompting;Sharpness Flattening', 'abstract': 'Few-shot Class Incremental Learning (FSCIL) aims to incrementally learn new classes with limited examples while retaining knowledge of previously learned classes. Recent advancements in prompt tuning for large pre-trained models have shown promise in FSCIL. However, current FSCIL methods still suffer from challenges like insufficient plasticity and limited generalization. To tackle these challenges, we propose a novel prompt tuning-based framework named A2GP-SF, which integrates attribute generative prompting (AGP) and adaptive sharpness flattening (ASF). The proposed AGP paradigm dynamically generates attribute-aware prompts for each instance, facilitating better semantics learning and enhancing plasticity. Additionally, the ASF mechanism aims to mitigate overfitting by applying adaptive perturbations to flatten sharpness, with these perturbations adjusted based on gradient norm changes, thereby enhancing the model’s robustness and generalization. Extensive experiments on multiple benchmark datasets consistently demonstrate the superiority of our proposed A2GP-SF framework.', 'pages': '1-5', 'number': '', 'volume': '', 'year': '2025', 'title': 'A2GP-SF: Enhancing Few-shot Class Incremental Learning via Attribute Generative Prompting and Adaptive Sharpness Flattening', 'booktitle': 'ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)', 'author': 'Chen, Zhiming and Wang, Desen and Fu, Sisi and Wen, Congcong and Lin, Hui and Chen, Bingzhi', 'ENTRYTYPE': 'inproceedings', 'ID': '10889691'}"
10859114,Intelligent Tourism Plan Making System Based on Machine Learning Technology,"Hu, Wenyue",Hu,10.1109/ICRSS65752.2024.00032,2024,"2024 International Conference on Computing, Robotics and System Sciences (ICRSS)","Generative artificial intelligence will conduct in-depth analysis and mining of massive data in the tourism industry to help tourism enterprises and government departments make more accurate and scientific decisions. In order to further improve the user experience of tourist terminals, this study proposes a personalized tourist route generation modeling method based on user interest model. Moreover, after the theoretical research of this method is completed, its application value is analyzed, and the expected design goal is achieved. Therefore, in the future research, this model can be used to solve the route planning problem, and provide users with more personal tourism routes and tourism schemes. Through the experimental results, it can be seen that the route generation result of this model is good, which proves that this model has high application value. In addition, generative artificial intelligence technology imitates human creative thinking to generate a series of data, images, text or audio content that tourists need, thereby effectively improving the user experience.",Technological innovation;Privacy;Generative AI;Service robots;Tourism industry;Government;Machine learning;User experience;Planning;Optimization;machine learning;travel;protocols;intelligence;generate,"{'month': 'Nov', 'issn': '', 'doi': '10.1109/ICRSS65752.2024.00032', 'keywords': 'Technological innovation;Privacy;Generative AI;Service robots;Tourism industry;Government;Machine learning;User experience;Planning;Optimization;machine learning;travel;protocols;intelligence;generate', 'abstract': 'Generative artificial intelligence will conduct in-depth analysis and mining of massive data in the tourism industry to help tourism enterprises and government departments make more accurate and scientific decisions. In order to further improve the user experience of tourist terminals, this study proposes a personalized tourist route generation modeling method based on user interest model. Moreover, after the theoretical research of this method is completed, its application value is analyzed, and the expected design goal is achieved. Therefore, in the future research, this model can be used to solve the route planning problem, and provide users with more personal tourism routes and tourism schemes. Through the experimental results, it can be seen that the route generation result of this model is good, which proves that this model has high application value. In addition, generative artificial intelligence technology imitates human creative thinking to generate a series of data, images, text or audio content that tourists need, thereby effectively improving the user experience.', 'pages': '140-146', 'number': '', 'volume': '', 'year': '2024', 'title': 'Intelligent Tourism Plan Making System Based on Machine Learning Technology', 'booktitle': '2024 International Conference on Computing, Robotics and System Sciences (ICRSS)', 'author': 'Hu, Wenyue', 'ENTRYTYPE': 'inproceedings', 'ID': '10859114'}"
10671462,Real-World Blind Face Restoration with Generative Facial Prior and Degradation Simulation,"Zhu, Weihong and Hu, Changhui and Xu, Lintao",Zhu,10.1109/ICSIP61881.2024.10671462,2024,2024 9th International Conference on Signal and Image Processing (ICSIP),"Real-world blind face restoration is very hard, since real-world blind face images are with unknown complex multiple degradations. Our previous work [1] proposed a joint image-to-image translation (JI2IT) method for real-world blind traffic monitoring driver face (TMDF) image restoration, whereas JI2IT is unsatisfactory to restore the facial details of real-world blind TMDF images. To tackle above issue, this paper proposes a RestormGAN for real-world image (i.e., TMDF image) restoration which consists of a channel axis attention based Restormer (CAR) encoder and a generative facial prior (i.e., GAN prior). Specifically, we propose to insert a novel channel height and width axes attention (CHWA) module into Restormer module to form CAR module, where CHWA contains the height-axis attention module (HAM) and the width-axis attention module (WAM). The HAM learns a weight for the height-axis of each channel while WAM learns a weight for the width-axis of each channel. Then the dual down-sampling (DDS) module is proposed to ensure the CAR encoder can be efficiently connected with GAN prior. DDS uses bilinear sampling and pixelunshuffle in parallel to generate more representative parameters. Finally, we propose to combine multi-handcraft degradations strategy and pretrained degradation model to construct massive image pairs for the training of RestormGAN. Extensive experiments demonstrate that our RestormGAN achieves state-of-the-art performances for real-world face restoration and superior results in traffic monitoring scenarios.",Degradation;Training;Generators;Image restoration;Automobiles;Faces;Monitoring;real-world blind face restoration;generative facial prior;transformer;traffic monitoring driver face image,"{'month': 'July', 'issn': '2642-6471', 'doi': '10.1109/ICSIP61881.2024.10671462', 'keywords': 'Degradation;Training;Generators;Image restoration;Automobiles;Faces;Monitoring;real-world blind face restoration;generative facial prior;transformer;traffic monitoring driver face image', 'abstract': 'Real-world blind face restoration is very hard, since real-world blind face images are with unknown complex multiple degradations. Our previous work [1] proposed a joint image-to-image translation (JI2IT) method for real-world blind traffic monitoring driver face (TMDF) image restoration, whereas JI2IT is unsatisfactory to restore the facial details of real-world blind TMDF images. To tackle above issue, this paper proposes a RestormGAN for real-world image (i.e., TMDF image) restoration which consists of a channel axis attention based Restormer (CAR) encoder and a generative facial prior (i.e., GAN prior). Specifically, we propose to insert a novel channel height and width axes attention (CHWA) module into Restormer module to form CAR module, where CHWA contains the height-axis attention module (HAM) and the width-axis attention module (WAM). The HAM learns a weight for the height-axis of each channel while WAM learns a weight for the width-axis of each channel. Then the dual down-sampling (DDS) module is proposed to ensure the CAR encoder can be efficiently connected with GAN prior. DDS uses bilinear sampling and pixelunshuffle in parallel to generate more representative parameters. Finally, we propose to combine multi-handcraft degradations strategy and pretrained degradation model to construct massive image pairs for the training of RestormGAN. Extensive experiments demonstrate that our RestormGAN achieves state-of-the-art performances for real-world face restoration and superior results in traffic monitoring scenarios.', 'pages': '765-769', 'number': '', 'volume': '', 'year': '2024', 'title': 'Real-World Blind Face Restoration with Generative Facial Prior and Degradation Simulation', 'booktitle': '2024 9th International Conference on Signal and Image Processing (ICSIP)', 'author': 'Zhu, Weihong and Hu, Changhui and Xu, Lintao', 'ENTRYTYPE': 'inproceedings', 'ID': '10671462'}"
11063972,GAN AI for Predictive Threat Detection with Explainable Risk Insights,"Ghadekar, Premanand and Paimode, Rupali and Pandav, Soham and Patange, Priyal and Pardeshi, Atharva and Patil, Paras",Ghadekar,10.1109/ICCSAI64074.2025.11063972,2025,"2025 3rd International Conference on Communication, Security, and Artificial Intelligence (ICCSAI)","An AI-based approach for predicting and analysis cyber-attacks using a mixture of advanced machine and deep learning techniques. Using a dataset having 40,000 cyber-attacks, we aim to detect attack from its primary symptoms before it affect system, calculating it's impact score, predicting attack and the appropriate response, severity levels, anomaly scores, and detect unauthorized users. We apply preprocessing methods like one-hot encoding, standardization, and feature selection using the Mean Decrease Gini impurity method. Key algorithms implemented include XGBoost, and Generative AI (GEN AI) to simulate potential attack scenarios. Explainable AI method such as SHAP are used to ensure model are transperant and trustworthy. Predicting cyber-attack based on primary symptoms before it affects the system using XGBoost model gives the good accuracy. The results demonstrate that our approach, with XGBoost and synthetic data from GEN AI, achieves a 99.17\% accuracy. The use of GAN make model Dynamic improve scalability and use of log loss, Softmax evaluation matrix improves XGBoost performance.",Deep learning;Accuracy;Generative AI;Explainable AI;Predictive models;Prediction algorithms;Threat assessment;Real-time systems;Cyberattack;Synthetic data;Cybersecurity;Generative AI;Explainable AI;XG-Boost;SHAP,"{'month': 'April', 'issn': '', 'doi': '10.1109/ICCSAI64074.2025.11063972', 'keywords': 'Deep learning;Accuracy;Generative AI;Explainable AI;Predictive models;Prediction algorithms;Threat assessment;Real-time systems;Cyberattack;Synthetic data;Cybersecurity;Generative AI;Explainable AI;XG-Boost;SHAP', 'abstract': ""An AI-based approach for predicting and analysis cyber-attacks using a mixture of advanced machine and deep learning techniques. Using a dataset having 40,000 cyber-attacks, we aim to detect attack from its primary symptoms before it affect system, calculating it's impact score, predicting attack and the appropriate response, severity levels, anomaly scores, and detect unauthorized users. We apply preprocessing methods like one-hot encoding, standardization, and feature selection using the Mean Decrease Gini impurity method. Key algorithms implemented include XGBoost, and Generative AI (GEN AI) to simulate potential attack scenarios. Explainable AI method such as SHAP are used to ensure model are transperant and trustworthy. Predicting cyber-attack based on primary symptoms before it affects the system using XGBoost model gives the good accuracy. The results demonstrate that our approach, with XGBoost and synthetic data from GEN AI, achieves a 99.17\\% accuracy. The use of GAN make model Dynamic improve scalability and use of log loss, Softmax evaluation matrix improves XGBoost performance."", 'pages': '1446-1452', 'number': '', 'volume': '3', 'year': '2025', 'title': 'GAN AI for Predictive Threat Detection with Explainable Risk Insights', 'booktitle': '2025 3rd International Conference on Communication, Security, and Artificial Intelligence (ICCSAI)', 'author': 'Ghadekar, Premanand and Paimode, Rupali and Pandav, Soham and Patange, Priyal and Pardeshi, Atharva and Patil, Paras', 'ENTRYTYPE': 'inproceedings', 'ID': '11063972'}"
10951261,APPENDIX: GENAI TOOLS,"Marr, Bernard",Marr,,2024,Generative AI in Practice: 100+ Amazing Ways Generative Artificial Intelligence is Changing Business and Society,,,"{'url': 'https://ieeexplore-ieee-org.crai.referencistas.com/document/10951261', 'isbn': '9781394254255', 'publisher': 'Wiley', 'issn': '', 'doi': '', 'keywords': '', 'abstract': '', 'pages': '249-270', 'number': '', 'volume': '', 'year': '2024', 'title': 'APPENDIX: GENAI TOOLS', 'booktitle': 'Generative AI in Practice: 100+ Amazing Ways Generative Artificial Intelligence is Changing Business and Society', 'author': 'Marr, Bernard', 'ENTRYTYPE': 'inbook', 'ID': '10951261'}"
9780976,A convolutional generative adversarial framework for data augmentation based on a robust optimal transport metric,"Su, Liyilei and Fu, Xianjun and Hu, Qingmao",Su,10.1109/HPCC-DSS-SmartCity-DependSys53884.2021.00178,2021,"2021 IEEE 23rd Int Conf on High Performance Computing \& Communications; 7th Int Conf on Data Science \& Systems; 19th Int Conf on Smart City; 7th Int Conf on Dependability in Sensor, Cloud \& Big Data Systems \& Application (HPCC/DSS/SmartCity/DependSys)","Enhancement of the vanilla generative adversarial network (GAN) to preserve data variability in the presence of real world noise is of paramount significance in deep learning. In this study, we proposed a new distance metric of cosine distance in the framework of optimal transport (OT), and presented and validated a convolutional neural network (CNN) based GAN framework. In comparison with state-of-the-art methods based on Graphics Processing Units (GPU), the proposed framework could maintain the data diversity and quality best in terms of inception score (IS), Fréchet inception distance (FID) and enhancing the classification network of bone age, and is robust to noise degradation. The proposed framework is independent of hardware and thus could also be extended to more advanced hardware such as specialized Tensor Processing Units (TPU), and could be a potential built-in component of a general deep learning networks for such applications as image classification, segmentation, registration, and object detection.",Measurement;Deep learning;Tensors;Smart cities;Neural networks;Graphics processing units;Object detection;Data augmentation;Generative adversarial network;Optimal transport;Convolutional neural network;Cosine distance,"{'month': 'Dec', 'issn': '', 'doi': '10.1109/HPCC-DSS-SmartCity-DependSys53884.2021.00178', 'keywords': 'Measurement;Deep learning;Tensors;Smart cities;Neural networks;Graphics processing units;Object detection;Data augmentation;Generative adversarial network;Optimal transport;Convolutional neural network;Cosine distance', 'abstract': 'Enhancement of the vanilla generative adversarial network (GAN) to preserve data variability in the presence of real world noise is of paramount significance in deep learning. In this study, we proposed a new distance metric of cosine distance in the framework of optimal transport (OT), and presented and validated a convolutional neural network (CNN) based GAN framework. In comparison with state-of-the-art methods based on Graphics Processing Units (GPU), the proposed framework could maintain the data diversity and quality best in terms of inception score (IS), Fréchet inception distance (FID) and enhancing the classification network of bone age, and is robust to noise degradation. The proposed framework is independent of hardware and thus could also be extended to more advanced hardware such as specialized Tensor Processing Units (TPU), and could be a potential built-in component of a general deep learning networks for such applications as image classification, segmentation, registration, and object detection.', 'pages': '1155-1162', 'number': '', 'volume': '', 'year': '2021', 'title': 'A convolutional generative adversarial framework for data augmentation based on a robust optimal transport metric', 'booktitle': '2021 IEEE 23rd Int Conf on High Performance Computing \\& Communications; 7th Int Conf on Data Science \\& Systems; 19th Int Conf on Smart City; 7th Int Conf on Dependability in Sensor, Cloud \\& Big Data Systems \\& Application (HPCC/DSS/SmartCity/DependSys)', 'author': 'Su, Liyilei and Fu, Xianjun and Hu, Qingmao', 'ENTRYTYPE': 'inproceedings', 'ID': '9780976'}"
10505401,Effective Information Guidance for Chinese Font Generation with Skeleton and Channel Expansion,"Zhou, Jie and Wang, Yefei and Yuan, Yiyang and Huang, Qing and Zeng, Jinshan",Zhou,10.1109/AIHCIR61661.2023.00053,2023,"2023 2nd International Conference on Artificial Intelligence, Human-Computer Interaction and Robotics (AIHCIR)","The automatic generation of Chinese fonts is an important problem involved in many applications. The predominated methods for the Chinese font generation are based on deep generative models, especially the generative adversarial networks (GANs). However, existing GAN-based methods (say, CycleGAN) for the Chinese font generation usually suffer from the mode collapse issue, mainly due to the lack of effective information guidance. This paper proposes a novel information guidance module called the skeleton guided channel expansion (SGCE) module for the Chinese font generation through integrating the skeleton information into the generator with the channel expansion way, motivated by the observation that the skeleton embodies both local and global structure information of Chinese characters. We conduct extensive experiments to show the effectiveness of the proposed module. Numerical results show that the mode collapse issue suffered by the known CycleGAN can be effectively alleviated by equipping with the proposed SGCE module, and the CycleGAN equipped with SGCE outperforms the state-of-the-art models in terms of four important evaluation metrics and visualization quality. Besides CycleGAN, we also show that the suggested SGCE module can be adapted to other models for Chinese font generation as a plug-and-play module to further improve their performance.",Measurement;Human computer interaction;Adaptation models;Visualization;Generative adversarial networks;Skeleton;Generators;Chinese font generation;skeleton guided channel expansion;generative adversarial networks;mode collapse;skeleton;channel expansion,"{'month': 'Dec', 'issn': '', 'doi': '10.1109/AIHCIR61661.2023.00053', 'keywords': 'Measurement;Human computer interaction;Adaptation models;Visualization;Generative adversarial networks;Skeleton;Generators;Chinese font generation;skeleton guided channel expansion;generative adversarial networks;mode collapse;skeleton;channel expansion', 'abstract': 'The automatic generation of Chinese fonts is an important problem involved in many applications. The predominated methods for the Chinese font generation are based on deep generative models, especially the generative adversarial networks (GANs). However, existing GAN-based methods (say, CycleGAN) for the Chinese font generation usually suffer from the mode collapse issue, mainly due to the lack of effective information guidance. This paper proposes a novel information guidance module called the skeleton guided channel expansion (SGCE) module for the Chinese font generation through integrating the skeleton information into the generator with the channel expansion way, motivated by the observation that the skeleton embodies both local and global structure information of Chinese characters. We conduct extensive experiments to show the effectiveness of the proposed module. Numerical results show that the mode collapse issue suffered by the known CycleGAN can be effectively alleviated by equipping with the proposed SGCE module, and the CycleGAN equipped with SGCE outperforms the state-of-the-art models in terms of four important evaluation metrics and visualization quality. Besides CycleGAN, we also show that the suggested SGCE module can be adapted to other models for Chinese font generation as a plug-and-play module to further improve their performance.', 'pages': '273-287', 'number': '', 'volume': '', 'year': '2023', 'title': 'Effective Information Guidance for Chinese Font Generation with Skeleton and Channel Expansion', 'booktitle': '2023 2nd International Conference on Artificial Intelligence, Human-Computer Interaction and Robotics (AIHCIR)', 'author': 'Zhou, Jie and Wang, Yefei and Yuan, Yiyang and Huang, Qing and Zeng, Jinshan', 'ENTRYTYPE': 'inproceedings', 'ID': '10505401'}"
11082971,Batik GAN for Generating Motif Synthesis using Multi-Discriminator and Self-Attention,"Minarno, Agus Eko and Soesanti, Indah and Nugroho, Hanung Adi",Minarno,10.1109/AIIT63112.2025.11082971,2025,2025 International Conference on Innovation in Artificial Intelligence and Internet of Things (AIIT),"This study proposes a novel approach for generating high-quality synthetic batik motifs by enhancing the Batik Generative Adversarial Network (Batik GAN) architecture with multi-discriminator and self-attention mechanisms. The proposed method addresses limitations in conventional GAN, particularly in capturing the intricate patterns, textures, and cultural elements inherent in traditional batik art. The Batik Nitik Sarimbit 120 dataset, comprising 120 images across 60 classes, serves as the foundation for training and evaluation, enriched with augmentation techniques to ensure robust learning. The multi discriminator framework enables comprehensive evaluation of synthetic motifs by focusing on distinct aspects such as global patterns, local details, and traditional elements, providing the generator with diverse and detailed feedback. Simultaneously, the self-attention layer allows the generator to dynamically prioritize critical motif elements, ensuring the preservation of spatial relationships and overall harmony in the patterns. Quantitative evaluation using FID, PSNR, and SSIM metrics demonstrates the superiority of the proposed method over the baseline Batik GAN SL and other tested variations. The proposed method achieved a significant reduction in FID, PSNR, and SSIM score. These results indicate enhanced visual fidelity, structural consistency, and noise reduction in the generated motifs. The findings highlight the effectiveness of integrating multi discriminator and self-attention mechanisms in Batik GAN, enabling the synthesis of realistic and culturally authentic batik motifs. This research contributes to the advancement of generative modeling in preserving and innovating traditional textile arts, with potential applications in digital heritage preservation and creative industries.",Measurement;Training;Visualization;Technological innovation;Art;Focusing;Generative adversarial networks;Generators;Cultural differences;Textiles;Generative Adversarial Networks;Batik GAN;Batik Nitik Sarimbit 120;Multi Discriminator;Self-Attention,"{'month': 'May', 'issn': '', 'doi': '10.1109/AIIT63112.2025.11082971', 'keywords': 'Measurement;Training;Visualization;Technological innovation;Art;Focusing;Generative adversarial networks;Generators;Cultural differences;Textiles;Generative Adversarial Networks;Batik GAN;Batik Nitik Sarimbit 120;Multi Discriminator;Self-Attention', 'abstract': 'This study proposes a novel approach for generating high-quality synthetic batik motifs by enhancing the Batik Generative Adversarial Network (Batik GAN) architecture with multi-discriminator and self-attention mechanisms. The proposed method addresses limitations in conventional GAN, particularly in capturing the intricate patterns, textures, and cultural elements inherent in traditional batik art. The Batik Nitik Sarimbit 120 dataset, comprising 120 images across 60 classes, serves as the foundation for training and evaluation, enriched with augmentation techniques to ensure robust learning. The multi discriminator framework enables comprehensive evaluation of synthetic motifs by focusing on distinct aspects such as global patterns, local details, and traditional elements, providing the generator with diverse and detailed feedback. Simultaneously, the self-attention layer allows the generator to dynamically prioritize critical motif elements, ensuring the preservation of spatial relationships and overall harmony in the patterns. Quantitative evaluation using FID, PSNR, and SSIM metrics demonstrates the superiority of the proposed method over the baseline Batik GAN SL and other tested variations. The proposed method achieved a significant reduction in FID, PSNR, and SSIM score. These results indicate enhanced visual fidelity, structural consistency, and noise reduction in the generated motifs. The findings highlight the effectiveness of integrating multi discriminator and self-attention mechanisms in Batik GAN, enabling the synthesis of realistic and culturally authentic batik motifs. This research contributes to the advancement of generative modeling in preserving and innovating traditional textile arts, with potential applications in digital heritage preservation and creative industries.', 'pages': '1-8', 'number': '', 'volume': '', 'year': '2025', 'title': 'Batik GAN for Generating Motif Synthesis using Multi-Discriminator and Self-Attention', 'booktitle': '2025 International Conference on Innovation in Artificial Intelligence and Internet of Things (AIIT)', 'author': 'Minarno, Agus Eko and Soesanti, Indah and Nugroho, Hanung Adi', 'ENTRYTYPE': 'inproceedings', 'ID': '11082971'}"
10081465,Extracting Semantic Knowledge From GANs With Unsupervised Learning,"Xu, Jianjin and Zhang, Zhaoxiang and Hu, Xiaolin",Xu,10.1109/TPAMI.2023.3262140,2023,IEEE Transactions on Pattern Analysis and Machine Intelligence,"Recently, unsupervised learning has made impressive progress on various tasks. Despite the dominance of discriminative models, increasing attention is drawn to representations learned by generative models and in particular, Generative Adversarial Networks (GANs). Previous works on the interpretation of GANs reveal that GANs encode semantics in feature maps in a linearly separable form. In this work, we further find that GAN's features can be well clustered with the linear separability assumption. We propose a novel clustering algorithm, named KLiSH, which leverages the linear separability to cluster GAN's features. KLiSH succeeds in extracting fine-grained semantics of GANs trained on datasets of various objects, e.g., car, portrait, animals, and so on. With KLiSH, we can sample images from GANs along with their segmentation masks and synthesize paired image-segmentation datasets. Using the synthesized datasets, we enable two downstream applications. First, we train semantic segmentation networks on these datasets and test them on real images, realizing unsupervised semantic segmentation. Second, we train image-to-image translation networks on the synthesized datasets, enabling semantic-conditional image synthesis without human annotations.",Semantics;Semantic segmentation;Generative adversarial networks;Clustering algorithms;Unsupervised learning;Image synthesis;Annotations;Conditional image synthesis;GAN;semantic segmentation;unsupervised learning,"{'month': 'Aug', 'issn': '1939-3539', 'doi': '10.1109/TPAMI.2023.3262140', 'keywords': 'Semantics;Semantic segmentation;Generative adversarial networks;Clustering algorithms;Unsupervised learning;Image synthesis;Annotations;Conditional image synthesis;GAN;semantic segmentation;unsupervised learning', 'abstract': ""Recently, unsupervised learning has made impressive progress on various tasks. Despite the dominance of discriminative models, increasing attention is drawn to representations learned by generative models and in particular, Generative Adversarial Networks (GANs). Previous works on the interpretation of GANs reveal that GANs encode semantics in feature maps in a linearly separable form. In this work, we further find that GAN's features can be well clustered with the linear separability assumption. We propose a novel clustering algorithm, named KLiSH, which leverages the linear separability to cluster GAN's features. KLiSH succeeds in extracting fine-grained semantics of GANs trained on datasets of various objects, e.g., car, portrait, animals, and so on. With KLiSH, we can sample images from GANs along with their segmentation masks and synthesize paired image-segmentation datasets. Using the synthesized datasets, we enable two downstream applications. First, we train semantic segmentation networks on these datasets and test them on real images, realizing unsupervised semantic segmentation. Second, we train image-to-image translation networks on the synthesized datasets, enabling semantic-conditional image synthesis without human annotations."", 'pages': '9654-9668', 'number': '8', 'volume': '45', 'year': '2023', 'title': 'Extracting Semantic Knowledge From GANs With Unsupervised Learning', 'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence', 'author': 'Xu, Jianjin and Zhang, Zhaoxiang and Hu, Xiaolin', 'ENTRYTYPE': 'article', 'ID': '10081465'}"
9980408,Generating Role-Playing Game Quests With GPT Language Models,"Värtinen, Susanna and Hämäläinen, Perttu and Guckelsberger, Christian",Värtinen,10.1109/TG.2022.3228480,2024,IEEE Transactions on Games,"Quests represent an integral part of role-playing games (RPGs). While evocative, narrative-rich quests are still mostly hand-authored, player demands toward more and richer game content, as well as business requirements for continuous player engagement necessitate alternative, procedural quest generation methods. While existing methods produce mostly uninteresting, mechanical quest descriptions, recent advances in AI have brought forth generative language models with promising computational storytelling capabilities. We leverage two of the most successful transformer models, 1) GPT-2 and 2) GPT-3, to procedurally generate RPG video game quest descriptions. We gathered, processed, and openly published a dataset of 978 quests and their descriptions from six RPGs. We fine-tuned GPT-2 on this dataset with a range of optimizations informed by several ministudies. We validated the resulting Quest-GPT-2 model via an online user study involving 349 RPG players. Our results indicate that one in five quest descriptions would be deemed acceptable by a human critic, yet the variation in quality across individual quests is large. We provide recommendations on current applications of Quest-GPT-2. This is complemented by case-studies on GPT-3 to highlight the future potential of state-of-the-art natural language models for quest generation.",Games;Computational modeling;Task analysis;Large language models;Data models;Artificial intelligence;Role playing games;Artificial intelligence;computational storytelling;games;generative models;procedural content generation;quests,"{'month': 'March', 'issn': '2475-1510', 'doi': '10.1109/TG.2022.3228480', 'keywords': 'Games;Computational modeling;Task analysis;Large language models;Data models;Artificial intelligence;Role playing games;Artificial intelligence;computational storytelling;games;generative models;procedural content generation;quests', 'abstract': 'Quests represent an integral part of role-playing games (RPGs). While evocative, narrative-rich quests are still mostly hand-authored, player demands toward more and richer game content, as well as business requirements for continuous player engagement necessitate alternative, procedural quest generation methods. While existing methods produce mostly uninteresting, mechanical quest descriptions, recent advances in AI have brought forth generative language models with promising computational storytelling capabilities. We leverage two of the most successful transformer models, 1) GPT-2 and 2) GPT-3, to procedurally generate RPG video game quest descriptions. We gathered, processed, and openly published a dataset of 978 quests and their descriptions from six RPGs. We fine-tuned GPT-2 on this dataset with a range of optimizations informed by several ministudies. We validated the resulting Quest-GPT-2 model via an online user study involving 349 RPG players. Our results indicate that one in five quest descriptions would be deemed acceptable by a human critic, yet the variation in quality across individual quests is large. We provide recommendations on current applications of Quest-GPT-2. This is complemented by case-studies on GPT-3 to highlight the future potential of state-of-the-art natural language models for quest generation.', 'pages': '127-139', 'number': '1', 'volume': '16', 'year': '2024', 'title': 'Generating Role-Playing Game Quests With GPT Language Models', 'journal': 'IEEE Transactions on Games', 'author': 'Värtinen, Susanna and Hämäläinen, Perttu and Guckelsberger, Christian', 'ENTRYTYPE': 'article', 'ID': '9980408'}"
10577853,The use of AI and student population: The change is inevitable,"Krašna, Marjan and Bratina, Tomaž",Krašna,10.1109/MECO62516.2024.10577853,2024,2024 13th Mediterranean Conference on Embedded Computing (MECO),"Generative AI systems have become a permanent fixture, with various authors highlighting their pros and cons. In education, a persistent technological gap often puts teachers at a disadvantage, trailing behind students who are more adept at embracing new technologies. The SETCOM project successfully introduced AI to both students and teachers, particularly focusing on educational study program students destined to become teachers themselves. Despite generational disparities between teachers and students, with younger generations showing more inclination towards ICT and AI, utilization remains sporadic. While students readily embrace AI, subscription-based advanced AI systems see surprising uptake, with around a fifth of students utilizing them. Primarily, students use AI to comprehend unfamiliar concepts, acknowledging its capacity to generate responses but exercising caution by verifying and correcting errors in over half of cases. For teachers, embracing AI becomes crucial to remain relevant in their field, despite potential financial implications. Failure to adapt risks being perceived as outdated by their students across all educational levels. In summary, generative AI systems are entrenched in education, offering both opportunities and challenges. Bridging the technological gap is imperative for teachers, as students increasingly rely on AI for learning support. Embracing AI becomes essential for educators to maintain relevance and effectiveness in teaching, ensuring they do not become obsolete in the eyes of their tech-savvy pupils.",Training;Generative AI;Fixtures;Focusing;Fasteners;Trajectory;Reliability;education;AI;student perspective;influence,"{'month': 'June', 'issn': '2637-9511', 'doi': '10.1109/MECO62516.2024.10577853', 'keywords': 'Training;Generative AI;Fixtures;Focusing;Fasteners;Trajectory;Reliability;education;AI;student perspective;influence', 'abstract': 'Generative AI systems have become a permanent fixture, with various authors highlighting their pros and cons. In education, a persistent technological gap often puts teachers at a disadvantage, trailing behind students who are more adept at embracing new technologies. The SETCOM project successfully introduced AI to both students and teachers, particularly focusing on educational study program students destined to become teachers themselves. Despite generational disparities between teachers and students, with younger generations showing more inclination towards ICT and AI, utilization remains sporadic. While students readily embrace AI, subscription-based advanced AI systems see surprising uptake, with around a fifth of students utilizing them. Primarily, students use AI to comprehend unfamiliar concepts, acknowledging its capacity to generate responses but exercising caution by verifying and correcting errors in over half of cases. For teachers, embracing AI becomes crucial to remain relevant in their field, despite potential financial implications. Failure to adapt risks being perceived as outdated by their students across all educational levels. In summary, generative AI systems are entrenched in education, offering both opportunities and challenges. Bridging the technological gap is imperative for teachers, as students increasingly rely on AI for learning support. Embracing AI becomes essential for educators to maintain relevance and effectiveness in teaching, ensuring they do not become obsolete in the eyes of their tech-savvy pupils.', 'pages': '1-5', 'number': '', 'volume': '', 'year': '2024', 'title': 'The use of AI and student population: The change is inevitable', 'booktitle': '2024 13th Mediterranean Conference on Embedded Computing (MECO)', 'author': 'Krašna, Marjan and Bratina, Tomaž', 'ENTRYTYPE': 'inproceedings', 'ID': '10577853'}"
6374126,Tutorials,"Nelson, Mark J. and Burelli, Paolo and Karpouzis, Kostas and Lucas, Simon and Cowling, Peter",Nelson,10.1109/CIG.2012.6374126,2012,2012 IEEE Conference on Computational Intelligence and Games (CIG),Provides an abstract for each of the tutorial presentations and a brief professional biography of each presenter. The complete presentations were not made available for publication as part of the conference proceedings.,,"{'month': 'Sep.', 'issn': '2325-4289', 'doi': '10.1109/CIG.2012.6374126', 'keywords': '', 'abstract': 'Provides an abstract for each of the tutorial presentations and a brief professional biography of each presenter. The complete presentations were not made available for publication as part of the conference proceedings.', 'pages': 'F-1-F-2', 'number': '', 'volume': '', 'year': '2012', 'title': 'Tutorials', 'booktitle': '2012 IEEE Conference on Computational Intelligence and Games (CIG)', 'author': 'Nelson, Mark J. and Burelli, Paolo and Karpouzis, Kostas and Lucas, Simon and Cowling, Peter', 'ENTRYTYPE': 'inproceedings', 'ID': '6374126'}"
11100261,Modeling Cross-Disciplinary Communication in Smart Education via Generative Agent,"Yang, Wen-Xi and Zhao, Tian-Fang",Yang,10.1109/MiTA66017.2025.11100261,2025,2025 12th International Conference on Machine Intelligence Theory and Applications (MiTA),"The application of generative agent models in smart education has become a hot topic in artificial intelligence research, with widespread use in fields such as personalized learning recommendations, intelligent question answering, and automated assessment. However, existing researches in smart education face challenges in cross-disciplinary communication, making it difficult to accurately simulate the language styles and thinking patterns of students from different disciplinary backgrounds. This leads to limited semantic transfer ability and weak knowledge integration capabilities, thereby limiting their effectiveness in multidisciplinary contexts. To address this issue, this study introduces the generative agent model driven by large language models (LLMs), based on Qwen-Max, to simulate interactions between humanities and science learners. Six metrics are designed to quantify and evaluate the quality of communication. Experimental results show that cross-disciplinary communication has lower information redundancy and stronger language adaptability compared to intra-disciplinary communication. The memory mechanism is introduced to improve communication coherence and knowledge integration. This research provides empirical support for the application of agents in cross-disciplinary collaboration and offers new directions for the development of smart education systems.",Measurement;Adaptation models;Humanities;Limiting;Redundancy;Semantics;Educational technology;Learning (artificial intelligence);Question answering (information retrieval);Machine intelligence;Generative agent;cross-disciplinary communication;quality evaluation;smart education,"{'month': 'May', 'issn': '', 'doi': '10.1109/MiTA66017.2025.11100261', 'keywords': 'Measurement;Adaptation models;Humanities;Limiting;Redundancy;Semantics;Educational technology;Learning (artificial intelligence);Question answering (information retrieval);Machine intelligence;Generative agent;cross-disciplinary communication;quality evaluation;smart education', 'abstract': 'The application of generative agent models in smart education has become a hot topic in artificial intelligence research, with widespread use in fields such as personalized learning recommendations, intelligent question answering, and automated assessment. However, existing researches in smart education face challenges in cross-disciplinary communication, making it difficult to accurately simulate the language styles and thinking patterns of students from different disciplinary backgrounds. This leads to limited semantic transfer ability and weak knowledge integration capabilities, thereby limiting their effectiveness in multidisciplinary contexts. To address this issue, this study introduces the generative agent model driven by large language models (LLMs), based on Qwen-Max, to simulate interactions between humanities and science learners. Six metrics are designed to quantify and evaluate the quality of communication. Experimental results show that cross-disciplinary communication has lower information redundancy and stronger language adaptability compared to intra-disciplinary communication. The memory mechanism is introduced to improve communication coherence and knowledge integration. This research provides empirical support for the application of agents in cross-disciplinary collaboration and offers new directions for the development of smart education systems.', 'pages': '1-8', 'number': '', 'volume': '', 'year': '2025', 'title': 'Modeling Cross-Disciplinary Communication in Smart Education via Generative Agent', 'booktitle': '2025 12th International Conference on Machine Intelligence Theory and Applications (MiTA)', 'author': 'Yang, Wen-Xi and Zhao, Tian-Fang', 'ENTRYTYPE': 'inproceedings', 'ID': '11100261'}"
9241434,InterFaceGAN: Interpreting the Disentangled Face Representation Learned by GANs,"Shen, Yujun and Yang, Ceyuan and Tang, Xiaoou and Zhou, Bolei",Shen,10.1109/TPAMI.2020.3034267,2022,IEEE Transactions on Pattern Analysis and Machine Intelligence,"Although generative adversarial networks (GANs) have made significant progress in face synthesis, there lacks enough understanding of what GANs have learned in the latent representation to map a random code to a photo-realistic image. In this work, we propose a framework called InterFaceGAN to interpret the disentangled face representation learned by the state-of-the-art GAN models and study the properties of the facial semantics encoded in the latent space. We first find that GANs learn various semantics in some linear subspaces of the latent space. After identifying these subspaces, we can realistically manipulate the corresponding facial attributes without retraining the model. We then conduct a detailed study on the correlation between different semantics and manage to better disentangle them via subspace projection, resulting in more precise control of the attribute manipulation. Besides manipulating the gender, age, expression, and presence of eyeglasses, we can even alter the face pose and fix the artifacts accidentally made by GANs. Furthermore, we perform an in-depth face identity analysis and a layer-wise analysis to evaluate the editing results quantitatively. Finally, we apply our approach to real face editing by employing GAN inversion approaches and explicitly training feed-forward models based on the synthetic data established by InterFaceGAN. Extensive experimental results suggest that learning to synthesize faces spontaneously brings a disentangled and controllable face representation.",Semantics;Faces;Gallium nitride;Generative adversarial networks;Generators;Aerospace electronics;Facial features;Generative adversarial network;face editing;interpretability;explainable artificial intelligence;disentanglement,"{'month': 'April', 'issn': '1939-3539', 'doi': '10.1109/TPAMI.2020.3034267', 'keywords': 'Semantics;Faces;Gallium nitride;Generative adversarial networks;Generators;Aerospace electronics;Facial features;Generative adversarial network;face editing;interpretability;explainable artificial intelligence;disentanglement', 'abstract': 'Although generative adversarial networks (GANs) have made significant progress in face synthesis, there lacks enough understanding of what GANs have learned in the latent representation to map a random code to a photo-realistic image. In this work, we propose a framework called InterFaceGAN to interpret the disentangled face representation learned by the state-of-the-art GAN models and study the properties of the facial semantics encoded in the latent space. We first find that GANs learn various semantics in some linear subspaces of the latent space. After identifying these subspaces, we can realistically manipulate the corresponding facial attributes without retraining the model. We then conduct a detailed study on the correlation between different semantics and manage to better disentangle them via subspace projection, resulting in more precise control of the attribute manipulation. Besides manipulating the gender, age, expression, and presence of eyeglasses, we can even alter the face pose and fix the artifacts accidentally made by GANs. Furthermore, we perform an in-depth face identity analysis and a layer-wise analysis to evaluate the editing results quantitatively. Finally, we apply our approach to real face editing by employing GAN inversion approaches and explicitly training feed-forward models based on the synthetic data established by InterFaceGAN. Extensive experimental results suggest that learning to synthesize faces spontaneously brings a disentangled and controllable face representation.', 'pages': '2004-2018', 'number': '4', 'volume': '44', 'year': '2022', 'title': 'InterFaceGAN: Interpreting the Disentangled Face Representation Learned by GANs', 'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence', 'author': 'Shen, Yujun and Yang, Ceyuan and Tang, Xiaoou and Zhou, Bolei', 'ENTRYTYPE': 'article', 'ID': '9241434'}"
8629024,Generative Adversarial Networks and Perceptual Losses for Video Super-Resolution,"Lucas, Alice and López-Tapia, Santiago and Molina, Rafael and Katsaggelos, Aggelos K.",Lucas,10.1109/TIP.2019.2895768,2019,IEEE Transactions on Image Processing,"Video super-resolution (VSR) has become one of the most critical problems in video processing. In the deep learning literature, recent works have shown the benefits of using adversarial-based and perceptual losses to improve the performance on various image restoration tasks; however, these have yet to be applied for video super-resolution. In this paper, we propose a generative adversarial network (GAN)-based formulation for VSR. We introduce a new generator network optimized for the VSR problem, named VSRResNet, along with new discriminator architecture to properly guide VSRResNet during the GAN training. We further enhance our VSR GAN formulation with two regularizers, a distance loss in feature-space and pixel-space, to obtain our final VSRResFeatGAN model. We show that pre-training our generator with the mean-squared-error loss only quantitatively surpasses the current state-of-the-art VSR models. Finally, we employ the PercepDist metric to compare the state-of-the-art VSR models. We show that this metric more accurately evaluates the perceptual quality of SR solutions obtained from neural networks, compared with the commonly used PSNR/SSIM metrics. Finally, we show that our proposed model, the VSRResFeatGAN model, outperforms the current state-of-the-art SR models, both quantitatively and qualitatively.",Neural networks;Training;Spatial resolution;Generators;Gallium nitride;Task analysis;Artificial neural networks;video signal processing;image resolution;image generation,"{'month': 'July', 'issn': '1941-0042', 'doi': '10.1109/TIP.2019.2895768', 'keywords': 'Neural networks;Training;Spatial resolution;Generators;Gallium nitride;Task analysis;Artificial neural networks;video signal processing;image resolution;image generation', 'abstract': 'Video super-resolution (VSR) has become one of the most critical problems in video processing. In the deep learning literature, recent works have shown the benefits of using adversarial-based and perceptual losses to improve the performance on various image restoration tasks; however, these have yet to be applied for video super-resolution. In this paper, we propose a generative adversarial network (GAN)-based formulation for VSR. We introduce a new generator network optimized for the VSR problem, named VSRResNet, along with new discriminator architecture to properly guide VSRResNet during the GAN training. We further enhance our VSR GAN formulation with two regularizers, a distance loss in feature-space and pixel-space, to obtain our final VSRResFeatGAN model. We show that pre-training our generator with the mean-squared-error loss only quantitatively surpasses the current state-of-the-art VSR models. Finally, we employ the PercepDist metric to compare the state-of-the-art VSR models. We show that this metric more accurately evaluates the perceptual quality of SR solutions obtained from neural networks, compared with the commonly used PSNR/SSIM metrics. Finally, we show that our proposed model, the VSRResFeatGAN model, outperforms the current state-of-the-art SR models, both quantitatively and qualitatively.', 'pages': '3312-3327', 'number': '7', 'volume': '28', 'year': '2019', 'title': 'Generative Adversarial Networks and Perceptual Losses for Video Super-Resolution', 'journal': 'IEEE Transactions on Image Processing', 'author': 'Lucas, Alice and López-Tapia, Santiago and Molina, Rafael and Katsaggelos, Aggelos K.', 'ENTRYTYPE': 'article', 'ID': '8629024'}"
9631286,Applications of Generative Adversarial Networks in Anomaly Detection: A Systematic Literature Review,"Sabuhi, Mikael and Zhou, Ming and Bezemer, Cor-Paul and Musilek, Petr",Sabuhi,10.1109/ACCESS.2021.3131949,2021,IEEE Access,"Anomaly detection has become an indispensable tool for modern society, applied in a wide range of applications, from detecting fraudulent transactions to malignant brain tumors. Over time, many anomaly detection techniques have been introduced. However, in general, they all suffer from the same problem: lack of data that represents anomalous behaviour. As anomalous behaviour is usually costly (or dangerous) for a system, it is difficult to gather enough data that represents such behaviour. This, in turn, makes it difficult to develop and evaluate anomaly detection techniques. Recently, generative adversarial networks (GANs) have attracted much attention in anomaly detection research, due to their unique ability to generate new data. In this paper, we present a systematic review of the literature in this area, covering 128 papers. The goal of this review paper is to analyze the relation between anomaly detection techniques and types of GANs, to identify the most common application domains for GAN-assisted and GAN-based anomaly detection, and to assemble information on datasets and performance metrics used to assess them. Our study helps researchers and practitioners to find the most suitable GAN-assisted anomaly detection technique for their application. In addition, we present a research roadmap for future studies in this area. In summary, GANs are used in anomaly detection to address the problem of insufficient amount of data for the anomalous behaviour, either through data augmentation or representation learning. The most commonly used GAN architectures are DCGANs, standard GANs, and cGANs. The primary application domains include medicine, surveillance and intrusion detection.",Anomaly detection;Data models;Systematics;Generative adversarial networks;Generators;Representation learning;Training;Anomaly detection;data augmentation;generative adversarial networks;outlier detection;representation learning,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2021.3131949', 'keywords': 'Anomaly detection;Data models;Systematics;Generative adversarial networks;Generators;Representation learning;Training;Anomaly detection;data augmentation;generative adversarial networks;outlier detection;representation learning', 'abstract': 'Anomaly detection has become an indispensable tool for modern society, applied in a wide range of applications, from detecting fraudulent transactions to malignant brain tumors. Over time, many anomaly detection techniques have been introduced. However, in general, they all suffer from the same problem: lack of data that represents anomalous behaviour. As anomalous behaviour is usually costly (or dangerous) for a system, it is difficult to gather enough data that represents such behaviour. This, in turn, makes it difficult to develop and evaluate anomaly detection techniques. Recently, generative adversarial networks (GANs) have attracted much attention in anomaly detection research, due to their unique ability to generate new data. In this paper, we present a systematic review of the literature in this area, covering 128 papers. The goal of this review paper is to analyze the relation between anomaly detection techniques and types of GANs, to identify the most common application domains for GAN-assisted and GAN-based anomaly detection, and to assemble information on datasets and performance metrics used to assess them. Our study helps researchers and practitioners to find the most suitable GAN-assisted anomaly detection technique for their application. In addition, we present a research roadmap for future studies in this area. In summary, GANs are used in anomaly detection to address the problem of insufficient amount of data for the anomalous behaviour, either through data augmentation or representation learning. The most commonly used GAN architectures are DCGANs, standard GANs, and cGANs. The primary application domains include medicine, surveillance and intrusion detection.', 'pages': '161003-161029', 'number': '', 'volume': '9', 'year': '2021', 'title': 'Applications of Generative Adversarial Networks in Anomaly Detection: A Systematic Literature Review', 'journal': 'IEEE Access', 'author': 'Sabuhi, Mikael and Zhou, Ming and Bezemer, Cor-Paul and Musilek, Petr', 'ENTRYTYPE': 'article', 'ID': '9631286'}"
10187144,A Comprehensive Survey of Generative Adversarial Networks (GANs) in Cybersecurity Intrusion Detection,"Dunmore, Aeryn and Jang-Jaccard, Julian and Sabrina, Fariza and Kwak, Jin",Dunmore,10.1109/ACCESS.2023.3296707,2023,IEEE Access,"Generative Adversarial Networks (GANs) have seen significant interest since their introduction in 2014. While originally focused primarily on image-based tasks, their capacity for generating new, synthetic data has brought them into many different fields of Machine Learning research. Their use in cybersecurity has grown swiftly, especially in tasks which require training on unbalanced datasets of attack classes. In this paper we examine the use of GANs in Intrusion Detection Systems (IDS) and how they are currently being employed in this area of research. GANs are currently in use for the creation of adversarial examples, editing the semantic information of data, creating polymorphic samples of malware, augmenting data for rare classes, and much more. We have endeavored to create a paper that may act as a primer for cybersecurity specialists and machine learning researchers alike. This paper details what GANs are and how they work, the current types of GAN in use in the area, datasets used in this research, metrics for evaluation, current areas of use in intrusion detection, and when and how they are best used.",Generative adversarial networks;Generators;Training;Surveys;Machine learning;Computer security;Games;Intrusion detection;Data augmentation;Generative adversarial networks (GAN);machine learning;research survey;attack modeling;threat detection;intrusion detection systems;data augmentation;zero-day attacks;adversarial examples,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2023.3296707', 'keywords': 'Generative adversarial networks;Generators;Training;Surveys;Machine learning;Computer security;Games;Intrusion detection;Data augmentation;Generative adversarial networks (GAN);machine learning;research survey;attack modeling;threat detection;intrusion detection systems;data augmentation;zero-day attacks;adversarial examples', 'abstract': 'Generative Adversarial Networks (GANs) have seen significant interest since their introduction in 2014. While originally focused primarily on image-based tasks, their capacity for generating new, synthetic data has brought them into many different fields of Machine Learning research. Their use in cybersecurity has grown swiftly, especially in tasks which require training on unbalanced datasets of attack classes. In this paper we examine the use of GANs in Intrusion Detection Systems (IDS) and how they are currently being employed in this area of research. GANs are currently in use for the creation of adversarial examples, editing the semantic information of data, creating polymorphic samples of malware, augmenting data for rare classes, and much more. We have endeavored to create a paper that may act as a primer for cybersecurity specialists and machine learning researchers alike. This paper details what GANs are and how they work, the current types of GAN in use in the area, datasets used in this research, metrics for evaluation, current areas of use in intrusion detection, and when and how they are best used.', 'pages': '76071-76094', 'number': '', 'volume': '11', 'year': '2023', 'title': 'A Comprehensive Survey of Generative Adversarial Networks (GANs) in Cybersecurity Intrusion Detection', 'journal': 'IEEE Access', 'author': 'Dunmore, Aeryn and Jang-Jaccard, Julian and Sabrina, Fariza and Kwak, Jin', 'ENTRYTYPE': 'article', 'ID': '10187144'}"
10395092,Adversarial Attacks on Generative AI Anomaly Detection in the Quantum Era,"A, Jenefa and Ebenezer, V and Isaac, A Joshua and Marshell, Joe and Pradeepa, P. and Naveen, V",A,10.1109/ICECA58529.2023.10395092,2023,"2023 7th International Conference on Electronics, Communication and Aerospace Technology (ICECA)","In the context of the rapidly evolving quantum era, where the capabilities of quantum computing are expanding exponentially, the security and reliability of generative AI anomaly detection systems have come under significant threat from adversarial attacks. This paper delves into the vulnerabilities inherent in existing models in this quantum era and introduces a novel approach aimed at fortifying their resilience. The problem at hand pertains to the susceptibility of conventional generative AI anomaly detection systems to adversarial manipulations, raising critical concerns about their trustworthiness in critical applications. Traditional methods in this domain rely on static models that lack adaptability to defend against quantum-based adversarial attacks, rendering them inadequate for the evolving threat landscape. Our proposed solution combines quantum-resistant algorithms with advanced generative AI techniques to dynamically adapt to emerging attack strategies, resulting in demonstrably enhanced robustness against quantum-based adversarial attacks, as substantiated by our experimental findings. In conclusion, safeguarding generative AI anomaly detection systems against adversarial threats in the quantum era is paramount, and our innovative approach offers a promising avenue for bolstering the security and reliability of these systems in this challenging environment.",Adaptation models;Quantum computing;Generative AI;Computational modeling;Robustness;Security;Anomaly detection;Adversarial Attacks;Generative Artificial Intelligence;Anomaly Detection;Quantum Era;Quantum Computing,"{'month': 'Nov', 'issn': '', 'doi': '10.1109/ICECA58529.2023.10395092', 'keywords': 'Adaptation models;Quantum computing;Generative AI;Computational modeling;Robustness;Security;Anomaly detection;Adversarial Attacks;Generative Artificial Intelligence;Anomaly Detection;Quantum Era;Quantum Computing', 'abstract': 'In the context of the rapidly evolving quantum era, where the capabilities of quantum computing are expanding exponentially, the security and reliability of generative AI anomaly detection systems have come under significant threat from adversarial attacks. This paper delves into the vulnerabilities inherent in existing models in this quantum era and introduces a novel approach aimed at fortifying their resilience. The problem at hand pertains to the susceptibility of conventional generative AI anomaly detection systems to adversarial manipulations, raising critical concerns about their trustworthiness in critical applications. Traditional methods in this domain rely on static models that lack adaptability to defend against quantum-based adversarial attacks, rendering them inadequate for the evolving threat landscape. Our proposed solution combines quantum-resistant algorithms with advanced generative AI techniques to dynamically adapt to emerging attack strategies, resulting in demonstrably enhanced robustness against quantum-based adversarial attacks, as substantiated by our experimental findings. In conclusion, safeguarding generative AI anomaly detection systems against adversarial threats in the quantum era is paramount, and our innovative approach offers a promising avenue for bolstering the security and reliability of these systems in this challenging environment.', 'pages': '1833-1840', 'number': '', 'volume': '', 'year': '2023', 'title': 'Adversarial Attacks on Generative AI Anomaly Detection in the Quantum Era', 'booktitle': '2023 7th International Conference on Electronics, Communication and Aerospace Technology (ICECA)', 'author': 'A, Jenefa and Ebenezer, V and Isaac, A Joshua and Marshell, Joe and Pradeepa, P. and Naveen, V', 'ENTRYTYPE': 'inproceedings', 'ID': '10395092'}"
11158675,Research on the AIGC-assisted Instructional Model of Intergrating Computational Thinking in STEM Education,"Zhang, Shaoying and Wu, Lan",Zhang,10.1109/ICAIE64856.2025.11158675,2025,2025 5th International Conference on Artificial Intelligence and Education (ICAIE),"Computational thinking (CT) is crucial in the smart era and cultivating students' CT has attracted attention all around the world. Science, Technology, Engineering, and Math-ematics (STEM) education, focusing on the cultivation of higher-order thinking, aids CT development, with group work being the essential part of integrating CT in STEM education. The advent of artificial intelligence in generative content (AIGC) offers new opportunities for improving CT, especially in self-learning and group work. Thus, this study aims to construct an AIGC-assisted instructional design model of integrating CT in STEM education to enhance students' CT learning while providing them with a good experience of group work. Based on that, we developed a course and conducted a two-week quasi-experiment in a senior high school in the South area of China. Data analysis showed that the students were very satisfied with the instructional design model and their CT was significantly improved with the assistance of AIGC in STEM contexts. This study provides some evidence for AIGC-assisted STEM education that supports the development of CT skills and and to provide a reference for AIGC to support teachers in conducting STEM courses.",Ethics;Data analysis;Computational modeling;Education;Focusing;Collaboration;Learning (artificial intelligence);Data models;STEM;Context modeling;artificial intelligence in generative content (AIGC);computational thinking (CT);STEM education;instructional design model,"{'month': 'May', 'issn': '', 'doi': '10.1109/ICAIE64856.2025.11158675', 'keywords': 'Ethics;Data analysis;Computational modeling;Education;Focusing;Collaboration;Learning (artificial intelligence);Data models;STEM;Context modeling;artificial intelligence in generative content (AIGC);computational thinking (CT);STEM education;instructional design model', 'abstract': ""Computational thinking (CT) is crucial in the smart era and cultivating students' CT has attracted attention all around the world. Science, Technology, Engineering, and Math-ematics (STEM) education, focusing on the cultivation of higher-order thinking, aids CT development, with group work being the essential part of integrating CT in STEM education. The advent of artificial intelligence in generative content (AIGC) offers new opportunities for improving CT, especially in self-learning and group work. Thus, this study aims to construct an AIGC-assisted instructional design model of integrating CT in STEM education to enhance students' CT learning while providing them with a good experience of group work. Based on that, we developed a course and conducted a two-week quasi-experiment in a senior high school in the South area of China. Data analysis showed that the students were very satisfied with the instructional design model and their CT was significantly improved with the assistance of AIGC in STEM contexts. This study provides some evidence for AIGC-assisted STEM education that supports the development of CT skills and and to provide a reference for AIGC to support teachers in conducting STEM courses."", 'pages': '382-387', 'number': '', 'volume': '', 'year': '2025', 'title': 'Research on the AIGC-assisted Instructional Model of Intergrating Computational Thinking in STEM Education', 'booktitle': '2025 5th International Conference on Artificial Intelligence and Education (ICAIE)', 'author': 'Zhang, Shaoying and Wu, Lan', 'ENTRYTYPE': 'inproceedings', 'ID': '11158675'}"
10979306,Novel Multi-Scale Attention Generative Adversarial Network for Photovoltaic Solar Cell Defect Inspection Using Electroluminescence Images,"Guan, Yuanjun and Liu, Yang and Wang, Jiayi and Wang, Tao and Yi, Qianchuan and Jiang, Wenxin and Gu, Xiaopu and Zhang, Yichen and Zhang, Li and Han, Tianyan and Huang, Binbing and Hu, Lilei",Guan,10.1109/ACCESS.2025.3565002,2025,IEEE Access,"In the pursuit of promoting green energy, efficient defect inspection in solar cell manufacturing is crucial in enhancing the reliability of solar energy systems. However, traditional deep learning models for automatic defect inspection in photovoltaic (PV) cell electroluminescence (EL) images encounter challenges in industrial settings due to difficulties associated with data acquisition, imbalance, and variability of defects. This paper presents a novel Multi-Scale Attention Generative Adversarial Network (MAGAN), an innovative GAN-based framework specifically designed for data augmentation in the context of solar cell defect detection. When integrated with automated detection techniques, MAGAN markedly improves the accuracy and efficiency of current models. A method for augmenting image datasets of EL was developed to generate a sufficient quantity of images for training machine learning models, addressing sample scarcity and bolstering CNN-based defect classification accuracy. The core of this approach lies in the application of the MCA (Multi-channel Spatial Attention Mechanism) and GLSA (Gate-like Spatial Attention Mechanism) modules, which enhance feature extraction by leveraging channel attention and spatial attention, respectively, thereby reflecting the most recent advancements in attention mechanism technology. The MCA dissects channels into sub-features across various scales, ensuring detailed attention mapping, whereas the GLSA refines spatial cues with a gating mechanism, shedding computational inefficiencies. The effectiveness of this approach is validated by comprehensive experiments against state-of-the-art deep learning models. The experiments demonstrate the exceptional performance of MAGAN, achieving a low FID score of 141.98 and KID score of 0.106 on complex EL images, surpassing previous models and emphasizing data augmentation’s importance in defect detection. With an industry-leading detection accuracy of 87.3\%, this study makes a substantial contribution to mitigating data imbalance. This method enhances quality control in solar cell manufacturing. Additionally, it advances defect inspection in the industrial semiconductor sector.",Generative adversarial networks;Photovoltaic cells;Computational modeling;Training;Inspection;Attention mechanisms;Accuracy;Deep learning;Photovoltaic systems;Image synthesis;Attention mechanism;convolution neural network;defect inspection;electroluminescence;generative adversarial network;photovoltaic solar cells,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2025.3565002', 'keywords': 'Generative adversarial networks;Photovoltaic cells;Computational modeling;Training;Inspection;Attention mechanisms;Accuracy;Deep learning;Photovoltaic systems;Image synthesis;Attention mechanism;convolution neural network;defect inspection;electroluminescence;generative adversarial network;photovoltaic solar cells', 'abstract': 'In the pursuit of promoting green energy, efficient defect inspection in solar cell manufacturing is crucial in enhancing the reliability of solar energy systems. However, traditional deep learning models for automatic defect inspection in photovoltaic (PV) cell electroluminescence (EL) images encounter challenges in industrial settings due to difficulties associated with data acquisition, imbalance, and variability of defects. This paper presents a novel Multi-Scale Attention Generative Adversarial Network (MAGAN), an innovative GAN-based framework specifically designed for data augmentation in the context of solar cell defect detection. When integrated with automated detection techniques, MAGAN markedly improves the accuracy and efficiency of current models. A method for augmenting image datasets of EL was developed to generate a sufficient quantity of images for training machine learning models, addressing sample scarcity and bolstering CNN-based defect classification accuracy. The core of this approach lies in the application of the MCA (Multi-channel Spatial Attention Mechanism) and GLSA (Gate-like Spatial Attention Mechanism) modules, which enhance feature extraction by leveraging channel attention and spatial attention, respectively, thereby reflecting the most recent advancements in attention mechanism technology. The MCA dissects channels into sub-features across various scales, ensuring detailed attention mapping, whereas the GLSA refines spatial cues with a gating mechanism, shedding computational inefficiencies. The effectiveness of this approach is validated by comprehensive experiments against state-of-the-art deep learning models. The experiments demonstrate the exceptional performance of MAGAN, achieving a low FID score of 141.98 and KID score of 0.106 on complex EL images, surpassing previous models and emphasizing data augmentation’s importance in defect detection. With an industry-leading detection accuracy of 87.3\\%, this study makes a substantial contribution to mitigating data imbalance. This method enhances quality control in solar cell manufacturing. Additionally, it advances defect inspection in the industrial semiconductor sector.', 'pages': '84409-84423', 'number': '', 'volume': '13', 'year': '2025', 'title': 'Novel Multi-Scale Attention Generative Adversarial Network for Photovoltaic Solar Cell Defect Inspection Using Electroluminescence Images', 'journal': 'IEEE Access', 'author': 'Guan, Yuanjun and Liu, Yang and Wang, Jiayi and Wang, Tao and Yi, Qianchuan and Jiang, Wenxin and Gu, Xiaopu and Zhang, Yichen and Zhang, Li and Han, Tianyan and Huang, Binbing and Hu, Lilei', 'ENTRYTYPE': 'article', 'ID': '10979306'}"
9454287,Learning Across Tasks for Zero-Shot Domain Adaptation From a Single Source Domain,"Wang, Jinghua and Jiang, Jianmin",Wang,10.1109/TPAMI.2021.3088859,2022,IEEE Transactions on Pattern Analysis and Machine Intelligence,"Domain adaptation techniques learn transferable knowledge from a source domain to a target domain and train models that generalize well in the target domain. Unfortunately, a majority of the existing techniques are only applicable to scenarios that the target-domain data in the task of interest is available for training, yet this is not often true in practice. In general, human beings are experts in generalization across domains. For example, a baby can easily identify the bear from a clipart image after learning this category of animal from the photo images. To reduce the gap between the generalization ability of human and that of machines, we propose a new solution to the challenging zero-shot domain adaptation (ZSDA) problem, where only a single source domain is available and the target domain for the task of interest is not accessible. Inspired by the observation that the knowledge about domain correlation can improve our generalization ability, we explore the correlation between source domain and target domain in an irrelevant knowledge task ($ \mathbb {K}$K-task), where dual-domain samples are available. We denote the task of interest as the question task ($ \mathbb {Q}$Q-task) and synthesize its non-accessible target-domain as such that these two tasks have the shared domain correlation. In order to realize our idea, we introduce a new network structure, i.e., conditional coupled generative adversarial networks (CoCoGAN), by extending the coupled generative adversarial networks (CoGAN) into a conditioning model. With a pair of coupling GANs, our CoCoGAN is able to capture the joint distribution of data samples across two domains and two tasks. For CoCoGAN training in a ZSDA task, we introduce three supervisory signals, i.e., semantic relationship consistency across domains, global representation alignment across tasks, and alignment consistency across domains. Experimental results demonstrate that our method can learn a suitable model for the non-accessible target domain and outperforms the existing state of the arts in both image classification and semantic segmentation.",Task analysis;Correlation;Training;Generative adversarial networks;Animals;Semantics;Adaptation models;Domain adaptation;zero-shot learning;adversarial learning;generative adversarial networks;domain generalization,"{'month': 'Oct', 'issn': '1939-3539', 'doi': '10.1109/TPAMI.2021.3088859', 'keywords': 'Task analysis;Correlation;Training;Generative adversarial networks;Animals;Semantics;Adaptation models;Domain adaptation;zero-shot learning;adversarial learning;generative adversarial networks;domain generalization', 'abstract': 'Domain adaptation techniques learn transferable knowledge from a source domain to a target domain and train models that generalize well in the target domain. Unfortunately, a majority of the existing techniques are only applicable to scenarios that the target-domain data in the task of interest is available for training, yet this is not often true in practice. In general, human beings are experts in generalization across domains. For example, a baby can easily identify the bear from a clipart image after learning this category of animal from the photo images. To reduce the gap between the generalization ability of human and that of machines, we propose a new solution to the challenging zero-shot domain adaptation (ZSDA) problem, where only a single source domain is available and the target domain for the task of interest is not accessible. Inspired by the observation that the knowledge about domain correlation can improve our generalization ability, we explore the correlation between source domain and target domain in an irrelevant knowledge task ($ \\mathbb {K}$K-task), where dual-domain samples are available. We denote the task of interest as the question task ($ \\mathbb {Q}$Q-task) and synthesize its non-accessible target-domain as such that these two tasks have the shared domain correlation. In order to realize our idea, we introduce a new network structure, i.e., conditional coupled generative adversarial networks (CoCoGAN), by extending the coupled generative adversarial networks (CoGAN) into a conditioning model. With a pair of coupling GANs, our CoCoGAN is able to capture the joint distribution of data samples across two domains and two tasks. For CoCoGAN training in a ZSDA task, we introduce three supervisory signals, i.e., semantic relationship consistency across domains, global representation alignment across tasks, and alignment consistency across domains. Experimental results demonstrate that our method can learn a suitable model for the non-accessible target domain and outperforms the existing state of the arts in both image classification and semantic segmentation.', 'pages': '6264-6279', 'number': '10', 'volume': '44', 'year': '2022', 'title': 'Learning Across Tasks for Zero-Shot Domain Adaptation From a Single Source Domain', 'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence', 'author': 'Wang, Jinghua and Jiang, Jianmin', 'ENTRYTYPE': 'article', 'ID': '9454287'}"
10936654,A Novel Approach to Generative AI-based Optimized Code Generation for Semiconductor Equipment Interfaces,"Lee, HyeokSoo and Yang, Minyeol and Jeong, Jongpil",Lee,10.23919/ICACT63878.2025.10936654,2025,2025 27th International Conference on Advanced Communications Technology (ICACT),"Generally, standardization defined by the SEMI association is well established and utilized in the semiconductor industry. In particular, most semiconductor equipment supports SECS / GEM communication protocols, and the automation and smart factory construction consist of equipment communication control programs using these standard protocols. We propose to improve development efficiency by automatically generating control program code using generative artificial intelligence technology to develop interface programs that control these semiconductor facilities. In addition, to improve the completeness and utilization of the automatically generated code, this paper presents a method to automatically generate semiconductor equipment control interface codes through generative artificial intelligence based on existing codes and minimize the constraints that may occur due to the hallucination effect, which is a significant weakness of generative artificial intelligence.",Codes;Protocols;Generative AI;Databases;Large language models;Retrieval augmented generation;Electronics industry;Documentation;Standards;Smart manufacturing;Generative AI;Code Generation;LLM (Large Language Model);RAG (Retrieval Augmented Generation);Prompt,"{'month': 'Feb', 'issn': '1738-9445', 'doi': '10.23919/ICACT63878.2025.10936654', 'keywords': 'Codes;Protocols;Generative AI;Databases;Large language models;Retrieval augmented generation;Electronics industry;Documentation;Standards;Smart manufacturing;Generative AI;Code Generation;LLM (Large Language Model);RAG (Retrieval Augmented Generation);Prompt', 'abstract': 'Generally, standardization defined by the SEMI association is well established and utilized in the semiconductor industry. In particular, most semiconductor equipment supports SECS / GEM communication protocols, and the automation and smart factory construction consist of equipment communication control programs using these standard protocols. We propose to improve development efficiency by automatically generating control program code using generative artificial intelligence technology to develop interface programs that control these semiconductor facilities. In addition, to improve the completeness and utilization of the automatically generated code, this paper presents a method to automatically generate semiconductor equipment control interface codes through generative artificial intelligence based on existing codes and minimize the constraints that may occur due to the hallucination effect, which is a significant weakness of generative artificial intelligence.', 'pages': '1-6', 'number': '', 'volume': '', 'year': '2025', 'title': 'A Novel Approach to Generative AI-based Optimized Code Generation for Semiconductor Equipment Interfaces', 'booktitle': '2025 27th International Conference on Advanced Communications Technology (ICACT)', 'author': 'Lee, HyeokSoo and Yang, Minyeol and Jeong, Jongpil', 'ENTRYTYPE': 'inproceedings', 'ID': '10936654'}"
11058678,"Towards a Framework for Intelligent Sampling: Comprehensive Review of Challenges, AI Techniques, and Tools","Bonilla, Lander and Aguirre-Usandizaga, Jon and Garcia-Perez, Asier and Osa, María José López and Belacortu, Idoia Murua and Diaz-de-Arcaya, J osu and Aroca, Jordi Arjona and Torre-Bastida, Ana Isabel and Milioñ, Raul and Almeida, Aitor",Bonilla,10.1109/SMARTCOMP65954.2025.00096,2025,2025 IEEE International Conference on Smart Computing (SMARTCOMP),"Intelligent data sampling is an innovative method that enhances conventional data sampling procedures by utilizing machine learning and artificial intelligence approaches. In this manuscript, we deep dive into the scientific and grey literature to find the main challenges faced by data sampling and elaborate on the various AI techniques utilized to mitigate them. We identify key issues such as class imbalance, overfitting, computational inefficiency, and bias, which often hinder traditional sampling methods. Furthermore, we explore AI-driven techniques that have been integrated into the sampling process to address these challenges effectively. As a result, we propose a novel framework for intelligent sampling that incorporates an AI-powered recommender system. This system dynamically selects the most appropriate sampling technique based on the specific characteristics of the data and the needs of the predictive model. By automating and optimizing the selection of sampling methods, our framework aims to enhance model performance, improve resource efficiency, and adapt to diverse real-world applications.",Adaptation models;Reviews;Computational modeling;Active learning;Machine learning;Predictive models;Sampling methods;Generative adversarial networks;Recommender systems;Overfitting;Intelligent Data Sampling;AI techniques;Framework;Challenges;Generative Adversarial Networks;Active Learning,"{'month': 'June', 'issn': '2693-8340', 'doi': '10.1109/SMARTCOMP65954.2025.00096', 'keywords': 'Adaptation models;Reviews;Computational modeling;Active learning;Machine learning;Predictive models;Sampling methods;Generative adversarial networks;Recommender systems;Overfitting;Intelligent Data Sampling;AI techniques;Framework;Challenges;Generative Adversarial Networks;Active Learning', 'abstract': 'Intelligent data sampling is an innovative method that enhances conventional data sampling procedures by utilizing machine learning and artificial intelligence approaches. In this manuscript, we deep dive into the scientific and grey literature to find the main challenges faced by data sampling and elaborate on the various AI techniques utilized to mitigate them. We identify key issues such as class imbalance, overfitting, computational inefficiency, and bias, which often hinder traditional sampling methods. Furthermore, we explore AI-driven techniques that have been integrated into the sampling process to address these challenges effectively. As a result, we propose a novel framework for intelligent sampling that incorporates an AI-powered recommender system. This system dynamically selects the most appropriate sampling technique based on the specific characteristics of the data and the needs of the predictive model. By automating and optimizing the selection of sampling methods, our framework aims to enhance model performance, improve resource efficiency, and adapt to diverse real-world applications.', 'pages': '504-509', 'number': '', 'volume': '', 'year': '2025', 'title': 'Towards a Framework for Intelligent Sampling: Comprehensive Review of Challenges, AI Techniques, and Tools', 'booktitle': '2025 IEEE International Conference on Smart Computing (SMARTCOMP)', 'author': 'Bonilla, Lander and Aguirre-Usandizaga, Jon and Garcia-Perez, Asier and Osa, María José López and Belacortu, Idoia Murua and Diaz-de-Arcaya, J osu and Aroca, Jordi Arjona and Torre-Bastida, Ana Isabel and Milioñ, Raul and Almeida, Aitor', 'ENTRYTYPE': 'inproceedings', 'ID': '11058678'}"
9376904,Unsupervised Adversarial Instance-Level Image Retrieval,"Bai, Cong and Li, Hongkai and Zhang, Jinglin and Huang, Ling and Zhang, Lu",Bai,10.1109/TMM.2021.3065578,2021,IEEE Transactions on Multimedia,"With the wide use of visual sensors in the Internet of Things (IoT) in the past decades, huge amounts of images are captured in people's daily lives, which poses challenges to traditional deep-learning-based image retrieval frameworks. Most such frameworks need a large amount of annotated training data, which are expensive. Moreover, machines still lack human intelligence, as illustrated by the fact that they pay less attention to the interesting regions that humans generally focus on when searching for images. Hence, this paper proposes a novel unsupervised framework that focuses on the instance object in the image and integrates human intelligence into the deep-learning-based image retrieval. This framework is called adversarial instance-level image retrieval (AILIR). We incorporate adversarial training and an attention mechanism into this framework that considers human intelligence with artificial intelligence. The generator and discriminator are redesigned to guarantee that the generator retrieves similar images while the discriminator selects unmatched images and creates an adversarial reward for the generator. A minimax game is conducted by the adversarial reward retrieval mechanism until the discriminator is unable to judge whether the image sequence retrieved matches the query. Comparison and ablation experiments on four benchmark datasets prove that the proposed adversarial training framework indeed improves instance retrieval and outperforms the state-of-the-art methods focused on instance retrieval.",Image retrieval;Training;Generators;Generative adversarial networks;Feature extraction;Gallium nitride;Task analysis;Generative adversarial training;human intelligence simulation;instance level image retrieval;unsupervised training,"{'month': '', 'issn': '1941-0077', 'doi': '10.1109/TMM.2021.3065578', 'keywords': 'Image retrieval;Training;Generators;Generative adversarial networks;Feature extraction;Gallium nitride;Task analysis;Generative adversarial training;human intelligence simulation;instance level image retrieval;unsupervised training', 'abstract': ""With the wide use of visual sensors in the Internet of Things (IoT) in the past decades, huge amounts of images are captured in people's daily lives, which poses challenges to traditional deep-learning-based image retrieval frameworks. Most such frameworks need a large amount of annotated training data, which are expensive. Moreover, machines still lack human intelligence, as illustrated by the fact that they pay less attention to the interesting regions that humans generally focus on when searching for images. Hence, this paper proposes a novel unsupervised framework that focuses on the instance object in the image and integrates human intelligence into the deep-learning-based image retrieval. This framework is called adversarial instance-level image retrieval (AILIR). We incorporate adversarial training and an attention mechanism into this framework that considers human intelligence with artificial intelligence. The generator and discriminator are redesigned to guarantee that the generator retrieves similar images while the discriminator selects unmatched images and creates an adversarial reward for the generator. A minimax game is conducted by the adversarial reward retrieval mechanism until the discriminator is unable to judge whether the image sequence retrieved matches the query. Comparison and ablation experiments on four benchmark datasets prove that the proposed adversarial training framework indeed improves instance retrieval and outperforms the state-of-the-art methods focused on instance retrieval."", 'pages': '2199-2207', 'number': '', 'volume': '23', 'year': '2021', 'title': 'Unsupervised Adversarial Instance-Level Image Retrieval', 'journal': 'IEEE Transactions on Multimedia', 'author': 'Bai, Cong and Li, Hongkai and Zhang, Jinglin and Huang, Ling and Zhang, Lu', 'ENTRYTYPE': 'article', 'ID': '9376904'}"
10578933,Leveraging GenAI for an Intelligent Tutoring System for R: A Quantitative Evaluation of Large Language Models,"Frank, Lukas and Herth, Fabian and Stuwe, Paul and Klaiber, Marco and Gerschner, Felix and Theissler, Andreas",Frank,10.1109/EDUCON60312.2024.10578933,2024,2024 IEEE Global Engineering Education Conference (EDUCON),"The tremendous advances in Artificial Intelligence (AI) open new opportunities for education, with Intelligent Tutoring Systems (ITS) powered by Generative Artificial Intelligence (GenAI) proving to be a promising prospect. Because of this, our work explores state-of-the-art (SOTA) ITS approaches with the integration of Large Language Models (LLMs) to improve programming education. We investigate whether and how a GenAI-based ITS can effectively support students in learning R programming skills. We measured the performance of three current pairings of LLMs and user interfaces: GPT-3.5 via ChatGPT, PaLM 2 via Google Bard, and GPT-4 via Bing. Therefore, we evaluated the LLMs on four types of problem settings when learning/teaching programming. Our experimental results show that the use of generative AI, specifically LLMs for R programming, is promising, where GPT-3.5 yielded the most satisfactory results. Furthermore, the advantages and limitations of our approach are addressed and revealed. Finally, open research directions towards explainable AI (XAI) and integrated self-assessment are pointed out.",Generative AI;Explainable AI;Current measurement;Benchmark testing;Chatbots;Internet;Task analysis;Generative AI;AI in Education;Intelligent Tutoring Systems;R Programming;Student Support,"{'month': 'May', 'issn': '2165-9567', 'doi': '10.1109/EDUCON60312.2024.10578933', 'keywords': 'Generative AI;Explainable AI;Current measurement;Benchmark testing;Chatbots;Internet;Task analysis;Generative AI;AI in Education;Intelligent Tutoring Systems;R Programming;Student Support', 'abstract': 'The tremendous advances in Artificial Intelligence (AI) open new opportunities for education, with Intelligent Tutoring Systems (ITS) powered by Generative Artificial Intelligence (GenAI) proving to be a promising prospect. Because of this, our work explores state-of-the-art (SOTA) ITS approaches with the integration of Large Language Models (LLMs) to improve programming education. We investigate whether and how a GenAI-based ITS can effectively support students in learning R programming skills. We measured the performance of three current pairings of LLMs and user interfaces: GPT-3.5 via ChatGPT, PaLM 2 via Google Bard, and GPT-4 via Bing. Therefore, we evaluated the LLMs on four types of problem settings when learning/teaching programming. Our experimental results show that the use of generative AI, specifically LLMs for R programming, is promising, where GPT-3.5 yielded the most satisfactory results. Furthermore, the advantages and limitations of our approach are addressed and revealed. Finally, open research directions towards explainable AI (XAI) and integrated self-assessment are pointed out.', 'pages': '1-9', 'number': '', 'volume': '', 'year': '2024', 'title': 'Leveraging GenAI for an Intelligent Tutoring System for R: A Quantitative Evaluation of Large Language Models', 'booktitle': '2024 IEEE Global Engineering Education Conference (EDUCON)', 'author': 'Frank, Lukas and Herth, Fabian and Stuwe, Paul and Klaiber, Marco and Gerschner, Felix and Theissler, Andreas', 'ENTRYTYPE': 'inproceedings', 'ID': '10578933'}"
9366373,Vehicle Trajectory Prediction Using Generative Adversarial Network With Temporal Logic Syntax Tree Features,"Li, Xiao and Rosman, Guy and Gilitschenski, Igor and Vasile, Cristian-Ioan and DeCastro, Jonathan A. and Karaman, Sertac and Rus, Daniela",Li,10.1109/LRA.2021.3062807,2021,IEEE Robotics and Automation Letters,"In this work, we propose a novel approach for integrating rules into traffic agent trajectory prediction. Consideration of rules is important for understanding how people behave-yet, it cannot be assumed that rules are always followed. To address this challenge, we evaluate different approaches of integrating rules as inductive biases into deep learning-based prediction models. We propose a framework based on generative adversarial networks that uses tools from formal methods, namely signal temporal logic and syntax trees. This allows us to leverage information on rule obedience as features in neural networks and improves prediction accuracy without biasing towards lawful behavior. We evaluate our method on a real-world driving dataset and show improvement in performance over off-the-shelf predictors.",Trajectory;Syntactics;Predictive models;Generators;Feature extraction;Robustness;Planning;Autonomous-driving;prediction;temporal logic,"{'month': 'April', 'issn': '2377-3766', 'doi': '10.1109/LRA.2021.3062807', 'keywords': 'Trajectory;Syntactics;Predictive models;Generators;Feature extraction;Robustness;Planning;Autonomous-driving;prediction;temporal logic', 'abstract': 'In this work, we propose a novel approach for integrating rules into traffic agent trajectory prediction. Consideration of rules is important for understanding how people behave-yet, it cannot be assumed that rules are always followed. To address this challenge, we evaluate different approaches of integrating rules as inductive biases into deep learning-based prediction models. We propose a framework based on generative adversarial networks that uses tools from formal methods, namely signal temporal logic and syntax trees. This allows us to leverage information on rule obedience as features in neural networks and improves prediction accuracy without biasing towards lawful behavior. We evaluate our method on a real-world driving dataset and show improvement in performance over off-the-shelf predictors.', 'pages': '3459-3466', 'number': '2', 'volume': '6', 'year': '2021', 'title': 'Vehicle Trajectory Prediction Using Generative Adversarial Network With Temporal Logic Syntax Tree Features', 'journal': 'IEEE Robotics and Automation Letters', 'author': 'Li, Xiao and Rosman, Guy and Gilitschenski, Igor and Vasile, Cristian-Ioan and DeCastro, Jonathan A. and Karaman, Sertac and Rus, Daniela', 'ENTRYTYPE': 'article', 'ID': '9366373'}"
9261530,Designing a Subsystem for Creating a Three-dimensional Model of an Orthopedic Insole Based on Data from a Laser 3D Scanning of the Patient's Feet,"Voronov, V. I. and Dovgolevskiy, P. A.",Voronov,10.1109/EMCTECH49634.2020.9261530,2020,2020 International Conference on Engineering Management of Communication and Technology (EMCTECH),"The article discusses the use of artificial intelligence methods for the design of orthopedic structures intended for the therapy and treatment of various pathologies of the human skeleton. The development of a software package for creating a three-dimensional model of an orthopedic insole based on an image of a patient's foot obtained using a specialized 3D scanner is described. The model was built using a modified generative adversarial network (GAN) Pix2Pix. This type of networks is used for the first time to obtain medically significant results in the field of orthopedics. The problems associated with obtaining a dataset suitable for neural network modeling are considered. Methods are described that make it possible to bring this set to a form suitable for training and further use in neural network modeling. A software module-data loader has been designed and implemented, which allows converting a three-dimensional model into a numpy array of a depth map with subsequent use as a training set. For the preprocessing of the original images, the authors used a wide range of methods, including dimensionality reduction, noise suppression (Gaussian filter). The results of neural network modeling of an orthopedic insole are presented. The presence of a three-dimensional model of the insole will allow it to be made on an industrial milling machine or printed on a printer using specialized medical material.",Training;Three-dimensional displays;Solid modeling;Software packages;Generators;Generative adversarial networks;Measurement by laser beam;software package;generative adversarial neural network;GAN;orthopedics;data preprocessing;software architecture;orthopedic insoles,"{'month': 'Oct', 'issn': '', 'doi': '10.1109/EMCTECH49634.2020.9261530', 'keywords': 'Training;Three-dimensional displays;Solid modeling;Software packages;Generators;Generative adversarial networks;Measurement by laser beam;software package;generative adversarial neural network;GAN;orthopedics;data preprocessing;software architecture;orthopedic insoles', 'abstract': ""The article discusses the use of artificial intelligence methods for the design of orthopedic structures intended for the therapy and treatment of various pathologies of the human skeleton. The development of a software package for creating a three-dimensional model of an orthopedic insole based on an image of a patient's foot obtained using a specialized 3D scanner is described. The model was built using a modified generative adversarial network (GAN) Pix2Pix. This type of networks is used for the first time to obtain medically significant results in the field of orthopedics. The problems associated with obtaining a dataset suitable for neural network modeling are considered. Methods are described that make it possible to bring this set to a form suitable for training and further use in neural network modeling. A software module-data loader has been designed and implemented, which allows converting a three-dimensional model into a numpy array of a depth map with subsequent use as a training set. For the preprocessing of the original images, the authors used a wide range of methods, including dimensionality reduction, noise suppression (Gaussian filter). The results of neural network modeling of an orthopedic insole are presented. The presence of a three-dimensional model of the insole will allow it to be made on an industrial milling machine or printed on a printer using specialized medical material."", 'pages': '1-6', 'number': '', 'volume': '', 'year': '2020', 'title': ""Designing a Subsystem for Creating a Three-dimensional Model of an Orthopedic Insole Based on Data from a Laser 3D Scanning of the Patient's Feet"", 'booktitle': '2020 International Conference on Engineering Management of Communication and Technology (EMCTECH)', 'author': 'Voronov, V. I. and Dovgolevskiy, P. A.', 'ENTRYTYPE': 'inproceedings', 'ID': '9261530'}"
11082009,Generative Adversarial Imitation Learning Method Based on TD3-SAC Hybrid Algorithm for Robot Motion Control,"Ben Amarat, Samia and Shi, Chaoxia and Wang, Yanqing",Ben Amarat,10.1109/ICAIBD64986.2025.11082009,2025,2025 8th International Conference on Artificial Intelligence and Big Data (ICAIBD),"The dependency on expert demonstrations and the difficulty in surpassing expert performance have been significant limitations in the field of imitation learning. This paper presents a TD3-SAC Hybrid Method that combines the advantages of two advanced RL algorithms, TD3 and SAC, integrating their exploration mechanisms to enhance the exploration capability of Generative Adversarial Imitation Learning (GAIL, an imitation learning method using generative adversarial networks). Furthermore, we designed a closed-loop method for generating expert demonstrations through a learning-based data generation and feedback approach, and proposed a GAIL model training pipeline based on TD3-SAC Hybrid Method, ultimately forming a robust robot motion control method. Through comparative experiments, we demonstrated the effectiveness of the TD3- SAC hybrid method in GAIL. It outperforms baselines such as C-GAIL, GDSG, DAC, and methods combining classical RL algorithms with GAIL, achieving better performance, overcoming the limitations of expert demonstrations and demonstrating a higher probability of outperforming expert demonstrations in MuJoCo simulation environments (46\% in HalfCheetah, 35\% in LunarLander, 14\% in Walker2d, and 21\% in Hopper). This method not only retains the advantages of imitation learning but also equips the agent with the necessary exploration mechanisms to overcome the limitations of expert-based learning, particularly in challenging and complex environments.",Robot motion;Training;Navigation;Imitation learning;Pipelines;Noise;Reinforcement learning;Generative adversarial networks;Hybrid power systems;Entropy;GAIL;Reinforcement learning;SAC;TD3,"{'month': 'May', 'issn': '2769-3554', 'doi': '10.1109/ICAIBD64986.2025.11082009', 'keywords': 'Robot motion;Training;Navigation;Imitation learning;Pipelines;Noise;Reinforcement learning;Generative adversarial networks;Hybrid power systems;Entropy;GAIL;Reinforcement learning;SAC;TD3', 'abstract': 'The dependency on expert demonstrations and the difficulty in surpassing expert performance have been significant limitations in the field of imitation learning. This paper presents a TD3-SAC Hybrid Method that combines the advantages of two advanced RL algorithms, TD3 and SAC, integrating their exploration mechanisms to enhance the exploration capability of Generative Adversarial Imitation Learning (GAIL, an imitation learning method using generative adversarial networks). Furthermore, we designed a closed-loop method for generating expert demonstrations through a learning-based data generation and feedback approach, and proposed a GAIL model training pipeline based on TD3-SAC Hybrid Method, ultimately forming a robust robot motion control method. Through comparative experiments, we demonstrated the effectiveness of the TD3- SAC hybrid method in GAIL. It outperforms baselines such as C-GAIL, GDSG, DAC, and methods combining classical RL algorithms with GAIL, achieving better performance, overcoming the limitations of expert demonstrations and demonstrating a higher probability of outperforming expert demonstrations in MuJoCo simulation environments (46\\% in HalfCheetah, 35\\% in LunarLander, 14\\% in Walker2d, and 21\\% in Hopper). This method not only retains the advantages of imitation learning but also equips the agent with the necessary exploration mechanisms to overcome the limitations of expert-based learning, particularly in challenging and complex environments.', 'pages': '846-852', 'number': '', 'volume': '', 'year': '2025', 'title': 'Generative Adversarial Imitation Learning Method Based on TD3-SAC Hybrid Algorithm for Robot Motion Control', 'booktitle': '2025 8th International Conference on Artificial Intelligence and Big Data (ICAIBD)', 'author': 'Ben Amarat, Samia and Shi, Chaoxia and Wang, Yanqing', 'ENTRYTYPE': 'inproceedings', 'ID': '11082009'}"
10933660,Integrating CLIP with Dynamic Memory Generative Adversarial Networks to Enhance Semantic Consistency in Text-to-Image Generation,"Yan, Dongxia and Cheng, Xien",Yan,10.1109/ICCBD-AI65562.2024.00083,2024,"2024 5th International Conference on Computer, Big Data and Artificial Intelligence (ICCBD+AI)","This paper focuses on the challenge of a lack of strong semantic coherence between generated images and their corresponding text descriptions, as well as the low quality of initial image generation and limited detail capture in the Dynamic Memory Generative Adversarial Networks model. To tackle these issues, we propose the DM-CLGAN model, an enhanced variant of DM-GAN. By incorporating the CLIP model, DM-CLGAN achieves improved alignment between textual and image features. Furthermore, it introduces the Text-Image Affine Combination Module and the Convolutional Block Attention Module to optimize image details and enhance the quality of the initially generated images. In the second stage of the model, the dynamic memory network further refines the generated images. We utilized a quantitative assessment of the model's performance on the CUB-200-2011 dataset by employing key metrics such as Inception Score and Fréchet Inception Distance. The results of our study demonstrate that the generation quality exceeds that of other models, as confirmed by manual evaluation methods. Additionally, the model demonstrates exceptional generation capabilities on our custom-built ceramic wine bottle dataset, further validating the adaptability and generative efficacy of the proposed approach.",Measurement;Adaptation models;Image synthesis;Computational modeling;Semantics;Text to image;Production;Generative adversarial networks;Product design;Ceramics;Text-to-image synthesis;DM-GAN;CLIP;DM-CLGAN;ACM,"{'month': 'Nov', 'issn': '', 'doi': '10.1109/ICCBD-AI65562.2024.00083', 'keywords': 'Measurement;Adaptation models;Image synthesis;Computational modeling;Semantics;Text to image;Production;Generative adversarial networks;Product design;Ceramics;Text-to-image synthesis;DM-GAN;CLIP;DM-CLGAN;ACM', 'abstract': ""This paper focuses on the challenge of a lack of strong semantic coherence between generated images and their corresponding text descriptions, as well as the low quality of initial image generation and limited detail capture in the Dynamic Memory Generative Adversarial Networks model. To tackle these issues, we propose the DM-CLGAN model, an enhanced variant of DM-GAN. By incorporating the CLIP model, DM-CLGAN achieves improved alignment between textual and image features. Furthermore, it introduces the Text-Image Affine Combination Module and the Convolutional Block Attention Module to optimize image details and enhance the quality of the initially generated images. In the second stage of the model, the dynamic memory network further refines the generated images. We utilized a quantitative assessment of the model's performance on the CUB-200-2011 dataset by employing key metrics such as Inception Score and Fréchet Inception Distance. The results of our study demonstrate that the generation quality exceeds that of other models, as confirmed by manual evaluation methods. Additionally, the model demonstrates exceptional generation capabilities on our custom-built ceramic wine bottle dataset, further validating the adaptability and generative efficacy of the proposed approach."", 'pages': '466-471', 'number': '', 'volume': '', 'year': '2024', 'title': 'Integrating CLIP with Dynamic Memory Generative Adversarial Networks to Enhance Semantic Consistency in Text-to-Image Generation', 'booktitle': '2024 5th International Conference on Computer, Big Data and Artificial Intelligence (ICCBD+AI)', 'author': 'Yan, Dongxia and Cheng, Xien', 'ENTRYTYPE': 'inproceedings', 'ID': '10933660'}"
10638024,The Impact of LLM Hallucinations on Motor Skill Learning: A Case Study in Badminton,"Qiu, Yepeng",Qiu,10.1109/ACCESS.2024.3444783,2024,IEEE Access,"The rise of Generative Artificial Intelligence, including Large Language Models (LLMs), has enabled users to engage in self-guided learning of sports skills through conversation-based interactions. However, studies have identified a phenomenon known as “hallucination” in which LLMs generate feedback that is inaccurate or non-existent. While this phenomenon has been observed in various domains, including medicine, academia, and news, its existence and implications in the context of physical exercises, particularly motor skill learning, remain unexplored. This study investigates the presence of LLM hallucinations in badminton skill learning and examines their potential impact on learning outcomes. This study aims to investigate whether LLMs hallucinations exist in the motor skill learning of physical exercises and what impact they may have. Eighty university freshmen with no prior badminton experience participated in a 16-week experiment, with 40 students assigned to the Experimental Group (EG) utilizing LLM-based applications (ChatGPT or New Bing) for self-guided learning, and 40 students in the Control Group (CG) learning under the supervision of 12 university sports teachers and 8 experts that specialized in badminton. Evaluation criteria for badminton skills were established, and assessments were conducted at baseline and 16 weeks using independent sample t-tests and paired-sample t-tests. One-way analysis of variance (One-Way ANCOVA) was employed to compare learning outcomes between the two groups. Interviews were conducted to gain insights into the causes of any observed differences in learning efficiency. Both CG and EG groups demonstrated motor skill improvement (clear: p <0.001; smash: p <0.001; footwork: p <0.001). CG exhibited significantly higher scores in long-distance shots and smashes in the post-test. No significant difference was observed in footwork scores between the two groups. High accordance in specific skill points among students in both groups indicated the common usage of prompts. Interviews with EG students revealed hallucinations in the text generated by LLMs, particularly in the context of “forearm internal rotation swing.” LLMs exhibit hallucinations in the context of intricate motor skill learning, such as badminton, where limited corpus data is available. These hallucinations can mislead users and impact learning outcomes. Future research should explore strategies to mitigate LLM hallucinations in physical exercise learning applications.",Artificial intelligence;Sports;Videos;Social networking (online);Fake news;Large language models;Motor coordination;Large language models;hallucination;motor skill learning;badminton skill,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2024.3444783', 'keywords': 'Artificial intelligence;Sports;Videos;Social networking (online);Fake news;Large language models;Motor coordination;Large language models;hallucination;motor skill learning;badminton skill', 'abstract': 'The rise of Generative Artificial Intelligence, including Large Language Models (LLMs), has enabled users to engage in self-guided learning of sports skills through conversation-based interactions. However, studies have identified a phenomenon known as “hallucination” in which LLMs generate feedback that is inaccurate or non-existent. While this phenomenon has been observed in various domains, including medicine, academia, and news, its existence and implications in the context of physical exercises, particularly motor skill learning, remain unexplored. This study investigates the presence of LLM hallucinations in badminton skill learning and examines their potential impact on learning outcomes. This study aims to investigate whether LLMs hallucinations exist in the motor skill learning of physical exercises and what impact they may have. Eighty university freshmen with no prior badminton experience participated in a 16-week experiment, with 40 students assigned to the Experimental Group (EG) utilizing LLM-based applications (ChatGPT or New Bing) for self-guided learning, and 40 students in the Control Group (CG) learning under the supervision of 12 university sports teachers and 8 experts that specialized in badminton. Evaluation criteria for badminton skills were established, and assessments were conducted at baseline and 16 weeks using independent sample t-tests and paired-sample t-tests. One-way analysis of variance (One-Way ANCOVA) was employed to compare learning outcomes between the two groups. Interviews were conducted to gain insights into the causes of any observed differences in learning efficiency. Both CG and EG groups demonstrated motor skill improvement (clear: p <0.001; smash: p <0.001; footwork: p <0.001). CG exhibited significantly higher scores in long-distance shots and smashes in the post-test. No significant difference was observed in footwork scores between the two groups. High accordance in specific skill points among students in both groups indicated the common usage of prompts. Interviews with EG students revealed hallucinations in the text generated by LLMs, particularly in the context of “forearm internal rotation swing.” LLMs exhibit hallucinations in the context of intricate motor skill learning, such as badminton, where limited corpus data is available. These hallucinations can mislead users and impact learning outcomes. Future research should explore strategies to mitigate LLM hallucinations in physical exercise learning applications.', 'pages': '139669-139682', 'number': '', 'volume': '12', 'year': '2024', 'title': 'The Impact of LLM Hallucinations on Motor Skill Learning: A Case Study in Badminton', 'journal': 'IEEE Access', 'author': 'Qiu, Yepeng', 'ENTRYTYPE': 'article', 'ID': '10638024'}"
10716437,Progressive Boundary Guided Anomaly Synthesis for Industrial Anomaly Detection,"Chen, Qiyu and Luo, Huiyuan and Gao, Han and Lv, Chengkan and Zhang, Zhengtao",Chen,10.1109/TCSVT.2024.3479887,2025,IEEE Transactions on Circuits and Systems for Video Technology,"Unsupervised anomaly detection methods can identify surface defects in industrial images by leveraging only normal samples for training. Due to the risk of overfitting when learning from a single class, anomaly synthesis strategies are introduced to enhance detection capability by generating artificial anomalies. However, existing strategies heavily rely on anomalous textures from auxiliary datasets. Moreover, their limitations in the coverage and directionality of anomaly synthesis may result in a failure to capture useful information and lead to significant redundancy. To address these issues, we propose a novel Progressive Boundary-guided Anomaly Synthesis (PBAS) strategy, which can directionally synthesize crucial feature-level anomalies without auxiliary textures. It consists of three core components: Approximate Boundary Learning (ABL), Anomaly Feature Synthesis (AFS), and Refined Boundary Optimization (RBO). To make the distribution of normal samples more compact, ABL first learns an approximate decision boundary by center constraint, which improves the center initialization through feature alignment. AFS then directionally synthesizes anomalies with more flexible scales guided by the hypersphere distribution of normal features. Since the boundary is so loose that it may contain real anomalies, RBO refines the decision boundary through the binary classification of artificial anomalies and normal features. Experimental results show that our method achieves state-of-the-art performance and the fastest detection speed on three widely used industrial datasets, including MVTec AD, VisA, and MPDD. The code will be available at: https://github.com/cqylunlun/PBAS.",Image reconstruction;Feature extraction;Anomaly detection;Training;Optimization;Learning (artificial intelligence);Gaussian noise;Vectors;Location awareness;Data models;Anomaly detection;industrial images;anomaly synthesis;progressive boundary guidance,"{'month': 'Feb', 'issn': '1558-2205', 'doi': '10.1109/TCSVT.2024.3479887', 'keywords': 'Image reconstruction;Feature extraction;Anomaly detection;Training;Optimization;Learning (artificial intelligence);Gaussian noise;Vectors;Location awareness;Data models;Anomaly detection;industrial images;anomaly synthesis;progressive boundary guidance', 'abstract': 'Unsupervised anomaly detection methods can identify surface defects in industrial images by leveraging only normal samples for training. Due to the risk of overfitting when learning from a single class, anomaly synthesis strategies are introduced to enhance detection capability by generating artificial anomalies. However, existing strategies heavily rely on anomalous textures from auxiliary datasets. Moreover, their limitations in the coverage and directionality of anomaly synthesis may result in a failure to capture useful information and lead to significant redundancy. To address these issues, we propose a novel Progressive Boundary-guided Anomaly Synthesis (PBAS) strategy, which can directionally synthesize crucial feature-level anomalies without auxiliary textures. It consists of three core components: Approximate Boundary Learning (ABL), Anomaly Feature Synthesis (AFS), and Refined Boundary Optimization (RBO). To make the distribution of normal samples more compact, ABL first learns an approximate decision boundary by center constraint, which improves the center initialization through feature alignment. AFS then directionally synthesizes anomalies with more flexible scales guided by the hypersphere distribution of normal features. Since the boundary is so loose that it may contain real anomalies, RBO refines the decision boundary through the binary classification of artificial anomalies and normal features. Experimental results show that our method achieves state-of-the-art performance and the fastest detection speed on three widely used industrial datasets, including MVTec AD, VisA, and MPDD. The code will be available at: https://github.com/cqylunlun/PBAS.', 'pages': '1193-1208', 'number': '2', 'volume': '35', 'year': '2025', 'title': 'Progressive Boundary Guided Anomaly Synthesis for Industrial Anomaly Detection', 'journal': 'IEEE Transactions on Circuits and Systems for Video Technology', 'author': 'Chen, Qiyu and Luo, Huiyuan and Gao, Han and Lv, Chengkan and Zhang, Zhengtao', 'ENTRYTYPE': 'article', 'ID': '10716437'}"
10143174,Validity Improvement in MolGAN-Based Molecular Generation,"Fan, Jiayi and Hong, Seul Ki and Lee, Yongkeun",Fan,10.1109/ACCESS.2023.3282248,2023,IEEE Access,"Designing molecules that have desired properties is one of the challenging tasks of drug design. Among the many molecular generative models, a generative adversarial network (GAN), is able to generate molecule structures with desirable chemical properties via reinforcement learning. Generating valid molecules is the foremost task of any molecular generative model, since invalid molecules cannot be synthesized. We base our research on a molecular generative adversarial network (MolGAN) architecture to investigate how the validity score is influenced in different scenarios. First, we verify that the Vanilla GAN structure can produce valid molecules in measure, and that the reward network, along with Vanilla GAN, can further increase the validity score in a reinforcement learning manner. Then, the procedure for solely optimizing the validity score is tested, followed by an assessment of validity score maintenance while other chemical properties are being optimized. We found that multiple aspects, including loss functions, hyper parameters, and training sequences, must be carefully considered and optimized to raise the validity score of molecular generation alone or in concurrence with the optimizing of other chemical property scores.",Generative adversarial networks;Generators;Chemicals;Tensors;Training;Drugs;Molecular biology;Drug design;molecular generation;generative adversarial network (GAN);molecular generative adversarial network (MolGAN),"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2023.3282248', 'keywords': 'Generative adversarial networks;Generators;Chemicals;Tensors;Training;Drugs;Molecular biology;Drug design;molecular generation;generative adversarial network (GAN);molecular generative adversarial network (MolGAN)', 'abstract': 'Designing molecules that have desired properties is one of the challenging tasks of drug design. Among the many molecular generative models, a generative adversarial network (GAN), is able to generate molecule structures with desirable chemical properties via reinforcement learning. Generating valid molecules is the foremost task of any molecular generative model, since invalid molecules cannot be synthesized. We base our research on a molecular generative adversarial network (MolGAN) architecture to investigate how the validity score is influenced in different scenarios. First, we verify that the Vanilla GAN structure can produce valid molecules in measure, and that the reward network, along with Vanilla GAN, can further increase the validity score in a reinforcement learning manner. Then, the procedure for solely optimizing the validity score is tested, followed by an assessment of validity score maintenance while other chemical properties are being optimized. We found that multiple aspects, including loss functions, hyper parameters, and training sequences, must be carefully considered and optimized to raise the validity score of molecular generation alone or in concurrence with the optimizing of other chemical property scores.', 'pages': '58359-58366', 'number': '', 'volume': '11', 'year': '2023', 'title': 'Validity Improvement in MolGAN-Based Molecular Generation', 'journal': 'IEEE Access', 'author': 'Fan, Jiayi and Hong, Seul Ki and Lee, Yongkeun', 'ENTRYTYPE': 'article', 'ID': '10143174'}"
10614282,AI Revolution: Mastering AI for Personal and Organizational Growth,"Ojanperä, Tero",Ojanperä,,2024,AI Revolution: Mastering AI for Personal and Organizational Growth,"""The AI Revolution"" is a practical guide to using new AI tools, such as ChatGPT, DALLE and Midjourney. Learn how to multiply your productivity by guiding or prompting AI in various ways. The book also introduces Microsoft Copilot, Google Bard, and Adobe Photoshop Generative Fill, among other new applications. ChatGPT reached a hundred million users in just two months after its release, faster than any other application before. This marked the advent of the generative AI era. Generative AI models generate text, images, music, videos, and even 3D models in ways previously thought impossible for machines. The book explains in an understandable manner how these AI models work. The book provides examples of how AI increases productivity, which professions are changing or disappearing, and how job markets will evolve in the coming years. With this book, you'll learn to recognize the opportunities and risks AI offers. Understand what this change demands from individuals and companies and what strategic skills are required. The book also covers legal questions caused by generative AI, like copyrights, data protection, and AI regulation. It also ponders societal impacts. AI produces content, thus influencing language, culture, and even worldviews. Therefore, it's crucial to understand by whom and how AI is trained. The AI revolution started by ChatGPT is just the beginning. This handbook is for you if you want to keep up with the rapid development of AI.",,"{'url': 'https://ieeexplore-ieee-org.crai.referencistas.com/document/10614282', 'isbn': '9788770042314', 'publisher': 'River Publishers', 'issn': '', 'doi': '', 'keywords': '', 'abstract': '""The AI Revolution"" is a practical guide to using new AI tools, such as ChatGPT, DALLE and Midjourney. Learn how to multiply your productivity by guiding or prompting AI in various ways. The book also introduces Microsoft Copilot, Google Bard, and Adobe Photoshop Generative Fill, among other new applications. ChatGPT reached a hundred million users in just two months after its release, faster than any other application before. This marked the advent of the generative AI era. Generative AI models generate text, images, music, videos, and even 3D models in ways previously thought impossible for machines. The book explains in an understandable manner how these AI models work. The book provides examples of how AI increases productivity, which professions are changing or disappearing, and how job markets will evolve in the coming years. With this book, you\'ll learn to recognize the opportunities and risks AI offers. Understand what this change demands from individuals and companies and what strategic skills are required. The book also covers legal questions caused by generative AI, like copyrights, data protection, and AI regulation. It also ponders societal impacts. AI produces content, thus influencing language, culture, and even worldviews. Therefore, it\'s crucial to understand by whom and how AI is trained. The AI revolution started by ChatGPT is just the beginning. This handbook is for you if you want to keep up with the rapid development of AI.', 'pages': 'i-xxvi', 'number': '', 'volume': '', 'year': '2024', 'title': 'AI Revolution: Mastering AI for Personal and Organizational Growth', 'booktitle': 'AI Revolution: Mastering AI for Personal and Organizational Growth', 'author': 'Ojanperä, Tero', 'ENTRYTYPE': 'inbook', 'ID': '10614282'}"
10990147,Empowering Future Spectrum Management and Regulation With Large Language Models,"Rutagemwa, Humphrey and Ghasemi, Amir and Guinand, Paul",Rutagemwa,10.1109/MIS.2025.3567372,2025,IEEE Intelligent Systems,"Spectrum management and regulation are becoming more complex due to rapid technological advancements, increasing demand for spectrum, and the presence of diverse stakeholders with conflicting interests. In response, governments worldwide are increasingly interested in leveraging advanced technologies, such as artificial intelligence (AI), to enhance efficiency and optimize policy outcomes. This article explores the application of large language models (LLMs), a subset of generative AI, to streamline tasks and improve decision making in spectrum management and regulation. It examines the various roles of LLMs in this field and addresses associated challenges. Through empirical case studies and experimental findings, the article demonstrates how LLMs can profoundly transform spectrum management and regulation practices. The study also offers insights into effectively integrating AI into regulatory frameworks, providing practical lessons and best practices for governmental AI initiatives.",Radio spectrum management;Data mining;Artificial intelligence;Regulation;Licenses;Question answering (information retrieval);Interference;Stakeholders;Large language models,"{'month': 'July', 'issn': '1941-1294', 'doi': '10.1109/MIS.2025.3567372', 'keywords': 'Radio spectrum management;Data mining;Artificial intelligence;Regulation;Licenses;Question answering (information retrieval);Interference;Stakeholders;Large language models', 'abstract': 'Spectrum management and regulation are becoming more complex due to rapid technological advancements, increasing demand for spectrum, and the presence of diverse stakeholders with conflicting interests. In response, governments worldwide are increasingly interested in leveraging advanced technologies, such as artificial intelligence (AI), to enhance efficiency and optimize policy outcomes. This article explores the application of large language models (LLMs), a subset of generative AI, to streamline tasks and improve decision making in spectrum management and regulation. It examines the various roles of LLMs in this field and addresses associated challenges. Through empirical case studies and experimental findings, the article demonstrates how LLMs can profoundly transform spectrum management and regulation practices. The study also offers insights into effectively integrating AI into regulatory frameworks, providing practical lessons and best practices for governmental AI initiatives.', 'pages': '46-54', 'number': '4', 'volume': '40', 'year': '2025', 'title': 'Empowering Future Spectrum Management and Regulation With Large Language Models', 'journal': 'IEEE Intelligent Systems', 'author': 'Rutagemwa, Humphrey and Ghasemi, Amir and Guinand, Paul', 'ENTRYTYPE': 'article', 'ID': '10990147'}"
9847028,Probabilistic Neural–Symbolic Models With Inductive Posterior Constraints,"Su, Ke and Su, Hang and Li, Chongxuan and Zhu, Jun and Zhang, Bo",Su,10.1109/TNNLS.2022.3190820,2024,IEEE Transactions on Neural Networks and Learning Systems,"Neural–symbolic models provide a powerful tool to tackle complex visual reasoning tasks by combining symbolic program execution for reasoning and deep representation learning for visual recognition. A probabilistic formulation of such models with stochastic latent variables can obtain an interpretable and legible reasoning system with less supervision. However, it is still nontrivial to generate reasonable symbolic structures without the guidance of domain knowledge, since it generally involves an optimization problem with both continuous and discrete variables. Despite the challenges, the interpretability of such symbolic structures provides an interface to regularize their generation by domain knowledge. In this article, we propose to incorporate the available domain knowledge into the learning process of probabilistic neural–symbolic (PNS) models via posterior constraints that directly regularize the structure posterior. In this way, our model is able to identify a middle point where the structure generation process mainly learns from data but also selectively borrows information from domain knowledge. We further present inductive reasoning where the posterior constraints can be automatically reweighted to handle noisy annotations. The experimental results show that our method achieves state-of-the-art performance on major abstract reasoning datasets and enjoys good generalization capability and data efficiency.",Cognition;Task analysis;Computational modeling;Artificial intelligence;Visualization;Probabilistic logic;Annotations;Abstract reasoning;deep learning;neural-symbolic integration;visual reasoning,"{'month': 'Feb', 'issn': '2162-2388', 'doi': '10.1109/TNNLS.2022.3190820', 'keywords': 'Cognition;Task analysis;Computational modeling;Artificial intelligence;Visualization;Probabilistic logic;Annotations;Abstract reasoning;deep learning;neural-symbolic integration;visual reasoning', 'abstract': 'Neural–symbolic models provide a powerful tool to tackle complex visual reasoning tasks by combining symbolic program execution for reasoning and deep representation learning for visual recognition. A probabilistic formulation of such models with stochastic latent variables can obtain an interpretable and legible reasoning system with less supervision. However, it is still nontrivial to generate reasonable symbolic structures without the guidance of domain knowledge, since it generally involves an optimization problem with both continuous and discrete variables. Despite the challenges, the interpretability of such symbolic structures provides an interface to regularize their generation by domain knowledge. In this article, we propose to incorporate the available domain knowledge into the learning process of probabilistic neural–symbolic (PNS) models via posterior constraints that directly regularize the structure posterior. In this way, our model is able to identify a middle point where the structure generation process mainly learns from data but also selectively borrows information from domain knowledge. We further present inductive reasoning where the posterior constraints can be automatically reweighted to handle noisy annotations. The experimental results show that our method achieves state-of-the-art performance on major abstract reasoning datasets and enjoys good generalization capability and data efficiency.', 'pages': '2667-2679', 'number': '2', 'volume': '35', 'year': '2024', 'title': 'Probabilistic Neural–Symbolic Models With Inductive Posterior Constraints', 'journal': 'IEEE Transactions on Neural Networks and Learning Systems', 'author': 'Su, Ke and Su, Hang and Li, Chongxuan and Zhu, Jun and Zhang, Bo', 'ENTRYTYPE': 'article', 'ID': '9847028'}"
9312402,A Two-Stage Unsupervised Approach for Low Light Image Enhancement,"Hu, Junjie and Guo, Xiyue and Chen, Junfeng and Liang, Guanqi and Deng, Fuqin and Lam, Tin Lun",Hu,10.1109/LRA.2020.3048667,2021,IEEE Robotics and Automation Letters,"As vision based perception methods are usually built on the normal light assumption, there will be a serious safety issue when deploying them into low light environments. Recently, deep learning based methods have been proposed to enhance low light images by penalizing the pixel-wise loss of low light and normal light images. However, most of them suffer from the following problems: 1) the need of pairs of low light and normal light images for training, 2) the poor performance for dark images, 3) the amplification of noise. To alleviate these problems, in this letter, we propose a two-stage unsupervised method that decomposes the low light image enhancement into a pre-enhancement and a post-refinement problem. In the first stage, we pre-enhance a low light image with a conventional Retinex based method. In the second stage, we use a refinement network learned with adversarial training for further improvement of the image quality. The experimental results show that our method outperforms previous methods on four benchmark datasets. In addition, we show that our method can significantly improve feature points matching and simultaneous localization and mapping in low light conditions.",Lighting;Image enhancement;Noise reduction;Training;Simultaneous localization and mapping;Robots;Image quality;Low light image enhancement;robot’s perception;SLAM;unsupervised method,"{'month': 'Oct', 'issn': '2377-3766', 'doi': '10.1109/LRA.2020.3048667', 'keywords': 'Lighting;Image enhancement;Noise reduction;Training;Simultaneous localization and mapping;Robots;Image quality;Low light image enhancement;robot’s perception;SLAM;unsupervised method', 'abstract': 'As vision based perception methods are usually built on the normal light assumption, there will be a serious safety issue when deploying them into low light environments. Recently, deep learning based methods have been proposed to enhance low light images by penalizing the pixel-wise loss of low light and normal light images. However, most of them suffer from the following problems: 1) the need of pairs of low light and normal light images for training, 2) the poor performance for dark images, 3) the amplification of noise. To alleviate these problems, in this letter, we propose a two-stage unsupervised method that decomposes the low light image enhancement into a pre-enhancement and a post-refinement problem. In the first stage, we pre-enhance a low light image with a conventional Retinex based method. In the second stage, we use a refinement network learned with adversarial training for further improvement of the image quality. The experimental results show that our method outperforms previous methods on four benchmark datasets. In addition, we show that our method can significantly improve feature points matching and simultaneous localization and mapping in low light conditions.', 'pages': '8363-8370', 'number': '4', 'volume': '6', 'year': '2021', 'title': 'A Two-Stage Unsupervised Approach for Low Light Image Enhancement', 'journal': 'IEEE Robotics and Automation Letters', 'author': 'Hu, Junjie and Guo, Xiyue and Chen, Junfeng and Liang, Guanqi and Deng, Fuqin and Lam, Tin Lun', 'ENTRYTYPE': 'article', 'ID': '9312402'}"
10552736,Art Innovation or Plagiarism? Chinese Students’ Attitudes Toward AI Painting Technology and Influencing Factors,"Wang, Changsheng",Wang,10.1109/ACCESS.2024.3412176,2024,IEEE Access,"The increasing integration of artificial intelligence (AI) in art, particularly AI painting technology, has captivated significant attention and sparked debate. However, little is understood about the attitudes of Chinese students toward this technology and the factors influencing their perspectives. This study employed a mixed-methods approach to comprehensively appraise Chinese students’ attitudes toward AI painting technology and the reasons behind these viewpoints. Data was collected from five universities and three high schools in China through questionnaire surveys and semi-structured interviews. Quantitative analysis demonstrated clear trends in students’ attitudes toward AI painting technology, with gender, educational level, and background in art and design identified as significant influencing factors. Specifically, students with higher levels of education demonstrated more favorable attitudes toward AI painting technology. This was evidenced by a strong positive correlation coefficient of 0.644 (p<0.01) between educational attainment and positive perceptions of this technology; whereas, a negative correlation with gender (coefficient of −0.263, p<0.01) indicated a difference in attitudes between male and female students, with males displaying more positive views. Specifically, background in art and design did not appear to significantly affect students’ attitudes, as presented by an insignificant correlation coefficient of −0.048 (p>0.05). In addition, regression analysis, with an R2 value of 0.419, suggests that these variables can account for 41.9\% of the variance in student attitudes toward AI painting, emphasizing the significant effect of gender and education level on their perspectives. Qualitative findings further indicated that concerns about copyright ethics, job displacement anxieties, personal values and aesthetic viewpoints, and broader social and environmental implications all affected students’ attitudes toward AI painting technology. These findings offer valuable insights into the attitudes toward AI-generated technologies.",Artificial intelligence;Painting;Art;Surveys;Training;Interviews;Collaboration;Educational programs;AI art;AI painting;student attitudes;education and AI;mixed methods research,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2024.3412176', 'keywords': 'Artificial intelligence;Painting;Art;Surveys;Training;Interviews;Collaboration;Educational programs;AI art;AI painting;student attitudes;education and AI;mixed methods research', 'abstract': 'The increasing integration of artificial intelligence (AI) in art, particularly AI painting technology, has captivated significant attention and sparked debate. However, little is understood about the attitudes of Chinese students toward this technology and the factors influencing their perspectives. This study employed a mixed-methods approach to comprehensively appraise Chinese students’ attitudes toward AI painting technology and the reasons behind these viewpoints. Data was collected from five universities and three high schools in China through questionnaire surveys and semi-structured interviews. Quantitative analysis demonstrated clear trends in students’ attitudes toward AI painting technology, with gender, educational level, and background in art and design identified as significant influencing factors. Specifically, students with higher levels of education demonstrated more favorable attitudes toward AI painting technology. This was evidenced by a strong positive correlation coefficient of 0.644 (p<0.01) between educational attainment and positive perceptions of this technology; whereas, a negative correlation with gender (coefficient of −0.263, p<0.01) indicated a difference in attitudes between male and female students, with males displaying more positive views. Specifically, background in art and design did not appear to significantly affect students’ attitudes, as presented by an insignificant correlation coefficient of −0.048 (p>0.05). In addition, regression analysis, with an R2 value of 0.419, suggests that these variables can account for 41.9\\% of the variance in student attitudes toward AI painting, emphasizing the significant effect of gender and education level on their perspectives. Qualitative findings further indicated that concerns about copyright ethics, job displacement anxieties, personal values and aesthetic viewpoints, and broader social and environmental implications all affected students’ attitudes toward AI painting technology. These findings offer valuable insights into the attitudes toward AI-generated technologies.', 'pages': '85795-85805', 'number': '', 'volume': '12', 'year': '2024', 'title': 'Art Innovation or Plagiarism? Chinese Students’ Attitudes Toward AI Painting Technology and Influencing Factors', 'journal': 'IEEE Access', 'author': 'Wang, Changsheng', 'ENTRYTYPE': 'article', 'ID': '10552736'}"
10585442,Unleashing Generative Non-Player Characters in Video Games: An AI Act Perspective,"Sas, Martin",Sas,10.1109/GEM61861.2024.10585442,2024,"2024 IEEE Gaming, Entertainment, and Media Conference (GEM)","The increased integration of large language models and generative AI in non-player characters (NPCs) is radically transforming the gaming industry. While these new technologies can enhance the immersiveness of the game experience, generative NPCs also introduce substantial risks of damaging misbehaviour. In this paper, we highlight the role of each actor along the supply chain of generative non-player characters (NPCs) and analyse how the forthcoming AI Act might apply vis-à-vis the allocation of responsibilities. Finally, we highlight unacceptable uses of generative NPCs.",Video games;Law;Large language models;Instruments;Supply chains;Media;Regulation;Generative AI;EU AI Act;video game;Non-player character (NPC);Large language models (LLM),"{'month': 'June', 'issn': '2766-6530', 'doi': '10.1109/GEM61861.2024.10585442', 'keywords': 'Video games;Law;Large language models;Instruments;Supply chains;Media;Regulation;Generative AI;EU AI Act;video game;Non-player character (NPC);Large language models (LLM)', 'abstract': 'The increased integration of large language models and generative AI in non-player characters (NPCs) is radically transforming the gaming industry. While these new technologies can enhance the immersiveness of the game experience, generative NPCs also introduce substantial risks of damaging misbehaviour. In this paper, we highlight the role of each actor along the supply chain of generative non-player characters (NPCs) and analyse how the forthcoming AI Act might apply vis-à-vis the allocation of responsibilities. Finally, we highlight unacceptable uses of generative NPCs.', 'pages': '1-4', 'number': '', 'volume': '', 'year': '2024', 'title': 'Unleashing Generative Non-Player Characters in Video Games: An AI Act Perspective', 'booktitle': '2024 IEEE Gaming, Entertainment, and Media Conference (GEM)', 'author': 'Sas, Martin', 'ENTRYTYPE': 'inproceedings', 'ID': '10585442'}"
10715728,Efficient GAN-Based Federated Optimization for Vehicular Task Offloading With Mobile Edge Computing in 6G Network,"Wu, Chunyi and Li, Lin and Zhang, Li and Gao, Chao and Wu, Xingchen and Xiao, Shan",Wu,10.1109/JIOT.2024.3479285,2025,IEEE Internet of Things Journal,"With the rapid development of 6G network technology and intelligent transportation system (ITS), the edge deployment and lightweight of network applications for modern users have gradually become a possibility. In this work, we propose a mobile intelligent vehicular task offloading method efficient mobile edge computing assisted task offloading using generative adversarial network (MEGAN) based on task representation learning and federated optimization for lightweight task recognition and energy consumption optimization of mobile edge computing (MEC) in 5G/6G transportation networks. The extended dataset of tasks is constructed based on generative adversarial network (GAN) to overcome the problems of data model overfitting and sample imbalance. A task preprocessing model between the edge server and the mobile users is established by using the federated deep learning with the knowledge distillation. The task classification accuracy, energy consumption of signal transmission and data computing are the optimization objectives to realize the MEC and lightweight application deployment. Experimental results show that compared with other state-of-the-art vehicular task offloading methods, the MEGAN method has great potential to promote the efficiency and energy consumption optimization of new transportation service processing in the future.",Optimization;Computational modeling;Generative adversarial networks;Big Data;Resource management;Training;Multi-access edge computing;Energy consumption;Servers;Faces;Federated optimization;generative adversarial network (GAN);mobile edge computing (MEC);representation learning;vehicular task offloading,"{'month': 'Feb', 'issn': '2327-4662', 'doi': '10.1109/JIOT.2024.3479285', 'keywords': 'Optimization;Computational modeling;Generative adversarial networks;Big Data;Resource management;Training;Multi-access edge computing;Energy consumption;Servers;Faces;Federated optimization;generative adversarial network (GAN);mobile edge computing (MEC);representation learning;vehicular task offloading', 'abstract': 'With the rapid development of 6G network technology and intelligent transportation system (ITS), the edge deployment and lightweight of network applications for modern users have gradually become a possibility. In this work, we propose a mobile intelligent vehicular task offloading method efficient mobile edge computing assisted task offloading using generative adversarial network (MEGAN) based on task representation learning and federated optimization for lightweight task recognition and energy consumption optimization of mobile edge computing (MEC) in 5G/6G transportation networks. The extended dataset of tasks is constructed based on generative adversarial network (GAN) to overcome the problems of data model overfitting and sample imbalance. A task preprocessing model between the edge server and the mobile users is established by using the federated deep learning with the knowledge distillation. The task classification accuracy, energy consumption of signal transmission and data computing are the optimization objectives to realize the MEC and lightweight application deployment. Experimental results show that compared with other state-of-the-art vehicular task offloading methods, the MEGAN method has great potential to promote the efficiency and energy consumption optimization of new transportation service processing in the future.', 'pages': '2736-2748', 'number': '3', 'volume': '12', 'year': '2025', 'title': 'Efficient GAN-Based Federated Optimization for Vehicular Task Offloading With Mobile Edge Computing in 6G Network', 'journal': 'IEEE Internet of Things Journal', 'author': 'Wu, Chunyi and Li, Lin and Zhang, Li and Gao, Chao and Wu, Xingchen and Xiao, Shan', 'ENTRYTYPE': 'article', 'ID': '10715728'}"
10823294,Lung Cancer Classification Using CNN with Data Augmentation,"Karunakaran, V and Akshaya, N and Harismita, B and Atchaya, S",Karunakaran,10.1109/ICICNIS64247.2024.10823294,2024,2024 International Conference on IoT Based Control Networks and Intelligent Systems (ICICNIS),"Lung cancer remains a leading cause of cancer-related deaths globally. This study investigates the application of Convolutional Neural Networks (CNNs) for the classification of lung cancer using the IQ-OTH/NCCD lung cancer dataset. The study compares the performance of CNN models with and without data augmentation techniques. Experimental results demonstrate that CNN models with data augmentation exhibit superior performance, achieving higher accuracy, precision, F1-score, and recall compared to models trained without data augmentation. These findings highlight the importance of data augmentation in improving the accuracy and robustness of deep learning models for lung cancer classification, ultimately contributing to early detection and improved patient outcomes.",Measurement;Accuracy;Lung cancer;Data augmentation;Data models;Robustness;Convolutional neural networks;Intelligent systems;Standards;Medical diagnostic imaging;Convolutional Neural Networks;Data Augmentation;Classification and Lung Cancer,"{'month': 'Dec', 'issn': '', 'doi': '10.1109/ICICNIS64247.2024.10823294', 'keywords': 'Measurement;Accuracy;Lung cancer;Data augmentation;Data models;Robustness;Convolutional neural networks;Intelligent systems;Standards;Medical diagnostic imaging;Convolutional Neural Networks;Data Augmentation;Classification and Lung Cancer', 'abstract': 'Lung cancer remains a leading cause of cancer-related deaths globally. This study investigates the application of Convolutional Neural Networks (CNNs) for the classification of lung cancer using the IQ-OTH/NCCD lung cancer dataset. The study compares the performance of CNN models with and without data augmentation techniques. Experimental results demonstrate that CNN models with data augmentation exhibit superior performance, achieving higher accuracy, precision, F1-score, and recall compared to models trained without data augmentation. These findings highlight the importance of data augmentation in improving the accuracy and robustness of deep learning models for lung cancer classification, ultimately contributing to early detection and improved patient outcomes.', 'pages': '1152-1157', 'number': '', 'volume': '', 'year': '2024', 'title': 'Lung Cancer Classification Using CNN with Data Augmentation', 'booktitle': '2024 International Conference on IoT Based Control Networks and Intelligent Systems (ICICNIS)', 'author': 'Karunakaran, V and Akshaya, N and Harismita, B and Atchaya, S', 'ENTRYTYPE': 'inproceedings', 'ID': '10823294'}"
10888884,Radar2ECG: Multi-Scale Bottleneck Fusion and Cross-modal Semantic Distillation for Conditional Electrocardiogram Generation from Radar Heart Sound,"Li, Jinye and Men, Aidong and Liu, Yang and Han, Pengda and Chen, Qingchao",Li,10.1109/ICASSP49660.2025.10888884,2025,"ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","The field of conditional Electrocardiogram(ECG) generation focuses on generating specified ECGs under given conditions for medical purposes. Existing methods are typically based on conditions of simple inputs like text or lead types. However, they struggle to handle the complexity of radar heart sound signals due to the lack of effective feature extraction, which hinders capturing the intricate waveform correlations between radar heart sounds and ECGs. Considering that radar-detected heart sound signals are contactless, the application is of essential value in a real-world deployment like sleep scenarios. Moreover, no prior approaches have addressed this specific task. To tackle this challenge, we propose a novel multi-scale feature fusion network framework, Radar2ECG. This model leverages pre-trained autoencoders for heart sound and ECG signals, aligning and integrating multi-layer features through a bottleneck structure to enhance receptive fields and reduce redundant features, thereby capturing the correlations between heart sounds and ECGs. Finally, we employ knowledge distillation to transfer knowledge from the ECG decoder to the heart sound decoder. We present three anomaly type datasets and extensive experiments conducted on both normal and abnormal datasets demonstrate that our method outperforms existing models in both accuracy and robustness. The multi-scale feature fusion significantly improves performance, showcasing strong potential in ECG generation and heart sound anomaly detection tasks.",Heart;Correlation;Semantics;Autoencoders;Radar;Electrocardiography;Feature extraction;Decoding;Speech processing;Anomaly detection;Radar-to-ECG;ECG synthesis;knowledge distillation,"{'month': 'April', 'issn': '2379-190X', 'doi': '10.1109/ICASSP49660.2025.10888884', 'keywords': 'Heart;Correlation;Semantics;Autoencoders;Radar;Electrocardiography;Feature extraction;Decoding;Speech processing;Anomaly detection;Radar-to-ECG;ECG synthesis;knowledge distillation', 'abstract': 'The field of conditional Electrocardiogram(ECG) generation focuses on generating specified ECGs under given conditions for medical purposes. Existing methods are typically based on conditions of simple inputs like text or lead types. However, they struggle to handle the complexity of radar heart sound signals due to the lack of effective feature extraction, which hinders capturing the intricate waveform correlations between radar heart sounds and ECGs. Considering that radar-detected heart sound signals are contactless, the application is of essential value in a real-world deployment like sleep scenarios. Moreover, no prior approaches have addressed this specific task. To tackle this challenge, we propose a novel multi-scale feature fusion network framework, Radar2ECG. This model leverages pre-trained autoencoders for heart sound and ECG signals, aligning and integrating multi-layer features through a bottleneck structure to enhance receptive fields and reduce redundant features, thereby capturing the correlations between heart sounds and ECGs. Finally, we employ knowledge distillation to transfer knowledge from the ECG decoder to the heart sound decoder. We present three anomaly type datasets and extensive experiments conducted on both normal and abnormal datasets demonstrate that our method outperforms existing models in both accuracy and robustness. The multi-scale feature fusion significantly improves performance, showcasing strong potential in ECG generation and heart sound anomaly detection tasks.', 'pages': '1-5', 'number': '', 'volume': '', 'year': '2025', 'title': 'Radar2ECG: Multi-Scale Bottleneck Fusion and Cross-modal Semantic Distillation for Conditional Electrocardiogram Generation from Radar Heart Sound', 'booktitle': 'ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)', 'author': 'Li, Jinye and Men, Aidong and Liu, Yang and Han, Pengda and Chen, Qingchao', 'ENTRYTYPE': 'inproceedings', 'ID': '10888884'}"
10984591,Predicting Stroke Through Health Data Analysis and Stacking Integration Algorithm,"Wang, Yi and Shang, Xinping and Dong, A'ni and Zhang, Chulan",Wang,10.1109/ICCECE65250.2025.10984591,2025,2025 5th International Conference on Consumer Electronics and Computer Engineering (ICCECE),"Stroke is the second leading cause of death worldwide and a significant contributor to the global burden of disability. This study aims to analyze health data from community residents using ensemble learning algorithms to screen for early stroke symptoms, enhancing the reliability and accuracy of predictions and reducing healthcare costs. We propose a Stacking Integration method, SIXCL, which combines three gradient boosting decision tree algorithms-XGBoost, CatBoost, and LightGBM-and integrates them with logistic regression with L2 regularization through stacking techniques. To address the extreme data imbalance, oversampling techniques were employed to balance the dataset. Experimental results demonstrate that the SIXCL method achieved a prediction accuracy of 96.4\%, significantly outperforming single models. Furthermore, the study identified key factors influencing stroke prediction. This research provides a cost-effective approach to early stroke prevention and screening and offers recommendations for future stroke prevention strategies.",Accuracy;Machine learning algorithms;Prevention and mitigation;Stacking;Predictive models;Stroke (medical condition);Prediction algorithms;Boosting;Decision trees;Reliability;stroke prediction;boosting decision tree;integration method;stacking techniques,"{'month': 'Feb', 'issn': '', 'doi': '10.1109/ICCECE65250.2025.10984591', 'keywords': 'Accuracy;Machine learning algorithms;Prevention and mitigation;Stacking;Predictive models;Stroke (medical condition);Prediction algorithms;Boosting;Decision trees;Reliability;stroke prediction;boosting decision tree;integration method;stacking techniques', 'abstract': 'Stroke is the second leading cause of death worldwide and a significant contributor to the global burden of disability. This study aims to analyze health data from community residents using ensemble learning algorithms to screen for early stroke symptoms, enhancing the reliability and accuracy of predictions and reducing healthcare costs. We propose a Stacking Integration method, SIXCL, which combines three gradient boosting decision tree algorithms-XGBoost, CatBoost, and LightGBM-and integrates them with logistic regression with L2 regularization through stacking techniques. To address the extreme data imbalance, oversampling techniques were employed to balance the dataset. Experimental results demonstrate that the SIXCL method achieved a prediction accuracy of 96.4\\%, significantly outperforming single models. Furthermore, the study identified key factors influencing stroke prediction. This research provides a cost-effective approach to early stroke prevention and screening and offers recommendations for future stroke prevention strategies.', 'pages': '1-5', 'number': '', 'volume': '', 'year': '2025', 'title': 'Predicting Stroke Through Health Data Analysis and Stacking Integration Algorithm', 'booktitle': '2025 5th International Conference on Consumer Electronics and Computer Engineering (ICCECE)', 'author': ""Wang, Yi and Shang, Xinping and Dong, A'ni and Zhang, Chulan"", 'ENTRYTYPE': 'inproceedings', 'ID': '10984591'}"
10402681,Non-uniform Deblurring by Deep Sharpness Edge Guided Model,"Wei, Hao and Ge, Chenyang and Qiao, Xin and Deng, Pengchao",Wei,10.1109/VCIP59821.2023.10402681,2023,2023 IEEE International Conference on Visual Communications and Image Processing (VCIP),"In this paper, we propose a two-branch deblurring framework. Given a blurred image, we first extract the edge map and employ an edge refinement network to recover the structure. Then the refined edge map is utilized to guide the subsequent deblurring process for correct structure recovery. Specifically, we develop a lightweight omni-dimensional attention module for long-range dependencies modeling and plug it into the edge refinement network, which effectively handles blur patterns with high variation. Furthermore, we propose a dynamic feature upsample module, which integrates dynamic convolution with upsampling and adaptively deals with the non-uniform blur. Extensive experiments show that our method outperforms state-of-the-art methods.",Adaptation models;Convolution;Visual communication;Image edge detection;Benchmark testing;Image restoration;Plugs;Image deblurring;edge guidance;long-range dependencies;dynamic convolution,"{'month': 'Dec', 'issn': '2642-9357', 'doi': '10.1109/VCIP59821.2023.10402681', 'keywords': 'Adaptation models;Convolution;Visual communication;Image edge detection;Benchmark testing;Image restoration;Plugs;Image deblurring;edge guidance;long-range dependencies;dynamic convolution', 'abstract': 'In this paper, we propose a two-branch deblurring framework. Given a blurred image, we first extract the edge map and employ an edge refinement network to recover the structure. Then the refined edge map is utilized to guide the subsequent deblurring process for correct structure recovery. Specifically, we develop a lightweight omni-dimensional attention module for long-range dependencies modeling and plug it into the edge refinement network, which effectively handles blur patterns with high variation. Furthermore, we propose a dynamic feature upsample module, which integrates dynamic convolution with upsampling and adaptively deals with the non-uniform blur. Extensive experiments show that our method outperforms state-of-the-art methods.', 'pages': '1-5', 'number': '', 'volume': '', 'year': '2023', 'title': 'Non-uniform Deblurring by Deep Sharpness Edge Guided Model', 'booktitle': '2023 IEEE International Conference on Visual Communications and Image Processing (VCIP)', 'author': 'Wei, Hao and Ge, Chenyang and Qiao, Xin and Deng, Pengchao', 'ENTRYTYPE': 'inproceedings', 'ID': '10402681'}"
10865505,Dual-Branch Speech Enhancement Network for Noise-Robust Automatic Speech Recognition,"Jiang, Haobin and Wang, Tianlei and Hu, Dinghan and Cao, Jiuwen",Jiang,10.1109/CAC63892.2024.10865505,2024,2024 China Automation Congress (CAC),"Automatic speech recognition (ASR) has achieved remarkable successes thanks to the end-to-end deep neural networks, but it is still challenging in the noisy and reverberation environments. The joint training of front-end speech enhancement (SE) and speech recognition system becomes a popular solution to the noise-robust ASR. However, the speech distortion problem arisen due to excessive information suppression by the SE module. To address this issue, in this paper, a novel dual-branch SE (DBSE) module is proposed as the front-end of the ASR system for joint training. Particularly, the two branches extract the clean speeches using different ways: one branch directly extracts clean speeches and the other branch utilizes spectral subtraction method. The final denoising speech signals are obtained by combining the outputs from both branches. In this way, the oversuppressed information can be compensated from each other. The two-stage joint training strategy is adopted for the noise-robust ASR model where the proposed DBSE is first pretrained by multitask reconstruction loss, and then the DBSE and ASR model is jointly trained using the speech recognition based loss function. Comparisons with several state-of-the-art ASR algorithms on benchmark dataset are conducted, and the results demonstrate the superior performance of our proposed algorithm.",Training;Automation;Noise;Speech enhancement;Benchmark testing;Distortion;Noise robustness;Reverberation;Noise measurement;Automatic speech recognition;Automatic speech recognition;speech enhancement;dual-branch network;speech denoising,"{'month': 'Nov', 'issn': '2688-0938', 'doi': '10.1109/CAC63892.2024.10865505', 'keywords': 'Training;Automation;Noise;Speech enhancement;Benchmark testing;Distortion;Noise robustness;Reverberation;Noise measurement;Automatic speech recognition;Automatic speech recognition;speech enhancement;dual-branch network;speech denoising', 'abstract': 'Automatic speech recognition (ASR) has achieved remarkable successes thanks to the end-to-end deep neural networks, but it is still challenging in the noisy and reverberation environments. The joint training of front-end speech enhancement (SE) and speech recognition system becomes a popular solution to the noise-robust ASR. However, the speech distortion problem arisen due to excessive information suppression by the SE module. To address this issue, in this paper, a novel dual-branch SE (DBSE) module is proposed as the front-end of the ASR system for joint training. Particularly, the two branches extract the clean speeches using different ways: one branch directly extracts clean speeches and the other branch utilizes spectral subtraction method. The final denoising speech signals are obtained by combining the outputs from both branches. In this way, the oversuppressed information can be compensated from each other. The two-stage joint training strategy is adopted for the noise-robust ASR model where the proposed DBSE is first pretrained by multitask reconstruction loss, and then the DBSE and ASR model is jointly trained using the speech recognition based loss function. Comparisons with several state-of-the-art ASR algorithms on benchmark dataset are conducted, and the results demonstrate the superior performance of our proposed algorithm.', 'pages': '5421-5426', 'number': '', 'volume': '', 'year': '2024', 'title': 'Dual-Branch Speech Enhancement Network for Noise-Robust Automatic Speech Recognition', 'booktitle': '2024 China Automation Congress (CAC)', 'author': 'Jiang, Haobin and Wang, Tianlei and Hu, Dinghan and Cao, Jiuwen', 'ENTRYTYPE': 'inproceedings', 'ID': '10865505'}"
10868922,LLM-Empowered Image Generation in the Neko Painter App: A Preliminary Application for Producing Teaching Materials,"Wu, Kaiyi and Ding, Jiaoyang and Li, Jingsen and Yang, Yuke and Zhang, Chen and Cao, Jiaxin",Wu,10.1109/ICET62460.2024.10868922,2024,2024 4th International Conference on Educational Technology (ICET),"This paper introduces the Neko Painter app and its key features, demonstrates the Large Language Models (LLMs)-empowered image generation with diffusion models and ContorlNet to be smarter and more automatic to control and optimise the image generation process, and shares some cases of using it to produce teaching materials. A preliminary application for producing teaching materials on General Studies using the Neko Painter app was conducted with 36 pre-service teachers from Hong Kong. The results showed that using LLM-empowered features positively impacts pre-service teachers’ motivation in producing teaching materials by using image generation. Future work will further explore the potential of LLM-empowered image generation in more educational subjects and scenarios.",Image synthesis;Large language models;Education;Process control;Educational technology;Diffusion models;Large Language Model (LLM);Image Generation;Neko Painter;Diffusion Models;Teaching Materials,"{'month': 'Sep.', 'issn': '', 'doi': '10.1109/ICET62460.2024.10868922', 'keywords': 'Image synthesis;Large language models;Education;Process control;Educational technology;Diffusion models;Large Language Model (LLM);Image Generation;Neko Painter;Diffusion Models;Teaching Materials', 'abstract': 'This paper introduces the Neko Painter app and its key features, demonstrates the Large Language Models (LLMs)-empowered image generation with diffusion models and ContorlNet to be smarter and more automatic to control and optimise the image generation process, and shares some cases of using it to produce teaching materials. A preliminary application for producing teaching materials on General Studies using the Neko Painter app was conducted with 36 pre-service teachers from Hong Kong. The results showed that using LLM-empowered features positively impacts pre-service teachers’ motivation in producing teaching materials by using image generation. Future work will further explore the potential of LLM-empowered image generation in more educational subjects and scenarios.', 'pages': '110-114', 'number': '', 'volume': '', 'year': '2024', 'title': 'LLM-Empowered Image Generation in the Neko Painter App: A Preliminary Application for Producing Teaching Materials', 'booktitle': '2024 4th International Conference on Educational Technology (ICET)', 'author': 'Wu, Kaiyi and Ding, Jiaoyang and Li, Jingsen and Yang, Yuke and Zhang, Chen and Cao, Jiaxin', 'ENTRYTYPE': 'inproceedings', 'ID': '10868922'}"
9959483,Inception-based Deep Learning Architecture for 3D Point Cloud Completion,"Saffi, Houda and Hmamouche, Youssef and Elharrouss, Omar and El Fallah Seghrouchni, Amal",Saffi,10.1109/AVSS56176.2022.9959483,2022,2022 18th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS),"3D point clouds are a simple and compact data format that represents the surface geometry of 3D objects. The output of the data acquisition process often yields incomplete shapes. Hence, it is crucial to infer the missing regions of 3D objects from incomplete ones for many real-world applications. By leveraging a framework of 3D point cloud completion architectures, the proposed inception module is an intermediate layer that aims to extract the hierarchical features, recognize the fine-grained details of point clouds and avoid overfitting. We conduct comprehensive experiments on three state-of-the-art datasets: ShapeNet-55, ShapeNet-34, and PCN. The experimental results demonstrate that the enhanced architectures outperform the state-of-the-art point cloud completion methods.",Point cloud compression;Geometry;Three-dimensional displays;Shape;Surveillance;Streaming media;Feature extraction,"{'month': 'Nov', 'issn': '', 'doi': '10.1109/AVSS56176.2022.9959483', 'keywords': 'Point cloud compression;Geometry;Three-dimensional displays;Shape;Surveillance;Streaming media;Feature extraction', 'abstract': '3D point clouds are a simple and compact data format that represents the surface geometry of 3D objects. The output of the data acquisition process often yields incomplete shapes. Hence, it is crucial to infer the missing regions of 3D objects from incomplete ones for many real-world applications. By leveraging a framework of 3D point cloud completion architectures, the proposed inception module is an intermediate layer that aims to extract the hierarchical features, recognize the fine-grained details of point clouds and avoid overfitting. We conduct comprehensive experiments on three state-of-the-art datasets: ShapeNet-55, ShapeNet-34, and PCN. The experimental results demonstrate that the enhanced architectures outperform the state-of-the-art point cloud completion methods.', 'pages': '1-7', 'number': '', 'volume': '', 'year': '2022', 'title': 'Inception-based Deep Learning Architecture for 3D Point Cloud Completion', 'booktitle': '2022 18th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)', 'author': 'Saffi, Houda and Hmamouche, Youssef and Elharrouss, Omar and El Fallah Seghrouchni, Amal', 'ENTRYTYPE': 'inproceedings', 'ID': '9959483'}"
10507390,A Hybrid Approach for Event Extraction in Safety Production,"Li, Mei and Ai, Xinbo and Gu, Yanjun and Chen, Zhanghui",Li,10.1109/ICCC59590.2023.10507390,2023,2023 9th International Conference on Computer and Communications (ICCC),"Event extraction aims to identify events of interest from unstructured text and represent them in a structured format. However, due to the diversity and complexity of events, the current accuracy of event extraction remains low, particularly for sentences containing multiple events. Furthermore, existing event extraction studies have primarily focused on domains such as news, finance, and biomedicine, with limited application in the field of safety production. To address these challenges, this paper divides the event extraction task into two phases tailored to the characteristics of the safety production domain: event triple extraction and event classification. This approach achieves the extraction of essential event information and the categorization of event types through a phase-based extraction method. By extracting event triples based on dependent syntactic analysis and semantic role annotation, we can successfully match 84.64\% of events according to the template and structure the event representations. Subsequently, the event classification task is performed on the event triples using the ELECTRA\_TextRCNN model. In comparison to the TextRCNN model, our proposed ELECTRA\_TextRCNN model demonstrates a 9\% improvement in accuracy and a 7\% increase in F1 value. Ultimately, we have significantly enhanced the accuracy of event extraction in the safety domain through a two-stage approach.",Semantics;Finance;Production;Organizations;Syntactics;Safety;Data mining;Event extraction;Information extraction;Text classification;safety production;HFACS,"{'month': 'Dec', 'issn': '2837-7109', 'doi': '10.1109/ICCC59590.2023.10507390', 'keywords': 'Semantics;Finance;Production;Organizations;Syntactics;Safety;Data mining;Event extraction;Information extraction;Text classification;safety production;HFACS', 'abstract': 'Event extraction aims to identify events of interest from unstructured text and represent them in a structured format. However, due to the diversity and complexity of events, the current accuracy of event extraction remains low, particularly for sentences containing multiple events. Furthermore, existing event extraction studies have primarily focused on domains such as news, finance, and biomedicine, with limited application in the field of safety production. To address these challenges, this paper divides the event extraction task into two phases tailored to the characteristics of the safety production domain: event triple extraction and event classification. This approach achieves the extraction of essential event information and the categorization of event types through a phase-based extraction method. By extracting event triples based on dependent syntactic analysis and semantic role annotation, we can successfully match 84.64\\% of events according to the template and structure the event representations. Subsequently, the event classification task is performed on the event triples using the ELECTRA\\_TextRCNN model. In comparison to the TextRCNN model, our proposed ELECTRA\\_TextRCNN model demonstrates a 9\\% improvement in accuracy and a 7\\% increase in F1 value. Ultimately, we have significantly enhanced the accuracy of event extraction in the safety domain through a two-stage approach.', 'pages': '2378-2382', 'number': '', 'volume': '', 'year': '2023', 'title': 'A Hybrid Approach for Event Extraction in Safety Production', 'booktitle': '2023 9th International Conference on Computer and Communications (ICCC)', 'author': 'Li, Mei and Ai, Xinbo and Gu, Yanjun and Chen, Zhanghui', 'ENTRYTYPE': 'inproceedings', 'ID': '10507390'}"
11115047,MVG-FD: Multi-Modal Visual Guidance and Feature Decomposition for Underwater Image Restoration,"Jiang, Guangqi and Zhang, Ao and Liu, Yi and Wang, Huibing and Xu, Shoukun",Jiang,10.1109/LSP.2025.3596447,2025,IEEE Signal Processing Letters,"Underwater images are frequently affected by light absorption and scattering, which lead to color distortion, reduced contrast, and blurred details, significantly degrading overall image quality. Most underwater image restoration methods are confined to the pixel space of the raw modality, overlooking the important role of other modalities and different frequency-domain features. As a result, the representational capacity of deep learning models is not fully realized, affecting the generation of high-quality images. To address the above issues, we propose Multi-modal Visual Guidance and Feature Decomposition (MVG-FD) method for underwater image restoration. Specifically, we introduce Modality Visual Guidance (MVG) module, which integrates the complementary information provided by depth modality features into the raw features to guide the model in restoring the color of underwater images. Meanwhile, we design Feature Decomposition (FD) module, which utilizes Learnable Wavelet Decomposition (LWD) to decompose and extract the high-frequency bands of the raw features to help restore the texture details of the image. MVG-FD significantly improves PSNR and SSIM on existing datasets. The code is available at: https://github.com/zhangao668/MVG-FD.",Feature extraction;Image restoration;Convolution;Visualization;Image color analysis;Training;Data mining;Transformers;Foundation models;Testing;Underwater image restoration;multi-modal;feature decomposition,"{'month': '', 'issn': '1558-2361', 'doi': '10.1109/LSP.2025.3596447', 'keywords': 'Feature extraction;Image restoration;Convolution;Visualization;Image color analysis;Training;Data mining;Transformers;Foundation models;Testing;Underwater image restoration;multi-modal;feature decomposition', 'abstract': 'Underwater images are frequently affected by light absorption and scattering, which lead to color distortion, reduced contrast, and blurred details, significantly degrading overall image quality. Most underwater image restoration methods are confined to the pixel space of the raw modality, overlooking the important role of other modalities and different frequency-domain features. As a result, the representational capacity of deep learning models is not fully realized, affecting the generation of high-quality images. To address the above issues, we propose Multi-modal Visual Guidance and Feature Decomposition (MVG-FD) method for underwater image restoration. Specifically, we introduce Modality Visual Guidance (MVG) module, which integrates the complementary information provided by depth modality features into the raw features to guide the model in restoring the color of underwater images. Meanwhile, we design Feature Decomposition (FD) module, which utilizes Learnable Wavelet Decomposition (LWD) to decompose and extract the high-frequency bands of the raw features to help restore the texture details of the image. MVG-FD significantly improves PSNR and SSIM on existing datasets. The code is available at: https://github.com/zhangao668/MVG-FD.', 'pages': '3305-3309', 'number': '', 'volume': '32', 'year': '2025', 'title': 'MVG-FD: Multi-Modal Visual Guidance and Feature Decomposition for Underwater Image Restoration', 'journal': 'IEEE Signal Processing Letters', 'author': 'Jiang, Guangqi and Zhang, Ao and Liu, Yi and Wang, Huibing and Xu, Shoukun', 'ENTRYTYPE': 'article', 'ID': '11115047'}"
11168496,Detecting AI-Generated Social Media Comments Using BERT-Extracted Semantic Style Features,"S, Mohammed Yaseen and Mohamed, Yaqub Moosa and Jacob, Chinnu",S,10.1109/ACCTHPA65749.2025.11168496,2025,2025 Advanced Computing and Communication Technologies for High Performance Applications (ACCTHPA),"The rapid rise of artificial intelligence (AI) in generating synthetic content has raised significant concerns regarding the spread of fake news and political propaganda, especially in the context of India's 2024 elections. Identifying human-written comments and tweets amidst AI-generated ones is critical for mitigating such challenges. This paper proposes a method for detecting AI-generated comment by fine-tuning a BERT (Bidirectional Encoder Representations from Transformers) model for classification tasks. The BERT model is fine-tuned on a custom AI text classification dataset, incorporating a neural network classification head to distinguish between machine-generated text (MGT) and human-written text (HWT). A custom dataset of YouTube comments, consisting of both human-written and AI-generated comments, is constructed to evaluate the model's performance in real-world social media scenarios. The model is fine-tuned on this dataset to detect AI-generated comments in social media discussions. To further analyze the discriminative capabilities of the model, Principal Component Analysis (PCA) is applied to visualize the embeddings. The results indicate that the fine-tuned BERT model effectively captures semantic patterns and separates AI comments and human comments into distinct regions in the embedding space, with a clear decision boundary between the two classes. This approach demonstrates the potential of fine-tuned transformer-based models for detecting AI-generated text and offers a robust solution to countering AI-driven misinformation on social media platforms.",Visualization;Social networking (online);Semantics;Text categorization;Bidirectional control;Transformers;Encoding;Natural language processing;Fake news;Principal component analysis;AI-generated text;BERT;Text classification;Transformer models;Natural Language Processing (NLP);Semantic analysis;Text embeddings,"{'month': 'July', 'issn': '', 'doi': '10.1109/ACCTHPA65749.2025.11168496', 'keywords': 'Visualization;Social networking (online);Semantics;Text categorization;Bidirectional control;Transformers;Encoding;Natural language processing;Fake news;Principal component analysis;AI-generated text;BERT;Text classification;Transformer models;Natural Language Processing (NLP);Semantic analysis;Text embeddings', 'abstract': ""The rapid rise of artificial intelligence (AI) in generating synthetic content has raised significant concerns regarding the spread of fake news and political propaganda, especially in the context of India's 2024 elections. Identifying human-written comments and tweets amidst AI-generated ones is critical for mitigating such challenges. This paper proposes a method for detecting AI-generated comment by fine-tuning a BERT (Bidirectional Encoder Representations from Transformers) model for classification tasks. The BERT model is fine-tuned on a custom AI text classification dataset, incorporating a neural network classification head to distinguish between machine-generated text (MGT) and human-written text (HWT). A custom dataset of YouTube comments, consisting of both human-written and AI-generated comments, is constructed to evaluate the model's performance in real-world social media scenarios. The model is fine-tuned on this dataset to detect AI-generated comments in social media discussions. To further analyze the discriminative capabilities of the model, Principal Component Analysis (PCA) is applied to visualize the embeddings. The results indicate that the fine-tuned BERT model effectively captures semantic patterns and separates AI comments and human comments into distinct regions in the embedding space, with a clear decision boundary between the two classes. This approach demonstrates the potential of fine-tuned transformer-based models for detecting AI-generated text and offers a robust solution to countering AI-driven misinformation on social media platforms."", 'pages': '1-6', 'number': '', 'volume': '', 'year': '2025', 'title': 'Detecting AI-Generated Social Media Comments Using BERT-Extracted Semantic Style Features', 'booktitle': '2025 Advanced Computing and Communication Technologies for High Performance Applications (ACCTHPA)', 'author': 'S, Mohammed Yaseen and Mohamed, Yaqub Moosa and Jacob, Chinnu', 'ENTRYTYPE': 'inproceedings', 'ID': '11168496'}"
10657743,EDM: Enhancing Quality Diversity of Medical Image Augmentation Under the User-specified Complex and Various Conditions,"Kim, S. Y. and Chung, H. B. and Lee, J. S. and Kang, M. J.",Kim,10.1109/NSS/MIC/RTSD57108.2024.10657743,2024,"2024 IEEE Nuclear Science Symposium (NSS), Medical Imaging Conference (MIC) and Room Temperature Semiconductor Detector Conference (RTSD)","In the medical domain, the scarcity of data leads to class imbalances and fairness issues, significantly impairing deep learning performance and leading to the underrepresentation of minority patient groups. To address these challenges, data augmentation techniques employing Generative Adversarial Networks (GANs) and diffusion models have been extensively explored, with diffusion models particularly noted for their ability to enhance data diversity and quality. Despite their achievements, it remains a challenging task to generate a diverse range of samples from limited datasets and to achieve precise generation of images that align with user intentions.To overcome these limitations, we propose a framework, EDM (Enhancing Quality Diversity of Medical image augmentation under the user-specified complex and various conditions), designed to generate diverse high-quality medical images under the various and complex multimodal conditions, representing age, sex, and diseases. Our framework comprises four steps. First, we propose a metric to quantize the necessity of data augmentation. Second, we prepare data augmentation. We generate medical sketch-image-text triplets using DiffSketcher from the real patients’ dataset. Then we employ these triplets to train ControlNet, enabling the generation of synthetic medical images from sketch data. Third, we generate synthetic medical images. We obtain multiple sketches for each image from a diversity-enhanced DiffSketcher, incorporating similarity loss. Using these sketches, we augment data from the fine-tuned ControlNet from the second step. Lastly, we introduce a metric to measure the diversity of the generated images, ensuring the effectiveness of our augmentation process.Our preliminary experimental results imply EDM can be the promising solution. Future experiments are planned to construct synthetic datasets of various modalities such as PET and CT for underrepresented patient groups.",Semiconductor device measurement;Microwave integrated circuits;Semiconductor detectors;Data augmentation;Diffusion models;Generative adversarial networks;Image augmentation,"{'month': 'Oct', 'issn': '2577-0829', 'doi': '10.1109/NSS/MIC/RTSD57108.2024.10657743', 'keywords': 'Semiconductor device measurement;Microwave integrated circuits;Semiconductor detectors;Data augmentation;Diffusion models;Generative adversarial networks;Image augmentation', 'abstract': 'In the medical domain, the scarcity of data leads to class imbalances and fairness issues, significantly impairing deep learning performance and leading to the underrepresentation of minority patient groups. To address these challenges, data augmentation techniques employing Generative Adversarial Networks (GANs) and diffusion models have been extensively explored, with diffusion models particularly noted for their ability to enhance data diversity and quality. Despite their achievements, it remains a challenging task to generate a diverse range of samples from limited datasets and to achieve precise generation of images that align with user intentions.To overcome these limitations, we propose a framework, EDM (Enhancing Quality Diversity of Medical image augmentation under the user-specified complex and various conditions), designed to generate diverse high-quality medical images under the various and complex multimodal conditions, representing age, sex, and diseases. Our framework comprises four steps. First, we propose a metric to quantize the necessity of data augmentation. Second, we prepare data augmentation. We generate medical sketch-image-text triplets using DiffSketcher from the real patients’ dataset. Then we employ these triplets to train ControlNet, enabling the generation of synthetic medical images from sketch data. Third, we generate synthetic medical images. We obtain multiple sketches for each image from a diversity-enhanced DiffSketcher, incorporating similarity loss. Using these sketches, we augment data from the fine-tuned ControlNet from the second step. Lastly, we introduce a metric to measure the diversity of the generated images, ensuring the effectiveness of our augmentation process.Our preliminary experimental results imply EDM can be the promising solution. Future experiments are planned to construct synthetic datasets of various modalities such as PET and CT for underrepresented patient groups.', 'pages': '1-1', 'number': '', 'volume': '', 'year': '2024', 'title': 'EDM: Enhancing Quality Diversity of Medical Image Augmentation Under the User-specified Complex and Various Conditions', 'booktitle': '2024 IEEE Nuclear Science Symposium (NSS), Medical Imaging Conference (MIC) and Room Temperature Semiconductor Detector Conference (RTSD)', 'author': 'Kim, S. Y. and Chung, H. B. and Lee, J. S. and Kang, M. J.', 'ENTRYTYPE': 'inproceedings', 'ID': '10657743'}"
10552157,MuSAnet: Monocular-to-3-D Human Modeling via Multi-Scale Spatial Awareness,"Du, Chenghu and Xiong, Shengwu",Du,10.1109/TCE.2024.3410989,2024,IEEE Transactions on Consumer Electronics,"Monocular-to-3D human modeling involves creating colored three-dimensional models of humans from monocular try-on images. This technology offers personalized services to consumers and has garnered considerable attention for its potential business value. However, current methods are unable to deform clothing images to align with the human body naturally. Additionally, the generation of low-quality monocular try-on images severely hinders the creation of high-precision human models. This paper presents a novel monocular-to-3D human modeling network capable of accurately generating 3D models from monocular try-on images. To improve the accuracy of clothing deformation, an enhanced non-rigid deformation constraint strategy is introduced. This strategy helps reduce excessive deformation by strengthening penalties for outliers. Additionally, occlusion is addressed by implementing strict boundary constraints, resulting in more realistic and natural deformation outcomes. Furthermore, a stepped spatial-aware block is proposed to fuse latent multi-scale shape features in person images during depth estimation. This approach allows for creating high-precision person models in a single stage, enhancing the overall quality of the generated 3D models. Experiments conducted on the MPV-3D dataset demonstrate the superiority of the method. Regarding human modeling, Abs. decreased from 7.88 to 7.38, Sq. from 0.39 to 0.34, and RMSE from 11.27 to 10.66.",Clothing;Deformation;Three-dimensional displays;Deformable models;Computational modeling;Generative adversarial networks;Monocular-to-3D human modeling;virtual try-on;generative adversarial network;consumer technology,"{'month': 'Aug', 'issn': '1558-4127', 'doi': '10.1109/TCE.2024.3410989', 'keywords': 'Clothing;Deformation;Three-dimensional displays;Deformable models;Computational modeling;Generative adversarial networks;Monocular-to-3D human modeling;virtual try-on;generative adversarial network;consumer technology', 'abstract': 'Monocular-to-3D human modeling involves creating colored three-dimensional models of humans from monocular try-on images. This technology offers personalized services to consumers and has garnered considerable attention for its potential business value. However, current methods are unable to deform clothing images to align with the human body naturally. Additionally, the generation of low-quality monocular try-on images severely hinders the creation of high-precision human models. This paper presents a novel monocular-to-3D human modeling network capable of accurately generating 3D models from monocular try-on images. To improve the accuracy of clothing deformation, an enhanced non-rigid deformation constraint strategy is introduced. This strategy helps reduce excessive deformation by strengthening penalties for outliers. Additionally, occlusion is addressed by implementing strict boundary constraints, resulting in more realistic and natural deformation outcomes. Furthermore, a stepped spatial-aware block is proposed to fuse latent multi-scale shape features in person images during depth estimation. This approach allows for creating high-precision person models in a single stage, enhancing the overall quality of the generated 3D models. Experiments conducted on the MPV-3D dataset demonstrate the superiority of the method. Regarding human modeling, Abs. decreased from 7.88 to 7.38, Sq. from 0.39 to 0.34, and RMSE from 11.27 to 10.66.', 'pages': '5115-5127', 'number': '3', 'volume': '70', 'year': '2024', 'title': 'MuSAnet: Monocular-to-3-D Human Modeling via Multi-Scale Spatial Awareness', 'journal': 'IEEE Transactions on Consumer Electronics', 'author': 'Du, Chenghu and Xiong, Shengwu', 'ENTRYTYPE': 'article', 'ID': '10552157'}"
10047554,A Review on Machine Learning and Deep Learning based Rainfall Prediction Methods,"Srinu, Nidamanuri and Bindu, Bejawada Hima",Srinu,10.1109/ICPECTS56089.2022.10047554,2022,"2022 International Conference on Power, Energy, Control and Transmission Systems (ICPECTS)","Water for farming and storage capacity in dams are both affected by the quantity of precipitation that falls. Predicting when and how much rain will fall is challenging because of global warming and other variables. Artificial Intelligence (AI), Machine Learning (ML), and Deep Learning (DL) may use factors like the amount of rainfall in previous years, climatic change, climatic conditions, and so on to forecast rainfall in a given area. Potential flooding and damage to man-made structures might result from unexpected rainfall. Comprehensive data categorization and investigation, evaluation, inquiry, and interpretations are needed to uncover rainfall that will help governments avert human deaths, agricultural failures, and animal fatalities. The development of ML and DL systems is ongoing to assess and manage crucial rainfall data. Therefore, the purpose of this research is to investigate the potential of using ML and DL approaches to forecast precipitation for future planning of human settlements and related activities. Prediction methods in ML are investigated, including regression, Convolutional Neural Networks (CNNs), and Long-Short Term Memory (LSTM). The benefits and drawbacks of using neural network algorithms and ML algorithms are also discussed in this paper, with each approach taking into account the data and features at hand. New developments in the field of precipitation forecasting are also covered, including studies that use a wide range of feature extraction and prediction methods. In this study, we discuss many ML methods and algorithms for predicting rainfall based on historical data and other criteria.",Deep learning;Rain;Biological system modeling;Weather forecasting;Predictive models;Prediction algorithms;Generative adversarial networks;Climate change;Farming;Rainfall;Review;Deep Learning;Machine Learning,"{'month': 'Dec', 'issn': '', 'doi': '10.1109/ICPECTS56089.2022.10047554', 'keywords': 'Deep learning;Rain;Biological system modeling;Weather forecasting;Predictive models;Prediction algorithms;Generative adversarial networks;Climate change;Farming;Rainfall;Review;Deep Learning;Machine Learning', 'abstract': 'Water for farming and storage capacity in dams are both affected by the quantity of precipitation that falls. Predicting when and how much rain will fall is challenging because of global warming and other variables. Artificial Intelligence (AI), Machine Learning (ML), and Deep Learning (DL) may use factors like the amount of rainfall in previous years, climatic change, climatic conditions, and so on to forecast rainfall in a given area. Potential flooding and damage to man-made structures might result from unexpected rainfall. Comprehensive data categorization and investigation, evaluation, inquiry, and interpretations are needed to uncover rainfall that will help governments avert human deaths, agricultural failures, and animal fatalities. The development of ML and DL systems is ongoing to assess and manage crucial rainfall data. Therefore, the purpose of this research is to investigate the potential of using ML and DL approaches to forecast precipitation for future planning of human settlements and related activities. Prediction methods in ML are investigated, including regression, Convolutional Neural Networks (CNNs), and Long-Short Term Memory (LSTM). The benefits and drawbacks of using neural network algorithms and ML algorithms are also discussed in this paper, with each approach taking into account the data and features at hand. New developments in the field of precipitation forecasting are also covered, including studies that use a wide range of feature extraction and prediction methods. In this study, we discuss many ML methods and algorithms for predicting rainfall based on historical data and other criteria.', 'pages': '1-4', 'number': '', 'volume': '', 'year': '2022', 'title': 'A Review on Machine Learning and Deep Learning based Rainfall Prediction Methods', 'booktitle': '2022 International Conference on Power, Energy, Control and Transmission Systems (ICPECTS)', 'author': 'Srinu, Nidamanuri and Bindu, Bejawada Hima', 'ENTRYTYPE': 'inproceedings', 'ID': '10047554'}"
10986870,Text-to-Voice Conversion for Indic Languages,"Hebbar, Aniketh and Rasheed, Shahil and Kl, Spoorthi and Mustafa, Sarah and Santhosh, Soumya",Hebbar,10.1109/AIDE64228.2025.10986870,2025,2025 International Conference on Artificial Intelligence and Data Engineering (AIDE),"The demand for sophisticated Text-to-Speech (TTS) systems is rising due to linguistic diversity, with Indic languages posing particular difficulties. Conventional text-to-speech (TTS) synthesis algorithms frequently fall short in capturing the rich expressiveness and subtle cultural nuances of Indic languages, resulting in an artificial voice output devoid of authentic prosody. By integrating the HiFi-GAN for high-fidelity voice cloning and the Tortoise TTS framework into a hybrid TTS architecture, this solution suggests a novel way to get beyond these restrictions. Allowing the system to pick up on and replicate the complex patterns and nuanced subtleties of Indic speech. High-fidelity waveforms are effectively synthesized from mel-spectrograms by the HiFi-GAN vocoder, while extremely expressive and natural-sounding speech waveforms are produced by the Tortoise TTS model, which is based on Denoising Diffusion Restoration Models. High-quality training data is ensured via a complex data processing pipeline that includes silence reduction, audio segmentation, and transcription filtering. The suggested system performs much better than current TTS systems on both Mean Opinion Score (MOS) and Comparative Mean Opinion Score (CMOS) according to extensive subjective evaluations.",Semiconductor device modeling;Filtering;Vocoders;Pipelines;Noise reduction;Training data;Linguistics;Generative adversarial networks;Hybrid power systems;Text to speech;Text-to-Speech;Indic languages;Deep learning;Hybrid architectures;Tortoise TTS;High-Fidelity Generative Adversarial Network (HiFi-GAN),"{'month': 'Feb', 'issn': '', 'doi': '10.1109/AIDE64228.2025.10986870', 'keywords': 'Semiconductor device modeling;Filtering;Vocoders;Pipelines;Noise reduction;Training data;Linguistics;Generative adversarial networks;Hybrid power systems;Text to speech;Text-to-Speech;Indic languages;Deep learning;Hybrid architectures;Tortoise TTS;High-Fidelity Generative Adversarial Network (HiFi-GAN)', 'abstract': 'The demand for sophisticated Text-to-Speech (TTS) systems is rising due to linguistic diversity, with Indic languages posing particular difficulties. Conventional text-to-speech (TTS) synthesis algorithms frequently fall short in capturing the rich expressiveness and subtle cultural nuances of Indic languages, resulting in an artificial voice output devoid of authentic prosody. By integrating the HiFi-GAN for high-fidelity voice cloning and the Tortoise TTS framework into a hybrid TTS architecture, this solution suggests a novel way to get beyond these restrictions. Allowing the system to pick up on and replicate the complex patterns and nuanced subtleties of Indic speech. High-fidelity waveforms are effectively synthesized from mel-spectrograms by the HiFi-GAN vocoder, while extremely expressive and natural-sounding speech waveforms are produced by the Tortoise TTS model, which is based on Denoising Diffusion Restoration Models. High-quality training data is ensured via a complex data processing pipeline that includes silence reduction, audio segmentation, and transcription filtering. The suggested system performs much better than current TTS systems on both Mean Opinion Score (MOS) and Comparative Mean Opinion Score (CMOS) according to extensive subjective evaluations.', 'pages': '346-352', 'number': '', 'volume': '', 'year': '2025', 'title': 'Text-to-Voice Conversion for Indic Languages', 'booktitle': '2025 International Conference on Artificial Intelligence and Data Engineering (AIDE)', 'author': 'Hebbar, Aniketh and Rasheed, Shahil and Kl, Spoorthi and Mustafa, Sarah and Santhosh, Soumya', 'ENTRYTYPE': 'inproceedings', 'ID': '10986870'}"
9703109,AI-Based Reconstruction for Fast MRI—A Systematic Review and Meta-Analysis,"Chen, Yutong and Schönlieb, Carola-Bibiane and Liò, Pietro and Leiner, Tim and Dragotti, Pier Luigi and Wang, Ge and Rueckert, Daniel and Firmin, David and Yang, Guang",Chen,10.1109/JPROC.2022.3141367,2022,Proceedings of the IEEE,"Compressed sensing (CS) has been playing a key role in accelerating the magnetic resonance imaging (MRI) acquisition process. With the resurgence of artificial intelligence, deep neural networks and CS algorithms are being integrated to redefine the state of the art of fast MRI. The past several years have witnessed substantial growth in the complexity, diversity, and performance of deep-learning-based CS techniques that are dedicated to fast MRI. In this meta-analysis, we systematically review the deep-learning-based CS techniques for fast MRI, describe key model designs, highlight breakthroughs, and discuss promising directions. We have also introduced a comprehensive analysis framework and a classification system to assess the pivotal role of deep learning in CS-based acceleration for MRI.",Deep learning;Systematics;Magnetic resonance imaging;Neural networks;Complexity theory;Artificial intelligence;Compressed sensing;Compressed sensing (CS);deep learning;magnetic resonance imaging (MRI);neural network,"{'month': 'Feb', 'issn': '1558-2256', 'doi': '10.1109/JPROC.2022.3141367', 'keywords': 'Deep learning;Systematics;Magnetic resonance imaging;Neural networks;Complexity theory;Artificial intelligence;Compressed sensing;Compressed sensing (CS);deep learning;magnetic resonance imaging (MRI);neural network', 'abstract': 'Compressed sensing (CS) has been playing a key role in accelerating the magnetic resonance imaging (MRI) acquisition process. With the resurgence of artificial intelligence, deep neural networks and CS algorithms are being integrated to redefine the state of the art of fast MRI. The past several years have witnessed substantial growth in the complexity, diversity, and performance of deep-learning-based CS techniques that are dedicated to fast MRI. In this meta-analysis, we systematically review the deep-learning-based CS techniques for fast MRI, describe key model designs, highlight breakthroughs, and discuss promising directions. We have also introduced a comprehensive analysis framework and a classification system to assess the pivotal role of deep learning in CS-based acceleration for MRI.', 'pages': '224-245', 'number': '2', 'volume': '110', 'year': '2022', 'title': 'AI-Based Reconstruction for Fast MRI—A Systematic Review and Meta-Analysis', 'journal': 'Proceedings of the IEEE', 'author': 'Chen, Yutong and Schönlieb, Carola-Bibiane and Liò, Pietro and Leiner, Tim and Dragotti, Pier Luigi and Wang, Ge and Rueckert, Daniel and Firmin, David and Yang, Guang', 'ENTRYTYPE': 'article', 'ID': '9703109'}"
4587722,Correspondence-free multi-camera activity analysis and scene modeling,"Wang, Xiaogang and Tieu, Kinh and Grimson, W. Eric L.",Wang,10.1109/CVPR.2008.4587722,2008,2008 IEEE Conference on Computer Vision and Pattern Recognition,"We propose a novel approach for activity analysis in multiple synchronized but uncalibrated static camera views. We assume that the topology of camera views is unknown and quite arbitrary, the fields of views covered by these cameras may have no overlap or any amount of overlap, and objects may move on different ground planes. Using low-level cues, objects are tracked in each of the camera views independently, and the positions and velocities of objects along trajectories are computed as features. Under a generative model, our approach jointly learns the distribution of an activity in the feature spaces of different camera views. It accomplishes two tasks: (1) grouping trajectories in different camera views belonging to the same activity into one cluster; (2) modeling paths commonly taken by objects across camera views. To our knowledge, no prior result of co-clustering trajectories in multiple camera views has been published. Advantages of this approach are that it does not require first solving the challenging correspondence problem, and the learning is unsupervised. Our approach is evaluated on two very large data sets with 22, 951 and 14, 985 trajectories.",Layout;Surveillance;Monitoring;Computer science;Artificial intelligence;Smart cameras;Trajectory;Streaming media;Network topology;History,"{'month': 'June', 'issn': '1063-6919', 'doi': '10.1109/CVPR.2008.4587722', 'keywords': 'Layout;Surveillance;Monitoring;Computer science;Artificial intelligence;Smart cameras;Trajectory;Streaming media;Network topology;History', 'abstract': 'We propose a novel approach for activity analysis in multiple synchronized but uncalibrated static camera views. We assume that the topology of camera views is unknown and quite arbitrary, the fields of views covered by these cameras may have no overlap or any amount of overlap, and objects may move on different ground planes. Using low-level cues, objects are tracked in each of the camera views independently, and the positions and velocities of objects along trajectories are computed as features. Under a generative model, our approach jointly learns the distribution of an activity in the feature spaces of different camera views. It accomplishes two tasks: (1) grouping trajectories in different camera views belonging to the same activity into one cluster; (2) modeling paths commonly taken by objects across camera views. To our knowledge, no prior result of co-clustering trajectories in multiple camera views has been published. Advantages of this approach are that it does not require first solving the challenging correspondence problem, and the learning is unsupervised. Our approach is evaluated on two very large data sets with 22, 951 and 14, 985 trajectories.', 'pages': '1-8', 'number': '', 'volume': '', 'year': '2008', 'title': 'Correspondence-free multi-camera activity analysis and scene modeling', 'booktitle': '2008 IEEE Conference on Computer Vision and Pattern Recognition', 'author': 'Wang, Xiaogang and Tieu, Kinh and Grimson, W. Eric L.', 'ENTRYTYPE': 'inproceedings', 'ID': '4587722'}"
1467429,Combining object and feature dynamics in probabilistic tracking,"Taycher, L. and Fisher, J.W. and Darrell, T.",Taycher,10.1109/CVPR.2005.102,2005,2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05),"Objects can exhibit different dynamics at different scales, and this is often exploited by visual tracking algorithms. A local dynamic model is typically used to extract image features that are then used as input to a system for tracking the entire object using a global dynamic model. Approximate local dynamics may be brittle - point trackers drift due to image noise and adaptive background models adapt to foreground objects that become stationary - but constraints from the global model can make them more robust. We propose a probabilistic framework for incorporating global dynamics knowledge into the local feature extraction processes. A global tracking algorithm can be formulated as a generative model and used to predict feature values that are incorporated into an observation process of the feature extractor. We combine such models in a multichain graphical model framework. We show the utility of our framework for improving feature tracking and thus shape and motion estimates in a batch factorization algorithm. We also propose an approximate filtering algorithm appropriate for online applications, and demonstrate its application to background subtraction.",Feature extraction;Hidden Markov models;Computer science;Artificial intelligence;Laboratories;Background noise;Noise robustness;Predictive models;Graphical models;Tracking,"{'month': 'June', 'issn': '1063-6919', 'doi': '10.1109/CVPR.2005.102', 'keywords': 'Feature extraction;Hidden Markov models;Computer science;Artificial intelligence;Laboratories;Background noise;Noise robustness;Predictive models;Graphical models;Tracking', 'abstract': 'Objects can exhibit different dynamics at different scales, and this is often exploited by visual tracking algorithms. A local dynamic model is typically used to extract image features that are then used as input to a system for tracking the entire object using a global dynamic model. Approximate local dynamics may be brittle - point trackers drift due to image noise and adaptive background models adapt to foreground objects that become stationary - but constraints from the global model can make them more robust. We propose a probabilistic framework for incorporating global dynamics knowledge into the local feature extraction processes. A global tracking algorithm can be formulated as a generative model and used to predict feature values that are incorporated into an observation process of the feature extractor. We combine such models in a multichain graphical model framework. We show the utility of our framework for improving feature tracking and thus shape and motion estimates in a batch factorization algorithm. We also propose an approximate filtering algorithm appropriate for online applications, and demonstrate its application to background subtraction.', 'pages': '106-113 vol. 2', 'number': '', 'volume': '2', 'year': '2005', 'title': 'Combining object and feature dynamics in probabilistic tracking', 'booktitle': ""2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"", 'author': 'Taycher, L. and Fisher, J.W. and Darrell, T.', 'ENTRYTYPE': 'inproceedings', 'ID': '1467429'}"
5543434,Nonparametric hierarchical Bayesian model for functional brain parcellation,"Lashkari, Danial and Sridharan, Ramesh and Vul, Edward and Hsieh, Po-Jang and Kanwisher, Nancy and Golland, Polina",Lashkari,10.1109/CVPRW.2010.5543434,2010,2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops,"We develop a method for unsupervised analysis of functional brain images that learns group-level patterns of functional response. Our algorithm is based on a generative model that comprises two main layers. At the lower level, we express the functional brain response to each stimulus as a binary activation variable. At the next level, we define a prior over the sets of activation variables in all subjects. We use a Hierarchical Dirichlet Process as the prior in order to simultaneously learn the patterns of response that are shared across the group, and to estimate the number of these patterns supported by data. Inference based on this model enables automatic discovery and characterization of salient and consistent patterns in functional signals. We apply our method to data from a study that explores the response of the visual cortex to a collection of images. The discovered profiles of activation correspond to selectivity to a number of image categories such as faces, bodies, and scenes. More generally, our results appear superior to the results of alternative data-driven methods in capturing the category structure in the space of stimuli.",Bayesian methods;Brain modeling;Independent component analysis;Layout;Testing;Computer science;Artificial intelligence;Laboratories;Image analysis;Pattern analysis,"{'month': 'June', 'issn': '2160-7516', 'doi': '10.1109/CVPRW.2010.5543434', 'keywords': 'Bayesian methods;Brain modeling;Independent component analysis;Layout;Testing;Computer science;Artificial intelligence;Laboratories;Image analysis;Pattern analysis', 'abstract': 'We develop a method for unsupervised analysis of functional brain images that learns group-level patterns of functional response. Our algorithm is based on a generative model that comprises two main layers. At the lower level, we express the functional brain response to each stimulus as a binary activation variable. At the next level, we define a prior over the sets of activation variables in all subjects. We use a Hierarchical Dirichlet Process as the prior in order to simultaneously learn the patterns of response that are shared across the group, and to estimate the number of these patterns supported by data. Inference based on this model enables automatic discovery and characterization of salient and consistent patterns in functional signals. We apply our method to data from a study that explores the response of the visual cortex to a collection of images. The discovered profiles of activation correspond to selectivity to a number of image categories such as faces, bodies, and scenes. More generally, our results appear superior to the results of alternative data-driven methods in capturing the category structure in the space of stimuli.', 'pages': '15-22', 'number': '', 'volume': '', 'year': '2010', 'title': 'Nonparametric hierarchical Bayesian model for functional brain parcellation', 'booktitle': '2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops', 'author': 'Lashkari, Danial and Sridharan, Ramesh and Vul, Edward and Hsieh, Po-Jang and Kanwisher, Nancy and Golland, Polina', 'ENTRYTYPE': 'inproceedings', 'ID': '5543434'}"
9668602,Collaborative adversary nodes learning on the logs of IoT devices in an IoT network,"Aneja, Sandhya and En, Melanie Ang Xuan and Aneja, Nagender",Aneja,10.1109/COMSNETS53615.2022.9668602,2022,2022 14th International Conference on COMmunication Systems \& NETworkS (COMSNETS),"Artificial Intelligence (AI) development has encouraged many new research areas, including AI-enabled Internet of Things (IoT) network. AI analytics and intelligent paradigms greatly improve learning efficiency and accuracy. Applying these learning paradigms to network scenarios provide technical advantages of new networking solutions. In this paper, we propose an improved approach for IoT security from data perspective. The network traffic of IoT devices can be analyzed using AI techniques. The Adversary Learning (AdLIoTLog) model is proposed using Recurrent Neural Network (RNN) with attention mechanism on sequences of network events in the network traffic. We define network events as a sequence of the time series packets of protocols captured in the log. We have considered different packets TCP packets, UDP packets, and HTTP packets in the network log to make the algorithm robust. The distributed IoT devices can collaborate to cripple our world which is extending to Internet of Intelligence. The time series packets are converted into structured data by removing noise and adding timestamps. The resulting data set is trained by RNN and can detect the node pairs collaborating with each other. We used the BLEU score to evaluate the model performance. Our results show that the predicting performance of the AdLIoTLog model trained by our method degrades by 3-4\% in the presence of attack in comparison to the scenario when the network is not under attack. AdLIoTLog can detect adversaries because when adversaries are present the model gets duped by the collaborative events and therefore predicts the next event with a biased event rather than a benign event. We conclude that AI can provision ubiquitous learning for the new generation of Internet of Things.",Performance evaluation;Recurrent neural networks;Protocols;Time series analysis;Collaboration;Telecommunication traffic;Predictive models;Deep learning;recurrent neural network;gated recurrent unit;internet of things;adversary,"{'month': 'Jan', 'issn': '2155-2509', 'doi': '10.1109/COMSNETS53615.2022.9668602', 'keywords': 'Performance evaluation;Recurrent neural networks;Protocols;Time series analysis;Collaboration;Telecommunication traffic;Predictive models;Deep learning;recurrent neural network;gated recurrent unit;internet of things;adversary', 'abstract': 'Artificial Intelligence (AI) development has encouraged many new research areas, including AI-enabled Internet of Things (IoT) network. AI analytics and intelligent paradigms greatly improve learning efficiency and accuracy. Applying these learning paradigms to network scenarios provide technical advantages of new networking solutions. In this paper, we propose an improved approach for IoT security from data perspective. The network traffic of IoT devices can be analyzed using AI techniques. The Adversary Learning (AdLIoTLog) model is proposed using Recurrent Neural Network (RNN) with attention mechanism on sequences of network events in the network traffic. We define network events as a sequence of the time series packets of protocols captured in the log. We have considered different packets TCP packets, UDP packets, and HTTP packets in the network log to make the algorithm robust. The distributed IoT devices can collaborate to cripple our world which is extending to Internet of Intelligence. The time series packets are converted into structured data by removing noise and adding timestamps. The resulting data set is trained by RNN and can detect the node pairs collaborating with each other. We used the BLEU score to evaluate the model performance. Our results show that the predicting performance of the AdLIoTLog model trained by our method degrades by 3-4\\% in the presence of attack in comparison to the scenario when the network is not under attack. AdLIoTLog can detect adversaries because when adversaries are present the model gets duped by the collaborative events and therefore predicts the next event with a biased event rather than a benign event. We conclude that AI can provision ubiquitous learning for the new generation of Internet of Things.', 'pages': '231-235', 'number': '', 'volume': '', 'year': '2022', 'title': 'Collaborative adversary nodes learning on the logs of IoT devices in an IoT network', 'booktitle': '2022 14th International Conference on COMmunication Systems \\& NETworkS (COMSNETS)', 'author': 'Aneja, Sandhya and En, Melanie Ang Xuan and Aneja, Nagender', 'ENTRYTYPE': 'inproceedings', 'ID': '9668602'}"
5074650,Spatial patterns and functional profiles for discovering structure in fMRI data,"Golland, Polina and Lashkari, Danial and Venkataraman, Archana",Golland,10.1109/ACSSC.2008.5074650,2008,"2008 42nd Asilomar Conference on Signals, Systems and Computers","We explore unsupervised, hypothesis-free methods for fMRI analysis in two different types of experiments. First, we employ clustering to identify large-scale functionally homogeneous systems. We formulate a generative mixture model, derive the EM algorithm and apply it to delineate functional systems. We also investigate spectral clustering in application to this problem and demonstrate that both methods give rise to similar partitions of the brain based on resting state fMRI data. Second, we demonstrate how to extend this approach to include information about the experimental protocol. Specifically, we formulate a mixture model in the space of possible profiles of brain response to stimuli. In both applications, our methods confirm previously known results in brain mapping and point to new research directions for exploratory analysis of fMRI data.",Independent component analysis;Pattern analysis;Protocols;Image analysis;Principal component analysis;Brain modeling;Computer science;Artificial intelligence;Laboratories;Paper technology,"{'month': 'Oct', 'issn': '1058-6393', 'doi': '10.1109/ACSSC.2008.5074650', 'keywords': 'Independent component analysis;Pattern analysis;Protocols;Image analysis;Principal component analysis;Brain modeling;Computer science;Artificial intelligence;Laboratories;Paper technology', 'abstract': 'We explore unsupervised, hypothesis-free methods for fMRI analysis in two different types of experiments. First, we employ clustering to identify large-scale functionally homogeneous systems. We formulate a generative mixture model, derive the EM algorithm and apply it to delineate functional systems. We also investigate spectral clustering in application to this problem and demonstrate that both methods give rise to similar partitions of the brain based on resting state fMRI data. Second, we demonstrate how to extend this approach to include information about the experimental protocol. Specifically, we formulate a mixture model in the space of possible profiles of brain response to stimuli. In both applications, our methods confirm previously known results in brain mapping and point to new research directions for exploratory analysis of fMRI data.', 'pages': '1402-1409', 'number': '', 'volume': '', 'year': '2008', 'title': 'Spatial patterns and functional profiles for discovering structure in fMRI data', 'booktitle': '2008 42nd Asilomar Conference on Signals, Systems and Computers', 'author': 'Golland, Polina and Lashkari, Danial and Venkataraman, Archana', 'ENTRYTYPE': 'inproceedings', 'ID': '5074650'}"
10985830,Exploring Grounding Abilities in Vision-Language Models through Contextual Perception,"Xu, Wei and Zhou, Tianfei and Zhang, Taoyuan and Li, Jie and Chen, Peiyin and Pan, Jia and Liu, Xiaofeng",Xu,10.1109/TCDS.2025.3566649,2025,IEEE Transactions on Cognitive and Developmental Systems,"Vision language models (VLMs) have demonstrated strong general capabilities and achieved great success in areas such as image understanding and reasoning. Visual prompts enhance the focus of VLMs on designated areas, but their fine-grained grounding has not been fully developed. Recent research has used Set-of-Mark (SoM) approach to unleash the grounding capabilities of Generative Pre-trained Transformer-4 with Vision (GPT-4V), achieving significant benchmark performance. However, SoM still has problems with label offset and hallucination of vision language models, and the grounding ability of VLMs remains limited, making it challenging to handle complex scenarios in human-robot interaction. To address these limitations and provide more accurate and less hallucinatory results, we propose Contextual Set-of-Mark (ConSoM), a new SoM-based prompting mechanism that leverages dual-image inputs and contextual semantic information of images. Experiments demonstrate that ConSoM has distinct advantages in visual grounding, improving by 11\% compared to the baseline on the dataset Refcocog. Furthermore, we evaluated ConSoM’s grounding abilities in five indoor scenarios, where it exhibited strong robustness in complex environments and under occlusion conditions. We also introduced a scalable annotation method for pixel-level question-answering dataset. The accuracy, scalability, and depth of world knowledge make ConSoM a highly effective approach for future human-robot interactions.",Robots;Visualization;Grounding;Large language models;Artificial intelligence;Human-robot interaction;Accuracy;Semantics;Robot kinematics;Prompt engineering;Large language model;prompt engineering;visual grounding;human-robot interaction,"{'month': '', 'issn': '2379-8939', 'doi': '10.1109/TCDS.2025.3566649', 'keywords': 'Robots;Visualization;Grounding;Large language models;Artificial intelligence;Human-robot interaction;Accuracy;Semantics;Robot kinematics;Prompt engineering;Large language model;prompt engineering;visual grounding;human-robot interaction', 'abstract': 'Vision language models (VLMs) have demonstrated strong general capabilities and achieved great success in areas such as image understanding and reasoning. Visual prompts enhance the focus of VLMs on designated areas, but their fine-grained grounding has not been fully developed. Recent research has used Set-of-Mark (SoM) approach to unleash the grounding capabilities of Generative Pre-trained Transformer-4 with Vision (GPT-4V), achieving significant benchmark performance. However, SoM still has problems with label offset and hallucination of vision language models, and the grounding ability of VLMs remains limited, making it challenging to handle complex scenarios in human-robot interaction. To address these limitations and provide more accurate and less hallucinatory results, we propose Contextual Set-of-Mark (ConSoM), a new SoM-based prompting mechanism that leverages dual-image inputs and contextual semantic information of images. Experiments demonstrate that ConSoM has distinct advantages in visual grounding, improving by 11\\% compared to the baseline on the dataset Refcocog. Furthermore, we evaluated ConSoM’s grounding abilities in five indoor scenarios, where it exhibited strong robustness in complex environments and under occlusion conditions. We also introduced a scalable annotation method for pixel-level question-answering dataset. The accuracy, scalability, and depth of world knowledge make ConSoM a highly effective approach for future human-robot interactions.', 'pages': '1-14', 'number': '', 'volume': '', 'year': '2025', 'title': 'Exploring Grounding Abilities in Vision-Language Models through Contextual Perception', 'journal': 'IEEE Transactions on Cognitive and Developmental Systems', 'author': 'Xu, Wei and Zhou, Tianfei and Zhang, Taoyuan and Li, Jie and Chen, Peiyin and Pan, Jia and Liu, Xiaofeng', 'ENTRYTYPE': 'article', 'ID': '10985830'}"
11079082,TMIQ: Quantifying Test and Measurement Domain Intelligence in Large Language Models,"Olowe, Emmanuel A. and Chitnis, Danial",Olowe,10.1109/I2MTC62753.2025.11079082,2025,2025 IEEE International Instrumentation and Measurement Technology Conference (I2MTC),"The Test and Measurement domain, known for its strict requirements for accuracy and efficiency, is increasingly adopting Generative AI technologies to enhance the performance of data analysis, automation, and decision-making processes. Among these, Large Language Models (LLMs) show significant promise for advancing automation and precision in testing. However, the evaluation of LLMs in this specialized area remains insufficiently explored. To address this gap, we introduce the Test and Measurement Intelligence Quotient (TMIQ), a benchmark designed to quantitatively assess LLMs across a wide range of electronic engineering tasks. TMIQ offers a comprehensive set of scenarios and metrics for detailed evaluation, including SCPI command matching accuracy, ranked response evaluation, Chain-of-Thought Reasoning (CoT), and the impact of output formatting variations required by LLMs on performance. In testing various LLMs, our findings indicate varying levels of proficiency, with exact SCPI command match accuracy ranging from around 56\% to 73\%, and ranked matching first-position scores achieving around 33\% for the best-performing model. We also assess token usage, cost-efficiency, and response times, identifying trade-offs between accuracy and operational efficiency. Additionally, we present a command-line interface (CLI) tool that enables users to generate datasets using the same methodology, allowing for tailored assessments of LLMs. TMIQ and the CLI tool provide a rigorous, reproducible means of evaluating LLMs for production environments, facilitating continuous monitoring and identifying strengths and areas for improvement, and driving innovation in their selections for applications within the Test and Measurement industry.",Industries;Technological innovation;Accuracy;Automation;Large language models;Production;Benchmark testing;Cognition;Time factors;Standards;Artificial intelligence;Large language models;Synthetic LLM Benchmark;Test and measurement;Electronic engineering;Automation,"{'month': 'May', 'issn': '2642-2077', 'doi': '10.1109/I2MTC62753.2025.11079082', 'keywords': 'Industries;Technological innovation;Accuracy;Automation;Large language models;Production;Benchmark testing;Cognition;Time factors;Standards;Artificial intelligence;Large language models;Synthetic LLM Benchmark;Test and measurement;Electronic engineering;Automation', 'abstract': 'The Test and Measurement domain, known for its strict requirements for accuracy and efficiency, is increasingly adopting Generative AI technologies to enhance the performance of data analysis, automation, and decision-making processes. Among these, Large Language Models (LLMs) show significant promise for advancing automation and precision in testing. However, the evaluation of LLMs in this specialized area remains insufficiently explored. To address this gap, we introduce the Test and Measurement Intelligence Quotient (TMIQ), a benchmark designed to quantitatively assess LLMs across a wide range of electronic engineering tasks. TMIQ offers a comprehensive set of scenarios and metrics for detailed evaluation, including SCPI command matching accuracy, ranked response evaluation, Chain-of-Thought Reasoning (CoT), and the impact of output formatting variations required by LLMs on performance. In testing various LLMs, our findings indicate varying levels of proficiency, with exact SCPI command match accuracy ranging from around 56\\% to 73\\%, and ranked matching first-position scores achieving around 33\\% for the best-performing model. We also assess token usage, cost-efficiency, and response times, identifying trade-offs between accuracy and operational efficiency. Additionally, we present a command-line interface (CLI) tool that enables users to generate datasets using the same methodology, allowing for tailored assessments of LLMs. TMIQ and the CLI tool provide a rigorous, reproducible means of evaluating LLMs for production environments, facilitating continuous monitoring and identifying strengths and areas for improvement, and driving innovation in their selections for applications within the Test and Measurement industry.', 'pages': '1-6', 'number': '', 'volume': '', 'year': '2025', 'title': 'TMIQ: Quantifying Test and Measurement Domain Intelligence in Large Language Models', 'booktitle': '2025 IEEE International Instrumentation and Measurement Technology Conference (I2MTC)', 'author': 'Olowe, Emmanuel A. and Chitnis, Danial', 'ENTRYTYPE': 'inproceedings', 'ID': '11079082'}"
9679936,Anomaly Detection with Dual Adversarial Training,"Liu, Shuo and Xu, Liwen",Liu,10.1109/ICDMW53433.2021.00063,2021,2021 International Conference on Data Mining Workshops (ICDMW),"Anomaly detection is of paramount importance in data mining and artificial intelligence. Deep generative models have been widely used in anomaly detection as a dominant paradigm to model complex and high-dimensional data distribution. However, developing effective and robust anomaly detection systems for complex and high-dimensional data using generative models remains a challenge. In this paper, we propose a novel Dual Adversarial Training method for Anomaly Detection (DAT-AD), which uses the adversarial training idea in Generative Adversarial Network (GAN) and the Virtual Adversarial Training (VAT) idea to improve the effectiveness and robustness of anomaly detector, respectively. In addition, we have also carefully designed the network architecture and the loss function of our method to ensure that the trained network can be utilized to the greatest extent. We demonstrate the superiority of our method by conducting various experiments on tabular and image data.",Training;Conferences;Detectors;Network architecture;Benchmark testing;Generative adversarial networks;Data models;data mining;anomaly detection;deep generative model;virtual adversarial training,"{'month': 'Dec', 'issn': '2375-9259', 'doi': '10.1109/ICDMW53433.2021.00063', 'keywords': 'Training;Conferences;Detectors;Network architecture;Benchmark testing;Generative adversarial networks;Data models;data mining;anomaly detection;deep generative model;virtual adversarial training', 'abstract': 'Anomaly detection is of paramount importance in data mining and artificial intelligence. Deep generative models have been widely used in anomaly detection as a dominant paradigm to model complex and high-dimensional data distribution. However, developing effective and robust anomaly detection systems for complex and high-dimensional data using generative models remains a challenge. In this paper, we propose a novel Dual Adversarial Training method for Anomaly Detection (DAT-AD), which uses the adversarial training idea in Generative Adversarial Network (GAN) and the Virtual Adversarial Training (VAT) idea to improve the effectiveness and robustness of anomaly detector, respectively. In addition, we have also carefully designed the network architecture and the loss function of our method to ensure that the trained network can be utilized to the greatest extent. We demonstrate the superiority of our method by conducting various experiments on tabular and image data.', 'pages': '466-473', 'number': '', 'volume': '', 'year': '2021', 'title': 'Anomaly Detection with Dual Adversarial Training', 'booktitle': '2021 International Conference on Data Mining Workshops (ICDMW)', 'author': 'Liu, Shuo and Xu, Liwen', 'ENTRYTYPE': 'inproceedings', 'ID': '9679936'}"
8930532,Dualattn-GAN: Text to Image Synthesis With Dual Attentional Generative Adversarial Network,"Cai, Yali and Wang, Xiaoru and Yu, Zhihong and Li, Fu and Xu, Peirong and Li, Yueli and Li, Lixian",Cai,10.1109/ACCESS.2019.2958864,2019,IEEE Access,"Recent generative adversarial network based methods have shown promising results for the charming but challenging task of synthesizing images from text descriptions. These approaches can generate images with general shape and color but often produce distorted global structures with unnatural local semantic details. It is due to ineffectiveness of convolutional neural networks in capturing the high-level semantic information for pixel-level image synthesis. In this paper, we propose a Dual Attentional Generative Adversarial Network (DualAttn-GAN) in which the dual attention modules are introduced to enhance local details and global structures by attending to related features from relevant words and different visual regions. As one of the dual modules, the textual attention module is designed to explore the fine-grained interaction between vision and language. On the other hand, visual attention module models internal representations of vision from channel and spatial axes, which can better capture the global structures. Meanwhile, we apply an attention embedding module to merge multi-path features. Furthermore, we present an inverted residual structure to boost representation power of CNNs and apply spectral normalization to stabilize GAN training. With extensive experimental validation on two benchmark datasets, our method significantly improves state-of-the-art models over the evaluation metrics of inception score and Fréchet inception distance.",Visualization;Generative adversarial networks;Gallium nitride;Semantics;Training;Image synthesis;Task analysis;Generative adversarial network;textual attention;visual attention;inverted residual structure;spectral normalization,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2019.2958864', 'keywords': 'Visualization;Generative adversarial networks;Gallium nitride;Semantics;Training;Image synthesis;Task analysis;Generative adversarial network;textual attention;visual attention;inverted residual structure;spectral normalization', 'abstract': 'Recent generative adversarial network based methods have shown promising results for the charming but challenging task of synthesizing images from text descriptions. These approaches can generate images with general shape and color but often produce distorted global structures with unnatural local semantic details. It is due to ineffectiveness of convolutional neural networks in capturing the high-level semantic information for pixel-level image synthesis. In this paper, we propose a Dual Attentional Generative Adversarial Network (DualAttn-GAN) in which the dual attention modules are introduced to enhance local details and global structures by attending to related features from relevant words and different visual regions. As one of the dual modules, the textual attention module is designed to explore the fine-grained interaction between vision and language. On the other hand, visual attention module models internal representations of vision from channel and spatial axes, which can better capture the global structures. Meanwhile, we apply an attention embedding module to merge multi-path features. Furthermore, we present an inverted residual structure to boost representation power of CNNs and apply spectral normalization to stabilize GAN training. With extensive experimental validation on two benchmark datasets, our method significantly improves state-of-the-art models over the evaluation metrics of inception score and Fréchet inception distance.', 'pages': '183706-183716', 'number': '', 'volume': '7', 'year': '2019', 'title': 'Dualattn-GAN: Text to Image Synthesis With Dual Attentional Generative Adversarial Network', 'journal': 'IEEE Access', 'author': 'Cai, Yali and Wang, Xiaoru and Yu, Zhihong and Li, Fu and Xu, Peirong and Li, Yueli and Li, Lixian', 'ENTRYTYPE': 'article', 'ID': '8930532'}"
8645630,Single Image Snow Removal via Composition Generative Adversarial Networks,"Li, Zhi and Zhang, Juan and Fang, Zhijun and Huang, Bo and Jiang, Xiaoyan and Gao, Yongbin and Hwang, Jenq-Neng",Li,10.1109/ACCESS.2019.2900323,2019,IEEE Access,"Snowflakes attached to the camera lens can severely affect the visibility of the background scene and compromise the image quality. In this paper, we solve this problem by visually removing snowflakes to convert the snowy image into a clean one. The problem is troublesome; the information about the background of the occluded regions is completely lost for the most part. For removing snowflakes from a single image, we proposed a composition generative adversarial network. Different from the previous generative adversarial networks, our generator network comprises clean background module and a snow mask estimate module. The clean background module aims to generate a clear image from an input snowy image, and snow mask estimate module is used to produce the snow mask in an input image. During the training step, we put forward a composition loss between the input snowy image and composition of the generated clean image and estimated snow mask. We use a dataset named Snow100K2 including indoor and outdoor scenes to train and test the proposed method. The extensive experiments on both synthetic and real-world images show that our network has a good effect and it is superior to the other state-of-the-art methods.",Snow;Rain;Generators;Gallium nitride;Generative adversarial networks;Image enhancement;Remove snowflakes;composition generative adversarial network;dataset,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2019.2900323', 'keywords': 'Snow;Rain;Generators;Gallium nitride;Generative adversarial networks;Image enhancement;Remove snowflakes;composition generative adversarial network;dataset', 'abstract': 'Snowflakes attached to the camera lens can severely affect the visibility of the background scene and compromise the image quality. In this paper, we solve this problem by visually removing snowflakes to convert the snowy image into a clean one. The problem is troublesome; the information about the background of the occluded regions is completely lost for the most part. For removing snowflakes from a single image, we proposed a composition generative adversarial network. Different from the previous generative adversarial networks, our generator network comprises clean background module and a snow mask estimate module. The clean background module aims to generate a clear image from an input snowy image, and snow mask estimate module is used to produce the snow mask in an input image. During the training step, we put forward a composition loss between the input snowy image and composition of the generated clean image and estimated snow mask. We use a dataset named Snow100K2 including indoor and outdoor scenes to train and test the proposed method. The extensive experiments on both synthetic and real-world images show that our network has a good effect and it is superior to the other state-of-the-art methods.', 'pages': '25016-25025', 'number': '', 'volume': '7', 'year': '2019', 'title': 'Single Image Snow Removal via Composition Generative Adversarial Networks', 'journal': 'IEEE Access', 'author': 'Li, Zhi and Zhang, Juan and Fang, Zhijun and Huang, Bo and Jiang, Xiaoyan and Gao, Yongbin and Hwang, Jenq-Neng', 'ENTRYTYPE': 'article', 'ID': '8645630'}"
10210017,Credit Card Fraud Detection Based on Improved Variational Autoencoder Generative Adversarial Network,"Ding, Yuanming and Kang, Wei and Feng, Jianxin and Peng, Bo and Yang, Anna",Ding,10.1109/ACCESS.2023.3302339,2023,IEEE Access,"The rapid spread of mobile banking and e-commerce has coincided with a dramatic increase in fraudulent online payments in recent years. Although machine learning and deep learning are widely used in credit card fraud detection, the typical credit card transaction data set is unbalanced, and the fraud data is much less than the normal transaction data, limiting the effectiveness of traditional binary classification algorithms. To overcome this issue, researchers oversample minority class data and utilize ensemble learning classification algorithms. However, oversampling still has disadvantages. Hence, we improve the generator part of the Variational Autoencoder Generative Adversarial Network (VAEGAN) and propose a new oversampling method that generates convincing and diverse minority class data. The training set is enhanced by generating minority class fraud data to train the ensemble learning classification model. The method is tested on an open credit card dataset, with the experimental results demonstrating that the oversampling method utilizing the improved VAEGAN is superior to the oversampling method of Generative Adversarial Network (GAN), Variational Autoencoder (VAE), and Synthetic Minority Oversampling Technique (SMOTE) in terms of Precision, F1\_score, and other indicators. The oversampling method based on the improved VAEGAN effectively deals with the classification problem of imbalanced data.",Fraud;Credit cards;Generative adversarial networks;Data models;Machine learning algorithms;Ensemble learning;Encoding;Electronic commerce;Banking;Credit card fraud;ensemble learning;variational autoencoder generative adversarial network;oversampling,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2023.3302339', 'keywords': 'Fraud;Credit cards;Generative adversarial networks;Data models;Machine learning algorithms;Ensemble learning;Encoding;Electronic commerce;Banking;Credit card fraud;ensemble learning;variational autoencoder generative adversarial network;oversampling', 'abstract': 'The rapid spread of mobile banking and e-commerce has coincided with a dramatic increase in fraudulent online payments in recent years. Although machine learning and deep learning are widely used in credit card fraud detection, the typical credit card transaction data set is unbalanced, and the fraud data is much less than the normal transaction data, limiting the effectiveness of traditional binary classification algorithms. To overcome this issue, researchers oversample minority class data and utilize ensemble learning classification algorithms. However, oversampling still has disadvantages. Hence, we improve the generator part of the Variational Autoencoder Generative Adversarial Network (VAEGAN) and propose a new oversampling method that generates convincing and diverse minority class data. The training set is enhanced by generating minority class fraud data to train the ensemble learning classification model. The method is tested on an open credit card dataset, with the experimental results demonstrating that the oversampling method utilizing the improved VAEGAN is superior to the oversampling method of Generative Adversarial Network (GAN), Variational Autoencoder (VAE), and Synthetic Minority Oversampling Technique (SMOTE) in terms of Precision, F1\\_score, and other indicators. The oversampling method based on the improved VAEGAN effectively deals with the classification problem of imbalanced data.', 'pages': '83680-83691', 'number': '', 'volume': '11', 'year': '2023', 'title': 'Credit Card Fraud Detection Based on Improved Variational Autoencoder Generative Adversarial Network', 'journal': 'IEEE Access', 'author': 'Ding, Yuanming and Kang, Wei and Feng, Jianxin and Peng, Bo and Yang, Anna', 'ENTRYTYPE': 'article', 'ID': '10210017'}"
8911320,Multi-Source Medical Image Fusion Based on Wasserstein Generative Adversarial Networks,"Yang, Zhiguang and Chen, Youping and Le, Zhuliang and Fan, Fan and Pan, Erting",Yang,10.1109/ACCESS.2019.2955382,2019,IEEE Access,"In this paper, we propose the medical Wasserstein generative adversarial networks (MWGAN), an end-to-end model, for fusing magnetic resonance imaging (MRI) and positron emission tomography (PET) medical images. Our method establishes two adversarial games between a generator and two discriminators to generate a fused image with the details of soft tissue structures in organs from MRI images and the functional and metabolic information from PET images. Different information from source images can be effectively adjusted with a specifically designed loss function. In addition, we use WGAN instead of the traditional generative adversarial networks to make the training process more stable and allow our architecture to deal with source images of different resolutions. Qualitative and quantitative comparisons on publicly available datasets demonstrate the superiority of MWGAN over the state-of-the-art networks. Furthermore, our MWGAN is applied to the fusion of MRI and computed tomography images of different resolutions, achieving a satisfactory performance.",Magnetic resonance imaging;Image fusion;Generative adversarial networks;Biomedical imaging;Deep learning;Training;Image resolution;Medical image fusion;Wasserstein generative adversarial networks;end-to-end;different resolutions,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2019.2955382', 'keywords': 'Magnetic resonance imaging;Image fusion;Generative adversarial networks;Biomedical imaging;Deep learning;Training;Image resolution;Medical image fusion;Wasserstein generative adversarial networks;end-to-end;different resolutions', 'abstract': 'In this paper, we propose the medical Wasserstein generative adversarial networks (MWGAN), an end-to-end model, for fusing magnetic resonance imaging (MRI) and positron emission tomography (PET) medical images. Our method establishes two adversarial games between a generator and two discriminators to generate a fused image with the details of soft tissue structures in organs from MRI images and the functional and metabolic information from PET images. Different information from source images can be effectively adjusted with a specifically designed loss function. In addition, we use WGAN instead of the traditional generative adversarial networks to make the training process more stable and allow our architecture to deal with source images of different resolutions. Qualitative and quantitative comparisons on publicly available datasets demonstrate the superiority of MWGAN over the state-of-the-art networks. Furthermore, our MWGAN is applied to the fusion of MRI and computed tomography images of different resolutions, achieving a satisfactory performance.', 'pages': '175947-175958', 'number': '', 'volume': '7', 'year': '2019', 'title': 'Multi-Source Medical Image Fusion Based on Wasserstein Generative Adversarial Networks', 'journal': 'IEEE Access', 'author': 'Yang, Zhiguang and Chen, Youping and Le, Zhuliang and Fan, Fan and Pan, Erting', 'ENTRYTYPE': 'article', 'ID': '8911320'}"
8793240,Resolution-Preserving Generative Adversarial Networks for Image Enhancement,"Lee, Donghyeon and Lee, Sangheon and Lee, Hoseong and Lee, Kyujoong and Lee, Hyuk-Jae",Lee,10.1109/ACCESS.2019.2934320,2019,IEEE Access,"Generative adversarial networks (GANs) are used for image enhancement such as single image super-resolution (SISR) and deblurring. The conventional GANs-based image enhancement suffers from two drawbacks that cause a quality degradation due to a loss of detailed information. First, the conventional discriminator network adopts strided convolution layers which cause a reduction in the resolution of the feature map, and thereby resulting in a loss of detailed information. Second, the previous GANs for image enhancement use the feature map of the visual geometry group (VGG) network for generating a content loss, which also causes visual artifacts because the maxpooling layers in the VGG network result in a loss of detailed information. To overcome these two drawbacks, this paper presents a proposal of a new resolution-preserving discriminator network architecture which removes the strided convolution layers, and a new content loss generated from the VGG network without maxpooling layers. The proposed discriminator network is applied to the super-resolution generative adversarial network (SRGAN), which is called a resolution-preserving SRGAN (RPSRGAN). Experimental results show that RPSRGAN generates more realistic super-resolution images than SRGAN does, and consequently, RPSRGAN with the new content loss improves the average peak signal-to-noise ratio (PSNR) by 0.75 dB and 0.32 dB for super-resolution images with the scale factors of 2 and 4, respectively. For deblurring, the visual appearance is also significantly improved, and the average PSNR is increased by 1.54 dB when the proposed discriminator and content loss are applied to the deblurring adversarial network.",Generative adversarial networks;Three-dimensional displays;Convolution;Image enhancement;Iron;Gallium nitride;Single image super-resolution;deblurring;generative adversarial networks;image enhancement,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2019.2934320', 'keywords': 'Generative adversarial networks;Three-dimensional displays;Convolution;Image enhancement;Iron;Gallium nitride;Single image super-resolution;deblurring;generative adversarial networks;image enhancement', 'abstract': 'Generative adversarial networks (GANs) are used for image enhancement such as single image super-resolution (SISR) and deblurring. The conventional GANs-based image enhancement suffers from two drawbacks that cause a quality degradation due to a loss of detailed information. First, the conventional discriminator network adopts strided convolution layers which cause a reduction in the resolution of the feature map, and thereby resulting in a loss of detailed information. Second, the previous GANs for image enhancement use the feature map of the visual geometry group (VGG) network for generating a content loss, which also causes visual artifacts because the maxpooling layers in the VGG network result in a loss of detailed information. To overcome these two drawbacks, this paper presents a proposal of a new resolution-preserving discriminator network architecture which removes the strided convolution layers, and a new content loss generated from the VGG network without maxpooling layers. The proposed discriminator network is applied to the super-resolution generative adversarial network (SRGAN), which is called a resolution-preserving SRGAN (RPSRGAN). Experimental results show that RPSRGAN generates more realistic super-resolution images than SRGAN does, and consequently, RPSRGAN with the new content loss improves the average peak signal-to-noise ratio (PSNR) by 0.75 dB and 0.32 dB for super-resolution images with the scale factors of 2 and 4, respectively. For deblurring, the visual appearance is also significantly improved, and the average PSNR is increased by 1.54 dB when the proposed discriminator and content loss are applied to the deblurring adversarial network.', 'pages': '110344-110357', 'number': '', 'volume': '7', 'year': '2019', 'title': 'Resolution-Preserving Generative Adversarial Networks for Image Enhancement', 'journal': 'IEEE Access', 'author': 'Lee, Donghyeon and Lee, Sangheon and Lee, Hoseong and Lee, Kyujoong and Lee, Hyuk-Jae', 'ENTRYTYPE': 'article', 'ID': '8793240'}"
9530576,Generative Adversarial Networks for Abnormal Event Detection in Videos Based on Self-Attention Mechanism,"Zhang, Weichao and Wang, Guanjun and Huang, Mengxing and Wang, Hongyu and Wen, Shaoping",Zhang,10.1109/ACCESS.2021.3110798,2021,IEEE Access,"Unsupervised anomaly detection defines an abnormal event as an event that does not conform to expected behavior. In the field of unsupervised anomaly detection, it is a pioneering work that leverages the difference between a future frame predicted by a generative adversarial network and its ground truth to detect an abnormal event. Based on the work, we improve the ability of video prediction framework to detect abnormal events by enhancing the difference between prediction results for normal and abnormal events. We incorporate super-resolution and self-attention mechanism to design a generative adversarial network. We propose an auto-encoder as a generator, which incorporates dense residual networks and self-attention. Moreover, we propose a new discriminator, which introduces self-attention on the basis of a relativistic discriminator. To predict a future frame with higher quality for normal events, we impose a constraint on the motion in video prediction by fusing optical flow and gradient difference between frames. We also introduce a perception constraint in video prediction to enrich the texture details of a frame. The AUC of our method on CUHK Avenue and Shanghai Tech datasets reaches 89.2\% and 75.7\% respectively, which is better than most existing methods. In addition, we propose a processing flow that can realize real-time anomaly detection in videos. The average running time of our video prediction framework is 37 frames per second. Among all real-time methods for abnormal event detection in videos, our method is competitive with the state-of-the-art methods.",Streaming media;Event detection;Anomaly detection;Feature extraction;Generative adversarial networks;Generators;Training;Abnormal event detection;generative adversarial networks (GANs);self-attention;video understanding,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2021.3110798', 'keywords': 'Streaming media;Event detection;Anomaly detection;Feature extraction;Generative adversarial networks;Generators;Training;Abnormal event detection;generative adversarial networks (GANs);self-attention;video understanding', 'abstract': 'Unsupervised anomaly detection defines an abnormal event as an event that does not conform to expected behavior. In the field of unsupervised anomaly detection, it is a pioneering work that leverages the difference between a future frame predicted by a generative adversarial network and its ground truth to detect an abnormal event. Based on the work, we improve the ability of video prediction framework to detect abnormal events by enhancing the difference between prediction results for normal and abnormal events. We incorporate super-resolution and self-attention mechanism to design a generative adversarial network. We propose an auto-encoder as a generator, which incorporates dense residual networks and self-attention. Moreover, we propose a new discriminator, which introduces self-attention on the basis of a relativistic discriminator. To predict a future frame with higher quality for normal events, we impose a constraint on the motion in video prediction by fusing optical flow and gradient difference between frames. We also introduce a perception constraint in video prediction to enrich the texture details of a frame. The AUC of our method on CUHK Avenue and Shanghai Tech datasets reaches 89.2\\% and 75.7\\% respectively, which is better than most existing methods. In addition, we propose a processing flow that can realize real-time anomaly detection in videos. The average running time of our video prediction framework is 37 frames per second. Among all real-time methods for abnormal event detection in videos, our method is competitive with the state-of-the-art methods.', 'pages': '124847-124860', 'number': '', 'volume': '9', 'year': '2021', 'title': 'Generative Adversarial Networks for Abnormal Event Detection in Videos Based on Self-Attention Mechanism', 'journal': 'IEEE Access', 'author': 'Zhang, Weichao and Wang, Guanjun and Huang, Mengxing and Wang, Hongyu and Wen, Shaoping', 'ENTRYTYPE': 'article', 'ID': '9530576'}"
9212411,SR-ITM-GAN: Learning 4K UHD HDR With a Generative Adversarial Network,"Zeng, Huimin and Zhang, Xinliang and Yu, Zhibin and Wang, Yubo",Zeng,10.1109/ACCESS.2020.3028584,2020,IEEE Access,"Currently, high dynamic range (HDR) videos with high resolution (HR) have become popular due to the display and the rendered technological advancements. However, making ultra-high definition (UHD) with HDR videos is expensive. The legacy low-resolution (LR) standard dynamic range (SDR) format is still largely used in practice. It is necessary to search for a solution to transform LR SDR videos into UHD HDR format. In this paper, we consider joint super resolution and learning inverse tone mapping an issue of high-frequency reconstruction and local contrast enhancement, and we propose an architecture based on a generative adversarial network to apply joint SR-ITM learning. Specifically, we include the residual ResNeXt block (RRXB) as a basic module to better capture high-frequency textures and adopt YUV interpolation to achieve local contrast enhancement. By adopting a generative adversarial network as a pivotal training mechanism, our designs show advantages in both integration and performance. Our code is now available on GitHub: SR-ITM-GAN.",Generative adversarial networks;Videos;Task analysis;UHDTV;Image reconstruction;Image resolution;Computer architecture;Super resolution;inverse tone mapping;generative adversarial network;high dynamic range,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2020.3028584', 'keywords': 'Generative adversarial networks;Videos;Task analysis;UHDTV;Image reconstruction;Image resolution;Computer architecture;Super resolution;inverse tone mapping;generative adversarial network;high dynamic range', 'abstract': 'Currently, high dynamic range (HDR) videos with high resolution (HR) have become popular due to the display and the rendered technological advancements. However, making ultra-high definition (UHD) with HDR videos is expensive. The legacy low-resolution (LR) standard dynamic range (SDR) format is still largely used in practice. It is necessary to search for a solution to transform LR SDR videos into UHD HDR format. In this paper, we consider joint super resolution and learning inverse tone mapping an issue of high-frequency reconstruction and local contrast enhancement, and we propose an architecture based on a generative adversarial network to apply joint SR-ITM learning. Specifically, we include the residual ResNeXt block (RRXB) as a basic module to better capture high-frequency textures and adopt YUV interpolation to achieve local contrast enhancement. By adopting a generative adversarial network as a pivotal training mechanism, our designs show advantages in both integration and performance. Our code is now available on GitHub: SR-ITM-GAN.', 'pages': '182815-182827', 'number': '', 'volume': '8', 'year': '2020', 'title': 'SR-ITM-GAN: Learning 4K UHD HDR With a Generative Adversarial Network', 'journal': 'IEEE Access', 'author': 'Zeng, Huimin and Zhang, Xinliang and Yu, Zhibin and Wang, Yubo', 'ENTRYTYPE': 'article', 'ID': '9212411'}"
10226181,High-Resolution 3D MRI With Deep Generative Networks via Novel Slice-Profile Transformation Super-Resolution,"Lin, Jiahao and Miao, Qi and Surawech, Chuthaporn and Raman, Steven S. and Zhao, Kai and Wu, Holden H. and Sung, Kyunghyun",Lin,10.1109/ACCESS.2023.3307577,2023,IEEE Access,"High-resolution magnetic resonance imaging (MRI) sequences, such as 3D turbo or fast spin-echo (TSE/FSE) imaging, are clinically desirable but suffer from long scanning time-related blurring when reformatted into preferred orientations. Instead, multi-slice two-dimensional (2D) TSE imaging is commonly used because of its high in-plane resolution but is limited clinically by poor through-plane resolution due to elongated voxels and the inability to generate multi-planar reformations due to staircase artifacts. Therefore, multiple 2D TSE scans are acquired in various orthogonal imaging planes, increasing the overall MRI scan time. In this study, we propose a novel slice-profile transformation super-resolution (SPTSR) framework with deep generative learning for through-plane super-resolution (SR) of multi-slice 2D TSE imaging. The deep generative networks were trained by synthesized low-resolution training input via slice-profile downsampling (SP-DS), and the trained networks inferred on the slice profile convolved (SP-conv) testing input for 5.5x through-plane SR. The network output was further slice-profile deconvolved (SP-deconv) to achieve an isotropic super-resolution. Compared to SMORE SR method and the networks trained by conventional downsampling, our SPTSR framework demonstrated the best overall image quality from 50 testing cases, evaluated by two abdominal radiologists. The quantitative analysis cross-validated the expert reader study results. 3D simulation experiments confirmed the quantitative improvement of the proposed SPTSR and the effectiveness of the SP-deconv step, compared to 3D ground-truths. Ablation studies were conducted on the individual contributions of SP-DS and SP-conv, networks structure, training dataset size, and different slice profiles.",Magnetic resonance imaging;Image resolution;Three-dimensional displays;Superresolution;Training;Image reconstruction;Generative adversarial networks;Deep learning;Generative adversarial networks;magnetic resonance imaging;turbo spin echo;slice profile;super-resolution,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2023.3307577', 'keywords': 'Magnetic resonance imaging;Image resolution;Three-dimensional displays;Superresolution;Training;Image reconstruction;Generative adversarial networks;Deep learning;Generative adversarial networks;magnetic resonance imaging;turbo spin echo;slice profile;super-resolution', 'abstract': 'High-resolution magnetic resonance imaging (MRI) sequences, such as 3D turbo or fast spin-echo (TSE/FSE) imaging, are clinically desirable but suffer from long scanning time-related blurring when reformatted into preferred orientations. Instead, multi-slice two-dimensional (2D) TSE imaging is commonly used because of its high in-plane resolution but is limited clinically by poor through-plane resolution due to elongated voxels and the inability to generate multi-planar reformations due to staircase artifacts. Therefore, multiple 2D TSE scans are acquired in various orthogonal imaging planes, increasing the overall MRI scan time. In this study, we propose a novel slice-profile transformation super-resolution (SPTSR) framework with deep generative learning for through-plane super-resolution (SR) of multi-slice 2D TSE imaging. The deep generative networks were trained by synthesized low-resolution training input via slice-profile downsampling (SP-DS), and the trained networks inferred on the slice profile convolved (SP-conv) testing input for 5.5x through-plane SR. The network output was further slice-profile deconvolved (SP-deconv) to achieve an isotropic super-resolution. Compared to SMORE SR method and the networks trained by conventional downsampling, our SPTSR framework demonstrated the best overall image quality from 50 testing cases, evaluated by two abdominal radiologists. The quantitative analysis cross-validated the expert reader study results. 3D simulation experiments confirmed the quantitative improvement of the proposed SPTSR and the effectiveness of the SP-deconv step, compared to 3D ground-truths. Ablation studies were conducted on the individual contributions of SP-DS and SP-conv, networks structure, training dataset size, and different slice profiles.', 'pages': '95022-95036', 'number': '', 'volume': '11', 'year': '2023', 'title': 'High-Resolution 3D MRI With Deep Generative Networks via Novel Slice-Profile Transformation Super-Resolution', 'journal': 'IEEE Access', 'author': 'Lin, Jiahao and Miao, Qi and Surawech, Chuthaporn and Raman, Steven S. and Zhao, Kai and Wu, Holden H. and Sung, Kyunghyun', 'ENTRYTYPE': 'article', 'ID': '10226181'}"
9803044,Generative Facial Prior and Semantic Guidance for Iterative Face Inpainting,"Zhang, Xin-Yu and Xie, Kai and Li, Mei-Ran and Wen, Chang and He, Jian-Biao",Zhang,10.1109/ACCESS.2022.3185210,2022,IEEE Access,"Image inpainting techniques have been greatly improved by relying on structure and texture priors. However, damaged original images or rough predictions cannot provide sufficient texture information and accurate structural priors, leading to a drop in image quality. Moreover, from the perspective of human visual perception, it is important to pay attention to facial symmetry and facial attribute consistency. In this paper, we present a face inpainting system with iteration structure, guided by generative facial priors contained in pretrained GANs and predicted semantic information. Specifically, generative facial priors generated by the GAN inversion techniques introduce sufficient textures and features to assist inpainting; semantic maps are able to provide facial structural information and semantic categories of different pixels for face reconstruction. In particular, we iteratively refine images multiple times, updating semantic maps at each iteration. The Weighted Prior-Guidance Modulation layer (WPGM) is devised for incorporating priors into networks through spatial modulation. We also propose facial feature self-symmetry loss to constrain the symmetry of faces in feature space. Experiments on CelebA-HQ and LaPa datasets demonstrate the superiority of our model for facial detail and attribute consistency. Meanwhile, under the background of COVID-19, it is worth trying recognition via inpainting to deal with recognition challenges brought by mask occlusion. Relevant experiments show that our inpainting model does help to recognition tasks to a certain degree, with higher accuracy.",Face recognition;Semantics;Faces;Iterative methods;Generative adversarial networks;Facial features;Shape;Face inpainting;semantic prior;generative facial prior;iterative structure;face symmetry;recognition via inpainting,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2022.3185210', 'keywords': 'Face recognition;Semantics;Faces;Iterative methods;Generative adversarial networks;Facial features;Shape;Face inpainting;semantic prior;generative facial prior;iterative structure;face symmetry;recognition via inpainting', 'abstract': 'Image inpainting techniques have been greatly improved by relying on structure and texture priors. However, damaged original images or rough predictions cannot provide sufficient texture information and accurate structural priors, leading to a drop in image quality. Moreover, from the perspective of human visual perception, it is important to pay attention to facial symmetry and facial attribute consistency. In this paper, we present a face inpainting system with iteration structure, guided by generative facial priors contained in pretrained GANs and predicted semantic information. Specifically, generative facial priors generated by the GAN inversion techniques introduce sufficient textures and features to assist inpainting; semantic maps are able to provide facial structural information and semantic categories of different pixels for face reconstruction. In particular, we iteratively refine images multiple times, updating semantic maps at each iteration. The Weighted Prior-Guidance Modulation layer (WPGM) is devised for incorporating priors into networks through spatial modulation. We also propose facial feature self-symmetry loss to constrain the symmetry of faces in feature space. Experiments on CelebA-HQ and LaPa datasets demonstrate the superiority of our model for facial detail and attribute consistency. Meanwhile, under the background of COVID-19, it is worth trying recognition via inpainting to deal with recognition challenges brought by mask occlusion. Relevant experiments show that our inpainting model does help to recognition tasks to a certain degree, with higher accuracy.', 'pages': '66757-66769', 'number': '', 'volume': '10', 'year': '2022', 'title': 'Generative Facial Prior and Semantic Guidance for Iterative Face Inpainting', 'journal': 'IEEE Access', 'author': 'Zhang, Xin-Yu and Xie, Kai and Li, Mei-Ran and Wen, Chang and He, Jian-Biao', 'ENTRYTYPE': 'article', 'ID': '9803044'}"
10662895,LabelGen: An Anomaly Label Generative Framework for Enhanced Graph Anomaly Detection,"Xia, Siqi and Rajasegarar, Sutharshan and Pan, Lei and Leckie, Christopher and Erfani, Sarah M. and Chan, Jeffrey",Xia,10.1109/ACCESS.2024.3453178,2024,IEEE Access,"Anomaly detection in graphs is increasingly used to reveal fraud, fakes, security attacks and unusual behaviours in networks, such as social networks, financial transaction networks and the Internet of Things. Accurately detecting such graph anomalies using deep learning approaches faces challenges in terms of obtaining sufficient labelled data, as well as an imbalance between normal and anomalous instances. These contribute to model bias or over-fitting problems and inferior anomaly detection outcomes. In order to address these challenges in graphs, we propose a novel generative framework, called LabelGen, that can generate additional anomalous labels, in terms of graph objects, such as nodes, through augmentation and provide updated deep embedding for the graph concurrently. In particular, we propose the use of a k-hop neighborhood sampling strategy, an anomaly scoring mechanism and an adversarial learning framework with a generator and discriminator pair in order to generate sufficient and informative anomalous nodes that closely resemble the characteristics of existing anomalies in the graph. Evaluation on benchmark network datasets, as well as ablation and comparison studies with random label generation processes and other existing works reveal that the proposed generative framework is superior in improving the anomaly detection accuracy in graphs, while achieving a balanced trade-off between accuracy and computational efficiency.",Anomaly detection;Generators;Training;Detectors;Data models;Data augmentation;Generative adversarial networks;Deep learning;Fraud;Network security;Graphical models;Anomalies in graphs;generative adversarial networks;k-hop neighborhood sampling;deep learning,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2024.3453178', 'keywords': 'Anomaly detection;Generators;Training;Detectors;Data models;Data augmentation;Generative adversarial networks;Deep learning;Fraud;Network security;Graphical models;Anomalies in graphs;generative adversarial networks;k-hop neighborhood sampling;deep learning', 'abstract': 'Anomaly detection in graphs is increasingly used to reveal fraud, fakes, security attacks and unusual behaviours in networks, such as social networks, financial transaction networks and the Internet of Things. Accurately detecting such graph anomalies using deep learning approaches faces challenges in terms of obtaining sufficient labelled data, as well as an imbalance between normal and anomalous instances. These contribute to model bias or over-fitting problems and inferior anomaly detection outcomes. In order to address these challenges in graphs, we propose a novel generative framework, called LabelGen, that can generate additional anomalous labels, in terms of graph objects, such as nodes, through augmentation and provide updated deep embedding for the graph concurrently. In particular, we propose the use of a k-hop neighborhood sampling strategy, an anomaly scoring mechanism and an adversarial learning framework with a generator and discriminator pair in order to generate sufficient and informative anomalous nodes that closely resemble the characteristics of existing anomalies in the graph. Evaluation on benchmark network datasets, as well as ablation and comparison studies with random label generation processes and other existing works reveal that the proposed generative framework is superior in improving the anomaly detection accuracy in graphs, while achieving a balanced trade-off between accuracy and computational efficiency.', 'pages': '121971-121982', 'number': '', 'volume': '12', 'year': '2024', 'title': 'LabelGen: An Anomaly Label Generative Framework for Enhanced Graph Anomaly Detection', 'journal': 'IEEE Access', 'author': 'Xia, Siqi and Rajasegarar, Sutharshan and Pan, Lei and Leckie, Christopher and Erfani, Sarah M. and Chan, Jeffrey', 'ENTRYTYPE': 'article', 'ID': '10662895'}"
10623426,Research on an Improved Wasserstein Generative Adversarial Network Early Fault Warning Method for Rotating Machinery,"Zhou, Chunlei and Xiao, Wang and Wang, Qingfeng and Feng, Zhipeng",Zhou,10.1109/ACCESS.2024.3438753,2024,IEEE Access,"Early fault warning for large-scale high-speed rotating machinery can effectively reduce unplanned downtime and avoid major safety accidents. Aiming at the problems of difficult screening of multi-source common sensitive features, the challenging training of neural networks with a small number of sensitive features, and the difficulty of directly using generative adversarial networks for early fault warning, this paper constructs an early fault warning model based on multi-source common sensitive features and an improved Wasserstein generative adversarial network, proposing an early fault warning method for rotating machinery. The model was verified by using the open XJTU-SY bearing laboratory data, the P3409A centrifugal pump bearing fault engineering case data of a petrochemical company and the rotor system engineering case data of a circulating hydrogen centrifugal compressor of a petrochemical company. The early fault warning method of rotating machinery proposed in this paper warns the bearing fault of centrifugal pump 160 hours in advance and the rotor system fault of centrifugal compressor 1330 minutes in advance. Compared with the two published methods, the proposed method has better early fault warning effect, better normal and abnormal health index discrimination and less false warning.",Generative adversarial networks;Feature extraction;Machinery;Training;Time-frequency analysis;Noise;Rolling bearings;Rotating machines;deep learning;generative adversarial networks;early fault warning,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2024.3438753', 'keywords': 'Generative adversarial networks;Feature extraction;Machinery;Training;Time-frequency analysis;Noise;Rolling bearings;Rotating machines;deep learning;generative adversarial networks;early fault warning', 'abstract': 'Early fault warning for large-scale high-speed rotating machinery can effectively reduce unplanned downtime and avoid major safety accidents. Aiming at the problems of difficult screening of multi-source common sensitive features, the challenging training of neural networks with a small number of sensitive features, and the difficulty of directly using generative adversarial networks for early fault warning, this paper constructs an early fault warning model based on multi-source common sensitive features and an improved Wasserstein generative adversarial network, proposing an early fault warning method for rotating machinery. The model was verified by using the open XJTU-SY bearing laboratory data, the P3409A centrifugal pump bearing fault engineering case data of a petrochemical company and the rotor system engineering case data of a circulating hydrogen centrifugal compressor of a petrochemical company. The early fault warning method of rotating machinery proposed in this paper warns the bearing fault of centrifugal pump 160 hours in advance and the rotor system fault of centrifugal compressor 1330 minutes in advance. Compared with the two published methods, the proposed method has better early fault warning effect, better normal and abnormal health index discrimination and less false warning.', 'pages': '109109-109127', 'number': '', 'volume': '12', 'year': '2024', 'title': 'Research on an Improved Wasserstein Generative Adversarial Network Early Fault Warning Method for Rotating Machinery', 'journal': 'IEEE Access', 'author': 'Zhou, Chunlei and Xiao, Wang and Wang, Qingfeng and Feng, Zhipeng', 'ENTRYTYPE': 'article', 'ID': '10623426'}"
9469035,Gated Feature Aggregation for Height Estimation From Single Aerial Images,"Xing, Siyuan and Dong, Qiulei and Hu, Zhanyi",Xing,10.1109/LGRS.2021.3090470,2022,IEEE Geoscience and Remote Sensing Letters,"Height estimation from single images, strictly speaking, is an ill-posed problem. However, recently, it is shown that it is both possible and feasible to learn a mapping from image statistics to height information. In spite of recent efforts in this field, how to learn fine-shape preserving features, such as object boundaries and contours, is still an open issue. In this work, we propose a progressive learning network to estimate height information from single aerial images in a coarse-to-fine manner. In particular, a gated feature aggregation module is introduced to effectively combine low-level and high-level features. The proposed method is validated on three public datasets, including the Vaihingen dataset, the Potsdam dataset, and the DFC2019 dataset. Both quantitative and qualitative experimental results demonstrate that the proposed method can achieve more accurate height estimation from single aerial images, especially with better object boundary and contour preserving capability, than four related height estimation methods.",Estimation;Decoding;Logic gates;Training;Feature extraction;Testing;Encoding;Convolutional neural networks (CNNs);gate mechanism;height estimation;progressive refinement,"{'month': '', 'issn': '1558-0571', 'doi': '10.1109/LGRS.2021.3090470', 'keywords': 'Estimation;Decoding;Logic gates;Training;Feature extraction;Testing;Encoding;Convolutional neural networks (CNNs);gate mechanism;height estimation;progressive refinement', 'abstract': 'Height estimation from single images, strictly speaking, is an ill-posed problem. However, recently, it is shown that it is both possible and feasible to learn a mapping from image statistics to height information. In spite of recent efforts in this field, how to learn fine-shape preserving features, such as object boundaries and contours, is still an open issue. In this work, we propose a progressive learning network to estimate height information from single aerial images in a coarse-to-fine manner. In particular, a gated feature aggregation module is introduced to effectively combine low-level and high-level features. The proposed method is validated on three public datasets, including the Vaihingen dataset, the Potsdam dataset, and the DFC2019 dataset. Both quantitative and qualitative experimental results demonstrate that the proposed method can achieve more accurate height estimation from single aerial images, especially with better object boundary and contour preserving capability, than four related height estimation methods.', 'pages': '1-5', 'number': '', 'volume': '19', 'year': '2022', 'title': 'Gated Feature Aggregation for Height Estimation From Single Aerial Images', 'journal': 'IEEE Geoscience and Remote Sensing Letters', 'author': 'Xing, Siyuan and Dong, Qiulei and Hu, Zhanyi', 'ENTRYTYPE': 'article', 'ID': '9469035'}"
10856197,Generative AI-Aided Multimodal Parallel Offloading for AIGC Metaverse Service in IoT Networks,"Zeng, Weizhe and Zheng, Jie and Gao, Ling and Niu, Jinping and Ren, Jie and Wang, Hai and Cao, Rui and Ji, Shuo",Zeng,10.1109/JIOT.2025.3535623,2025,IEEE Internet of Things Journal,"Mobile edge computing (MEC) enabled artificial intelligence-generated content (AIGC) has garnered considerable attention. To support AIGC metaverse applications within MEC in Internet of Things (IoT) networks, it is effective to offload computation tasks, particularly those involving neural networks generative in AIGC, from mobile devices to edge clouds. Existing solutions typically assume the availability of a dedicated and powerful edge server for each user with single modal data, which can handle the entire AIGC service offloading. However, the practical availability of such dedicated and powerful servers may be limited, necessitating the utilization of less capable alternatives. Thus, we propose the multimodal parallel offloading AIGC framework which partitions multimodal content and offloads partial diffusion tasks to multiple servers. Our proposed scheme accelerates mobile deep vision multimodal metaverse applications through parallel offloading provided by multiple servers. We further utilize the generative AI scheme to solve offloading problems to adapt the dynamic and available communication and computing resource in wireless IoT network. Our framework proposed a multimodal parallel diffusion offloading scheme with integrating the recurrent region proposal prediction algorithm to optimize communication and computing resources while minimizing delay. Simulation results show that our approach can significantly reduce delay compared to conventional algorithms.",Servers;Metaverse;Resource management;Edge computing;Proposals;Heuristic algorithms;Generative AI;Wireless communication;Dynamic scheduling;Prediction algorithms;Artificial intelligence-generated content (AIGC);multimodal parallel offloading;region proposal prediction,"{'month': 'May', 'issn': '2327-4662', 'doi': '10.1109/JIOT.2025.3535623', 'keywords': 'Servers;Metaverse;Resource management;Edge computing;Proposals;Heuristic algorithms;Generative AI;Wireless communication;Dynamic scheduling;Prediction algorithms;Artificial intelligence-generated content (AIGC);multimodal parallel offloading;region proposal prediction', 'abstract': 'Mobile edge computing (MEC) enabled artificial intelligence-generated content (AIGC) has garnered considerable attention. To support AIGC metaverse applications within MEC in Internet of Things (IoT) networks, it is effective to offload computation tasks, particularly those involving neural networks generative in AIGC, from mobile devices to edge clouds. Existing solutions typically assume the availability of a dedicated and powerful edge server for each user with single modal data, which can handle the entire AIGC service offloading. However, the practical availability of such dedicated and powerful servers may be limited, necessitating the utilization of less capable alternatives. Thus, we propose the multimodal parallel offloading AIGC framework which partitions multimodal content and offloads partial diffusion tasks to multiple servers. Our proposed scheme accelerates mobile deep vision multimodal metaverse applications through parallel offloading provided by multiple servers. We further utilize the generative AI scheme to solve offloading problems to adapt the dynamic and available communication and computing resource in wireless IoT network. Our framework proposed a multimodal parallel diffusion offloading scheme with integrating the recurrent region proposal prediction algorithm to optimize communication and computing resources while minimizing delay. Simulation results show that our approach can significantly reduce delay compared to conventional algorithms.', 'pages': '13273-13285', 'number': '10', 'volume': '12', 'year': '2025', 'title': 'Generative AI-Aided Multimodal Parallel Offloading for AIGC Metaverse Service in IoT Networks', 'journal': 'IEEE Internet of Things Journal', 'author': 'Zeng, Weizhe and Zheng, Jie and Gao, Ling and Niu, Jinping and Ren, Jie and Wang, Hai and Cao, Rui and Ji, Shuo', 'ENTRYTYPE': 'article', 'ID': '10856197'}"
8372083,Identification of Generalized Communities with Semantics in Networks with Content,"Jin, Di and Wang, Xiaobao and He, Dongxiao and Lu, Wenhuan and Fogelman-Soulié, Francoise and Dang, Jianwu",Jin,10.1109/ICTAI.2017.00180,2017,2017 IEEE 29th International Conference on Tools with Artificial Intelligence (ICTAI),"Discovery of communities in networks is a fundamental data analysis task. Recently, researchers have tried to improve its performance by exploiting node contents, and further interpret the communities using the derived semantics. However, the existing methods typically assume that the communities are assortative (i.e. members of each group are mostly connected to other members of the same group), and are unable to find the generalized community structure, e.g. structures with either assortative or disassortative communities (i.e. vertices of the same group have most of their connections outside their group), or a combination. In addition, these methods often assume that the network topology and node contents share the same group memberships, and thus cannot perform well when the contents mismatch with network structure. Also, they are limited to using only one topic to interpret each community. To address these two issues, we propose a new generative probabilistic model which is learned by using a nested expectation-maximization algorithm. It describes the generalized communities (based on network) and the content clusters (based on contents) separately, and further explores and models their correlation to improve as much as possible each of the communities and clusters based on the other. By depicting and utilizing this correlation, our model is not only robust with respect to the above problems, but is also able to interpret each community using more than one topic, which provides richer explanations. We validate the robustness of this proposed new approach on an artificial benchmark, and test its interpretability using a case study analysis. We finally show its definite superiority for community detection by comparing with seven state-of-the-art algorithms on eight real networks.",Conferences;Tools;Artificial intelligence;Social networks;attributed network;community detection;generalized communities;probabilistic model;EM algorithm;semantics,"{'month': 'Nov', 'issn': '2375-0197', 'doi': '10.1109/ICTAI.2017.00180', 'keywords': 'Conferences;Tools;Artificial intelligence;Social networks;attributed network;community detection;generalized communities;probabilistic model;EM algorithm;semantics', 'abstract': 'Discovery of communities in networks is a fundamental data analysis task. Recently, researchers have tried to improve its performance by exploiting node contents, and further interpret the communities using the derived semantics. However, the existing methods typically assume that the communities are assortative (i.e. members of each group are mostly connected to other members of the same group), and are unable to find the generalized community structure, e.g. structures with either assortative or disassortative communities (i.e. vertices of the same group have most of their connections outside their group), or a combination. In addition, these methods often assume that the network topology and node contents share the same group memberships, and thus cannot perform well when the contents mismatch with network structure. Also, they are limited to using only one topic to interpret each community. To address these two issues, we propose a new generative probabilistic model which is learned by using a nested expectation-maximization algorithm. It describes the generalized communities (based on network) and the content clusters (based on contents) separately, and further explores and models their correlation to improve as much as possible each of the communities and clusters based on the other. By depicting and utilizing this correlation, our model is not only robust with respect to the above problems, but is also able to interpret each community using more than one topic, which provides richer explanations. We validate the robustness of this proposed new approach on an artificial benchmark, and test its interpretability using a case study analysis. We finally show its definite superiority for community detection by comparing with seven state-of-the-art algorithms on eight real networks.', 'pages': '1182-1189', 'number': '', 'volume': '', 'year': '2017', 'title': 'Identification of Generalized Communities with Semantics in Networks with Content', 'booktitle': '2017 IEEE 29th International Conference on Tools with Artificial Intelligence (ICTAI)', 'author': 'Jin, Di and Wang, Xiaobao and He, Dongxiao and Lu, Wenhuan and Fogelman-Soulié, Francoise and Dang, Jianwu', 'ENTRYTYPE': 'inproceedings', 'ID': '8372083'}"
10880605,"Inclusive Role of Internet of (Healthcare) Things in Digital Health: Challenges, Methods, and Future Directions","Abdalla, Mohammed",Abdalla,10.1002/9781394280735.ch12,2025,Generative Artificial Intelligence for Biomedical and Smart Health Informatics,"Summary <p>Healthcare systems could undergo a change with the incorporation of Internet of Things (IoT) technology, which would allow for enhanced analytics, real\&\#x2010;time data monitoring, and seamless communication. This chapter offers a thorough analysis of IoT applications in healthcare systems, emphasizing their influence on several facets of healthcare delivery, such as patient care, digital health, preventative medicine, remote monitoring, and future directions of IoT\&\#x2010;enabled healthcare systems. The first section of the chapter covers the basic elements of the IoT in the healthcare industry, including data networks, sensors, linked devices, and cloud\&\#x2010;based platforms. It looks at how wearable technology, smart healthcare devices that allow for continuous health tracking and real\&\#x2010;time data processing, and remote patient monitoring can all improve patient care. In the context of preventative healthcare, the potential of IoT to support personalized treatment and early diagnosis and intervention is explored. In addition, the chapter investigates how IoT integration affects healthcare systems, including smart medical devices, electronic health records, and hospital management systems. It looks at how IoT and digital health could help healthcare organizations make better decisions, use their resources more effectively, and run their operations more efficiently. There is also a discussion of the difficulties and factors involved in implementing IoT, such as privacy, interoperability, data security, and regulatory compliance. This chapter also emphasizes how IoT\&\#x2010;enabled healthcare systems have the potential to revolutionize the way that global healthcare issues, including managing chronic diseases, aging populations, and restricted access to healthcare services in remote places, are addressed. It highlights that to remove obstacles and promote the widespread use of IoT in healthcare, stakeholders, including technology developers, policymakers, researchers, and healthcare providers, must work together. Finally, the application of IoT technology to healthcare systems presents a plethora of chances to boost patient care, increase operational effectiveness, and spur innovation in the provision of healthcare services. The difficulties with data security, privacy, and interoperability, however, need to be carefully considered. The healthcare system may adopt a patient\&\#x2010;centered, data\&\#x2010;driven strategy and enhance health outcomes by utilizing the potential of the IoT. This will change the way healthcare is delivered in the digital age.</p>",Medical services;Medical diagnostic imaging;Pediatrics;Monitoring;Software;Medical devices;Heart;Electronic healthcare;Diabetes;Costs,"{'url': 'https://ieeexplore-ieee-org.crai.referencistas.com/document/10880605', 'isbn': '9781394280728', 'publisher': 'IEEE', 'issn': '', 'doi': '10.1002/9781394280735.ch12', 'keywords': 'Medical services;Medical diagnostic imaging;Pediatrics;Monitoring;Software;Medical devices;Heart;Electronic healthcare;Diabetes;Costs', 'abstract': 'Summary <p>Healthcare systems could undergo a change with the incorporation of Internet of Things (IoT) technology, which would allow for enhanced analytics, real\\&\\#x2010;time data monitoring, and seamless communication. This chapter offers a thorough analysis of IoT applications in healthcare systems, emphasizing their influence on several facets of healthcare delivery, such as patient care, digital health, preventative medicine, remote monitoring, and future directions of IoT\\&\\#x2010;enabled healthcare systems. The first section of the chapter covers the basic elements of the IoT in the healthcare industry, including data networks, sensors, linked devices, and cloud\\&\\#x2010;based platforms. It looks at how wearable technology, smart healthcare devices that allow for continuous health tracking and real\\&\\#x2010;time data processing, and remote patient monitoring can all improve patient care. In the context of preventative healthcare, the potential of IoT to support personalized treatment and early diagnosis and intervention is explored. In addition, the chapter investigates how IoT integration affects healthcare systems, including smart medical devices, electronic health records, and hospital management systems. It looks at how IoT and digital health could help healthcare organizations make better decisions, use their resources more effectively, and run their operations more efficiently. There is also a discussion of the difficulties and factors involved in implementing IoT, such as privacy, interoperability, data security, and regulatory compliance. This chapter also emphasizes how IoT\\&\\#x2010;enabled healthcare systems have the potential to revolutionize the way that global healthcare issues, including managing chronic diseases, aging populations, and restricted access to healthcare services in remote places, are addressed. It highlights that to remove obstacles and promote the widespread use of IoT in healthcare, stakeholders, including technology developers, policymakers, researchers, and healthcare providers, must work together. Finally, the application of IoT technology to healthcare systems presents a plethora of chances to boost patient care, increase operational effectiveness, and spur innovation in the provision of healthcare services. The difficulties with data security, privacy, and interoperability, however, need to be carefully considered. The healthcare system may adopt a patient\\&\\#x2010;centered, data\\&\\#x2010;driven strategy and enhance health outcomes by utilizing the potential of the IoT. This will change the way healthcare is delivered in the digital age.</p>', 'pages': '239-258', 'number': '', 'volume': '', 'year': '2025', 'title': 'Inclusive Role of Internet of (Healthcare) Things in Digital Health: Challenges, Methods, and Future Directions', 'booktitle': 'Generative Artificial Intelligence for Biomedical and Smart Health Informatics', 'author': 'Abdalla, Mohammed', 'ENTRYTYPE': 'inbook', 'ID': '10880605'}"
10237060,Generative Machine Learning for Photonic Design,"Zhu, Dayu and Liu, Zhaocheng and Cai, Wenshan",Zhu,10.1002/9781119853923.ch6,2023,Advances in Electromagnetics Empowered by Artificial Intelligence and Deep Learning,"Machine learning, as a study of algorithms that automate prediction and decision\&\#x2010;making based on complex data, has become one of the most effective tools in the exploitation of artificial intelligence. In recent years, scientific communities have been gradually merging data\&\#x2010;driven approaches with research endeavors, enabling dramatic progress in revealing underlying mechanisms, predicting essential properties, and discovering unconventional phenomena. Very recently, generative machine learning models have attracted growing attention in the design and optimization of complex photonic structures. In this chapter, we overview the advantages of the generative design strategy for photonic components and devices over traditional optimization schemes with both theoretical analysis and practical design examples.",Optimization;Probabilistic logic;Mathematical models;Generators;Stochastic processes;Predictive models;Photonics,"{'url': 'https://ieeexplore-ieee-org.crai.referencistas.com/document/10237060', 'isbn': '9781119853909', 'publisher': 'IEEE', 'issn': '', 'doi': '10.1002/9781119853923.ch6', 'keywords': 'Optimization;Probabilistic logic;Mathematical models;Generators;Stochastic processes;Predictive models;Photonics', 'abstract': 'Machine learning, as a study of algorithms that automate prediction and decision\\&\\#x2010;making based on complex data, has become one of the most effective tools in the exploitation of artificial intelligence. In recent years, scientific communities have been gradually merging data\\&\\#x2010;driven approaches with research endeavors, enabling dramatic progress in revealing underlying mechanisms, predicting essential properties, and discovering unconventional phenomena. Very recently, generative machine learning models have attracted growing attention in the design and optimization of complex photonic structures. In this chapter, we overview the advantages of the generative design strategy for photonic components and devices over traditional optimization schemes with both theoretical analysis and practical design examples.', 'pages': '197-224', 'number': '', 'volume': '', 'year': '2023', 'title': 'Generative Machine Learning for Photonic Design', 'booktitle': 'Advances in Electromagnetics Empowered by Artificial Intelligence and Deep Learning', 'author': 'Zhu, Dayu and Liu, Zhaocheng and Cai, Wenshan', 'ENTRYTYPE': 'inbook', 'ID': '10237060'}"
9186017,Usage of Deep Learning and Blockchain in Compilation and Copyright Protection of Digital Music,"Cai, Zhini",Cai,10.1109/ACCESS.2020.3021523,2020,IEEE Access,"In order to explore the application of deep learning algorithms in arrangement and composition, and the role of blockchain in the protection of digital music copyright, a monophonic melody composition model based on the deep generative adversarial networks (DCGANs) is constructed firstly, and the composition performance of the model is analyzed using hymn as input sample in this study. Later, the multi-instrument co-arrangement (MICA) model based on the multi-task learning is proposed, and the composition performance is analyzed by taking the actual music as an input sample. Finally, the improved practical byzantine fault tolerance (IPBFT) algorithm is proposed, and a digital music copyright protection system is designed based on the blockchain in this study. The results indicate that the accuracies constructed DCGANs model in predicting the Soprano and Alto voice melody are higher than those of the DeepBatch model by 2.29\% and 3.32\%, respectively. The performance on the harmony score, note accuracy, Levenshtein similarity (LS), notes distribution mean square error, and empty as well as the convergence speed of the constructed MICA model are better than those of other models. The average transaction per second (TPS) value of the proposed IPBFT algorithm in the real digital music copyright protection system is 3469, which is superior to other blockchain technologies. Finally, the digital music copyright protection system is achieved, the error rate of completing the request is 0\% in the state of many users operating concurrently, and a high TPS value can be guaranteed. In short, the DCGANs and MICA models pointed out in this study can be used in the composition of monophonic melodies and complex melodies, and the digital music copyright protection system based on the blockchain has excellent performance in practical applications.",Copyright protection;Blockchain;Deep learning;Data models;Music;Generative adversarial networks;Mathematical model;Deep generative adversarial networks;multi-instrument co-arrangement;practical byzantine fault tolerance;blockchain;digital music copyright protection system,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2020.3021523', 'keywords': 'Copyright protection;Blockchain;Deep learning;Data models;Music;Generative adversarial networks;Mathematical model;Deep generative adversarial networks;multi-instrument co-arrangement;practical byzantine fault tolerance;blockchain;digital music copyright protection system', 'abstract': 'In order to explore the application of deep learning algorithms in arrangement and composition, and the role of blockchain in the protection of digital music copyright, a monophonic melody composition model based on the deep generative adversarial networks (DCGANs) is constructed firstly, and the composition performance of the model is analyzed using hymn as input sample in this study. Later, the multi-instrument co-arrangement (MICA) model based on the multi-task learning is proposed, and the composition performance is analyzed by taking the actual music as an input sample. Finally, the improved practical byzantine fault tolerance (IPBFT) algorithm is proposed, and a digital music copyright protection system is designed based on the blockchain in this study. The results indicate that the accuracies constructed DCGANs model in predicting the Soprano and Alto voice melody are higher than those of the DeepBatch model by 2.29\\% and 3.32\\%, respectively. The performance on the harmony score, note accuracy, Levenshtein similarity (LS), notes distribution mean square error, and empty as well as the convergence speed of the constructed MICA model are better than those of other models. The average transaction per second (TPS) value of the proposed IPBFT algorithm in the real digital music copyright protection system is 3469, which is superior to other blockchain technologies. Finally, the digital music copyright protection system is achieved, the error rate of completing the request is 0\\% in the state of many users operating concurrently, and a high TPS value can be guaranteed. In short, the DCGANs and MICA models pointed out in this study can be used in the composition of monophonic melodies and complex melodies, and the digital music copyright protection system based on the blockchain has excellent performance in practical applications.', 'pages': '164144-164154', 'number': '', 'volume': '8', 'year': '2020', 'title': 'Usage of Deep Learning and Blockchain in Compilation and Copyright Protection of Digital Music', 'journal': 'IEEE Access', 'author': 'Cai, Zhini', 'ENTRYTYPE': 'article', 'ID': '9186017'}"
10192759,Skin Disease Detection and Recommendation System using Deep Learning and Cloud Computing,"Chowdary, Nama Deepak and Inturu, Siddhartha and Katta, Jithendra and Yashwanth, Chiluka and Kanaparthi, Naga Sri Harsha Vardhan and Voore, Srinivas",Chowdary,10.1109/ICCES57224.2023.10192759,2023,2023 8th International Conference on Communication and Electronics Systems (ICCES),"The main objective of this research is to develop an application based on Deep learning, Computer vision and cloud computing that detects the different kinds of skin diseases caused by different types of viruses, Bacteria, Fungus and Environment. This study has also developed and integrated a recommendation system, which recommends the medicines and care taking process for a particular disease. The application also suggests preventive methods for different kinds of skin infections. This study used an ensemble of convolution neural networks (CNN) with generative adversarial network (GAN) and Computer vision for construction of the model. Further, Amazon Personalize is used to build recommendation system in the proposed web application. The proposed application detects the disease based on symptoms, pictures, and videos of infected skin area. The application will be helpful for dermatologists and common people to perform early detection and prevention of skin diseases in India. This study also compared the accuracy of ensemble of convolution neural networks (CNN) with GAN and other algorithms like CNN. In comparison of accuracy, this study found that the Ensembles of CNN with GAN give best results for the proposed dataset.",Deep learning;Visualization;Computer vision;Computational modeling;Neural networks;Generative adversarial networks;Prediction algorithms;Convolutional neural network;Generative Adversarial Networks;Cloud Computing;Amazon web services personalize;Deep learning;Computer Vision,"{'month': 'June', 'issn': '', 'doi': '10.1109/ICCES57224.2023.10192759', 'keywords': 'Deep learning;Visualization;Computer vision;Computational modeling;Neural networks;Generative adversarial networks;Prediction algorithms;Convolutional neural network;Generative Adversarial Networks;Cloud Computing;Amazon web services personalize;Deep learning;Computer Vision', 'abstract': 'The main objective of this research is to develop an application based on Deep learning, Computer vision and cloud computing that detects the different kinds of skin diseases caused by different types of viruses, Bacteria, Fungus and Environment. This study has also developed and integrated a recommendation system, which recommends the medicines and care taking process for a particular disease. The application also suggests preventive methods for different kinds of skin infections. This study used an ensemble of convolution neural networks (CNN) with generative adversarial network (GAN) and Computer vision for construction of the model. Further, Amazon Personalize is used to build recommendation system in the proposed web application. The proposed application detects the disease based on symptoms, pictures, and videos of infected skin area. The application will be helpful for dermatologists and common people to perform early detection and prevention of skin diseases in India. This study also compared the accuracy of ensemble of convolution neural networks (CNN) with GAN and other algorithms like CNN. In comparison of accuracy, this study found that the Ensembles of CNN with GAN give best results for the proposed dataset.', 'pages': '1064-1068', 'number': '', 'volume': '', 'year': '2023', 'title': 'Skin Disease Detection and Recommendation System using Deep Learning and Cloud Computing', 'booktitle': '2023 8th International Conference on Communication and Electronics Systems (ICCES)', 'author': 'Chowdary, Nama Deepak and Inturu, Siddhartha and Katta, Jithendra and Yashwanth, Chiluka and Kanaparthi, Naga Sri Harsha Vardhan and Voore, Srinivas', 'ENTRYTYPE': 'inproceedings', 'ID': '10192759'}"
11014501,Interpretable and Adaptive GAN-BiLSTM Approach for Cyber Threat Detection in IoMT-based Healthcare 5.0,"Ullah, Zabeeh and Arif, Fahim and Khan, Nauman Ali and Khan, Mudassar Ali and Din, Ikram Ud and Almogren, Ahmad and Altameem, Ayman",Ullah,10.1109/JBHI.2025.3573097,2025,IEEE Journal of Biomedical and Health Informatics,"Healthcare 5.0, driven by the Internet of Medical Things (IoMT), introduces transformative changes in the medical field but also exposes systems to growing cybersecurity threats. While Deep Learning (DL) offers high accuracy in attack detection, its effectiveness is often limited by data imbalance and difficulty in identifying key features dynamically. Additionally, DL models are often criticized for their lack of interpretability, as their internal decisionmaking remains obscure. To overcome these limitations, this paper presents an explainable and adaptive DL-based security framework. It integrates a Generative Adversarial Network (GAN) to balance the dataset by generating realistic samples for underrepresented attack classes, and employs Bidirectional Long Short-Term Memory (BiLSTM) to identify temporal patterns and critical features. To enhance transparency, SHapley Additive exPlanations (SHAP) and Permutation Feature Importance (PFI) are used for interpreting the model's decisions. Experiments conducted on the NSL-KDD dataset demonstrate the effectiveness of the proposed method, achieving 93.81\% accuracy and an F1-score of 82.95\%.",Medical services;Explainable AI;Threat assessment;Bidirectional long short term memory;Security;NSL-KDD;Training;Feature extraction;Adaptation models;Generative adversarial networks;Healthcare 5.0;IoMT;Deep Learning;Explainable AI;Generative Adversarial Network,"{'month': '', 'issn': '2168-2208', 'doi': '10.1109/JBHI.2025.3573097', 'keywords': 'Medical services;Explainable AI;Threat assessment;Bidirectional long short term memory;Security;NSL-KDD;Training;Feature extraction;Adaptation models;Generative adversarial networks;Healthcare 5.0;IoMT;Deep Learning;Explainable AI;Generative Adversarial Network', 'abstract': ""Healthcare 5.0, driven by the Internet of Medical Things (IoMT), introduces transformative changes in the medical field but also exposes systems to growing cybersecurity threats. While Deep Learning (DL) offers high accuracy in attack detection, its effectiveness is often limited by data imbalance and difficulty in identifying key features dynamically. Additionally, DL models are often criticized for their lack of interpretability, as their internal decisionmaking remains obscure. To overcome these limitations, this paper presents an explainable and adaptive DL-based security framework. It integrates a Generative Adversarial Network (GAN) to balance the dataset by generating realistic samples for underrepresented attack classes, and employs Bidirectional Long Short-Term Memory (BiLSTM) to identify temporal patterns and critical features. To enhance transparency, SHapley Additive exPlanations (SHAP) and Permutation Feature Importance (PFI) are used for interpreting the model's decisions. Experiments conducted on the NSL-KDD dataset demonstrate the effectiveness of the proposed method, achieving 93.81\\% accuracy and an F1-score of 82.95\\%."", 'pages': '1-8', 'number': '', 'volume': '', 'year': '2025', 'title': 'Interpretable and Adaptive GAN-BiLSTM Approach for Cyber Threat Detection in IoMT-based Healthcare 5.0', 'journal': 'IEEE Journal of Biomedical and Health Informatics', 'author': 'Ullah, Zabeeh and Arif, Fahim and Khan, Nauman Ali and Khan, Mudassar Ali and Din, Ikram Ud and Almogren, Ahmad and Altameem, Ayman', 'ENTRYTYPE': 'article', 'ID': '11014501'}"
9585370,Medical Term and Status Generation From Chinese Clinical Dialogue With Multi-Granularity Transformer,"Li, Mei and Xiang, Lu and Kang, Xiaomian and Zhao, Yang and Zhou, Yu and Zong, Chengqing",Li,10.1109/TASLP.2021.3122301,2021,"IEEE/ACM Transactions on Audio, Speech, and Language Processing","This paper describes a generative model for extracting medical terms and their status from Chinese medical dialogues. Notably, the extracted semantic information is particularly important to downstream tasks like automatic medical scribe and automatic diagnosis systems. However, how to effectively leverage dialogue context to generate medical terms and their corresponding status accurately remains less explored. Existing generative approaches treat dialogue text as a single continuous text, ignoring conversational characteristics like colloquialism, redundancy and interactions. Between the doctor and the patient, a variety of colloquial medical information is frequently discussed. Each speaker (doctor and patient) plays a specific role in the interaction's goals. As a result, the importance of role information and interactions between utterances cannot be overstated. Furthermore, existing generative approaches only use character-level tokens, disregarding word-level tokens, which are the shortest meaningful utterances in Chinese. In this paper, we propose a Multi-granularity Transformer (MGT) model to enhance the dialogue context understanding from multi-granularity features. We incorporate word-level information by adapting a Lattice-based encoder with our proposed relative position encoding method. We further propose a Role Access Controlled Attention (RaCa) mechanism for introducing utterance-level interaction information. Experimental results on two benchmark datasets illustrate our model's validity and effectiveness, achieving state-of-the-art performance on both datasets.",Transformers;Medical services;Encoding;Semantics;Hierarchical systems;Medical dialogue;multi-granularity;attention mechanism;natural language understanding;sequence to sequence learning,"{'month': '', 'issn': '2329-9304', 'doi': '10.1109/TASLP.2021.3122301', 'keywords': 'Transformers;Medical services;Encoding;Semantics;Hierarchical systems;Medical dialogue;multi-granularity;attention mechanism;natural language understanding;sequence to sequence learning', 'abstract': ""This paper describes a generative model for extracting medical terms and their status from Chinese medical dialogues. Notably, the extracted semantic information is particularly important to downstream tasks like automatic medical scribe and automatic diagnosis systems. However, how to effectively leverage dialogue context to generate medical terms and their corresponding status accurately remains less explored. Existing generative approaches treat dialogue text as a single continuous text, ignoring conversational characteristics like colloquialism, redundancy and interactions. Between the doctor and the patient, a variety of colloquial medical information is frequently discussed. Each speaker (doctor and patient) plays a specific role in the interaction's goals. As a result, the importance of role information and interactions between utterances cannot be overstated. Furthermore, existing generative approaches only use character-level tokens, disregarding word-level tokens, which are the shortest meaningful utterances in Chinese. In this paper, we propose a Multi-granularity Transformer (MGT) model to enhance the dialogue context understanding from multi-granularity features. We incorporate word-level information by adapting a Lattice-based encoder with our proposed relative position encoding method. We further propose a Role Access Controlled Attention (RaCa) mechanism for introducing utterance-level interaction information. Experimental results on two benchmark datasets illustrate our model's validity and effectiveness, achieving state-of-the-art performance on both datasets."", 'pages': '3362-3374', 'number': '', 'volume': '29', 'year': '2021', 'title': 'Medical Term and Status Generation From Chinese Clinical Dialogue With Multi-Granularity Transformer', 'journal': 'IEEE/ACM Transactions on Audio, Speech, and Language Processing', 'author': 'Li, Mei and Xiang, Lu and Kang, Xiaomian and Zhao, Yang and Zhou, Yu and Zong, Chengqing', 'ENTRYTYPE': 'article', 'ID': '9585370'}"
9097251,Single Image Reflection Removal via Attention Model and SN-GAN,"Cheng, Kuanhong and Song, Jiangluqi and Du, Juan and Rong, Shenghui and Zhou, Huixin",Cheng,10.1109/ACCESS.2020.2995871,2020,IEEE Access,"Single image reflection removal is of great practical importance for various computer vision tasks. Most non-learning methods try to solve this problem through the model-optimization scheme, which fails to produce promising results due to the shortage of suitable priors to model the difference between the reflection layer and the transmission layer. This paper presents an improved generative adversarial network to resolve this problem. First, we suggest that reflection removal is not only a channel-wise separation problem, but also a spatial variational occlusion removal task, which is sensitive to both spatial and channel-wise features. To this end, we integrate the CBAM module into the generator to enhance both spatial and channel-wise feature representation. Second, we consider the reflection layer as a spatial mask with space-relevant reflection intensity information, which can be used to elevate the performance of the discriminator. We then design a novel SNGAN structure with utilize the predicted reflection as a guidance to achieve better adversarial supervision. Specifically, our new generative network has an encoder-decoder structure with skip-connections, where the attention enhancement block is integrated into each skip-connection of the encoder-decoder subnet, and followed by an eight-layer fully convolutional subnet. Furthermore, the SNGAN loss is combined with L2 pixel loss and L1 VGG19 perceptual loss for training. The experimental results with benchmark datasets indicate that our method outperforms several state-of-the-art networks.",Reflection;Task analysis;Convolution;Generators;Generative adversarial networks;Computer vision;Computational modeling;Inverse problems;artificial neural networks;image processing;image restoration;computer vision;artificial intelligence;supervised learning;multi-layer neural network;knowledge-based systems,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2020.2995871', 'keywords': 'Reflection;Task analysis;Convolution;Generators;Generative adversarial networks;Computer vision;Computational modeling;Inverse problems;artificial neural networks;image processing;image restoration;computer vision;artificial intelligence;supervised learning;multi-layer neural network;knowledge-based systems', 'abstract': 'Single image reflection removal is of great practical importance for various computer vision tasks. Most non-learning methods try to solve this problem through the model-optimization scheme, which fails to produce promising results due to the shortage of suitable priors to model the difference between the reflection layer and the transmission layer. This paper presents an improved generative adversarial network to resolve this problem. First, we suggest that reflection removal is not only a channel-wise separation problem, but also a spatial variational occlusion removal task, which is sensitive to both spatial and channel-wise features. To this end, we integrate the CBAM module into the generator to enhance both spatial and channel-wise feature representation. Second, we consider the reflection layer as a spatial mask with space-relevant reflection intensity information, which can be used to elevate the performance of the discriminator. We then design a novel SNGAN structure with utilize the predicted reflection as a guidance to achieve better adversarial supervision. Specifically, our new generative network has an encoder-decoder structure with skip-connections, where the attention enhancement block is integrated into each skip-connection of the encoder-decoder subnet, and followed by an eight-layer fully convolutional subnet. Furthermore, the SNGAN loss is combined with L2 pixel loss and L1 VGG19 perceptual loss for training. The experimental results with benchmark datasets indicate that our method outperforms several state-of-the-art networks.', 'pages': '96046-96054', 'number': '', 'volume': '8', 'year': '2020', 'title': 'Single Image Reflection Removal via Attention Model and SN-GAN', 'journal': 'IEEE Access', 'author': 'Cheng, Kuanhong and Song, Jiangluqi and Du, Juan and Rong, Shenghui and Zhou, Huixin', 'ENTRYTYPE': 'article', 'ID': '9097251'}"
10717316,Augmenting Sentiments into Chat-GPT Using FacialEmotion Recognition,"Iyer, Aditya A and Vojjala, Saipranav and J, Andrew",Iyer,10.1109/ICACCS60874.2024.10717316,2024,2024 10th International Conference on Advanced Computing and Communication Systems (ICACCS),"This research initiative addresses the task of enhancing Chat Generative Pre-trained Transformer's (ChatGPT's) conversational capabilities by integrating the comprehension and response to user emotions conveyed through facial expressions. The central challenge lies in refining the AI system's proficiency to tailor responses according to users' detected emotional states. In this pursuit, our study adopts a comprehensive approach, aiming to seamlessly incorporate emotional intelligence into AI-driven interactions. To achieve this objective, the methodology involves integrating real-time sentiment analysis based on facial expressions into the ChatGPT framework. This is carried out through the utilization of a deep convolutional neural network (DCNN) architecture, designed to recognize and interpret various emotions exhibited in facial expressions. The primary goal is to enable ChatGPT to dynamically adjust its responses, fostering a more empathetic and contextually relevant interaction with users. In terms of evaluation metrics for facial expression recog- nition, our assessment employs a confusion matrix to quantify the model's performance across different emotional categories. Additionally, a heuristic approach is implemented, wherein the sum total probability of each detected emotion is calculated over the duration the user enters the prompt. These evaluation methodologies aim to provide a comprehensive understanding of the model's accuracy and effectiveness in discerning and responding to user emotions. Overall, this research contributes to the ongoing endeavor of imbuing AI systems with emotional intelligence, paving the way for more nuanced and human-like interactions.",Emotion recognition;Sentiment analysis;Face recognition;Computer architecture;Chatbots;Transformers;Real-time systems;User experience;Convolutional neural networks;Artificial intelligence;Fine-tuning;Natural Language Processing;Sentiment Analysis;Machine Learning;GPT;Facial Expression Recognition;Interaction adaptation;Emotional intelligence in AI;Conversational Agents,"{'month': 'March', 'issn': '2575-7288', 'doi': '10.1109/ICACCS60874.2024.10717316', 'keywords': 'Emotion recognition;Sentiment analysis;Face recognition;Computer architecture;Chatbots;Transformers;Real-time systems;User experience;Convolutional neural networks;Artificial intelligence;Fine-tuning;Natural Language Processing;Sentiment Analysis;Machine Learning;GPT;Facial Expression Recognition;Interaction adaptation;Emotional intelligence in AI;Conversational Agents', 'abstract': ""This research initiative addresses the task of enhancing Chat Generative Pre-trained Transformer's (ChatGPT's) conversational capabilities by integrating the comprehension and response to user emotions conveyed through facial expressions. The central challenge lies in refining the AI system's proficiency to tailor responses according to users' detected emotional states. In this pursuit, our study adopts a comprehensive approach, aiming to seamlessly incorporate emotional intelligence into AI-driven interactions. To achieve this objective, the methodology involves integrating real-time sentiment analysis based on facial expressions into the ChatGPT framework. This is carried out through the utilization of a deep convolutional neural network (DCNN) architecture, designed to recognize and interpret various emotions exhibited in facial expressions. The primary goal is to enable ChatGPT to dynamically adjust its responses, fostering a more empathetic and contextually relevant interaction with users. In terms of evaluation metrics for facial expression recog- nition, our assessment employs a confusion matrix to quantify the model's performance across different emotional categories. Additionally, a heuristic approach is implemented, wherein the sum total probability of each detected emotion is calculated over the duration the user enters the prompt. These evaluation methodologies aim to provide a comprehensive understanding of the model's accuracy and effectiveness in discerning and responding to user emotions. Overall, this research contributes to the ongoing endeavor of imbuing AI systems with emotional intelligence, paving the way for more nuanced and human-like interactions."", 'pages': '69-74', 'number': '', 'volume': '1', 'year': '2024', 'title': 'Augmenting Sentiments into Chat-GPT Using FacialEmotion Recognition', 'booktitle': '2024 10th International Conference on Advanced Computing and Communication Systems (ICACCS)', 'author': 'Iyer, Aditya A and Vojjala, Saipranav and J, Andrew', 'ENTRYTYPE': 'inproceedings', 'ID': '10717316'}"
10042407,Detection of Tomato Leaf Diseases for Agro-Based Industries Using Novel PCA DeepNet,"Roy, Kyamelia and Chaudhuri, Sheli Sinha and Frnda, Jaroslav and Bandopadhyay, Srijita and Ray, Ishan Jyoti and Banerjee, Soumen and Nedoma, Jan",Roy,10.1109/ACCESS.2023.3244499,2023,IEEE Access,The advancement of Deep Learning and Computer Vision in the field of agriculture has been found to be an effective tool in detecting harmful plant diseases. Classification and detection of healthy and diseased crops play a very crucial role in determining the rate and quality of production. Thus the present work highlights a well-proposed novel method of detecting Tomato leaf diseases using Deep Neural Networks to strengthen agro-based industries. The present novel framework is utilized with a combination of classical Machine Learning model Principal Component Analysis (PCA) and a customized Deep Neural Network which has been named as PCA DeepNet. The hybridized framework also consists of Generative Adversarial Network (GAN) for obtaining a good mixture of datasets. The detection is carried out using the Faster Region-Based Convolutional Neural Network (F-RCNN). The overall work generated a classification accuracy of 99.60\% with an average precision of 98.55\%; giving a promising Intersection over Union (IOU) score of 0.95 in detection. Thus the presented work outperforms any other reported state-of-the-art.,Diseases;Deep learning;Feature extraction;Principal component analysis;Convolutional neural networks;Generative adversarial networks;Computer architecture;Crops;Tomato leaf diseases;artificial intelligence;deep learning;computer vision;generative adversarial networks;convolutional neural network;faster region-based convolutional neural network,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2023.3244499', 'keywords': 'Diseases;Deep learning;Feature extraction;Principal component analysis;Convolutional neural networks;Generative adversarial networks;Computer architecture;Crops;Tomato leaf diseases;artificial intelligence;deep learning;computer vision;generative adversarial networks;convolutional neural network;faster region-based convolutional neural network', 'abstract': 'The advancement of Deep Learning and Computer Vision in the field of agriculture has been found to be an effective tool in detecting harmful plant diseases. Classification and detection of healthy and diseased crops play a very crucial role in determining the rate and quality of production. Thus the present work highlights a well-proposed novel method of detecting Tomato leaf diseases using Deep Neural Networks to strengthen agro-based industries. The present novel framework is utilized with a combination of classical Machine Learning model Principal Component Analysis (PCA) and a customized Deep Neural Network which has been named as PCA DeepNet. The hybridized framework also consists of Generative Adversarial Network (GAN) for obtaining a good mixture of datasets. The detection is carried out using the Faster Region-Based Convolutional Neural Network (F-RCNN). The overall work generated a classification accuracy of 99.60\\% with an average precision of 98.55\\%; giving a promising Intersection over Union (IOU) score of 0.95 in detection. Thus the presented work outperforms any other reported state-of-the-art.', 'pages': '14983-15001', 'number': '', 'volume': '11', 'year': '2023', 'title': 'Detection of Tomato Leaf Diseases for Agro-Based Industries Using Novel PCA DeepNet', 'journal': 'IEEE Access', 'author': 'Roy, Kyamelia and Chaudhuri, Sheli Sinha and Frnda, Jaroslav and Bandopadhyay, Srijita and Ray, Ishan Jyoti and Banerjee, Soumen and Nedoma, Jan', 'ENTRYTYPE': 'article', 'ID': '10042407'}"
9919840,Detection and Classification of Fault Types in Distribution Lines by Applying Contrastive Learning to GAN Encoded Time-Series of Pulse Reflectometry Signals,"Fornás, Javier Granado and Jaraba, Elías Herrero and Estopiñan, Andrés Llombart and Saldana, Jose",Fornás,10.1109/ACCESS.2022.3214994,2022,IEEE Access,"This study proposes a new method for detecting and classifying faults in distribution lines. The physical principle of classification is based on time-domain pulse reflectometry (TDR). These high-frequency pulses are injected into the line, propagate through all of its bifurcations, and are reflected back to the injection point. According to the impedances encountered along the way, these signals carry information regarding the state of the line. In the present work, an initial signal database was obtained using the TDR technique, simulating a real distribution line using (PSCAD™). By transforming these signals into images and reducing their dimensionality, these signals are processed using convolutional neural networks (CNN). In particular, in this study, contrastive learning in Siamese networks was used for the classification of different types of faults (ToF). In addition, to avoid the problem of overfitting owing to the scarcity of examples, generative adversarial neural networks (GAN) have been used to synthesise new examples, enlarging the initial database. The combination of Siamese neural networks and GAN allows the classification of this type of signal using only synthesised examples to train and validate and only the original examples to test the network. This solves the problem of the lack of original examples in this type of signal of natural phenomena which are difficult to obtain and simulate.",Circuit faults;Databases;Generative adversarial networks;Task analysis;Generators;Reflectometry;Mathematical models;Artificial Neural Networks (ANNs);deep learning;siamese networks;generative adversarial neural networks (GAN’s);fault classification;fault detection;transmission lines,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2022.3214994', 'keywords': 'Circuit faults;Databases;Generative adversarial networks;Task analysis;Generators;Reflectometry;Mathematical models;Artificial Neural Networks (ANNs);deep learning;siamese networks;generative adversarial neural networks (GAN’s);fault classification;fault detection;transmission lines', 'abstract': 'This study proposes a new method for detecting and classifying faults in distribution lines. The physical principle of classification is based on time-domain pulse reflectometry (TDR). These high-frequency pulses are injected into the line, propagate through all of its bifurcations, and are reflected back to the injection point. According to the impedances encountered along the way, these signals carry information regarding the state of the line. In the present work, an initial signal database was obtained using the TDR technique, simulating a real distribution line using (PSCAD™). By transforming these signals into images and reducing their dimensionality, these signals are processed using convolutional neural networks (CNN). In particular, in this study, contrastive learning in Siamese networks was used for the classification of different types of faults (ToF). In addition, to avoid the problem of overfitting owing to the scarcity of examples, generative adversarial neural networks (GAN) have been used to synthesise new examples, enlarging the initial database. The combination of Siamese neural networks and GAN allows the classification of this type of signal using only synthesised examples to train and validate and only the original examples to test the network. This solves the problem of the lack of original examples in this type of signal of natural phenomena which are difficult to obtain and simulate.', 'pages': '110521-110536', 'number': '', 'volume': '10', 'year': '2022', 'title': 'Detection and Classification of Fault Types in Distribution Lines by Applying Contrastive Learning to GAN Encoded Time-Series of Pulse Reflectometry Signals', 'journal': 'IEEE Access', 'author': 'Fornás, Javier Granado and Jaraba, Elías Herrero and Estopiñan, Andrés Llombart and Saldana, Jose', 'ENTRYTYPE': 'article', 'ID': '9919840'}"
10537072,Leveraging Generative AI for Rapid Design and Verification of a Vector Processor SoC,"Salcedo, William and Achour, Sara and McBeth, Courtney",Salcedo,10.1109/MDAT.2024.3404117,2024,IEEE Design \& Test,"This article presents a novel approach to using generative artificial intelligence (AI), specifically GPT-4, to accelerate the design and verification of a vector processor SoC, demonstrating the potential of AI to streamline chip development processes and reduce time to market. —Matthew Guthaus, University of California at Santa Cruz, USA",Registers;Hardware design languages;Vector processors;Computer architecture;Codes;Generative AI;Artificial intelligence;Vector processors,"{'month': 'Dec', 'issn': '2168-2364', 'doi': '10.1109/MDAT.2024.3404117', 'keywords': 'Registers;Hardware design languages;Vector processors;Computer architecture;Codes;Generative AI;Artificial intelligence;Vector processors', 'abstract': 'This article presents a novel approach to using generative artificial intelligence (AI), specifically GPT-4, to accelerate the design and verification of a vector processor SoC, demonstrating the potential of AI to streamline chip development processes and reduce time to market. —Matthew Guthaus, University of California at Santa Cruz, USA', 'pages': '8-18', 'number': '6', 'volume': '41', 'year': '2024', 'title': 'Leveraging Generative AI for Rapid Design and Verification of a Vector Processor SoC', 'journal': 'IEEE Design \\& Test', 'author': 'Salcedo, William and Achour, Sara and McBeth, Courtney', 'ENTRYTYPE': 'article', 'ID': '10537072'}"
11020592,LLM-Enabled Multi-Modal Data Synthesis via Cross-Domain Collaboration,"Zhou, Xiaomao and Hu, Yujiao and Jia, Qingmin and Xie, Renchao",Zhou,10.1109/MCOM.002.2400435,2025,IEEE Communications Magazine,"Data is pivotal to the advancement of intelligent communication and network systems. However, the availability of relevant data is hampered by different challenges, including scarcity, privacy issues, and the high cost of acquisition. Furthermore, existing data generation models are of low quality, uncontrollable, and lack generalizability. In this article, we propose a large language models (LLMs) driven framework that leverages the power of LLMs in concert with various domain-specific generative models (DGMs) to realize general multi-modal data synthesis. Specifically, our method employs LLMs as the core to interpret user requests, decompose a complex task into a manageable set of sub-tasks, and delegate each sub-task to the most suitable DGM, thereby automatically constructing customized data generation pipelines. Meanwhile, DGMs contribute their expertise to generate high-fidelity, domain-relevant data, whose specialized knowledge can be further enhanced by the LLM's broad linguistic knowledge via knowledge transfer. In addition, the integration of reinforcement learning (RL) is promising to enhance the framework's ability to optimally utilize DGMs, resulting in data generation with superior quality and control flexibility. Experimental results demonstrate the effectiveness of LLMs in augmenting domain-specific generative models via knowledge transfer and in facilitating multi-modal data synthesis through collaborative interactions with diverse DGMs.",Data models;Data collection;Knowledge transfer;Pipelines;Artificial intelligence;Analytical models;Training data;Integrated circuit modeling;Linguistics;Context modeling,"{'month': '', 'issn': '1558-1896', 'doi': '10.1109/MCOM.002.2400435', 'keywords': 'Data models;Data collection;Knowledge transfer;Pipelines;Artificial intelligence;Analytical models;Training data;Integrated circuit modeling;Linguistics;Context modeling', 'abstract': ""Data is pivotal to the advancement of intelligent communication and network systems. However, the availability of relevant data is hampered by different challenges, including scarcity, privacy issues, and the high cost of acquisition. Furthermore, existing data generation models are of low quality, uncontrollable, and lack generalizability. In this article, we propose a large language models (LLMs) driven framework that leverages the power of LLMs in concert with various domain-specific generative models (DGMs) to realize general multi-modal data synthesis. Specifically, our method employs LLMs as the core to interpret user requests, decompose a complex task into a manageable set of sub-tasks, and delegate each sub-task to the most suitable DGM, thereby automatically constructing customized data generation pipelines. Meanwhile, DGMs contribute their expertise to generate high-fidelity, domain-relevant data, whose specialized knowledge can be further enhanced by the LLM's broad linguistic knowledge via knowledge transfer. In addition, the integration of reinforcement learning (RL) is promising to enhance the framework's ability to optimally utilize DGMs, resulting in data generation with superior quality and control flexibility. Experimental results demonstrate the effectiveness of LLMs in augmenting domain-specific generative models via knowledge transfer and in facilitating multi-modal data synthesis through collaborative interactions with diverse DGMs."", 'pages': '1-8', 'number': '', 'volume': '', 'year': '2025', 'title': 'LLM-Enabled Multi-Modal Data Synthesis via Cross-Domain Collaboration', 'journal': 'IEEE Communications Magazine', 'author': 'Zhou, Xiaomao and Hu, Yujiao and Jia, Qingmin and Xie, Renchao', 'ENTRYTYPE': 'article', 'ID': '11020592'}"
10559829,Generating Social-Aware Locations for a Robot in a Human Group by Image Generation Using Adversarial Learning,"Poluan, Sevendi Eldrige Rifki and Chen, Yan-Ann",Poluan,10.1109/ACCESS.2024.3415708,2024,IEEE Access,"With the advance of deep learning techniques, social robots can have more powerful perception and interaction capabilities. However, the problem of finding a socially aware standing location for the robot to join a conversation group is not well addressed. Thus, we propose a generative-based and image-based approach to generate a social-aware group formation to obtain the possible locations for the robot. Furthermore, to overcome the problem of formulating human comforts, we try to leverage human behaviors with the concerns of human comforts when joining the conversation group. We utilize a self-supervised technique to generate this kind of human experience from the real-world dataset. Through extensive experiments, we show that the proposed method outperforms the social force method by 62\% with respect to data from human experiences. In addition, our approach also provides controllable parameters to generate the location with the required features using the GAN noise vector.",Robots;Social robots;Robot kinematics;Navigation;Image synthesis;Oral communication;Trajectory planning;Adversarial machine learning;Generative AI;Adversarial learning;conversation group;edge artificial intelligence;generative AI;Internet of Things;robot standing position;social robot,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2024.3415708', 'keywords': 'Robots;Social robots;Robot kinematics;Navigation;Image synthesis;Oral communication;Trajectory planning;Adversarial machine learning;Generative AI;Adversarial learning;conversation group;edge artificial intelligence;generative AI;Internet of Things;robot standing position;social robot', 'abstract': 'With the advance of deep learning techniques, social robots can have more powerful perception and interaction capabilities. However, the problem of finding a socially aware standing location for the robot to join a conversation group is not well addressed. Thus, we propose a generative-based and image-based approach to generate a social-aware group formation to obtain the possible locations for the robot. Furthermore, to overcome the problem of formulating human comforts, we try to leverage human behaviors with the concerns of human comforts when joining the conversation group. We utilize a self-supervised technique to generate this kind of human experience from the real-world dataset. Through extensive experiments, we show that the proposed method outperforms the social force method by 62\\% with respect to data from human experiences. In addition, our approach also provides controllable parameters to generate the location with the required features using the GAN noise vector.', 'pages': '85681-85693', 'number': '', 'volume': '12', 'year': '2024', 'title': 'Generating Social-Aware Locations for a Robot in a Human Group by Image Generation Using Adversarial Learning', 'journal': 'IEEE Access', 'author': 'Poluan, Sevendi Eldrige Rifki and Chen, Yan-Ann', 'ENTRYTYPE': 'article', 'ID': '10559829'}"
10758932,Mitigating manipulation in generative AI,"Wu, Yanyi",Wu,10.1109/MPOT.2024.3491338,2024,IEEE Potentials,"In an era where generative artificial intelligence (GAI) can compose symphonies and generate news articles with equal facility, the boundary between creation and manipulation becomes increasingly nebulous. This article discusses the fascinating and risky field of GAI and explores its potential to manipulate through deepfake videos and AI-generated text. Rather than presenting a fatalistic outlook, it is a call to action to ethicists, engineers, and policy makers. It delineates strategies to harness GAI’s potential while implementing safeguards against misuse, proposing solutions from “trust by design” principles to adaptive regulatory frameworks.",Psychology;Ethics;Artificial intelligence;Transforms;Media;Watersheds;Social networking (online);Shape;Labeling;Generative AI,"{'month': 'Nov', 'issn': '1558-1772', 'doi': '10.1109/MPOT.2024.3491338', 'keywords': 'Psychology;Ethics;Artificial intelligence;Transforms;Media;Watersheds;Social networking (online);Shape;Labeling;Generative AI', 'abstract': 'In an era where generative artificial intelligence (GAI) can compose symphonies and generate news articles with equal facility, the boundary between creation and manipulation becomes increasingly nebulous. This article discusses the fascinating and risky field of GAI and explores its potential to manipulate through deepfake videos and AI-generated text. Rather than presenting a fatalistic outlook, it is a call to action to ethicists, engineers, and policy makers. It delineates strategies to harness GAI’s potential while implementing safeguards against misuse, proposing solutions from “trust by design” principles to adaptive regulatory frameworks.', 'pages': '39-45', 'number': '6', 'volume': '43', 'year': '2024', 'title': 'Mitigating manipulation in generative AI', 'journal': 'IEEE Potentials', 'author': 'Wu, Yanyi', 'ENTRYTYPE': 'article', 'ID': '10758932'}"
9773982,Learning Robust Deep State Space for Unsupervised Anomaly Detection in Contaminated Time-Series,"Li, Longyuan and Yan, Junchi and Wen, Qingsong and Jin, Yaohui and Yang, Xiaokang",Li,10.1109/TKDE.2022.3171562,2023,IEEE Transactions on Knowledge and Data Engineering,"Anomalies are ubiquitous in real-world time-series data which call for effective and timely detection, especially in an unsupervised setting for labeling cost saving. In this paper, we develop an unsupervised density reconstruction model for multi-dimensional time-series anomaly detection. In particular, it directly handles an important realistic setting that the detection is achieved towards raw time-series contaminated with noise for training, in contrast to most existing anomaly detection works that assume the training data is in general clean i.e., not contaminated with anomaly. It extends recent advancements in deep generative models and state space models to achieve robust anomaly detection. Our approach comprises of a novel state space based generative model, a filtering based inference model, together with a carefully-designated emission model based on robust statistics theory. Extensive experimental results are conducted to show that our approach can adapt to complex patterns even given severely contaminated training data. We also develop visualization techniques to help better understand the behavior of the anomaly detection models. Empirical results show that our method outperforms state-of-the-arts on both synthetic and real-world datasets.",Data models;Anomaly detection;Training data;Hidden Markov models;Training;Adaptation models;Estimation;Anomaly detection;density estimation;time-series;variational auto-encoder;deep state space model,"{'month': 'June', 'issn': '1558-2191', 'doi': '10.1109/TKDE.2022.3171562', 'keywords': 'Data models;Anomaly detection;Training data;Hidden Markov models;Training;Adaptation models;Estimation;Anomaly detection;density estimation;time-series;variational auto-encoder;deep state space model', 'abstract': 'Anomalies are ubiquitous in real-world time-series data which call for effective and timely detection, especially in an unsupervised setting for labeling cost saving. In this paper, we develop an unsupervised density reconstruction model for multi-dimensional time-series anomaly detection. In particular, it directly handles an important realistic setting that the detection is achieved towards raw time-series contaminated with noise for training, in contrast to most existing anomaly detection works that assume the training data is in general clean i.e., not contaminated with anomaly. It extends recent advancements in deep generative models and state space models to achieve robust anomaly detection. Our approach comprises of a novel state space based generative model, a filtering based inference model, together with a carefully-designated emission model based on robust statistics theory. Extensive experimental results are conducted to show that our approach can adapt to complex patterns even given severely contaminated training data. We also develop visualization techniques to help better understand the behavior of the anomaly detection models. Empirical results show that our method outperforms state-of-the-arts on both synthetic and real-world datasets.', 'pages': '6058-6072', 'number': '6', 'volume': '35', 'year': '2023', 'title': 'Learning Robust Deep State Space for Unsupervised Anomaly Detection in Contaminated Time-Series', 'journal': 'IEEE Transactions on Knowledge and Data Engineering', 'author': 'Li, Longyuan and Yan, Junchi and Wen, Qingsong and Jin, Yaohui and Yang, Xiaokang', 'ENTRYTYPE': 'article', 'ID': '9773982'}"
9662066,Learning Context Restrained Correlation Tracking Filters via Adversarial Negative Instance Generation,"Huang, Bo and Xu, Tingfa and Li, Jianan and Luo, Fei and Qin, Qingwang and Chen, Junjie",Huang,10.1109/TNNLS.2021.3133441,2023,IEEE Transactions on Neural Networks and Learning Systems,"The tracking performance of discriminative correlation filters (DCFs) is often subject to unwanted boundary effects. Many attempts have already been made to address the above issue by enlarging searching regions over the last years. However, introducing excessive background information makes the discriminative filter prone to learn from the surrounding context rather than the target. In this article, we propose a novel context restrained correlation tracking filter (CRCTF) that can effectively suppress background interference via incorporating high-quality adversarial generative negative instances. Concretely, we first construct an adversarial context generation network to simulate the central target area with surrounding background information at the initial frame. Then, we suggest a coarse background estimation network to accelerate the background generation in subsequent frames. By introducing a suppression convolution term, we utilize generative background patches to reformulate the original ridge regression objective through circulant property of correlation and a cropping operator. Finally, our tracking filter is efficiently solved by the alternating direction method of multipliers (ADMM). CRCTF demonstrates the accuracy performance on par with several well-established and highly optimized baselines on multiple challenging tracking datasets, verifying the effectiveness of our proposed approach.",Target tracking;Correlation;Training;Background noise;Interference;Adversarial machine learning;Feature extraction;Adversarial context generation;background interference;correlation filters;object tracking,"{'month': 'Sep.', 'issn': '2162-2388', 'doi': '10.1109/TNNLS.2021.3133441', 'keywords': 'Target tracking;Correlation;Training;Background noise;Interference;Adversarial machine learning;Feature extraction;Adversarial context generation;background interference;correlation filters;object tracking', 'abstract': 'The tracking performance of discriminative correlation filters (DCFs) is often subject to unwanted boundary effects. Many attempts have already been made to address the above issue by enlarging searching regions over the last years. However, introducing excessive background information makes the discriminative filter prone to learn from the surrounding context rather than the target. In this article, we propose a novel context restrained correlation tracking filter (CRCTF) that can effectively suppress background interference via incorporating high-quality adversarial generative negative instances. Concretely, we first construct an adversarial context generation network to simulate the central target area with surrounding background information at the initial frame. Then, we suggest a coarse background estimation network to accelerate the background generation in subsequent frames. By introducing a suppression convolution term, we utilize generative background patches to reformulate the original ridge regression objective through circulant property of correlation and a cropping operator. Finally, our tracking filter is efficiently solved by the alternating direction method of multipliers (ADMM). CRCTF demonstrates the accuracy performance on par with several well-established and highly optimized baselines on multiple challenging tracking datasets, verifying the effectiveness of our proposed approach.', 'pages': '6132-6145', 'number': '9', 'volume': '34', 'year': '2023', 'title': 'Learning Context Restrained Correlation Tracking Filters via Adversarial Negative Instance Generation', 'journal': 'IEEE Transactions on Neural Networks and Learning Systems', 'author': 'Huang, Bo and Xu, Tingfa and Li, Jianan and Luo, Fei and Qin, Qingwang and Chen, Junjie', 'ENTRYTYPE': 'article', 'ID': '9662066'}"
10184689,DBAugur: An Adversarial-based Trend Forecasting System for Diversified Workloads,"Gao, Yuanning and Huang, Xiuqi and Zhou, Xuanhe and Gao, Xiaofeng and Li, Guoliang and Chen, Guihai",Gao,10.1109/ICDE55515.2023.00385,2023,2023 IEEE 39th International Conference on Data Engineering (ICDE),"Trend forecasting is vital to optimize the workload performance. It becomes even more urgent with an increasing number of applications and database configurations. However, DBAs mainly target at historical workloads and may give suboptimal configuration advice when the workload trends have changed. Although there are some studies on trend forecasting, they have several limitations. First, they mainly predict the changes of query numbers, which do not combine other critical factors (e.g., disk utilization) and cannot fully reflect the future workload trends. Besides, there are numerous queries in the workloads and exact clustering algorithms like K-means cannot effectively merge similar queries which contain noises like time shifts. Second, basic machine learning models like RNN may have relatively low prediction accuracy on complex workloads (e.g., no cycles but random bursts). Third, real-world workloads may have diverse patterns, while previous models cannot efficiently and reliably predict for all the different workload patterns.To address these challenges, we propose a trend forecasting system (DBAugur) that utilizes adversarial neural networks to predict the trends of different workloads. First, DBAugur collects the important features (e.g., queries, resource metrics) to characterize workloads, and reduces the number of involved queries by separately merging similar queries based on the SQL semantics and trend patterns. Second, DBAugur utilizes Generative Adversarial Networks (GANs) to capture the latent patterns, correlations between different metrics, and occasional bursts within the complicated and time-varying workloads. Moreover, we further propose a time-sensitive ensemble algorithm that takes advantage of various machine learning models (e.g., generative models, convolutional models, feed-forward models) to accommodate the various workload patterns. The experimental results show that DBAugur outperformed state-of-the-art methods on various real-world workloads.",Measurement;Machine learning algorithms;Databases;Clustering algorithms;Machine learning;Predictive models;Market research;Workload Forecasting;GAN;DBMS,"{'month': 'April', 'issn': '2375-026X', 'doi': '10.1109/ICDE55515.2023.00385', 'keywords': 'Measurement;Machine learning algorithms;Databases;Clustering algorithms;Machine learning;Predictive models;Market research;Workload Forecasting;GAN;DBMS', 'abstract': 'Trend forecasting is vital to optimize the workload performance. It becomes even more urgent with an increasing number of applications and database configurations. However, DBAs mainly target at historical workloads and may give suboptimal configuration advice when the workload trends have changed. Although there are some studies on trend forecasting, they have several limitations. First, they mainly predict the changes of query numbers, which do not combine other critical factors (e.g., disk utilization) and cannot fully reflect the future workload trends. Besides, there are numerous queries in the workloads and exact clustering algorithms like K-means cannot effectively merge similar queries which contain noises like time shifts. Second, basic machine learning models like RNN may have relatively low prediction accuracy on complex workloads (e.g., no cycles but random bursts). Third, real-world workloads may have diverse patterns, while previous models cannot efficiently and reliably predict for all the different workload patterns.To address these challenges, we propose a trend forecasting system (DBAugur) that utilizes adversarial neural networks to predict the trends of different workloads. First, DBAugur collects the important features (e.g., queries, resource metrics) to characterize workloads, and reduces the number of involved queries by separately merging similar queries based on the SQL semantics and trend patterns. Second, DBAugur utilizes Generative Adversarial Networks (GANs) to capture the latent patterns, correlations between different metrics, and occasional bursts within the complicated and time-varying workloads. Moreover, we further propose a time-sensitive ensemble algorithm that takes advantage of various machine learning models (e.g., generative models, convolutional models, feed-forward models) to accommodate the various workload patterns. The experimental results show that DBAugur outperformed state-of-the-art methods on various real-world workloads.', 'pages': '27-39', 'number': '', 'volume': '', 'year': '2023', 'title': 'DBAugur: An Adversarial-based Trend Forecasting System for Diversified Workloads', 'booktitle': '2023 IEEE 39th International Conference on Data Engineering (ICDE)', 'author': 'Gao, Yuanning and Huang, Xiuqi and Zhou, Xuanhe and Gao, Xiaofeng and Li, Guoliang and Chen, Guihai', 'ENTRYTYPE': 'inproceedings', 'ID': '10184689'}"
10229440,Debunk online rumors via generative-contrastive learning based on tree-transformer,"Luo, Xi and Deng, Yuhui and Liu, Junjie and Wu, Sirong and Sun, Gengchen",Luo,10.1109/AITest58265.2023.00032,2023,2023 IEEE International Conference On Artificial Intelligence Testing (AITest),"Rumors have been treated as the online pandemic on social media, impeding the truth. Existing claim-guided approaches tend to capture the structural, temporal, or relational indicative features among responsive posts to enhance representation learning, but few of them can integrate these features properly to comprehensively explore them. Additionally, large pre-trained language models are widely used for rumor debunking, such as Transformer-based models, which can result in collapsed issues and achieve poor performance on semantic textual similarity. Given these circumstances, we treat the rumor thread propagation as a conversation tree and propose a hybrid model, TRANS-CVAE, to learn a thread embedding encoded with Bidirectional Encoder Representations from Transformers (BERT). We then adopt a temporal-based Variational Auto-Encoder (VAE) on this thread embedding to extract a compressed and latent representation from its latent semantic space. This latent embedding is revised by label-anchored contrastive loss. Using labels, this loss could pull together or push apart different threads to obtain better thread representations, and it could also alleviate the collapsed issue caused by BERT. Extensive experiments on the PHEME dataset show that our proposed approach outperforms many state-of-the-art rumor debunking models, and the ablation study also reflects the necessity of all three components to upgrade post-representation learning.",Representation learning;Social networking (online);Pandemics;Semantics;Learning (artificial intelligence);Bidirectional control;Transformers;Rumor Debunking;Transformer;Variational Auto-Encoder;Contrastive Learning,"{'month': 'July', 'issn': '2835-3560', 'doi': '10.1109/AITest58265.2023.00032', 'keywords': 'Representation learning;Social networking (online);Pandemics;Semantics;Learning (artificial intelligence);Bidirectional control;Transformers;Rumor Debunking;Transformer;Variational Auto-Encoder;Contrastive Learning', 'abstract': 'Rumors have been treated as the online pandemic on social media, impeding the truth. Existing claim-guided approaches tend to capture the structural, temporal, or relational indicative features among responsive posts to enhance representation learning, but few of them can integrate these features properly to comprehensively explore them. Additionally, large pre-trained language models are widely used for rumor debunking, such as Transformer-based models, which can result in collapsed issues and achieve poor performance on semantic textual similarity. Given these circumstances, we treat the rumor thread propagation as a conversation tree and propose a hybrid model, TRANS-CVAE, to learn a thread embedding encoded with Bidirectional Encoder Representations from Transformers (BERT). We then adopt a temporal-based Variational Auto-Encoder (VAE) on this thread embedding to extract a compressed and latent representation from its latent semantic space. This latent embedding is revised by label-anchored contrastive loss. Using labels, this loss could pull together or push apart different threads to obtain better thread representations, and it could also alleviate the collapsed issue caused by BERT. Extensive experiments on the PHEME dataset show that our proposed approach outperforms many state-of-the-art rumor debunking models, and the ablation study also reflects the necessity of all three components to upgrade post-representation learning.', 'pages': '152-159', 'number': '', 'volume': '', 'year': '2023', 'title': 'Debunk online rumors via generative-contrastive learning based on tree-transformer', 'booktitle': '2023 IEEE International Conference On Artificial Intelligence Testing (AITest)', 'author': 'Luo, Xi and Deng, Yuhui and Liu, Junjie and Wu, Sirong and Sun, Gengchen', 'ENTRYTYPE': 'inproceedings', 'ID': '10229440'}"
10839466,CDS-Net: Contextual Difference Sensitivity Network for Pixel-Wise Road Crack Detection,"Tan, Qinzhong and Li, Ao and Dong, Le and Dong, Weisheng and Li, Xin and Shi, Guangming",Tan,10.1109/TCSVT.2025.3529039,2025,IEEE Transactions on Circuits and Systems for Video Technology,"Road crack detection is a key computer vision task that identifies and locates cracks in road surface images, which usually have an irregular shape and contain only a few pixels in width. Generative and unsupervised methods are popular these years, but generative methods require a lot of training data and computational power while unsupervised methods are not so satisfactory in pixel-level segmentation. The process is challenged by the irregularity of crack shapes and complex road image backgrounds. To alleviate these problems, we propose a novel method in this paper, CDS-Net, that significantly improves road crack detection performance through multiple practical modules, including the Multi-Directional Hierarchical Attention (MDHA) module and the Difference Sensitivity Reconstruction Block (DSRB). Specifically, the MDHA module employs a multi-directional feature extraction strategy to capture detailed information of cracks, thereby enhancing the discriminative power of the features. The DSRB module, designed to address the inefficiency of traditional skip-connections, utilizes masked convolution and graph convolution attention to reconstruct and refine feature representations. Additionally, we propose an improved weighted cross-entropy loss function to address the inherent class imbalance problem in road crack detection. Extensive experiments on five public datasets demonstrate that CDS-Net achieves superior performance compared to other state-of-the-art methods, showcasing its effectiveness and robustness in road crack detection. It also has a stronger generalization ability compared with other methods. Code is available at https://github.com/ttttqz/CDS-Net/tree/master.",Roads;Image segmentation;Feature extraction;Accuracy;Transformers;Surface cracks;Shape;Sensitivity;Convolution;Computational modeling;Crack detection;hierarchical attention;sensitivity reconstruction;cross-entropy loss,"{'month': 'June', 'issn': '1558-2205', 'doi': '10.1109/TCSVT.2025.3529039', 'keywords': 'Roads;Image segmentation;Feature extraction;Accuracy;Transformers;Surface cracks;Shape;Sensitivity;Convolution;Computational modeling;Crack detection;hierarchical attention;sensitivity reconstruction;cross-entropy loss', 'abstract': 'Road crack detection is a key computer vision task that identifies and locates cracks in road surface images, which usually have an irregular shape and contain only a few pixels in width. Generative and unsupervised methods are popular these years, but generative methods require a lot of training data and computational power while unsupervised methods are not so satisfactory in pixel-level segmentation. The process is challenged by the irregularity of crack shapes and complex road image backgrounds. To alleviate these problems, we propose a novel method in this paper, CDS-Net, that significantly improves road crack detection performance through multiple practical modules, including the Multi-Directional Hierarchical Attention (MDHA) module and the Difference Sensitivity Reconstruction Block (DSRB). Specifically, the MDHA module employs a multi-directional feature extraction strategy to capture detailed information of cracks, thereby enhancing the discriminative power of the features. The DSRB module, designed to address the inefficiency of traditional skip-connections, utilizes masked convolution and graph convolution attention to reconstruct and refine feature representations. Additionally, we propose an improved weighted cross-entropy loss function to address the inherent class imbalance problem in road crack detection. Extensive experiments on five public datasets demonstrate that CDS-Net achieves superior performance compared to other state-of-the-art methods, showcasing its effectiveness and robustness in road crack detection. It also has a stronger generalization ability compared with other methods. Code is available at https://github.com/ttttqz/CDS-Net/tree/master.', 'pages': '5223-5235', 'number': '6', 'volume': '35', 'year': '2025', 'title': 'CDS-Net: Contextual Difference Sensitivity Network for Pixel-Wise Road Crack Detection', 'journal': 'IEEE Transactions on Circuits and Systems for Video Technology', 'author': 'Tan, Qinzhong and Li, Ao and Dong, Le and Dong, Weisheng and Li, Xin and Shi, Guangming', 'ENTRYTYPE': 'article', 'ID': '10839466'}"
8665362,The Effect of Explicit Structure Encoding of Deep Neural Networks for Symbolic Music Generation,"Chen, Ke and Zhang, Weilin and Dubnov, Shlomo and Xia, Gus and Li, Wei",Chen,10.1109/MMRP.2019.00022,2019,2019 International Workshop on Multilayer Music Representation and Processing (MMRP),"With recent breakthroughs in artificial neural networks, deep generative models have become one of the leading techniques for computational creativity. Despite very promising progress on image and short sequence generation, symbolic music generation remains a challenging problem since the structure of compositions are usually complicated. In this study, we attempt to solve the melody generation problem constrained by the given chord progression. In particular, we explore the effect of explicit architectural encoding of musical structure via comparing two sequential generative models: LSTM (a type of RNN) and WaveNet (dilated temporal-CNN). As far as we know, this is the first study of applying WaveNet to symbolic music generation, as well as the first systematic comparison between temporal-CNN and RNN for music generation. We conduct a survey for evaluation in our generations and implemented Variable Markov Oracle in music pattern discovery. Experimental results show that to encode structure more explicitly using a stack of dilated convolution layers improved the performance significantly, and a global encoding of underlying chord progression into the generation procedure gains even more.",Music;Autoregressive processes;Encoding;Neural networks;Computational modeling;Convolution;Data models;symbolic music generation;artificial intelligence;deep generative model;machine learning and understanding of music;Variable Markov Oracle;analysis of variance;music structure analysis,"{'month': 'Jan', 'issn': '', 'doi': '10.1109/MMRP.2019.00022', 'keywords': 'Music;Autoregressive processes;Encoding;Neural networks;Computational modeling;Convolution;Data models;symbolic music generation;artificial intelligence;deep generative model;machine learning and understanding of music;Variable Markov Oracle;analysis of variance;music structure analysis', 'abstract': 'With recent breakthroughs in artificial neural networks, deep generative models have become one of the leading techniques for computational creativity. Despite very promising progress on image and short sequence generation, symbolic music generation remains a challenging problem since the structure of compositions are usually complicated. In this study, we attempt to solve the melody generation problem constrained by the given chord progression. In particular, we explore the effect of explicit architectural encoding of musical structure via comparing two sequential generative models: LSTM (a type of RNN) and WaveNet (dilated temporal-CNN). As far as we know, this is the first study of applying WaveNet to symbolic music generation, as well as the first systematic comparison between temporal-CNN and RNN for music generation. We conduct a survey for evaluation in our generations and implemented Variable Markov Oracle in music pattern discovery. Experimental results show that to encode structure more explicitly using a stack of dilated convolution layers improved the performance significantly, and a global encoding of underlying chord progression into the generation procedure gains even more.', 'pages': '77-84', 'number': '', 'volume': '', 'year': '2019', 'title': 'The Effect of Explicit Structure Encoding of Deep Neural Networks for Symbolic Music Generation', 'booktitle': '2019 International Workshop on Multilayer Music Representation and Processing (MMRP)', 'author': 'Chen, Ke and Zhang, Weilin and Dubnov, Shlomo and Xia, Gus and Li, Wei', 'ENTRYTYPE': 'inproceedings', 'ID': '8665362'}"
10128698,Source-Free Multidomain Adaptation With Fuzzy Rule-Based Deep Neural Networks,"Li, Keqiuyin and Lu, Jie and Zuo, Hua and Zhang, Guangquan",Li,10.1109/TFUZZ.2023.3276978,2023,IEEE Transactions on Fuzzy Systems,"Unsupervised domain adaptation deals with a task from an unlabeled target domain by leveraging the knowledge gained from labeled source domain(s). The fuzzy system is adopted in domain adaptation to better tackle the uncertainty caused by information scarcity in the transfer. Most existing fuzzy and nonfuzzy domain adaptation methods depend on data-level distribution matching to eliminate the domain shift. However, data sharing can trigger privacy concerns. This situation results in the unavailability of source data, wherein most domain adaptation methods cannot be applied. Source-free domain adaptation is then proposed to handle this problem. But the existing source-free domain adaptation methods rarely deal with any soft information component due to data imprecision. Besides, fewer methods handle multiple source domains that provide richer transfer information. Thus, in this article, we propose source-free multidomain adaptation with fuzzy rule-based deep neural networks, which takes advantage of a fuzzy system to handle data uncertainty in domain adaptation without source data. To learn source private models with high generality, which is important to collect low-noise pseudotarget labels, auxiliary tasks are designed by jointly training source models from multiple domains, which share source parameters and fuzzy rules while protecting source data. To transfer fuzzy rules and fit source private parameters to the target domain, self-supervised learning and anchor-based alignment are built to force target data into source feature spaces. Experiments on real-world datasets under both homogeneous and heterogeneous label space scenarios are carried out to validate the proposed method. The results indicate the superiority of the proposed fuzzy rule-based source-free multidomain adaptation method.",Adaptation models;Feature extraction;Data models;Neural networks;Deep learning;Transfer learning;Machine learning;Classification;domain adaptation;fuzzy rules;machine learning;transfer learning,"{'month': 'Dec', 'issn': '1941-0034', 'doi': '10.1109/TFUZZ.2023.3276978', 'keywords': 'Adaptation models;Feature extraction;Data models;Neural networks;Deep learning;Transfer learning;Machine learning;Classification;domain adaptation;fuzzy rules;machine learning;transfer learning', 'abstract': 'Unsupervised domain adaptation deals with a task from an unlabeled target domain by leveraging the knowledge gained from labeled source domain(s). The fuzzy system is adopted in domain adaptation to better tackle the uncertainty caused by information scarcity in the transfer. Most existing fuzzy and nonfuzzy domain adaptation methods depend on data-level distribution matching to eliminate the domain shift. However, data sharing can trigger privacy concerns. This situation results in the unavailability of source data, wherein most domain adaptation methods cannot be applied. Source-free domain adaptation is then proposed to handle this problem. But the existing source-free domain adaptation methods rarely deal with any soft information component due to data imprecision. Besides, fewer methods handle multiple source domains that provide richer transfer information. Thus, in this article, we propose source-free multidomain adaptation with fuzzy rule-based deep neural networks, which takes advantage of a fuzzy system to handle data uncertainty in domain adaptation without source data. To learn source private models with high generality, which is important to collect low-noise pseudotarget labels, auxiliary tasks are designed by jointly training source models from multiple domains, which share source parameters and fuzzy rules while protecting source data. To transfer fuzzy rules and fit source private parameters to the target domain, self-supervised learning and anchor-based alignment are built to force target data into source feature spaces. Experiments on real-world datasets under both homogeneous and heterogeneous label space scenarios are carried out to validate the proposed method. The results indicate the superiority of the proposed fuzzy rule-based source-free multidomain adaptation method.', 'pages': '4180-4194', 'number': '12', 'volume': '31', 'year': '2023', 'title': 'Source-Free Multidomain Adaptation With Fuzzy Rule-Based Deep Neural Networks', 'journal': 'IEEE Transactions on Fuzzy Systems', 'author': 'Li, Keqiuyin and Lu, Jie and Zuo, Hua and Zhang, Guangquan', 'ENTRYTYPE': 'article', 'ID': '10128698'}"
9786028,Classification of Breast Cancer Histopathological Images using DensNet201,"Djouima, Hossena and Zitouni, Athmane and Megherbi, Ahmed Chaouki and Sbaa, Salim",Djouima,10.1109/ISPA54004.2022.9786028,2022,2022 7th International Conference on Image and Signal Processing and their Applications (ISPA),"Diagnosing and classifying breast cancer tumors is a rather complex activity for pathologists due to the heterogeneous nature of the tumor cells. The wide use of artificial intelligence (AI) and the rise of Deep Learning (DL) have led to promising results in terms of breast histopathology images classification. The outcomes depend largely on two main factors, namely, the number and quality of images. BreaKhis dataset shows an imbalance in the image classes distribution, thus generating the performance degradation of the classifier model due to a biased classification towards the majority class. In this paper, a Deep Convolution Generative Adversial Network (DCGAN) is applied to give the number of images consistence in the minority (benign) class with that of the majority (malignant) class. Data augmentation is a technique used later to create more data from the limited ones. The DenseNet201 pre-trained model is chosen and used with the concatenation of features from various DensNet blocks. Instead of considering all the layers of the pre-trained network, the features are extracted from the lower layers of DensNet201, via a global average pooling (GAP). These features are passed to the softmax classifier to classify breast cancer. The model is evaluated using a two-class BreaKhis, provided at four magnification levels 40x, 100x, 200x, and 400x. The proposed method yielded test accuracies of 96\%, 95\%, 88\%, and 92\% respectively for each magnification factor. As indicated in the results, the proposed method based on data augmentation by DCGAN and feature concatenation using DenseNet201 pre-trained models could produce an efficient prediction for breast cancer image classification.",Deep learning;Degradation;Histopathology;Predictive models;Feature extraction;Breast cancer;Data models;deep learning;breast cancer;classification;DCGAN;transfer learning,"{'month': 'May', 'issn': '', 'doi': '10.1109/ISPA54004.2022.9786028', 'keywords': 'Deep learning;Degradation;Histopathology;Predictive models;Feature extraction;Breast cancer;Data models;deep learning;breast cancer;classification;DCGAN;transfer learning', 'abstract': 'Diagnosing and classifying breast cancer tumors is a rather complex activity for pathologists due to the heterogeneous nature of the tumor cells. The wide use of artificial intelligence (AI) and the rise of Deep Learning (DL) have led to promising results in terms of breast histopathology images classification. The outcomes depend largely on two main factors, namely, the number and quality of images. BreaKhis dataset shows an imbalance in the image classes distribution, thus generating the performance degradation of the classifier model due to a biased classification towards the majority class. In this paper, a Deep Convolution Generative Adversial Network (DCGAN) is applied to give the number of images consistence in the minority (benign) class with that of the majority (malignant) class. Data augmentation is a technique used later to create more data from the limited ones. The DenseNet201 pre-trained model is chosen and used with the concatenation of features from various DensNet blocks. Instead of considering all the layers of the pre-trained network, the features are extracted from the lower layers of DensNet201, via a global average pooling (GAP). These features are passed to the softmax classifier to classify breast cancer. The model is evaluated using a two-class BreaKhis, provided at four magnification levels 40x, 100x, 200x, and 400x. The proposed method yielded test accuracies of 96\\%, 95\\%, 88\\%, and 92\\% respectively for each magnification factor. As indicated in the results, the proposed method based on data augmentation by DCGAN and feature concatenation using DenseNet201 pre-trained models could produce an efficient prediction for breast cancer image classification.', 'pages': '1-6', 'number': '', 'volume': '', 'year': '2022', 'title': 'Classification of Breast Cancer Histopathological Images using DensNet201', 'booktitle': '2022 7th International Conference on Image and Signal Processing and their Applications (ISPA)', 'author': 'Djouima, Hossena and Zitouni, Athmane and Megherbi, Ahmed Chaouki and Sbaa, Salim', 'ENTRYTYPE': 'inproceedings', 'ID': '9786028'}"
11010595,Automated UI Interface Generation via Diffusion Models: Enhancing Personalization and Efficiency,"Duan, Yifei and Yang, Liuqingqing and Zhang, Tong and Song, Zhijun and Shao, Fenghua",Duan,10.1109/ISCAIT64916.2025.11010595,2025,2025 4th International Symposium on Computer Applications and Information Technology (ISCAIT),"This study proposes a UI interface generation method based on a diffusion model, aiming to achieve high-quality, diversified, and personalized interface design through generative artificial intelligence technology. The diffusion model is based on its step-by-step denoising generation process. By combining the conditional generation mechanism, design optimization module, and user feedback mechanism, the model can generate a UI interface that meets the requirements based on multimodal inputs such as text descriptions and sketches provided by users. In the study, a complete experimental evaluation framework was designed, and mainstream generation models (such as GAN, VAE, DALL·E, etc.) were selected for comparative experiments. The generation results were quantitatively analyzed from indicators such as PSNR, SSIM, and FID. The results show that the model proposed in this study is superior to other models in terms of generation quality and user satisfaction, especially in terms of logical clarity of information transmission and visual aesthetics. The ablation experiment further verifies the key role of conditional generation and design optimization modules in improving interface quality. This study provides a new technical path for UI design automation and lays the foundation for the intelligent and personalized development of human-computer interaction interfaces. In the future, the application potential of the model in virtual reality, game design, and other fields will be further explored.",Human computer interaction;Solid modeling;Adaptation models;Visualization;Design automation;Virtual reality;Games;Diffusion models;Optimization;Design optimization;Diffusion model;UI interface generation;User experience;Human-computer interaction,"{'month': 'March', 'issn': '', 'doi': '10.1109/ISCAIT64916.2025.11010595', 'keywords': 'Human computer interaction;Solid modeling;Adaptation models;Visualization;Design automation;Virtual reality;Games;Diffusion models;Optimization;Design optimization;Diffusion model;UI interface generation;User experience;Human-computer interaction', 'abstract': 'This study proposes a UI interface generation method based on a diffusion model, aiming to achieve high-quality, diversified, and personalized interface design through generative artificial intelligence technology. The diffusion model is based on its step-by-step denoising generation process. By combining the conditional generation mechanism, design optimization module, and user feedback mechanism, the model can generate a UI interface that meets the requirements based on multimodal inputs such as text descriptions and sketches provided by users. In the study, a complete experimental evaluation framework was designed, and mainstream generation models (such as GAN, VAE, DALL·E, etc.) were selected for comparative experiments. The generation results were quantitatively analyzed from indicators such as PSNR, SSIM, and FID. The results show that the model proposed in this study is superior to other models in terms of generation quality and user satisfaction, especially in terms of logical clarity of information transmission and visual aesthetics. The ablation experiment further verifies the key role of conditional generation and design optimization modules in improving interface quality. This study provides a new technical path for UI design automation and lays the foundation for the intelligent and personalized development of human-computer interaction interfaces. In the future, the application potential of the model in virtual reality, game design, and other fields will be further explored.', 'pages': '780-783', 'number': '', 'volume': '', 'year': '2025', 'title': 'Automated UI Interface Generation via Diffusion Models: Enhancing Personalization and Efficiency', 'booktitle': '2025 4th International Symposium on Computer Applications and Information Technology (ISCAIT)', 'author': 'Duan, Yifei and Yang, Liuqingqing and Zhang, Tong and Song, Zhijun and Shao, Fenghua', 'ENTRYTYPE': 'inproceedings', 'ID': '11010595'}"
8836978,History and Development Tendency of Human - Computer Dialogue System,"Lin, Xiaoyu and Yuan, Tommy and Lei, Gang",Lin,10.1109/ICAIBD.2019.8836978,2019,2019 2nd International Conference on Artificial Intelligence and Big Data (ICAIBD),"As the core technology in the field of artificial intelligence, human-computer dialogue system is developing rapidly, and it is more widely used in various fields. The development of science and technology is increasing people and computer communications. In order to clarify the direction, the development process and complex nature of human-computer dialogue system are classified and analysed. The development of human-computer dialogue system can be considered as three stages: embryonic, development and breakthrough according to chronological order and technical characteristics. We comprehensively combine and summarize the representative results and shortcomings of each stage, and analyze the opportunities and challenges faced by human-computer dialogue systems in terms of capabilities and technical means to adapt to human needs. It is anticipated that the work will help to understand the direction of this line of research and contribute to the development of human-computer dialogue.",Task analysis;Information retrieval;Robots;History;Pipelines;Personal digital assistants;human-computer dialogue system;task-oriented;non-task-oriented;Intelligent Personal Assistant;social chatbot,"{'month': 'May', 'issn': '', 'doi': '10.1109/ICAIBD.2019.8836978', 'keywords': 'Task analysis;Information retrieval;Robots;History;Pipelines;Personal digital assistants;human-computer dialogue system;task-oriented;non-task-oriented;Intelligent Personal Assistant;social chatbot', 'abstract': 'As the core technology in the field of artificial intelligence, human-computer dialogue system is developing rapidly, and it is more widely used in various fields. The development of science and technology is increasing people and computer communications. In order to clarify the direction, the development process and complex nature of human-computer dialogue system are classified and analysed. The development of human-computer dialogue system can be considered as three stages: embryonic, development and breakthrough according to chronological order and technical characteristics. We comprehensively combine and summarize the representative results and shortcomings of each stage, and analyze the opportunities and challenges faced by human-computer dialogue systems in terms of capabilities and technical means to adapt to human needs. It is anticipated that the work will help to understand the direction of this line of research and contribute to the development of human-computer dialogue.', 'pages': '154-160', 'number': '', 'volume': '', 'year': '2019', 'title': 'History and Development Tendency of Human - Computer Dialogue System', 'booktitle': '2019 2nd International Conference on Artificial Intelligence and Big Data (ICAIBD)', 'author': 'Lin, Xiaoyu and Yuan, Tommy and Lei, Gang', 'ENTRYTYPE': 'inproceedings', 'ID': '8836978'}"
10365001,Intelligent electricity theft detection system in low-voltage stations considering data with low-quality and imbalance,"Gao, B. and Kong, X.",Gao,10.1049/icp.2023.2822,2023,Tianjin University-IET Electrical \& Information Engineering Doctoral Forum (TJU IET 2023),"Accurate locating and timely processing of electricity theft targets are of great significance to guarantee the stability of the grid operation. With the development of the electric internet of things and artificial intelligence technologies, various efficient and intelligent electricity theft detection methods have been researched. However, the problems of the data imbalance, missing and exception are often encountered in actual conditions, which have led to the inferior performance for detection model. In this paper, the DLMVI-VAE-rACGANs-RD model, as a potential solution, is proposed to solve the dilemmas existing in the actual situation of electricity theft detection. Firstly, deep learning-based missing value imputation (DLMVI) method was used to solve the problem of data missing and exception. After that, the variational autoencoder (VAE) combined with label-noise robust auxiliary classifier generative adversarial networks (rACGANs) was developed to generate electricity theft data to eliminate the impact of data imbalance. Finally, the retrained discriminator (RD) of rACGANs is used to complete the mission of electricity theft detection. The comprehensive experimental results indicate that the DLMVI-VAE-rACGANs-RD model has a better detection performance.",,"{'month': 'Sep.', 'issn': '', 'doi': '10.1049/icp.2023.2822', 'keywords': '', 'abstract': 'Accurate locating and timely processing of electricity theft targets are of great significance to guarantee the stability of the grid operation. With the development of the electric internet of things and artificial intelligence technologies, various efficient and intelligent electricity theft detection methods have been researched. However, the problems of the data imbalance, missing and exception are often encountered in actual conditions, which have led to the inferior performance for detection model. In this paper, the DLMVI-VAE-rACGANs-RD model, as a potential solution, is proposed to solve the dilemmas existing in the actual situation of electricity theft detection. Firstly, deep learning-based missing value imputation (DLMVI) method was used to solve the problem of data missing and exception. After that, the variational autoencoder (VAE) combined with label-noise robust auxiliary classifier generative adversarial networks (rACGANs) was developed to generate electricity theft data to eliminate the impact of data imbalance. Finally, the retrained discriminator (RD) of rACGANs is used to complete the mission of electricity theft detection. The comprehensive experimental results indicate that the DLMVI-VAE-rACGANs-RD model has a better detection performance.', 'pages': '16-21', 'number': '', 'volume': '2023', 'year': '2023', 'title': 'Intelligent electricity theft detection system in low-voltage stations considering data with low-quality and imbalance', 'booktitle': 'Tianjin University-IET Electrical \\& Information Engineering Doctoral Forum (TJU IET 2023)', 'author': 'Gao, B. and Kong, X.', 'ENTRYTYPE': 'inproceedings', 'ID': '10365001'}"
10695083,Applications of LLMs for Generating Cyber Security Exercise Scenarios,"Mudassar Yamin, Muhammad and Hashmi, Ehtesham and Ullah, Mohib and Katt, Basel",Mudassar Yamin,10.1109/ACCESS.2024.3468914,2024,IEEE Access,"This study proposes a novel approach leveraging Large Language Models (LLMs) to generate dynamic and complex adaptable cybersecurity exercise scenarios. Motivated by Turing’s seminal exploration into machine cognition, which questions the ability of machines to mimic human thought and intelligence. By exploiting the generative potential of LLMs, our methodology simulates a wide range of cyber threats, both known and novel, thereby enhancing cybersecurity training and awareness. This approach transforms the potential for ‘hallucination’ inherent in LLMs into a potential advantage, enabling the creation of complex exercise scenarios that push the boundaries of traditional cybersecurity training. The innovation lies in the sophisticated application of AI, aiming to advance the preparedness of security professionals against diverse cyber threats. The scenarios generated through this method were subject to meticulous testing and a rigorous evaluation process involving (Generated Pre-Trained Transformer) GPT models and expert review to ensure their realism and applicability. In this paper, we introduce ‘CyExec,’ a novel approach leveraging GPT to dynamically generate cybersecurity training scenarios. Furthermore, the prompts provided to the LLMs were meticulously designed to adopt a Retrieval-Augmented Generation (RAG) approach, enriching the complexity and relevance of the scenarios. This incorporation of RAG, alongside the inspiration drawn from Turing’s exploration of machine intelligence, showcases an advanced application of AI in cybersecurity training, reflecting a deep understanding of how machines can augment our capabilities to anticipate and mitigate cyber threats.",Training;Computer security;Computer crime;Security;Manuals;Transformers;Testing;Organizations;Ethics;Data models;Cyber security exercise scenarios;large language models;bounded rationality;generative configurations;Halluciation in LLMs,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2024.3468914', 'keywords': 'Training;Computer security;Computer crime;Security;Manuals;Transformers;Testing;Organizations;Ethics;Data models;Cyber security exercise scenarios;large language models;bounded rationality;generative configurations;Halluciation in LLMs', 'abstract': 'This study proposes a novel approach leveraging Large Language Models (LLMs) to generate dynamic and complex adaptable cybersecurity exercise scenarios. Motivated by Turing’s seminal exploration into machine cognition, which questions the ability of machines to mimic human thought and intelligence. By exploiting the generative potential of LLMs, our methodology simulates a wide range of cyber threats, both known and novel, thereby enhancing cybersecurity training and awareness. This approach transforms the potential for ‘hallucination’ inherent in LLMs into a potential advantage, enabling the creation of complex exercise scenarios that push the boundaries of traditional cybersecurity training. The innovation lies in the sophisticated application of AI, aiming to advance the preparedness of security professionals against diverse cyber threats. The scenarios generated through this method were subject to meticulous testing and a rigorous evaluation process involving (Generated Pre-Trained Transformer) GPT models and expert review to ensure their realism and applicability. In this paper, we introduce ‘CyExec,’ a novel approach leveraging GPT to dynamically generate cybersecurity training scenarios. Furthermore, the prompts provided to the LLMs were meticulously designed to adopt a Retrieval-Augmented Generation (RAG) approach, enriching the complexity and relevance of the scenarios. This incorporation of RAG, alongside the inspiration drawn from Turing’s exploration of machine intelligence, showcases an advanced application of AI in cybersecurity training, reflecting a deep understanding of how machines can augment our capabilities to anticipate and mitigate cyber threats.', 'pages': '143806-143822', 'number': '', 'volume': '12', 'year': '2024', 'title': 'Applications of LLMs for Generating Cyber Security Exercise Scenarios', 'journal': 'IEEE Access', 'author': 'Mudassar Yamin, Muhammad and Hashmi, Ehtesham and Ullah, Mohib and Katt, Basel', 'ENTRYTYPE': 'article', 'ID': '10695083'}"
10896802,Enhancing Image Retrieval Performance With Generative Models in Siamese Networks,"Golfe, Alejandro and Colomer, Adrián and Prades, José and Naranjo, Valery",Golfe,10.1109/JBHI.2025.3543907,2025,IEEE Journal of Biomedical and Health Informatics,"Prostate cancer is a critical healthcare challenge globally and is one of the most prevalent types of cancer in men. Early and accurate diagnosis is essential for effective treatment and improved patient outcomes. In the existing literature, computer-aided diagnosis (CAD) solutions have been developed to assist pathologists in various tasks, including classification, diagnosis, and prostate cancer grading. Content-based image retrieval (CBIR) techniques provide valuable approaches to enhance these computer-aided solutions. This study evaluates how generative deep learning models can improve the quality of retrievals within a CBIR system. Specifically, we propose applying a Siamese Network approach, which enables us to learn how to encode image patches into latent representations for retrieval purposes. We used the ProGleason-GAN framework trained on the SiCAPv2 dataset to create similar pairs of input patches. Our observations indicate that introducing synthetic patches leads to notable improvements in the evaluated metrics, underscoring the utility of generative models within CBIR tasks. Furthermore, this work is the first in the literature where latent representations optimized for CBIR are used to train an attention mechanism for performing Gleason Scoring of a WSI.",Feature extraction;Biological system modeling;Deep learning;Solid modeling;Image retrieval;Histopathology;Vectors;Prostate cancer;Biopsy;Bioinformatics;Generative deep learning;prostate cancer;image retrieval;siamese network;histology,"{'month': 'July', 'issn': '2168-2208', 'doi': '10.1109/JBHI.2025.3543907', 'keywords': 'Feature extraction;Biological system modeling;Deep learning;Solid modeling;Image retrieval;Histopathology;Vectors;Prostate cancer;Biopsy;Bioinformatics;Generative deep learning;prostate cancer;image retrieval;siamese network;histology', 'abstract': 'Prostate cancer is a critical healthcare challenge globally and is one of the most prevalent types of cancer in men. Early and accurate diagnosis is essential for effective treatment and improved patient outcomes. In the existing literature, computer-aided diagnosis (CAD) solutions have been developed to assist pathologists in various tasks, including classification, diagnosis, and prostate cancer grading. Content-based image retrieval (CBIR) techniques provide valuable approaches to enhance these computer-aided solutions. This study evaluates how generative deep learning models can improve the quality of retrievals within a CBIR system. Specifically, we propose applying a Siamese Network approach, which enables us to learn how to encode image patches into latent representations for retrieval purposes. We used the ProGleason-GAN framework trained on the SiCAPv2 dataset to create similar pairs of input patches. Our observations indicate that introducing synthetic patches leads to notable improvements in the evaluated metrics, underscoring the utility of generative models within CBIR tasks. Furthermore, this work is the first in the literature where latent representations optimized for CBIR are used to train an attention mechanism for performing Gleason Scoring of a WSI.', 'pages': '4956-4968', 'number': '7', 'volume': '29', 'year': '2025', 'title': 'Enhancing Image Retrieval Performance With Generative Models in Siamese Networks', 'journal': 'IEEE Journal of Biomedical and Health Informatics', 'author': 'Golfe, Alejandro and Colomer, Adrián and Prades, José and Naranjo, Valery', 'ENTRYTYPE': 'article', 'ID': '10896802'}"
10860622,Improving Programmer Work Quality with ChatGPT Assistance,"Rumondor, Abraham David and Abimanyu, Aditya and Abiyyu’Ammaar, Muhammad and Achmad, Said and Sutoyo, Rhio",Rumondor,10.1109/CHIuXiD64022.2024.10860622,2024,2024 10th International HCI and UX Conference in Indonesia (CHIuXiD),"The potential of artificial intelligence as a tool that can be an assistant for humans is very close. This is reinforced by the breakthrough of a generative model that is considered to be able to help many tasks that are usually done by humans, namely ChatGPT. ChatGPT successfully demonstrated its potential by providing complete and intact code generation. ChatGPT, with GPT-3, is one of many language models currently available for people. In previous research, ChatGPT was tested to see if it could provide appropriate programming code results and perform large-scale data analysis on a specific dataset described in sentence form. The limitations of ChatGPT include understanding and generating code, which requires further training data. This opportunity positions ChatGPT as a provider of programming materials and a place to seek solutions to problems faced by programmers. This paper investigates the capabilities and also limitations of ChatGPT to help with the works of programmer’s daily tasks by examining overall ChatGPT capabilities within the assigned environment as a Programmer Assistant and examining the overall probability of ChatGPT limitations and failures to generate the results from specified prompts from the programmer. This study formulated the research question and performed the PRISMA step to answer the question. This study found significant potential for ChatGPT as a programmer’s assistant, catering to requests related to programming and design, and also functions as a tool for designing programming code to be implemented in software development.",Codes;Training data;Programming;Chatbots;Artificial intelligence;Software development management;Programmer;Programming;AI Assistant;and ChatGPT,"{'month': 'Nov', 'issn': '', 'doi': '10.1109/CHIuXiD64022.2024.10860622', 'keywords': 'Codes;Training data;Programming;Chatbots;Artificial intelligence;Software development management;Programmer;Programming;AI Assistant;and ChatGPT', 'abstract': 'The potential of artificial intelligence as a tool that can be an assistant for humans is very close. This is reinforced by the breakthrough of a generative model that is considered to be able to help many tasks that are usually done by humans, namely ChatGPT. ChatGPT successfully demonstrated its potential by providing complete and intact code generation. ChatGPT, with GPT-3, is one of many language models currently available for people. In previous research, ChatGPT was tested to see if it could provide appropriate programming code results and perform large-scale data analysis on a specific dataset described in sentence form. The limitations of ChatGPT include understanding and generating code, which requires further training data. This opportunity positions ChatGPT as a provider of programming materials and a place to seek solutions to problems faced by programmers. This paper investigates the capabilities and also limitations of ChatGPT to help with the works of programmer’s daily tasks by examining overall ChatGPT capabilities within the assigned environment as a Programmer Assistant and examining the overall probability of ChatGPT limitations and failures to generate the results from specified prompts from the programmer. This study formulated the research question and performed the PRISMA step to answer the question. This study found significant potential for ChatGPT as a programmer’s assistant, catering to requests related to programming and design, and also functions as a tool for designing programming code to be implemented in software development.', 'pages': '42-47', 'number': '', 'volume': '', 'year': '2024', 'title': 'Improving Programmer Work Quality with ChatGPT Assistance', 'booktitle': '2024 10th International HCI and UX Conference in Indonesia (CHIuXiD)', 'author': 'Rumondor, Abraham David and Abimanyu, Aditya and Abiyyu’Ammaar, Muhammad and Achmad, Said and Sutoyo, Rhio', 'ENTRYTYPE': 'inproceedings', 'ID': '10860622'}"
9761928,Unsupervised Learning for Seismic Internal Multiple Suppression Based on Adaptive Virtual Events,"Wang, Kunxi and Hu, Tianyue and Wang, Shangxu",Wang,10.1109/TGRS.2022.3169481,2022,IEEE Transactions on Geoscience and Remote Sensing,"Seismic internal multiples are the key factors affecting the accuracy and reliability of velocity analysis and migration. The removal of internal multiples is a challenging direction. To effectively remove the internal multiples from the seismic data, we propose the unsupervised deep neural network (DNN) combined with the adaptive virtual events (AVEs) method. First, we use the AVE method to get the predicted internal multiples, which can calibrate the true internal multiples in the original data, also called the full wavefield data. Second, the unsupervised learning with the DNN is used as a nonlinear operator to minimize the difference between the estimated internal multiples and original data. The trained DNN can obtain the estimated internal multiples through the predicted internal multiples, thereby completing the suppression of the internal multiples. Since our proposed unsupervised learning is essentially an optimization process, it does not require true primaries as the label data to participate in the training process for the DNN. Therefore, our proposed method can deal with the problem of lack of training set and would have some good practical application value with low computational cost. The effectiveness and efficiency of our proposed method are verified through two sets of synthetic data and one land field data examples.",Deep learning;Scattering;Convolutional neural networks;Surface waves;Unsupervised learning;Training;Three-dimensional displays;Adaptive virtual events (AVE);deep neural network (DNN);internal multiple;unsupervised learning,"{'month': '', 'issn': '1558-0644', 'doi': '10.1109/TGRS.2022.3169481', 'keywords': 'Deep learning;Scattering;Convolutional neural networks;Surface waves;Unsupervised learning;Training;Three-dimensional displays;Adaptive virtual events (AVE);deep neural network (DNN);internal multiple;unsupervised learning', 'abstract': 'Seismic internal multiples are the key factors affecting the accuracy and reliability of velocity analysis and migration. The removal of internal multiples is a challenging direction. To effectively remove the internal multiples from the seismic data, we propose the unsupervised deep neural network (DNN) combined with the adaptive virtual events (AVEs) method. First, we use the AVE method to get the predicted internal multiples, which can calibrate the true internal multiples in the original data, also called the full wavefield data. Second, the unsupervised learning with the DNN is used as a nonlinear operator to minimize the difference between the estimated internal multiples and original data. The trained DNN can obtain the estimated internal multiples through the predicted internal multiples, thereby completing the suppression of the internal multiples. Since our proposed unsupervised learning is essentially an optimization process, it does not require true primaries as the label data to participate in the training process for the DNN. Therefore, our proposed method can deal with the problem of lack of training set and would have some good practical application value with low computational cost. The effectiveness and efficiency of our proposed method are verified through two sets of synthetic data and one land field data examples.', 'pages': '1-13', 'number': '', 'volume': '60', 'year': '2022', 'title': 'Unsupervised Learning for Seismic Internal Multiple Suppression Based on Adaptive Virtual Events', 'journal': 'IEEE Transactions on Geoscience and Remote Sensing', 'author': 'Wang, Kunxi and Hu, Tianyue and Wang, Shangxu', 'ENTRYTYPE': 'article', 'ID': '9761928'}"
10614265,5 AI Changes Society,"Ojanperä, Tero",Ojanperä,,2024,AI Revolution: Mastering AI for Personal and Organizational Growth,"""The AI Revolution"" is a practical guide to using new AI tools, such as ChatGPT, DALLE and Midjourney. Learn how to multiply your productivity by guiding or prompting AI in various ways. The book also introduces Microsoft Copilot, Google Bard, and Adobe Photoshop Generative Fill, among other new applications. ChatGPT reached a hundred million users in just two months after its release, faster than any other application before. This marked the advent of the generative AI era. Generative AI models generate text, images, music, videos, and even 3D models in ways previously thought impossible for machines. The book explains in an understandable manner how these AI models work. The book provides examples of how AI increases productivity, which professions are changing or disappearing, and how job markets will evolve in the coming years. With this book, you'll learn to recognize the opportunities and risks AI offers. Understand what this change demands from individuals and companies and what strategic skills are required. The book also covers legal questions caused by generative AI, like copyrights, data protection, and AI regulation. It also ponders societal impacts. AI produces content, thus influencing language, culture, and even worldviews. Therefore, it's crucial to understand by whom and how AI is trained. The AI revolution started by ChatGPT is just the beginning. This handbook is for you if you want to keep up with the rapid development of AI.",,"{'url': 'https://ieeexplore-ieee-org.crai.referencistas.com/document/10614265', 'isbn': '9788770042314', 'publisher': 'River Publishers', 'issn': '', 'doi': '', 'keywords': '', 'abstract': '""The AI Revolution"" is a practical guide to using new AI tools, such as ChatGPT, DALLE and Midjourney. Learn how to multiply your productivity by guiding or prompting AI in various ways. The book also introduces Microsoft Copilot, Google Bard, and Adobe Photoshop Generative Fill, among other new applications. ChatGPT reached a hundred million users in just two months after its release, faster than any other application before. This marked the advent of the generative AI era. Generative AI models generate text, images, music, videos, and even 3D models in ways previously thought impossible for machines. The book explains in an understandable manner how these AI models work. The book provides examples of how AI increases productivity, which professions are changing or disappearing, and how job markets will evolve in the coming years. With this book, you\'ll learn to recognize the opportunities and risks AI offers. Understand what this change demands from individuals and companies and what strategic skills are required. The book also covers legal questions caused by generative AI, like copyrights, data protection, and AI regulation. It also ponders societal impacts. AI produces content, thus influencing language, culture, and even worldviews. Therefore, it\'s crucial to understand by whom and how AI is trained. The AI revolution started by ChatGPT is just the beginning. This handbook is for you if you want to keep up with the rapid development of AI.', 'pages': '91-100', 'number': '', 'volume': '', 'year': '2024', 'title': '5 AI Changes Society', 'booktitle': 'AI Revolution: Mastering AI for Personal and Organizational Growth', 'author': 'Ojanperä, Tero', 'ENTRYTYPE': 'inbook', 'ID': '10614265'}"
10834365,Tailoring Your Code Companion: Leveraging LLMs and RAG to Develop a Chatbot to Support Students in a Programming Course,"Alario-Hoyos, Carlos and Kemcha, Rebiha and Kloos, Carlos Delgado and Callejo, Patricia and Estévez-Ayres, Iria and Santín-Cristóbal, David and Cruz-Argudo, Francisco and López-Sánchez, José Luis",Alario-Hoyos,10.1109/TALE62452.2024.10834365,2024,"2024 IEEE International Conference on Teaching, Assessment and Learning for Engineering (TALE)","Students frequently rely on chatbots powered by generative Artificial Intelligence (GenAI), such as ChatGPT, Copilot, Gemini, and Claude, to assist with a wide range of academic tasks. However, these chatbots are not specifically designed for the context of particular courses, which can lead to responses that are sometimes inaccurate or insufficiently relevant. This paper introduces a chatbot specifically designed to support first-year engineering students in a Java programming course. Developed using the Retrieval-Augmented Generation (RAG) technique, the chatbot draws on course-specific resources such as videos, quizzes, programming exercises, and other materials, while using OpenAI’s Large Language Models (LLMs) GPT-4 and GPT-3.5 for information analysis and response generation. The data collected, consisting of logs from 1,059 messages sent by students to the chatbot and 30 responses to a survey, indicate that students primarily used the chatbot to clarify concepts and explain code snippets. Moreover, most of the students reported that the responses provided by the chatbot were well suited to the Java programming course.",Surveys;Java;Codes;Large language models;Retrieval augmented generation;Learning (artificial intelligence);Programming;Chatbots;Videos;Information analysis;Large Language Models (LLMs);Retrieval-Augmented Generation (RAG);Generative Artificial Intelligence (GenAI);Chatbots;and Programming Course,"{'month': 'Dec', 'issn': '', 'doi': '10.1109/TALE62452.2024.10834365', 'keywords': 'Surveys;Java;Codes;Large language models;Retrieval augmented generation;Learning (artificial intelligence);Programming;Chatbots;Videos;Information analysis;Large Language Models (LLMs);Retrieval-Augmented Generation (RAG);Generative Artificial Intelligence (GenAI);Chatbots;and Programming Course', 'abstract': 'Students frequently rely on chatbots powered by generative Artificial Intelligence (GenAI), such as ChatGPT, Copilot, Gemini, and Claude, to assist with a wide range of academic tasks. However, these chatbots are not specifically designed for the context of particular courses, which can lead to responses that are sometimes inaccurate or insufficiently relevant. This paper introduces a chatbot specifically designed to support first-year engineering students in a Java programming course. Developed using the Retrieval-Augmented Generation (RAG) technique, the chatbot draws on course-specific resources such as videos, quizzes, programming exercises, and other materials, while using OpenAI’s Large Language Models (LLMs) GPT-4 and GPT-3.5 for information analysis and response generation. The data collected, consisting of logs from 1,059 messages sent by students to the chatbot and 30 responses to a survey, indicate that students primarily used the chatbot to clarify concepts and explain code snippets. Moreover, most of the students reported that the responses provided by the chatbot were well suited to the Java programming course.', 'pages': '1-8', 'number': '', 'volume': '', 'year': '2024', 'title': 'Tailoring Your Code Companion: Leveraging LLMs and RAG to Develop a Chatbot to Support Students in a Programming Course', 'booktitle': '2024 IEEE International Conference on Teaching, Assessment and Learning for Engineering (TALE)', 'author': 'Alario-Hoyos, Carlos and Kemcha, Rebiha and Kloos, Carlos Delgado and Callejo, Patricia and Estévez-Ayres, Iria and Santín-Cristóbal, David and Cruz-Argudo, Francisco and López-Sánchez, José Luis', 'ENTRYTYPE': 'inproceedings', 'ID': '10834365'}"
10634792,Towards the Vehicular Metaverse: Exploring Distributed Inference With Transformer-Based Diffusion Model,"Xie, Gaochang and Xiong, Zehui and Zhang, Xinyuan and Xie, Renchao and Liu, Yunjie and Shen, Xuemin",Xie,10.1109/TVT.2024.3442292,2024,IEEE Transactions on Vehicular Technology,"Generative artificial intelligence (GAI) is emerging as a promising solution for the vehicular metaverse due to its adaptable, high-quality, and multi-modal content generation capabilities. Particularly noteworthy is the recent introduction of the Sora model, a Transformer-based diffusion model, which exhibits exceptional performance in visual scenarios. However, diffusion vision transformer (DViT) models face limitations in terms of device resources, inference latency, and personalized requirements at the edge, despite their practical effectiveness in clouds. In response, we propose a DViT-enabled system to enhance vehicular metaverse services. Our approach involves a distributed DViT inference mechanism where road-side units (RSUs) and vehicles collaborate to execute the diffusion process and generate personalized content within vehicles using local prompts. Additionally, we address users' latency-sensitive service demands by formulating a distributed latency optimization problem that considers bandwidth, computation power, and dynamic positioning of heterogeneous devices. We then propose a value iteration-based distributed inference algorithm capable of adaptively determining optimal inference strategies within resource-constrained vehicular networks. Numerical simulations demonstrate that our approach achieves superior performance in reducing latency and enhancing success rates for inference tasks.",Metaverse;Diffusion models;Transformers;Vehicle dynamics;Optimization;Computational modeling;Artificial intelligence;Distributed inference;generative artificial intelligence (GAI);latency optimization;vehicular metaverse,"{'month': 'Dec', 'issn': '1939-9359', 'doi': '10.1109/TVT.2024.3442292', 'keywords': 'Metaverse;Diffusion models;Transformers;Vehicle dynamics;Optimization;Computational modeling;Artificial intelligence;Distributed inference;generative artificial intelligence (GAI);latency optimization;vehicular metaverse', 'abstract': ""Generative artificial intelligence (GAI) is emerging as a promising solution for the vehicular metaverse due to its adaptable, high-quality, and multi-modal content generation capabilities. Particularly noteworthy is the recent introduction of the Sora model, a Transformer-based diffusion model, which exhibits exceptional performance in visual scenarios. However, diffusion vision transformer (DViT) models face limitations in terms of device resources, inference latency, and personalized requirements at the edge, despite their practical effectiveness in clouds. In response, we propose a DViT-enabled system to enhance vehicular metaverse services. Our approach involves a distributed DViT inference mechanism where road-side units (RSUs) and vehicles collaborate to execute the diffusion process and generate personalized content within vehicles using local prompts. Additionally, we address users' latency-sensitive service demands by formulating a distributed latency optimization problem that considers bandwidth, computation power, and dynamic positioning of heterogeneous devices. We then propose a value iteration-based distributed inference algorithm capable of adaptively determining optimal inference strategies within resource-constrained vehicular networks. Numerical simulations demonstrate that our approach achieves superior performance in reducing latency and enhancing success rates for inference tasks."", 'pages': '19931-19936', 'number': '12', 'volume': '73', 'year': '2024', 'title': 'Towards the Vehicular Metaverse: Exploring Distributed Inference With Transformer-Based Diffusion Model', 'journal': 'IEEE Transactions on Vehicular Technology', 'author': 'Xie, Gaochang and Xiong, Zehui and Zhang, Xinyuan and Xie, Renchao and Liu, Yunjie and Shen, Xuemin', 'ENTRYTYPE': 'article', 'ID': '10634792'}"
10614277,"8 Will AI Define our Culture, Language, and Worldview?","Ojanperä, Tero",Ojanperä,,2024,AI Revolution: Mastering AI for Personal and Organizational Growth,"""The AI Revolution"" is a practical guide to using new AI tools, such as ChatGPT, DALLE and Midjourney. Learn how to multiply your productivity by guiding or prompting AI in various ways. The book also introduces Microsoft Copilot, Google Bard, and Adobe Photoshop Generative Fill, among other new applications. ChatGPT reached a hundred million users in just two months after its release, faster than any other application before. This marked the advent of the generative AI era. Generative AI models generate text, images, music, videos, and even 3D models in ways previously thought impossible for machines. The book explains in an understandable manner how these AI models work. The book provides examples of how AI increases productivity, which professions are changing or disappearing, and how job markets will evolve in the coming years. With this book, you'll learn to recognize the opportunities and risks AI offers. Understand what this change demands from individuals and companies and what strategic skills are required. The book also covers legal questions caused by generative AI, like copyrights, data protection, and AI regulation. It also ponders societal impacts. AI produces content, thus influencing language, culture, and even worldviews. Therefore, it's crucial to understand by whom and how AI is trained. The AI revolution started by ChatGPT is just the beginning. This handbook is for you if you want to keep up with the rapid development of AI.",,"{'url': 'https://ieeexplore-ieee-org.crai.referencistas.com/document/10614277', 'isbn': '9788770042314', 'publisher': 'River Publishers', 'issn': '', 'doi': '', 'keywords': '', 'abstract': '""The AI Revolution"" is a practical guide to using new AI tools, such as ChatGPT, DALLE and Midjourney. Learn how to multiply your productivity by guiding or prompting AI in various ways. The book also introduces Microsoft Copilot, Google Bard, and Adobe Photoshop Generative Fill, among other new applications. ChatGPT reached a hundred million users in just two months after its release, faster than any other application before. This marked the advent of the generative AI era. Generative AI models generate text, images, music, videos, and even 3D models in ways previously thought impossible for machines. The book explains in an understandable manner how these AI models work. The book provides examples of how AI increases productivity, which professions are changing or disappearing, and how job markets will evolve in the coming years. With this book, you\'ll learn to recognize the opportunities and risks AI offers. Understand what this change demands from individuals and companies and what strategic skills are required. The book also covers legal questions caused by generative AI, like copyrights, data protection, and AI regulation. It also ponders societal impacts. AI produces content, thus influencing language, culture, and even worldviews. Therefore, it\'s crucial to understand by whom and how AI is trained. The AI revolution started by ChatGPT is just the beginning. This handbook is for you if you want to keep up with the rapid development of AI.', 'pages': '125-134', 'number': '', 'volume': '', 'year': '2024', 'title': '8 Will AI Define our Culture, Language, and Worldview?', 'booktitle': 'AI Revolution: Mastering AI for Personal and Organizational Growth', 'author': 'Ojanperä, Tero', 'ENTRYTYPE': 'inbook', 'ID': '10614277'}"
11152548,Empowering Preservice Teachers Through Textbook Design Activities With GAI-Based Chatbot,"Luo, Jiutong and Zhu, Chunying and Hu, Lixin and Sun, Meng",Luo,10.1109/TLT.2025.3606757,2025,IEEE Transactions on Learning Technologies,"Generative artificial intelligence (GAI) has become an epoch-making technology in the educational context. With a quasi-experimental repeated measure design and mixed-method data collection, this study examined the effects of the GAI-based chatbot in assisting preservice teachers in implementing the new national curriculum standards in Mainland China and their perceptions accordingly. A sample of 26 preservice teachers (divided into 13 teams) was included in this two-phrase study. Results showed that textbook design activities with the chatbot effectively promoted participants’ acquisition of content knowledge and improved self-efficacy, although it did not reduce teaching anxiety. Evidence was also extracted from participants’ open-ended responses with an extended COSTEM (i.e., content, others, self, tasks, ethics, and model) framework. Meanwhile, preservice teachers perceived both advantages and disadvantages regarding the utility of the GAI-based chatbot in learning. Implications of this study were also discussed.",Education;Standards;Chatbots;Anxiety disorders;Artificial intelligence;Training;Engineering profession;Electronic mail;Technological innovation;Hands;Advantages;disadvantages;generative artificial intelligence (GAI);preservice teachers;textbook design,"{'month': '', 'issn': '1939-1382', 'doi': '10.1109/TLT.2025.3606757', 'keywords': 'Education;Standards;Chatbots;Anxiety disorders;Artificial intelligence;Training;Engineering profession;Electronic mail;Technological innovation;Hands;Advantages;disadvantages;generative artificial intelligence (GAI);preservice teachers;textbook design', 'abstract': 'Generative artificial intelligence (GAI) has become an epoch-making technology in the educational context. With a quasi-experimental repeated measure design and mixed-method data collection, this study examined the effects of the GAI-based chatbot in assisting preservice teachers in implementing the new national curriculum standards in Mainland China and their perceptions accordingly. A sample of 26 preservice teachers (divided into 13 teams) was included in this two-phrase study. Results showed that textbook design activities with the chatbot effectively promoted participants’ acquisition of content knowledge and improved self-efficacy, although it did not reduce teaching anxiety. Evidence was also extracted from participants’ open-ended responses with an extended COSTEM (i.e., content, others, self, tasks, ethics, and model) framework. Meanwhile, preservice teachers perceived both advantages and disadvantages regarding the utility of the GAI-based chatbot in learning. Implications of this study were also discussed.', 'pages': '822-832', 'number': '', 'volume': '18', 'year': '2025', 'title': 'Empowering Preservice Teachers Through Textbook Design Activities With GAI-Based Chatbot', 'journal': 'IEEE Transactions on Learning Technologies', 'author': 'Luo, Jiutong and Zhu, Chunying and Hu, Lixin and Sun, Meng', 'ENTRYTYPE': 'article', 'ID': '11152548'}"
8389206,Instance Map Based Image Synthesis With a Denoising Generative Adversarial Network,"Zheng, Ziqiang and Wang, Chao and Yu, Zhibin and Zheng, Haiyong and Zheng, Bing",Zheng,10.1109/ACCESS.2018.2849108,2018,IEEE Access,"Semantic layout-based image synthesizing, which has benefited from the success of generative adversarial networks (GANs), has received a substantial amount of attention recently. How to enhance the synthesis image equality while maintaining the stochasticity of the GAN remains a challenge. We propose a novel denoising framework to handle this problem. The generation of overlapping objects is another challenging task when synthesizing images from a semantic layout to a realistic RGB photograph. To overcome this deficiency, we include a one-hot semantic label map to force the generator to pay more attention to the generation of overlapping objects. Furthermore, we improve the loss function of the discriminator by considering the perturbed loss and cascade layer loss to guide the generation process. We applied our methods to the Cityscapes, photo-sketch, day-night, facades, and NYU datasets to demonstrate the image generation ability of our model.",Gallium nitride;Semantics;Generators;Task analysis;Convolution;Layout;Neural networks;Underwater technology;artificial neural networks;image processing,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2018.2849108', 'keywords': 'Gallium nitride;Semantics;Generators;Task analysis;Convolution;Layout;Neural networks;Underwater technology;artificial neural networks;image processing', 'abstract': 'Semantic layout-based image synthesizing, which has benefited from the success of generative adversarial networks (GANs), has received a substantial amount of attention recently. How to enhance the synthesis image equality while maintaining the stochasticity of the GAN remains a challenge. We propose a novel denoising framework to handle this problem. The generation of overlapping objects is another challenging task when synthesizing images from a semantic layout to a realistic RGB photograph. To overcome this deficiency, we include a one-hot semantic label map to force the generator to pay more attention to the generation of overlapping objects. Furthermore, we improve the loss function of the discriminator by considering the perturbed loss and cascade layer loss to guide the generation process. We applied our methods to the Cityscapes, photo-sketch, day-night, facades, and NYU datasets to demonstrate the image generation ability of our model.', 'pages': '33654-33665', 'number': '', 'volume': '6', 'year': '2018', 'title': 'Instance Map Based Image Synthesis With a Denoising Generative Adversarial Network', 'journal': 'IEEE Access', 'author': 'Zheng, Ziqiang and Wang, Chao and Yu, Zhibin and Zheng, Haiyong and Zheng, Bing', 'ENTRYTYPE': 'article', 'ID': '8389206'}"
10849478,Power of Suggestion: Strategic Feature Manipulation in Transformer-Based Models,"Benamara, Issam and Viennet, Emmanuel",Benamara,10.1109/ICTAI62512.2024.00033,2024,2024 IEEE 36th International Conference on Tools with Artificial Intelligence (ICTAI),"This paper presents ASGARD, a novel generative framework for personalized advertising strategy design. ASGARD integrates user-preferences while maintaining the ability to autonomously generate high-quality outputs. The core challenge is balancing user preferences with the model's knowledge, enabling it to align with user inputs or propose optimal alternatives. Our novel approach uses a token-driven method for strategy generation, adapts a token masking strategy for training, and refines the loss function to prevent issues like mode collapse. We evaluate ASGARD to demonstrate its effectiveness, identify limitations, and suggest future enhancements. To our knowledge, this is the first approach that meets all constraints of advertising strategy design while allowing user-preference integration, showing potential for generalization across transformer-based generative models.",Training;Degradation;Adaptation models;Transformers;User preference;Advertising;Artificial intelligence;Generative Model;Transformers;Preference Injection;Adaptive Control Mechanism;AI Assistant;Digital Advertising,"{'month': 'Oct', 'issn': '2375-0197', 'doi': '10.1109/ICTAI62512.2024.00033', 'keywords': 'Training;Degradation;Adaptation models;Transformers;User preference;Advertising;Artificial intelligence;Generative Model;Transformers;Preference Injection;Adaptive Control Mechanism;AI Assistant;Digital Advertising', 'abstract': ""This paper presents ASGARD, a novel generative framework for personalized advertising strategy design. ASGARD integrates user-preferences while maintaining the ability to autonomously generate high-quality outputs. The core challenge is balancing user preferences with the model's knowledge, enabling it to align with user inputs or propose optimal alternatives. Our novel approach uses a token-driven method for strategy generation, adapts a token masking strategy for training, and refines the loss function to prevent issues like mode collapse. We evaluate ASGARD to demonstrate its effectiveness, identify limitations, and suggest future enhancements. To our knowledge, this is the first approach that meets all constraints of advertising strategy design while allowing user-preference integration, showing potential for generalization across transformer-based generative models."", 'pages': '174-180', 'number': '', 'volume': '', 'year': '2024', 'title': 'Power of Suggestion: Strategic Feature Manipulation in Transformer-Based Models', 'booktitle': '2024 IEEE 36th International Conference on Tools with Artificial Intelligence (ICTAI)', 'author': 'Benamara, Issam and Viennet, Emmanuel', 'ENTRYTYPE': 'inproceedings', 'ID': '10849478'}"
10438453,A Revolution of Personalized Healthcare: Enabling Human Digital Twin With Mobile AIGC,"Chen, Jiayuan and Yi, Changyan and Du, Hongyang and Niyato, Dusit and Kang, Jiawen and Cai, Jun and Shen, Xuemin",Chen,10.1109/MNET.2024.3366560,2024,IEEE Network,"Mobile artificial intelligence-generated content (AIGC) refers to the adoption of generative artificial intelligence (GAI) algorithms deployed at mobile edge networks to automate the information creation process while fulfilling the requirements of end users. Mobile AIGC has recently attracted phenomenal attentions and can be a key enabling technology for an emerging application, called human digital twin (HDT). HDT empowered by the mobile AIGC is expected to revolutionize the personalized healthcare by generating rare disease data, modeling highfidelity digital twin, building versatile testbeds, and providing 24/7 customized medical services. To promote the development of this new breed of paradigm, in this article, we propose a system architecture of mobile AIGC-driven HDT and highlight the corresponding design requirements and challenges. Moreover, we illustrate two use cases, i.e., mobile AIGC-driven HDT in customized surgery planning and personalized medication. In addition, we conduct an experimental study to prove the effectiveness of the proposed mobile AIGC-driven HDT solution, which shows a particular application in a virtual physical therapy teaching platform. Finally, we conclude this article by briefly discussing several open issues and future directions.",Medical services;Data models;Solid modeling;Digital twins;Data collection;Distributed databases;Mobile communication;Artificial intelligence;Generative AI;Content management;Multi-access edge computing,"{'month': 'Nov', 'issn': '1558-156X', 'doi': '10.1109/MNET.2024.3366560', 'keywords': 'Medical services;Data models;Solid modeling;Digital twins;Data collection;Distributed databases;Mobile communication;Artificial intelligence;Generative AI;Content management;Multi-access edge computing', 'abstract': 'Mobile artificial intelligence-generated content (AIGC) refers to the adoption of generative artificial intelligence (GAI) algorithms deployed at mobile edge networks to automate the information creation process while fulfilling the requirements of end users. Mobile AIGC has recently attracted phenomenal attentions and can be a key enabling technology for an emerging application, called human digital twin (HDT). HDT empowered by the mobile AIGC is expected to revolutionize the personalized healthcare by generating rare disease data, modeling highfidelity digital twin, building versatile testbeds, and providing 24/7 customized medical services. To promote the development of this new breed of paradigm, in this article, we propose a system architecture of mobile AIGC-driven HDT and highlight the corresponding design requirements and challenges. Moreover, we illustrate two use cases, i.e., mobile AIGC-driven HDT in customized surgery planning and personalized medication. In addition, we conduct an experimental study to prove the effectiveness of the proposed mobile AIGC-driven HDT solution, which shows a particular application in a virtual physical therapy teaching platform. Finally, we conclude this article by briefly discussing several open issues and future directions.', 'pages': '234-242', 'number': '6', 'volume': '38', 'year': '2024', 'title': 'A Revolution of Personalized Healthcare: Enabling Human Digital Twin With Mobile AIGC', 'journal': 'IEEE Network', 'author': 'Chen, Jiayuan and Yi, Changyan and Du, Hongyang and Niyato, Dusit and Kang, Jiawen and Cai, Jun and Shen, Xuemin', 'ENTRYTYPE': 'article', 'ID': '10438453'}"
9684443,Cross-Spectrum Thermal Face Pattern Generator,"Cao, Xingdong and Lai, Kenneth and Hsu, Gee-Sern Jison and Smith, Michael and Yanushkevich, Svetlana N.",Cao,10.1109/ACCESS.2022.3144308,2022,IEEE Access,"Conversion of a visible face image into a thermal face image (V2T), or one thermal face image into another one given a different target temperature (T2T), is required in applications such as thermography, human body thermal pattern analysis, and surveillance using cross-spectral imaging. In this work, we propose to use conditional generative adversarial networks (cGAN) with cGAN loss, perceptual loss, and temperature loss to solve the conversion tasks. In our experiment, we used Carl and SpeakingFaces Databases. Frèchet Inception Distance (FID) is used to evaluate the generated images. As well, face recognition was applied to assess the performance of our models. For the V2T task, the FID of the generated thermal images reached a low value of 57.3. For the T2T task, we achieved a rank-1 face recognition rate of 91.0\% which indicates that the generated thermal images preserve the majority of the identity information.",Face recognition;Task analysis;Databases;Generators;Training;Temperature measurement;Generative adversarial networks;Generative adversarial networks;image-to-image translation;thermal pattern generation;face recognition;biometrics,"{'month': '', 'issn': '2169-3536', 'doi': '10.1109/ACCESS.2022.3144308', 'keywords': 'Face recognition;Task analysis;Databases;Generators;Training;Temperature measurement;Generative adversarial networks;Generative adversarial networks;image-to-image translation;thermal pattern generation;face recognition;biometrics', 'abstract': 'Conversion of a visible face image into a thermal face image (V2T), or one thermal face image into another one given a different target temperature (T2T), is required in applications such as thermography, human body thermal pattern analysis, and surveillance using cross-spectral imaging. In this work, we propose to use conditional generative adversarial networks (cGAN) with cGAN loss, perceptual loss, and temperature loss to solve the conversion tasks. In our experiment, we used Carl and SpeakingFaces Databases. Frèchet Inception Distance (FID) is used to evaluate the generated images. As well, face recognition was applied to assess the performance of our models. For the V2T task, the FID of the generated thermal images reached a low value of 57.3. For the T2T task, we achieved a rank-1 face recognition rate of 91.0\\% which indicates that the generated thermal images preserve the majority of the identity information.', 'pages': '9576-9586', 'number': '', 'volume': '10', 'year': '2022', 'title': 'Cross-Spectrum Thermal Face Pattern Generator', 'journal': 'IEEE Access', 'author': 'Cao, Xingdong and Lai, Kenneth and Hsu, Gee-Sern Jison and Smith, Michael and Yanushkevich, Svetlana N.', 'ENTRYTYPE': 'article', 'ID': '9684443'}"
11084045,Multi-Scale Attentional Networks with Adaptive Normalization for Real-to-Cartoon Facial Image Translation,"Deng, Xiaofei",Deng,10.1109/ICETCI64844.2025.11084045,2025,"2025 IEEE 5th International Conference on Electronic Technology, Communication and Information (ICETCI)","Facial cartoonization, crucial for applications in virtual reality, multimedia entertainment, and online communication, faces challenges in maintaining semantic coherence and high-fidelity outputs under hardware constraints. Traditional Cycle-GAN-based methods often suffer from mode collapse and discriminators’ overemphasis on local features, leading to artifacts and semantic fragmentation. This paper proposes an enhanced framework, U-GAT-IT (Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization), integrating multi-scale attention mechanisms and adaptive normalization (AdaLIN) to address these limitations. The attention module dynamically prioritizes critical facial features across hierarchical levels, balancing local details and global structures. AdaLIN adaptively fuses Instance and Layer Normalization, enabling content-aware stylization intensity adjustment. Experiments on a dataset of 10,000 Asian faces and 10,000 anime images demonstrate U-GAT-IT’s superiority: it achieves a 25\% improvement in eye transformation accuracy (SSIM) and 30\% faster convergence compared to Cycle-GAN. Results show realistic cartoon avatars with preserved geometric consistency, complete background removal, and reduced noise artifacts. Additionally, U-GAT-IT exhibits robustness in cross-domain tasks, such as medical imaging. Future directions include 3D expression synchronization, lightweight deployment, and multi-task collaboration with speech-driven modules. This work establishes a novel paradigm for GANs in complex cross-domain image translation, offering theoretical and practical advancements.",Adaptive systems;Attention mechanisms;Translation;Image color analysis;Shape;Semantics;Robustness;Generators;Facial features;Biomedical imaging;Generative Adversarial Network;Cycle-GAN;image transformation;Attention mechanism,"{'month': 'May', 'issn': '', 'doi': '10.1109/ICETCI64844.2025.11084045', 'keywords': 'Adaptive systems;Attention mechanisms;Translation;Image color analysis;Shape;Semantics;Robustness;Generators;Facial features;Biomedical imaging;Generative Adversarial Network;Cycle-GAN;image transformation;Attention mechanism', 'abstract': 'Facial cartoonization, crucial for applications in virtual reality, multimedia entertainment, and online communication, faces challenges in maintaining semantic coherence and high-fidelity outputs under hardware constraints. Traditional Cycle-GAN-based methods often suffer from mode collapse and discriminators’ overemphasis on local features, leading to artifacts and semantic fragmentation. This paper proposes an enhanced framework, U-GAT-IT (Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization), integrating multi-scale attention mechanisms and adaptive normalization (AdaLIN) to address these limitations. The attention module dynamically prioritizes critical facial features across hierarchical levels, balancing local details and global structures. AdaLIN adaptively fuses Instance and Layer Normalization, enabling content-aware stylization intensity adjustment. Experiments on a dataset of 10,000 Asian faces and 10,000 anime images demonstrate U-GAT-IT’s superiority: it achieves a 25\\% improvement in eye transformation accuracy (SSIM) and 30\\% faster convergence compared to Cycle-GAN. Results show realistic cartoon avatars with preserved geometric consistency, complete background removal, and reduced noise artifacts. Additionally, U-GAT-IT exhibits robustness in cross-domain tasks, such as medical imaging. Future directions include 3D expression synchronization, lightweight deployment, and multi-task collaboration with speech-driven modules. This work establishes a novel paradigm for GANs in complex cross-domain image translation, offering theoretical and practical advancements.', 'pages': '389-393', 'number': '', 'volume': '', 'year': '2025', 'title': 'Multi-Scale Attentional Networks with Adaptive Normalization for Real-to-Cartoon Facial Image Translation', 'booktitle': '2025 IEEE 5th International Conference on Electronic Technology, Communication and Information (ICETCI)', 'author': 'Deng, Xiaofei', 'ENTRYTYPE': 'inproceedings', 'ID': '11084045'}"
10768582,Fault Diagnosis of Wind Turbines Based on TimeGAN-Stacking,"Chu, Jieping and Wu, Junjing and Yuan, Nana and Lu, Tingyi and Gu, Lejun and He, Xing",Chu,10.1109/ICPRE62586.2024.10768582,2024,2024 The 9th International Conference on Power and Renewable Energy (ICPRE),"The lack of sufficient fault samples to support the training of fault diagnosis models in wind turbines often leads to monitoring systems missing or misreporting faults. In response to the above issues, this article proposes a fault diagnosis method for wind turbine pitch system based on TimeGAN-Stacking. At the data level, due to the imbalance of the original sample categories, a Time Generative Adversarial Network (TimeGAN) is used to track the dynamic changes in the probability distribution of wind turbine operation data step by step, while optimizing the global and local distribution of the generated samples, effectively balancing and expanding the comprehensive sample set of wind turbine faults; At the model level, establish a Stacking integrated model that integrates the advantages of multiple fault diagnosis devices to further improve fault diagnosis capabilities. Finally, the proposed method was tested based on actual wind field data, and the results showed that the TimeGAN-Stacking fault recognition method can effectively diagnose wind turbine faults and has good recognition performance.",Fault diagnosis;Training;Performance evaluation;Renewable energy sources;Stacking;Wind farms;Feature extraction;Data models;Probability distribution;Wind turbines;timeseries generative adversarial network;sample enhancement;data driven,"{'month': 'Sep.', 'issn': '2768-0525', 'doi': '10.1109/ICPRE62586.2024.10768582', 'keywords': 'Fault diagnosis;Training;Performance evaluation;Renewable energy sources;Stacking;Wind farms;Feature extraction;Data models;Probability distribution;Wind turbines;timeseries generative adversarial network;sample enhancement;data driven', 'abstract': 'The lack of sufficient fault samples to support the training of fault diagnosis models in wind turbines often leads to monitoring systems missing or misreporting faults. In response to the above issues, this article proposes a fault diagnosis method for wind turbine pitch system based on TimeGAN-Stacking. At the data level, due to the imbalance of the original sample categories, a Time Generative Adversarial Network (TimeGAN) is used to track the dynamic changes in the probability distribution of wind turbine operation data step by step, while optimizing the global and local distribution of the generated samples, effectively balancing and expanding the comprehensive sample set of wind turbine faults; At the model level, establish a Stacking integrated model that integrates the advantages of multiple fault diagnosis devices to further improve fault diagnosis capabilities. Finally, the proposed method was tested based on actual wind field data, and the results showed that the TimeGAN-Stacking fault recognition method can effectively diagnose wind turbine faults and has good recognition performance.', 'pages': '735-739', 'number': '', 'volume': '', 'year': '2024', 'title': 'Fault Diagnosis of Wind Turbines Based on TimeGAN-Stacking', 'booktitle': '2024 The 9th International Conference on Power and Renewable Energy (ICPRE)', 'author': 'Chu, Jieping and Wu, Junjing and Yuan, Nana and Lu, Tingyi and Gu, Lejun and He, Xing', 'ENTRYTYPE': 'inproceedings', 'ID': '10768582'}"
10688255,DNAF: Diffusion with Noise-Aware Feature for Pose-Guided Person Image Synthesis,"Guo, Liyan and Song, Kaiyu and Xu, Mengying and Lai, Hanjiang",Guo,10.1109/ICME57554.2024.10688255,2024,2024 IEEE International Conference on Multimedia and Expo (ICME),"Pose-guided person image synthesis aims at generating images based on the related pose skeleton and the appearance of a source image. As a popular generative model, the diffusion model shows its potential. However, there are two gaps to hinder the fusion between pose information and appearance: 1) Directly injecting pixel-level pose information into semantic features leads to the representation gap. 2) The timestep-dependent nature of the diffusion model introduces the noise-induced gap. To alleviate these, we propose Diffusion with Noise-Aware Feature(DNAF). Concretely, we leverage the T2I-Adapter-based pose adapter to achieve the mapping from the pixel level to the feature level. Then, we propose a lightweight trainable layer to infuse the multi-scale constant feature adaptively. In the end, we construct noise-aware features to more effectively guide the diffusion process. Experimental results show that DNAF achieves competitive results on DeepFashion and Market-1501 datasets.",Image synthesis;Semantics;Diffusion processes;Diffusion models;Skeleton;pose transfer;diffusion model;person image synthesis;conditional image generation;generative model,"{'month': 'July', 'issn': '1945-788X', 'doi': '10.1109/ICME57554.2024.10688255', 'keywords': 'Image synthesis;Semantics;Diffusion processes;Diffusion models;Skeleton;pose transfer;diffusion model;person image synthesis;conditional image generation;generative model', 'abstract': 'Pose-guided person image synthesis aims at generating images based on the related pose skeleton and the appearance of a source image. As a popular generative model, the diffusion model shows its potential. However, there are two gaps to hinder the fusion between pose information and appearance: 1) Directly injecting pixel-level pose information into semantic features leads to the representation gap. 2) The timestep-dependent nature of the diffusion model introduces the noise-induced gap. To alleviate these, we propose Diffusion with Noise-Aware Feature(DNAF). Concretely, we leverage the T2I-Adapter-based pose adapter to achieve the mapping from the pixel level to the feature level. Then, we propose a lightweight trainable layer to infuse the multi-scale constant feature adaptively. In the end, we construct noise-aware features to more effectively guide the diffusion process. Experimental results show that DNAF achieves competitive results on DeepFashion and Market-1501 datasets.', 'pages': '1-6', 'number': '', 'volume': '', 'year': '2024', 'title': 'DNAF: Diffusion with Noise-Aware Feature for Pose-Guided Person Image Synthesis', 'booktitle': '2024 IEEE International Conference on Multimedia and Expo (ICME)', 'author': 'Guo, Liyan and Song, Kaiyu and Xu, Mengying and Lai, Hanjiang', 'ENTRYTYPE': 'inproceedings', 'ID': '10688255'}"
10444916,A Dense Multicross Self-Attention and Adaptive Gated Perceptual Unit Method for Few-Shot Semantic Segmentation,"Xiao, Feng and Liu, Ruyu and Zhu, Yunrui and Zhang, Haoyu and Zhang, Jianhua and Chen, Shengyong",Xiao,10.1109/TAI.2024.3369553,2024,IEEE Transactions on Artificial Intelligence,"Few-shot semantic segmentation (FSSS) is a pivotal and prevalent research task for advancing the field of artificial intelligence. The task entails learning to differentiate between various classes in a support set and leveraging this knowledge on samples within a query set. However, traditional deep learning methods tend to underperform in this context due to limited training samples and subtle correlations between query and support images that are inadequately utilized. Existing methods for FSSS often compress support information into prototype categories or utilize only partial pixel-level support information, resulting in a significant impact. In this article, we propose a novel auto FSSS method that employs dense multicross self-attention and adaptive gate perception units to tackle this challenge. Specifically, our proposed method treats each query pixel as a label and predicts its segmentation label as the sum of labels of all support pixels. The method fully utilizes foreground and background support information through multilevel pixel correlations between paired query and support features to achieve state-of-the-art performance with only 1–5 annotated images. Moreover, our proposed adaptive gating perception unit filters and weighs each support image information by adaptively learning the gating values. This ensures the model selects only the most relevant support image information to the current query image. The proposed method is evaluated on several popular FSSS datasets and compared with state-of-the-art methods. Additionally, a visual analysis of our method is conducted to demonstrate its ability to distinguish different semantic categories and exhibit robustness at segmentation boundaries.",Semantic segmentation;Transformers;Task analysis;Semantics;Visualization;Artificial intelligence;Adaptation models;Auto machine learning;computer vision;few-shot semantic segmentation (FSSS);vision transformer,"{'month': 'June', 'issn': '2691-4581', 'doi': '10.1109/TAI.2024.3369553', 'keywords': 'Semantic segmentation;Transformers;Task analysis;Semantics;Visualization;Artificial intelligence;Adaptation models;Auto machine learning;computer vision;few-shot semantic segmentation (FSSS);vision transformer', 'abstract': 'Few-shot semantic segmentation (FSSS) is a pivotal and prevalent research task for advancing the field of artificial intelligence. The task entails learning to differentiate between various classes in a support set and leveraging this knowledge on samples within a query set. However, traditional deep learning methods tend to underperform in this context due to limited training samples and subtle correlations between query and support images that are inadequately utilized. Existing methods for FSSS often compress support information into prototype categories or utilize only partial pixel-level support information, resulting in a significant impact. In this article, we propose a novel auto FSSS method that employs dense multicross self-attention and adaptive gate perception units to tackle this challenge. Specifically, our proposed method treats each query pixel as a label and predicts its segmentation label as the sum of labels of all support pixels. The method fully utilizes foreground and background support information through multilevel pixel correlations between paired query and support features to achieve state-of-the-art performance with only 1–5 annotated images. Moreover, our proposed adaptive gating perception unit filters and weighs each support image information by adaptively learning the gating values. This ensures the model selects only the most relevant support image information to the current query image. The proposed method is evaluated on several popular FSSS datasets and compared with state-of-the-art methods. Additionally, a visual analysis of our method is conducted to demonstrate its ability to distinguish different semantic categories and exhibit robustness at segmentation boundaries.', 'pages': '2493-2504', 'number': '6', 'volume': '5', 'year': '2024', 'title': 'A Dense Multicross Self-Attention and Adaptive Gated Perceptual Unit Method for Few-Shot Semantic Segmentation', 'journal': 'IEEE Transactions on Artificial Intelligence', 'author': 'Xiao, Feng and Liu, Ruyu and Zhu, Yunrui and Zhang, Haoyu and Zhang, Jianhua and Chen, Shengyong', 'ENTRYTYPE': 'article', 'ID': '10444916'}"
10645262,Enhancing Deepfake Detection using SE Block Attention with CNN,"Dasgupta, Subhram and Mason, Janelle and Yuan, Xiaohong and Odeyomi, Olusola and Roy, Kaushik",Dasgupta,10.1109/icABCD62167.2024.10645262,2024,"2024 International Conference on Artificial Intelligence, Big Data, Computing and Data Communication Systems (icABCD)","In the digital age, Deepfake present a formidable challenge by using advanced artificial intelligence to create highly convincing manipulated content, undermining information authenticity and security. These sophisticated fabrications surpass traditional detection methods in complexity and realism. To address this issue, we aim to harness cutting-edge deep learning methodologies to engineer an innovative deepfake detection model. However, most of the models designed for deepfake detection are large, causing heavy storage and mem-ory consumption. In this research, we propose a lightweight convolution neural network (CNN) with squeeze and excitation block attention (SE) for Deepfake detection. The SE block module is designed to perform dynamic channel-wise feature recalibration. The SE block allows the network to emphasize informative features and suppress less useful ones, which leads to a more efficient and effective learning module. This module is integrated with a simple sequential model to perform Deepfake detection. The model is smaller in size and it achieves competing accuracy with the existing models for deepfake detection tasks. The model achieved an overall classification accuracy of 94.14\% and AVC-ROC score of 0.985 on the Style GAN dataset from the Diverse Fake Face Dataset. Our proposed approach presents a promising avenue for combating the Deepfake challenge with minimal computational resources, developing efficient and scalable solutions for digital content verification.",Deep learning;Deepfakes;Accuracy;Convolution;Computational modeling;Predictive models;Artificial intelligence;SE Block;CNN;Deepfake Detection;Entire Face Synthesis,"{'month': 'Aug', 'issn': '', 'doi': '10.1109/icABCD62167.2024.10645262', 'keywords': 'Deep learning;Deepfakes;Accuracy;Convolution;Computational modeling;Predictive models;Artificial intelligence;SE Block;CNN;Deepfake Detection;Entire Face Synthesis', 'abstract': 'In the digital age, Deepfake present a formidable challenge by using advanced artificial intelligence to create highly convincing manipulated content, undermining information authenticity and security. These sophisticated fabrications surpass traditional detection methods in complexity and realism. To address this issue, we aim to harness cutting-edge deep learning methodologies to engineer an innovative deepfake detection model. However, most of the models designed for deepfake detection are large, causing heavy storage and mem-ory consumption. In this research, we propose a lightweight convolution neural network (CNN) with squeeze and excitation block attention (SE) for Deepfake detection. The SE block module is designed to perform dynamic channel-wise feature recalibration. The SE block allows the network to emphasize informative features and suppress less useful ones, which leads to a more efficient and effective learning module. This module is integrated with a simple sequential model to perform Deepfake detection. The model is smaller in size and it achieves competing accuracy with the existing models for deepfake detection tasks. The model achieved an overall classification accuracy of 94.14\\% and AVC-ROC score of 0.985 on the Style GAN dataset from the Diverse Fake Face Dataset. Our proposed approach presents a promising avenue for combating the Deepfake challenge with minimal computational resources, developing efficient and scalable solutions for digital content verification.', 'pages': '1-6', 'number': '', 'volume': '', 'year': '2024', 'title': 'Enhancing Deepfake Detection using SE Block Attention with CNN', 'booktitle': '2024 International Conference on Artificial Intelligence, Big Data, Computing and Data Communication Systems (icABCD)', 'author': 'Dasgupta, Subhram and Mason, Janelle and Yuan, Xiaohong and Odeyomi, Olusola and Roy, Kaushik', 'ENTRYTYPE': 'inproceedings', 'ID': '10645262'}"
10271595,Use of ChatGPT as Configuration Support Tool and Network Analysis,"Marques, Stella Azevedo and Rodriguez, Demostenes Zegarra and Rosa, Renata Lopes",Marques,10.23919/SoftCOM58365.2023.10271595,2023,"2023 International Conference on Software, Telecommunications and Computer Networks (SoftCOM)","This article presents the configuration of development environments intended for the analysis of computer networks, ranging from the configuration of virtual machines to the analysis of packet traffic on the network using the Generative Pre-Trained Transformer (GPT). This article investigates the application of the ChatGPT tool, a variant based on the GPT-3.5 architecture developed by OpenAI, which employs a natural language model. The tool is employed as a facilitator in the configuration of network simulation scenarios. The study shows ChatGPT as an intelligent automatic assistant, being a great ally mainly in studies, and as a configuration assistant in the area of Computer Networks. In addition, it is also useful for beginners in this field, as it thoroughly describes the procedures needed to set up a development and network analysis environment. The work also presents the existing limitations inherent to this technology, such as the need for internet access and the possibility of inaccurate or incomplete answers. Making it clear that the use of artificial intelligence is a great ally, but until the present study there is no remote chance of replacing human expertise.",Analytical models;Computational modeling;Natural languages;Network analyzers;Chatbots;Transformers;Computer networks;Generative Pre-Trained Transformer;intelligent automatic assistant;ChatGPT;computer networks,"{'month': 'Sep.', 'issn': '1847-358X', 'doi': '10.23919/SoftCOM58365.2023.10271595', 'keywords': 'Analytical models;Computational modeling;Natural languages;Network analyzers;Chatbots;Transformers;Computer networks;Generative Pre-Trained Transformer;intelligent automatic assistant;ChatGPT;computer networks', 'abstract': 'This article presents the configuration of development environments intended for the analysis of computer networks, ranging from the configuration of virtual machines to the analysis of packet traffic on the network using the Generative Pre-Trained Transformer (GPT). This article investigates the application of the ChatGPT tool, a variant based on the GPT-3.5 architecture developed by OpenAI, which employs a natural language model. The tool is employed as a facilitator in the configuration of network simulation scenarios. The study shows ChatGPT as an intelligent automatic assistant, being a great ally mainly in studies, and as a configuration assistant in the area of Computer Networks. In addition, it is also useful for beginners in this field, as it thoroughly describes the procedures needed to set up a development and network analysis environment. The work also presents the existing limitations inherent to this technology, such as the need for internet access and the possibility of inaccurate or incomplete answers. Making it clear that the use of artificial intelligence is a great ally, but until the present study there is no remote chance of replacing human expertise.', 'pages': '1-6', 'number': '', 'volume': '', 'year': '2023', 'title': 'Use of ChatGPT as Configuration Support Tool and Network Analysis', 'booktitle': '2023 International Conference on Software, Telecommunications and Computer Networks (SoftCOM)', 'author': 'Marques, Stella Azevedo and Rodriguez, Demostenes Zegarra and Rosa, Renata Lopes', 'ENTRYTYPE': 'inproceedings', 'ID': '10271595'}"
10098084,Measuring Machine Learning Robustness in front of Static and Dynamic Adversaries*,"Menéndez, Héctor D.",Menéndez,10.1109/ICTAI56018.2022.00033,2022,2022 IEEE 34th International Conference on Tools with Artificial Intelligence (ICTAI),"Adversarial machine learning brought a new way of understanding the reliability of different learning systems. Knowing that the learning confidence depends significantly on small changes, such as noise, created a mind change in the artificial intelligence community, who started to consider the boundaries and limitations of machine learning methods. However, if we can measure these limitations, we can improve the strength of our machine learning models and their robustness. Following this motivation, this work introduces different measures of robustness for machine learning models based on false negatives. These measures can be evaluated for either static or dynamic scenarios, where an adversary performs intelligent actions to evade the system. To evaluate the metrics I have applied 11 classifiers to different benchmark datasets and created an adversary that performs an evolutionary search process aiming to reduce the classification accuracy. The results show that the most robust models are related to K-Nearest Neighbours, Logistic regression, and neural networks, although none of the systems is robust enough when the target is to reach a single misclassification.",Measurement;Learning systems;Neural networks;Learning (artificial intelligence);Benchmark testing;Robustness;Adversarial machine learning;Machine Learning Testing;Adversarial Machine Learning;Robustness;Blindspots,"{'month': 'Oct', 'issn': '2375-0197', 'doi': '10.1109/ICTAI56018.2022.00033', 'keywords': 'Measurement;Learning systems;Neural networks;Learning (artificial intelligence);Benchmark testing;Robustness;Adversarial machine learning;Machine Learning Testing;Adversarial Machine Learning;Robustness;Blindspots', 'abstract': 'Adversarial machine learning brought a new way of understanding the reliability of different learning systems. Knowing that the learning confidence depends significantly on small changes, such as noise, created a mind change in the artificial intelligence community, who started to consider the boundaries and limitations of machine learning methods. However, if we can measure these limitations, we can improve the strength of our machine learning models and their robustness. Following this motivation, this work introduces different measures of robustness for machine learning models based on false negatives. These measures can be evaluated for either static or dynamic scenarios, where an adversary performs intelligent actions to evade the system. To evaluate the metrics I have applied 11 classifiers to different benchmark datasets and created an adversary that performs an evolutionary search process aiming to reduce the classification accuracy. The results show that the most robust models are related to K-Nearest Neighbours, Logistic regression, and neural networks, although none of the systems is robust enough when the target is to reach a single misclassification.', 'pages': '174-181', 'number': '', 'volume': '', 'year': '2022', 'title': 'Measuring Machine Learning Robustness in front of Static and Dynamic Adversaries*', 'booktitle': '2022 IEEE 34th International Conference on Tools with Artificial Intelligence (ICTAI)', 'author': 'Menéndez, Héctor D.', 'ENTRYTYPE': 'inproceedings', 'ID': '10098084'}"
10848485,Privacy and Fairness in Machine Learning: A Survey,"Shaham, Sina and Hajisafi, Arash and Quan, Minh K. and Nguyen, Dinh C. and Krishnamachari, Bhaskar and Peris, Charith and Ghinita, Gabriel and Shahabi, Cyrus and Pathirana, Pubudu N.",Shaham,10.1109/TAI.2025.3531326,2025,IEEE Transactions on Artificial Intelligence,"Privacy and fairness are two crucial pillars of responsible artificial intelligence (AI) and trustworthy machine learning (ML). Each objective has been independently studied in the literature with the aim of reducing utility loss in achieving them. Despite the significant interest attracted from both academia and industry, there remains an immediate demand for more in-depth research to unravel how these two objectives can be simultaneously integrated into ML models. As opposed to well-accepted trade-offs, i.e., privacy-utility and fairness-utility, the interrelation between privacy and fairness is not well-understood. While some works suggest a trade-off between the two objective functions, there are others that demonstrate the alignment of these functions in certain scenarios. To fill this research gap, we provide a thorough review of privacy and fairness in ML, including supervised, unsupervised, semisupervised, and reinforcement learning. After examining and consolidating the literature on both objectives, we present a holistic survey on the impact of privacy on fairness, the impact of fairness on privacy, existing architectures, their interaction in application domains, and algorithms that aim to achieve both objectives while minimizing the utility sacrificed. Finally, we identify research challenges in achieving concurrently privacy and fairness in ML, particularly focusing on large language models.",Privacy;Data privacy;Surveys;Artificial intelligence;Reviews;Mathematical models;Decision making;Biological system modeling;Terminology;Social networking (online);Fairness;machine learning;privacy;supervised learning;unsupervised learning,"{'month': 'July', 'issn': '2691-4581', 'doi': '10.1109/TAI.2025.3531326', 'keywords': 'Privacy;Data privacy;Surveys;Artificial intelligence;Reviews;Mathematical models;Decision making;Biological system modeling;Terminology;Social networking (online);Fairness;machine learning;privacy;supervised learning;unsupervised learning', 'abstract': 'Privacy and fairness are two crucial pillars of responsible artificial intelligence (AI) and trustworthy machine learning (ML). Each objective has been independently studied in the literature with the aim of reducing utility loss in achieving them. Despite the significant interest attracted from both academia and industry, there remains an immediate demand for more in-depth research to unravel how these two objectives can be simultaneously integrated into ML models. As opposed to well-accepted trade-offs, i.e., privacy-utility and fairness-utility, the interrelation between privacy and fairness is not well-understood. While some works suggest a trade-off between the two objective functions, there are others that demonstrate the alignment of these functions in certain scenarios. To fill this research gap, we provide a thorough review of privacy and fairness in ML, including supervised, unsupervised, semisupervised, and reinforcement learning. After examining and consolidating the literature on both objectives, we present a holistic survey on the impact of privacy on fairness, the impact of fairness on privacy, existing architectures, their interaction in application domains, and algorithms that aim to achieve both objectives while minimizing the utility sacrificed. Finally, we identify research challenges in achieving concurrently privacy and fairness in ML, particularly focusing on large language models.', 'pages': '1706-1726', 'number': '7', 'volume': '6', 'year': '2025', 'title': 'Privacy and Fairness in Machine Learning: A Survey', 'journal': 'IEEE Transactions on Artificial Intelligence', 'author': 'Shaham, Sina and Hajisafi, Arash and Quan, Minh K. and Nguyen, Dinh C. and Krishnamachari, Bhaskar and Peris, Charith and Ghinita, Gabriel and Shahabi, Cyrus and Pathirana, Pubudu N.', 'ENTRYTYPE': 'article', 'ID': '10848485'}"
11072501,Redefining Human-Machine Collaboration: Industry 5.0 to Improve Safety and Efficiency,"Lloret Abrisqueta, Francisco Antonio and Guerrero González, Antonio and Zapata Martinez, Roberto",Lloret Abrisqueta,10.1109/TLA.2025.11072501,2025,IEEE Latin America Transactions,"This study presents an innovative implementation of Industry 5.0 principles in a window production line, integrating advanced robotics and artificial intelligence technologies to improve operational efficiency and worker well-being. A robotic cell was designed to automate the handling of heavy components in the final production stage, resulting in a 35\% reduction in cycle times and a significant decrease in ergonomic risks. Additionally, an interactive voice assistant based on generative AI was implemented, allowing operators to access system data and technical information in real-time through cognitive interaction. The results show a substantial improvement in job satisfaction, with a 278\% increase in the perception of occupational health. This approach not only optimizes productivity but also redefines workers' roles, aligning with the human-centered vision of Industry 5.0. The study demonstrates how the integration of advanced technologies can create safer, more efficient, and adaptable work environments in modern manufacturing.",Production;Robots;Fifth Industrial Revolution;Real-time systems;Service robots;Personal voice assistants;Manufacturing;Collaboration;Accuracy;Unified modeling language;generative AI;advanced robotics;industry 5.0;occupational health,"{'month': 'Aug', 'issn': '1548-0992', 'doi': '10.1109/TLA.2025.11072501', 'keywords': 'Production;Robots;Fifth Industrial Revolution;Real-time systems;Service robots;Personal voice assistants;Manufacturing;Collaboration;Accuracy;Unified modeling language;generative AI;advanced robotics;industry 5.0;occupational health', 'abstract': ""This study presents an innovative implementation of Industry 5.0 principles in a window production line, integrating advanced robotics and artificial intelligence technologies to improve operational efficiency and worker well-being. A robotic cell was designed to automate the handling of heavy components in the final production stage, resulting in a 35\\% reduction in cycle times and a significant decrease in ergonomic risks. Additionally, an interactive voice assistant based on generative AI was implemented, allowing operators to access system data and technical information in real-time through cognitive interaction. The results show a substantial improvement in job satisfaction, with a 278\\% increase in the perception of occupational health. This approach not only optimizes productivity but also redefines workers' roles, aligning with the human-centered vision of Industry 5.0. The study demonstrates how the integration of advanced technologies can create safer, more efficient, and adaptable work environments in modern manufacturing."", 'pages': '729-735', 'number': '8', 'volume': '23', 'year': '2025', 'title': 'Redefining Human-Machine Collaboration: Industry 5.0 to Improve Safety and Efficiency', 'journal': 'IEEE Latin America Transactions', 'author': 'Lloret Abrisqueta, Francisco Antonio and Guerrero González, Antonio and Zapata Martinez, Roberto', 'ENTRYTYPE': 'article', 'ID': '11072501'}"
10730450,Generating and Integrating Diffusion Model-Based Panoramic Views for Virtual Interview Platform,"Si, Jongwook and Yang, Seongeun and Song, Jeyong and Son, Seungjae and Lee, Sangjin and Kim, Daemin and Kim, Sungyoung",Si,10.1109/IICAIET62352.2024.10730450,2024,2024 IEEE International Conference on Artificial Intelligence in Engineering and Technology (IICAIET),"This paper presents a new approach to improve virtual interview platforms in education, which are gaining significant attention. This study aims to simplify the complex manual process of equipment setup to enhance the realism and reliability of virtual interviews. To this end, this study proposes a method for automatically constructing 3D virtual interview environments using diffusion technology in generative AI. In this research, we exploit a diffusion model capable of generating high-quality panoramic images. We generate images of interview rooms capable of delivering immersive interview experiences via refined text prompts. The resulting imagery is then reconstituted 3D VR content utilizing the Unity engine, facilitating enhanced interaction and engagement within virtual environments. This research compares and analyzes various methods presented in related research and proposes a new process for efficiently constructing 360-degree virtual environments. When wearing Oculus Quest 2 and experiencing the virtual environment created using the proposed method, a high sense of immersion was experienced, similar to the actual interview environment.",Three-dimensional displays;Generative AI;Education;Virtual environments;Manuals;Learning (artificial intelligence);Diffusion models;Reliability;Interviews;Engines;Virtual Interview;Diffusion;AI;Panorama;Deep Learning,"{'month': 'Aug', 'issn': '', 'doi': '10.1109/IICAIET62352.2024.10730450', 'keywords': 'Three-dimensional displays;Generative AI;Education;Virtual environments;Manuals;Learning (artificial intelligence);Diffusion models;Reliability;Interviews;Engines;Virtual Interview;Diffusion;AI;Panorama;Deep Learning', 'abstract': 'This paper presents a new approach to improve virtual interview platforms in education, which are gaining significant attention. This study aims to simplify the complex manual process of equipment setup to enhance the realism and reliability of virtual interviews. To this end, this study proposes a method for automatically constructing 3D virtual interview environments using diffusion technology in generative AI. In this research, we exploit a diffusion model capable of generating high-quality panoramic images. We generate images of interview rooms capable of delivering immersive interview experiences via refined text prompts. The resulting imagery is then reconstituted 3D VR content utilizing the Unity engine, facilitating enhanced interaction and engagement within virtual environments. This research compares and analyzes various methods presented in related research and proposes a new process for efficiently constructing 360-degree virtual environments. When wearing Oculus Quest 2 and experiencing the virtual environment created using the proposed method, a high sense of immersion was experienced, similar to the actual interview environment.', 'pages': '343-348', 'number': '', 'volume': '', 'year': '2024', 'title': 'Generating and Integrating Diffusion Model-Based Panoramic Views for Virtual Interview Platform', 'booktitle': '2024 IEEE International Conference on Artificial Intelligence in Engineering and Technology (IICAIET)', 'author': 'Si, Jongwook and Yang, Seongeun and Song, Jeyong and Son, Seungjae and Lee, Sangjin and Kim, Daemin and Kim, Sungyoung', 'ENTRYTYPE': 'inproceedings', 'ID': '10730450'}"
10951654,Deep Supervised Learning on Radiological Images to Classify Bone Fractures,"Malviya, Rishabha and Rajput, Shivam and Vaidya, Makarand",Malviya,10.1002/9781394230914.ch3,2024,Artificial Intelligence for Bone Disorder: Diagnosis and Treatment,"Summary <p>Several researchers in recent years have proposed several ways to classify bone fractures. Despite this, there is no well\&\#x2010;defined system for categorizing the many fractures that might occur in a human body. The use of X\&\#x2010;rays to identify bone fractures is just one such application, yet this can also occur with smaller fissures. This highlights the need to quickly diagnose a patient with a fracture and start treatment. Because of this, it is crucial to design efficient and intelligent systems. Still it is a regular emergency when a fracture is not properly diagnosed. Patients\&\#x2019; access to care and treatment is limited and often delayed as a result. The potential use of deep learning (DL) and other forms of artificial intelligence to aid radiologists in the identification of bone fractures is now receiving a great deal of attention. The analysis of medical images is a promising area for the use of DL. In this chapter, we investigate how deep supervised learning can be used to analyze X\&\#x2010;ray pictures and categorize the many types of bone fractures.</p>",Bones;Deep learning;Artificial intelligence;X-rays;Radiology;Medical diagnostic imaging;Supervised learning;Labeling;Computed tomography;Magnetic resonance imaging,"{'url': 'https://ieeexplore-ieee-org.crai.referencistas.com/document/10951654', 'isbn': '9781394230907', 'publisher': 'Wiley', 'issn': '', 'doi': '10.1002/9781394230914.ch3', 'keywords': 'Bones;Deep learning;Artificial intelligence;X-rays;Radiology;Medical diagnostic imaging;Supervised learning;Labeling;Computed tomography;Magnetic resonance imaging', 'abstract': 'Summary <p>Several researchers in recent years have proposed several ways to classify bone fractures. Despite this, there is no well\\&\\#x2010;defined system for categorizing the many fractures that might occur in a human body. The use of X\\&\\#x2010;rays to identify bone fractures is just one such application, yet this can also occur with smaller fissures. This highlights the need to quickly diagnose a patient with a fracture and start treatment. Because of this, it is crucial to design efficient and intelligent systems. Still it is a regular emergency when a fracture is not properly diagnosed. Patients\\&\\#x2019; access to care and treatment is limited and often delayed as a result. The potential use of deep learning (DL) and other forms of artificial intelligence to aid radiologists in the identification of bone fractures is now receiving a great deal of attention. The analysis of medical images is a promising area for the use of DL. In this chapter, we investigate how deep supervised learning can be used to analyze X\\&\\#x2010;ray pictures and categorize the many types of bone fractures.</p>', 'pages': '59-77', 'number': '', 'volume': '', 'year': '2024', 'title': 'Deep Supervised Learning on Radiological Images to Classify Bone Fractures', 'booktitle': 'Artificial Intelligence for Bone Disorder: Diagnosis and Treatment', 'author': 'Malviya, Rishabha and Rajput, Shivam and Vaidya, Makarand', 'ENTRYTYPE': 'inbook', 'ID': '10951654'}"
10622943,AI on the Defensive and Offensive: Securing Multi-Environment Networks from AI Agents,"Rustam, Furqan and Ranaweera, Pasika and Jurcut, Anca Delia",Rustam,10.1109/ICC51166.2024.10622943,2024,ICC 2024 - IEEE International Conference on Communications,"The role of artificial intelligence (AI) in cybersecu-rity has grown due to increasing threats from malicious actors. It aids in threat detection, behavioral analysis, malware detection, phishing identification, and enhancing security measures. However, AI can also be weaponized for cyberattacks, as malicious actors use AI-based tools for sophisticated and adaptable assaults on security systems. This study contributes to cybersecurity by defending against AI-based threats. Machine learning models were trained on diverse, complex datasets to counter sophisticated AI-based attacks in multi-environments (M-En). We have utilized auto-encoders to generate our M-En dataset by combining two benchmark datasets: UNSW-NB15 and IoTID-20, that represent traditional IP-based and IoT-based traffic, respectively. Three generative models (CTGAN, CopulaGAN, and TVAE) produced AI-based traffic, leading to a dataset comprising traditional and AI-generated traffic. Machine learning and deep learning models were deployed on this M-En dataset. The ensemble Extra Trees classifier achieved the highest accuracy score of 0.983 for binary classification and 0.968 for multiclass problems. Our proposed approach demonstrates its effectiveness in countering AI-based traffic as well as traditional network traffic within the M-En networks.",Training;Accuracy;Weapons;Phishing;Diversity reception;Telecommunication traffic;Threat assessment;Cyber Security;Machine Learning;AI-based Attacks;Complex Networks;Generative Models,"{'month': 'June', 'issn': '1938-1883', 'doi': '10.1109/ICC51166.2024.10622943', 'keywords': 'Training;Accuracy;Weapons;Phishing;Diversity reception;Telecommunication traffic;Threat assessment;Cyber Security;Machine Learning;AI-based Attacks;Complex Networks;Generative Models', 'abstract': 'The role of artificial intelligence (AI) in cybersecu-rity has grown due to increasing threats from malicious actors. It aids in threat detection, behavioral analysis, malware detection, phishing identification, and enhancing security measures. However, AI can also be weaponized for cyberattacks, as malicious actors use AI-based tools for sophisticated and adaptable assaults on security systems. This study contributes to cybersecurity by defending against AI-based threats. Machine learning models were trained on diverse, complex datasets to counter sophisticated AI-based attacks in multi-environments (M-En). We have utilized auto-encoders to generate our M-En dataset by combining two benchmark datasets: UNSW-NB15 and IoTID-20, that represent traditional IP-based and IoT-based traffic, respectively. Three generative models (CTGAN, CopulaGAN, and TVAE) produced AI-based traffic, leading to a dataset comprising traditional and AI-generated traffic. Machine learning and deep learning models were deployed on this M-En dataset. The ensemble Extra Trees classifier achieved the highest accuracy score of 0.983 for binary classification and 0.968 for multiclass problems. Our proposed approach demonstrates its effectiveness in countering AI-based traffic as well as traditional network traffic within the M-En networks.', 'pages': '4287-4292', 'number': '', 'volume': '', 'year': '2024', 'title': 'AI on the Defensive and Offensive: Securing Multi-Environment Networks from AI Agents', 'booktitle': 'ICC 2024 - IEEE International Conference on Communications', 'author': 'Rustam, Furqan and Ranaweera, Pasika and Jurcut, Anca Delia', 'ENTRYTYPE': 'inproceedings', 'ID': '10622943'}"
10813439,Employing a CNN Detector to Identify AI-Generated Images and Against Attacks on AI Systems,"Truong, Phi-Ho and Nguyen, Tien-Dung and Truong, Xuan-Hung and Nguyen, Nhat-Hai and Pham, Duy-Trung",Truong,10.1109/VCRIS63677.2024.10813439,2024,2024 1st International Conference On Cryptography And Information Security (VCRIS),"The advancement of artificial intelligence (AI) technology has made Generative AI a significant concern for many. This technology generates fake images through highly complex algorithms. It involves a detailed analysis of the original image, extracting features like color, context, and feature, which are then used to build a neural network. This network is combined with computer graphics to produce an image that resembles the original. Attackers often use these fake images to attack AI systems, making the detection and prevention of such images a pressing issue. In this study, we propose a method to detect fake images using a detector built on CNN models. Our experimental results demonstrate that the proposed detectors achieve over 95\% average accuracy on the test datasets, indicating their potential applicability to real-world problems.",Training;Accuracy;Image color analysis;Prevention and mitigation;Transfer learning;Neural networks;Information security;Detectors;Pressing;Feature extraction;Generative AI;Detect fake images;Convolutional Neural Networks;Pre-Training model,"{'month': 'Dec', 'issn': '', 'doi': '10.1109/VCRIS63677.2024.10813439', 'keywords': 'Training;Accuracy;Image color analysis;Prevention and mitigation;Transfer learning;Neural networks;Information security;Detectors;Pressing;Feature extraction;Generative AI;Detect fake images;Convolutional Neural Networks;Pre-Training model', 'abstract': 'The advancement of artificial intelligence (AI) technology has made Generative AI a significant concern for many. This technology generates fake images through highly complex algorithms. It involves a detailed analysis of the original image, extracting features like color, context, and feature, which are then used to build a neural network. This network is combined with computer graphics to produce an image that resembles the original. Attackers often use these fake images to attack AI systems, making the detection and prevention of such images a pressing issue. In this study, we propose a method to detect fake images using a detector built on CNN models. Our experimental results demonstrate that the proposed detectors achieve over 95\\% average accuracy on the test datasets, indicating their potential applicability to real-world problems.', 'pages': '1-6', 'number': '', 'volume': '', 'year': '2024', 'title': 'Employing a CNN Detector to Identify AI-Generated Images and Against Attacks on AI Systems', 'booktitle': '2024 1st International Conference On Cryptography And Information Security (VCRIS)', 'author': 'Truong, Phi-Ho and Nguyen, Tien-Dung and Truong, Xuan-Hung and Nguyen, Nhat-Hai and Pham, Duy-Trung', 'ENTRYTYPE': 'inproceedings', 'ID': '10813439'}"
11042380,Detecting Deception: A Comparative Study of CNN Approaches for Deepfake Image Detection,"M, Nithish and P, Thiyagarajan and S, Rajalakshmi",M,10.1109/RMKMATE64874.2025.11042380,2025,"2025 2nd International Conference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication Engineering (RMKMATE)","The exponential growth of Artificial Intelligence (AI) has given rise to deepfake images, which present a major impact on digital safety, mistaken identity, and the spread of fake information. Due to their highly realistic nature, detecting deepfakes has become a major challenge, highlighting the critical need for robust detection systems. This research investigates the performance of nine advanced Convolutional Neural Network (CNN) architectures, namely: Custom CNN, ResNet50, VGG16, InceptionV3, DenseNet, MobileNetV2, XceptionNet, MesoNet, and InceptionResNetV2 which were used to detect deepfakes. To evaluate the effectiveness of the above CNN architectures, two large-scale image datasets were used: the Real \& Deepfake Dataset and the 140K Real and Fake Faces Dataset. These datasets include more than 2,00,000 real and fake images. The training was done using a GPU to increase the speed and accuracy of the models. Among all the models, the Custom CNN provided the best results, reaching 99\% accuracy on the Real \& Deepfake Dataset and 98\% on the 140K dataset. ResNet50, DenseNet, and InceptionV3 also performed very well, each scoring above 90\% accuracy in both datasets. Other models like VGG16, MobileNetV2, XceptionNet, and InceptionResNetV2 also showed strong results, proving they are useful for deepfake detection. This analysis study aims to identify the most efficient and accurate CNN architecture for image-based deepfake detection, ensuring digital content safety, and identifying the fastest model for real-time deepfake detection in future applications.","Training;Deepfakes;Accuracy;Graphics processing units;Real-time systems;Knowledge management;Safety;Convolutional neural networks;Artificial intelligence;Residual neural networks;CNN Models;Deep fake;Images;Deep Learning,Comparison","{'month': 'May', 'issn': '', 'doi': '10.1109/RMKMATE64874.2025.11042380', 'keywords': 'Training;Deepfakes;Accuracy;Graphics processing units;Real-time systems;Knowledge management;Safety;Convolutional neural networks;Artificial intelligence;Residual neural networks;CNN Models;Deep fake;Images;Deep Learning,Comparison', 'abstract': 'The exponential growth of Artificial Intelligence (AI) has given rise to deepfake images, which present a major impact on digital safety, mistaken identity, and the spread of fake information. Due to their highly realistic nature, detecting deepfakes has become a major challenge, highlighting the critical need for robust detection systems. This research investigates the performance of nine advanced Convolutional Neural Network (CNN) architectures, namely: Custom CNN, ResNet50, VGG16, InceptionV3, DenseNet, MobileNetV2, XceptionNet, MesoNet, and InceptionResNetV2 which were used to detect deepfakes. To evaluate the effectiveness of the above CNN architectures, two large-scale image datasets were used: the Real \\& Deepfake Dataset and the 140K Real and Fake Faces Dataset. These datasets include more than 2,00,000 real and fake images. The training was done using a GPU to increase the speed and accuracy of the models. Among all the models, the Custom CNN provided the best results, reaching 99\\% accuracy on the Real \\& Deepfake Dataset and 98\\% on the 140K dataset. ResNet50, DenseNet, and InceptionV3 also performed very well, each scoring above 90\\% accuracy in both datasets. Other models like VGG16, MobileNetV2, XceptionNet, and InceptionResNetV2 also showed strong results, proving they are useful for deepfake detection. This analysis study aims to identify the most efficient and accurate CNN architecture for image-based deepfake detection, ensuring digital content safety, and identifying the fastest model for real-time deepfake detection in future applications.', 'pages': '1-8', 'number': '', 'volume': '', 'year': '2025', 'title': 'Detecting Deception: A Comparative Study of CNN Approaches for Deepfake Image Detection', 'booktitle': '2025 2nd International Conference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication Engineering (RMKMATE)', 'author': 'M, Nithish and P, Thiyagarajan and S, Rajalakshmi', 'ENTRYTYPE': 'inproceedings', 'ID': '11042380'}"
10800948,Intelligent Document Interaction with Advanced Vector Embeddings and FAISS-CPU Indexing,"Sriman, B and Annie Silviya, S H and Mouleesh, Y and Vinod, S and Nishanthini, S and Nikitha, Polimera",Sriman,10.1109/ICECA63461.2024.10800948,2024,"2024 8th International Conference on Electronics, Communication and Aerospace Technology (ICECA)","Our research project aims to introduce an artificial intelligence (AI) conversational model that can be used to efficiently communicate with multiple PDF documents. Our solution uses natural language processing algorithms to enable users to have lively conversations with PDFs that have been uploaded. We have built a comprehensive platform that can interpret user queries and provide pertinent responses based on the content of uploaded documents by integrating Python with Streamlit for interface development, FAISSCPU for storage and similarity search, PyPDF2 for text extraction, ChromaDB, Langchain, and Google Gemini Generative AI for enhanced functionality. Our goals include developing a UI that is easy to use, processing documents correctly, and having strong conversational capabilities.",Navigation;Oral communication;Portable document format;Information retrieval;Vectors;Natural language processing;Libraries;Internet;Reliability;Indexing;Google Gemini Generative AI;Conversational Model;Natural Language Processing,"{'month': 'Nov', 'issn': '', 'doi': '10.1109/ICECA63461.2024.10800948', 'keywords': 'Navigation;Oral communication;Portable document format;Information retrieval;Vectors;Natural language processing;Libraries;Internet;Reliability;Indexing;Google Gemini Generative AI;Conversational Model;Natural Language Processing', 'abstract': 'Our research project aims to introduce an artificial intelligence (AI) conversational model that can be used to efficiently communicate with multiple PDF documents. Our solution uses natural language processing algorithms to enable users to have lively conversations with PDFs that have been uploaded. We have built a comprehensive platform that can interpret user queries and provide pertinent responses based on the content of uploaded documents by integrating Python with Streamlit for interface development, FAISSCPU for storage and similarity search, PyPDF2 for text extraction, ChromaDB, Langchain, and Google Gemini Generative AI for enhanced functionality. Our goals include developing a UI that is easy to use, processing documents correctly, and having strong conversational capabilities.', 'pages': '1-6', 'number': '', 'volume': '', 'year': '2024', 'title': 'Intelligent Document Interaction with Advanced Vector Embeddings and FAISS-CPU Indexing', 'booktitle': '2024 8th International Conference on Electronics, Communication and Aerospace Technology (ICECA)', 'author': 'Sriman, B and Annie Silviya, S H and Mouleesh, Y and Vinod, S and Nishanthini, S and Nikitha, Polimera', 'ENTRYTYPE': 'inproceedings', 'ID': '10800948'}"
10405735,A Framework to Improve the Comparability and Reproducibility of Morphing Attack Detectors,"Di Domenico, Nicolò and Borghi, Guido and Franco, Annalisa and Ferrara, Matteo and Maltoni, Davide",Di Domenico,10.1109/MetroXRAINE58569.2023.10405735,2023,"2023 IEEE International Conference on Metrology for eXtended Reality, Artificial Intelligence and Neural Engineering (MetroXRAINE)","Morphing Attack, i.e. the deception of Face Recognition Systems (FRS) through a face morphing process between the identity of two subjects with criminal intent, has recently emerged as a serious security threat. Due to its significance, recently several Morphing Attack Detection (MAD) systems, i.e. methods based on Artificial Intelligence able to automatically detect the presence of morphing, have been proposed in the literature. Unfortunately, developing, comparing, and reproducing these MAD algorithms is challenging, particularly for deep learning-based solutions, since they are usually evaluated on private datasets and the source code is not publicly released. Therefore, we observe the need for an open-source framework that aims to simplify the development of new MAD systems, in combination with their evaluation. Thus, in this paper, after a discussion about the current limits of existing studies on the MAD task, we examine the desired properties and features of this framework, with a particular focus on its modularity, usability, and effectiveness.",Face recognition;Source coding;Reproducibility of results;Security;Artificial intelligence;Usability;Task analysis;Morphing Attack;Morphing Attack Detection (MAD);Single-image MAD (S-MAD);Differential MAD (D-MAD);Automated Border Control (ABC);Face Recognition Systems (FRS),"{'month': 'Oct', 'issn': '', 'doi': '10.1109/MetroXRAINE58569.2023.10405735', 'keywords': 'Face recognition;Source coding;Reproducibility of results;Security;Artificial intelligence;Usability;Task analysis;Morphing Attack;Morphing Attack Detection (MAD);Single-image MAD (S-MAD);Differential MAD (D-MAD);Automated Border Control (ABC);Face Recognition Systems (FRS)', 'abstract': 'Morphing Attack, i.e. the deception of Face Recognition Systems (FRS) through a face morphing process between the identity of two subjects with criminal intent, has recently emerged as a serious security threat. Due to its significance, recently several Morphing Attack Detection (MAD) systems, i.e. methods based on Artificial Intelligence able to automatically detect the presence of morphing, have been proposed in the literature. Unfortunately, developing, comparing, and reproducing these MAD algorithms is challenging, particularly for deep learning-based solutions, since they are usually evaluated on private datasets and the source code is not publicly released. Therefore, we observe the need for an open-source framework that aims to simplify the development of new MAD systems, in combination with their evaluation. Thus, in this paper, after a discussion about the current limits of existing studies on the MAD task, we examine the desired properties and features of this framework, with a particular focus on its modularity, usability, and effectiveness.', 'pages': '525-530', 'number': '', 'volume': '', 'year': '2023', 'title': 'A Framework to Improve the Comparability and Reproducibility of Morphing Attack Detectors', 'booktitle': '2023 IEEE International Conference on Metrology for eXtended Reality, Artificial Intelligence and Neural Engineering (MetroXRAINE)', 'author': 'Di Domenico, Nicolò and Borghi, Guido and Franco, Annalisa and Ferrara, Matteo and Maltoni, Davide', 'ENTRYTYPE': 'inproceedings', 'ID': '10405735'}"
10857545,Deep Learning Strategies for Identifying Machine-Generated Text,"Yadagiri, Annepaka and Pakray, Partha",Yadagiri,10.1109/IMCOM64595.2025.10857545,2025,2025 19th International Conference on Ubiquitous Information Management and Communication (IMCOM),"Generative AIs like LLMs are now accessible to the general public. For example, students can utilize these tools to create essays or complete theses. However, how is a teacher supposed to determine if a text was composed by the student or an AI? Using deep learning techniques, we investigate novel and classic approaches for detecting text created by artificial intelligence. We also study the more complex instance when the AI is asked to write the text in a way that a human would not recognize as AI-generated, as we discovered that categorization is more challenging in this scenario. For our studies, we used llm-detect-ai-generated text from the Kaggle competition dataset, which included texts written by students and texts produced using different LLMs. Our top systems achieve an accuracy of 0.98\% and F1 scores of more than 0.98\% in classifying simple and complex texts produced by humans and AI-Genereted. The systems combine features such as TF-IDF vectorization, word2Vec, and word embedding properties. Our findings demonstrate that these additional characteristics significantly enhance the performance of several classifiers. Compared to deep learning models, our best-performing model for detecting AI-generated text outperforms even the fine-tuned ROBERTA-Open-AI classifier, achieving an accuracy of 0.98\%. This underscores the efficacy of our proposed approach in distinguishing between human and AI-generated content.",Deep learning;Training;Adaptation models;Accuracy;Text recognition;Text categorization;Text detection;Detectors;Transformers;Information management;Large Language Models;Natural Language Processing;Neural Networks;Generative AI,"{'month': 'Jan', 'issn': '', 'doi': '10.1109/IMCOM64595.2025.10857545', 'keywords': 'Deep learning;Training;Adaptation models;Accuracy;Text recognition;Text categorization;Text detection;Detectors;Transformers;Information management;Large Language Models;Natural Language Processing;Neural Networks;Generative AI', 'abstract': 'Generative AIs like LLMs are now accessible to the general public. For example, students can utilize these tools to create essays or complete theses. However, how is a teacher supposed to determine if a text was composed by the student or an AI? Using deep learning techniques, we investigate novel and classic approaches for detecting text created by artificial intelligence. We also study the more complex instance when the AI is asked to write the text in a way that a human would not recognize as AI-generated, as we discovered that categorization is more challenging in this scenario. For our studies, we used llm-detect-ai-generated text from the Kaggle competition dataset, which included texts written by students and texts produced using different LLMs. Our top systems achieve an accuracy of 0.98\\% and F1 scores of more than 0.98\\% in classifying simple and complex texts produced by humans and AI-Genereted. The systems combine features such as TF-IDF vectorization, word2Vec, and word embedding properties. Our findings demonstrate that these additional characteristics significantly enhance the performance of several classifiers. Compared to deep learning models, our best-performing model for detecting AI-generated text outperforms even the fine-tuned ROBERTA-Open-AI classifier, achieving an accuracy of 0.98\\%. This underscores the efficacy of our proposed approach in distinguishing between human and AI-generated content.', 'pages': '1-8', 'number': '', 'volume': '', 'year': '2025', 'title': 'Deep Learning Strategies for Identifying Machine-Generated Text', 'booktitle': '2025 19th International Conference on Ubiquitous Information Management and Communication (IMCOM)', 'author': 'Yadagiri, Annepaka and Pakray, Partha', 'ENTRYTYPE': 'inproceedings', 'ID': '10857545'}"
10950904,Using AI in Curriculum Development,"Shah, Priten",Shah,,2023,AI and the Future of Education: Teaching in the Age of Artificial Intelligence,"Summary <p>Generative AI tools have the potential to dramatically reduce the amount of time teachers spend creating and revising materials for their classrooms. The content generated by AI can be customized quickly, adapted to current events, personalized based on classroom needs and preferences, and match formats and guidelines from administrators. AI tools can also be used to help plan out a curriculum or unit while helping us think about time allocation, standards, and the unique needs that their classroom might have. Thinking about AI\&\#x2010;partnered lesson and curriculum planning as an iterative process will allow us to make the most of it by offloading as much work as possible to the AI. Everything from presentations and worksheets to posters and custom classroom resources can be enhanced with AI\&\#x2010;generated art. ChatGPT came up with a fun, interactive, problem\&\#x2010;based assessment to replace the essay prompt.</p>",Artificial intelligence;Standards;Education;Chatbots;Generative AI;Curriculum development;Videos;Refining;Planning;Particle swarm optimization,"{'url': 'https://ieeexplore-ieee-org.crai.referencistas.com/document/10950904', 'isbn': '9781394219261', 'publisher': 'Wiley', 'issn': '', 'doi': '', 'keywords': 'Artificial intelligence;Standards;Education;Chatbots;Generative AI;Curriculum development;Videos;Refining;Planning;Particle swarm optimization', 'abstract': 'Summary <p>Generative AI tools have the potential to dramatically reduce the amount of time teachers spend creating and revising materials for their classrooms. The content generated by AI can be customized quickly, adapted to current events, personalized based on classroom needs and preferences, and match formats and guidelines from administrators. AI tools can also be used to help plan out a curriculum or unit while helping us think about time allocation, standards, and the unique needs that their classroom might have. Thinking about AI\\&\\#x2010;partnered lesson and curriculum planning as an iterative process will allow us to make the most of it by offloading as much work as possible to the AI. Everything from presentations and worksheets to posters and custom classroom resources can be enhanced with AI\\&\\#x2010;generated art. ChatGPT came up with a fun, interactive, problem\\&\\#x2010;based assessment to replace the essay prompt.</p>', 'pages': '90-135', 'number': '', 'volume': '', 'year': '2023', 'title': 'Using AI in Curriculum Development', 'booktitle': 'AI and the Future of Education: Teaching in the Age of Artificial Intelligence', 'author': 'Shah, Priten', 'ENTRYTYPE': 'inbook', 'ID': '10950904'}"
11022294,Identification of Benign and Malignant Using GA and ResNet-50 for Pulmonary Nodules,"Xie, Xin and Guan, Enguang and Wang, Yao",Xie,10.1109/ACAIT63902.2024.11022294,2024,2024 8th Asian Conference on Artificial Intelligence Technology (ACAIT),"Artificial intelligence (AI) algorithms have been proven to be an effective and feasible technique for early diagnosing of medical images, such as lung or pulmonary, thyroid and other nodules diseases. Although numerous studies have been presented to improve diagnostic performance, there is still no absolutely reliable method for screening and identifying benign and malignant nodules, which significantly affects the predictions and universalities. In this study, a pulmonary nodule image identification framework which is based on deep-learning networks of ResNet-50 and genetic algorithm (GA) is developed, i.e. GA+ResNet-50, aiming at the classification problem of Computed Tomography (CT) images from LIDC-IDRI public data set. Six different algorithms have been used for comparative analysis in terms of the accuracy, the sensitivity, the precision and the F1 score. The results show that GA+ResNet-50 has a good performance on the identification of benign and malignant lung nodules, and the diagnostic capability is significantly improved. In addition, the method also performs well on three other datasets, further validating its broad applicability and reliability. This work can provide effective suggestions to doctors’ diagnosis for pulmonary nodule diseases.",Accuracy;Sensitivity;Lungs;Computed tomography;Transformers;Classification algorithms;Reliability;Artificial intelligence;Cancer;Genetic algorithms;lung or pulmonary nodule;benign and malignant;diagnostic imaging;deep learning;GA+ResNet-50,"{'month': 'Nov', 'issn': '', 'doi': '10.1109/ACAIT63902.2024.11022294', 'keywords': 'Accuracy;Sensitivity;Lungs;Computed tomography;Transformers;Classification algorithms;Reliability;Artificial intelligence;Cancer;Genetic algorithms;lung or pulmonary nodule;benign and malignant;diagnostic imaging;deep learning;GA+ResNet-50', 'abstract': 'Artificial intelligence (AI) algorithms have been proven to be an effective and feasible technique for early diagnosing of medical images, such as lung or pulmonary, thyroid and other nodules diseases. Although numerous studies have been presented to improve diagnostic performance, there is still no absolutely reliable method for screening and identifying benign and malignant nodules, which significantly affects the predictions and universalities. In this study, a pulmonary nodule image identification framework which is based on deep-learning networks of ResNet-50 and genetic algorithm (GA) is developed, i.e. GA+ResNet-50, aiming at the classification problem of Computed Tomography (CT) images from LIDC-IDRI public data set. Six different algorithms have been used for comparative analysis in terms of the accuracy, the sensitivity, the precision and the F1 score. The results show that GA+ResNet-50 has a good performance on the identification of benign and malignant lung nodules, and the diagnostic capability is significantly improved. In addition, the method also performs well on three other datasets, further validating its broad applicability and reliability. This work can provide effective suggestions to doctors’ diagnosis for pulmonary nodule diseases.', 'pages': '954-963', 'number': '', 'volume': '', 'year': '2024', 'title': 'Identification of Benign and Malignant Using GA and ResNet-50 for Pulmonary Nodules', 'booktitle': '2024 8th Asian Conference on Artificial Intelligence Technology (ACAIT)', 'author': 'Xie, Xin and Guan, Enguang and Wang, Yao', 'ENTRYTYPE': 'inproceedings', 'ID': '11022294'}"
10765144,APR-Net: Defense Against Adversarial Examples Based on Universal Adversarial Perturbation Removal Network,"Liao, Wenxing and Liu, Zhuxian and Shen, Minghuang and Chen, Riqing and Liu, Xiaolong",Liao,10.1109/TAI.2024.3504478,2025,IEEE Transactions on Artificial Intelligence,"Adversarial attack, a bleeding-edge technique that attempts to fool deep learning classification model by generating adversarial examples with imperceptible perturbations, is becoming a growing threat in artificial intelligence fields. Preprocessing models that remove perturbations are an effective approach for enhancing the robustness of classification models. However, most existing methods overlook a critical issue: although powerful preprocessing operations can remove adversarial perturbations, they may also weaken the representation of key features in the image, leading to decreased defense performance. To address this, we propose a novel universal defense model, APR-Net, which aims to remove adversarial perturbations while effectively preserving high-quality images. The key innovation of APR-Net lies in its dual-module design, which consists of a denoising module and an image restoration module. This design not only effectively eliminates imperceptible adversarial perturbations but also ensures the restoration of high-quality images. Unlike existing methods, APR-Net does not require modifications to the classifier architecture or specialized adversarial training, making it highly versatile. Extensive experiments on the ImageNet dataset demonstrate that APR-Net provides strong defense against various adversarial attack algorithms, significantly improves image quality, and outperforms other state-of-the-art defense methods in terms of overall performance.",Perturbation methods;Training;Robustness;Iterative methods;Computational modeling;Artificial intelligence;Image restoration;Deep learning;Technological innovation;Optimization;Adversarial defense;adversarial examples;deep neural networks (DNNs);image denoising;image restoration.,"{'month': 'April', 'issn': '2691-4581', 'doi': '10.1109/TAI.2024.3504478', 'keywords': 'Perturbation methods;Training;Robustness;Iterative methods;Computational modeling;Artificial intelligence;Image restoration;Deep learning;Technological innovation;Optimization;Adversarial defense;adversarial examples;deep neural networks (DNNs);image denoising;image restoration.', 'abstract': 'Adversarial attack, a bleeding-edge technique that attempts to fool deep learning classification model by generating adversarial examples with imperceptible perturbations, is becoming a growing threat in artificial intelligence fields. Preprocessing models that remove perturbations are an effective approach for enhancing the robustness of classification models. However, most existing methods overlook a critical issue: although powerful preprocessing operations can remove adversarial perturbations, they may also weaken the representation of key features in the image, leading to decreased defense performance. To address this, we propose a novel universal defense model, APR-Net, which aims to remove adversarial perturbations while effectively preserving high-quality images. The key innovation of APR-Net lies in its dual-module design, which consists of a denoising module and an image restoration module. This design not only effectively eliminates imperceptible adversarial perturbations but also ensures the restoration of high-quality images. Unlike existing methods, APR-Net does not require modifications to the classifier architecture or specialized adversarial training, making it highly versatile. Extensive experiments on the ImageNet dataset demonstrate that APR-Net provides strong defense against various adversarial attack algorithms, significantly improves image quality, and outperforms other state-of-the-art defense methods in terms of overall performance.', 'pages': '945-954', 'number': '4', 'volume': '6', 'year': '2025', 'title': 'APR-Net: Defense Against Adversarial Examples Based on Universal Adversarial Perturbation Removal Network', 'journal': 'IEEE Transactions on Artificial Intelligence', 'author': 'Liao, Wenxing and Liu, Zhuxian and Shen, Minghuang and Chen, Riqing and Liu, Xiaolong', 'ENTRYTYPE': 'article', 'ID': '10765144'}"
9054438,Generative Pre-Training for Speech with Autoregressive Predictive Coding,"Chung, Yu-An and Glass, James",Chung,10.1109/ICASSP40776.2020.9054438,2020,"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","Learning meaningful and general representations from unannotated speech that are applicable to a wide range of tasks remains challenging. In this paper we propose to use autoregressive predictive coding (APC), a recently proposed self-supervised objective, as a generative pre-training approach for learning meaningful, non-specific, and transferable speech representations. We pre-train APC on large-scale unlabeled data and conduct transfer learning experiments on three speech applications that require different information about speech characteristics to perform well: speech recognition, speech translation, and speaker identification. Extensive experiments show that APC not only outperforms surface features (e.g., log Mel spectrograms) and other popular representation learning methods on all three tasks, but is also effective at reducing downstream labeled data size and model parameters. We also investigate the use of Transformers for modeling APC and find it superior to RNNs.",Training;Speech coding;Transfer learning;Speech recognition;Predictive coding;Data models;Task analysis;representation learning;self-supervised learning;pre-training;transfer learning;autoregressive modeling,"{'month': 'May', 'issn': '2379-190X', 'doi': '10.1109/ICASSP40776.2020.9054438', 'keywords': 'Training;Speech coding;Transfer learning;Speech recognition;Predictive coding;Data models;Task analysis;representation learning;self-supervised learning;pre-training;transfer learning;autoregressive modeling', 'abstract': 'Learning meaningful and general representations from unannotated speech that are applicable to a wide range of tasks remains challenging. In this paper we propose to use autoregressive predictive coding (APC), a recently proposed self-supervised objective, as a generative pre-training approach for learning meaningful, non-specific, and transferable speech representations. We pre-train APC on large-scale unlabeled data and conduct transfer learning experiments on three speech applications that require different information about speech characteristics to perform well: speech recognition, speech translation, and speaker identification. Extensive experiments show that APC not only outperforms surface features (e.g., log Mel spectrograms) and other popular representation learning methods on all three tasks, but is also effective at reducing downstream labeled data size and model parameters. We also investigate the use of Transformers for modeling APC and find it superior to RNNs.', 'pages': '3497-3501', 'number': '', 'volume': '', 'year': '2020', 'title': 'Generative Pre-Training for Speech with Autoregressive Predictive Coding', 'booktitle': 'ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)', 'author': 'Chung, Yu-An and Glass, James', 'ENTRYTYPE': 'inproceedings', 'ID': '9054438'}"
9274911,Driving Accident Detection by Self-Supervised Adversarial Appearance-Motion Prediction in First-Person Videos,"Qiao, Jiahuan and Fang, Jianwu and Yan, Dingxin and Xue, Jianru",Qiao,10.1109/ICUS50048.2020.9274911,2020,2020 3rd International Conference on Unmanned Systems (ICUS),"Driving accident is the event that occur unexpectedly and should be detected effectively for autonomous driving systems. In this paper, we propose a method based on adversarial appearance-motion prediction for driving accident detection in dashcam videos. The novelty of this method is to consider the predictability of the frame-level and object-level motion and appearance from the current to the future. Through a self-supervised adversarial learning between the real observation in next frame and the predicted motion and appearance, the objects that may occur accident are detected. In order to evaluate the accuracy of the detection results of our method, we evaluate the performance on A3D dataset, and the effectiveness of the method is validated.",Accidents;Videos;Optical network units;Generative adversarial networks;Gallium nitride;Feature extraction;Control engineering;Driving accident detection;motion and appearance consistency;adversarial learning,"{'month': 'Nov', 'issn': '', 'doi': '10.1109/ICUS50048.2020.9274911', 'keywords': 'Accidents;Videos;Optical network units;Generative adversarial networks;Gallium nitride;Feature extraction;Control engineering;Driving accident detection;motion and appearance consistency;adversarial learning', 'abstract': 'Driving accident is the event that occur unexpectedly and should be detected effectively for autonomous driving systems. In this paper, we propose a method based on adversarial appearance-motion prediction for driving accident detection in dashcam videos. The novelty of this method is to consider the predictability of the frame-level and object-level motion and appearance from the current to the future. Through a self-supervised adversarial learning between the real observation in next frame and the predicted motion and appearance, the objects that may occur accident are detected. In order to evaluate the accuracy of the detection results of our method, we evaluate the performance on A3D dataset, and the effectiveness of the method is validated.', 'pages': '1083-1088', 'number': '', 'volume': '', 'year': '2020', 'title': 'Driving Accident Detection by Self-Supervised Adversarial Appearance-Motion Prediction in First-Person Videos', 'booktitle': '2020 3rd International Conference on Unmanned Systems (ICUS)', 'author': 'Qiao, Jiahuan and Fang, Jianwu and Yan, Dingxin and Xue, Jianru', 'ENTRYTYPE': 'inproceedings', 'ID': '9274911'}"
10497435,Medical Image Generation based on Latent Diffusion Models,"Song, Wenbo and Jiang, Yan and Fang, Yin and Cao, Xinyu and Wu, Peiyan and Xing, Hanshuo and Wu, Xinglong",Song,10.1109/ICAII59460.2023.10497435,2023,2023 International Conference on Artificial Intelligence Innovation (ICAII),"The application of deep learning networks in med-ical image analysis has become increasingly mature. However, the availability of a large amount of medical image data for training deep learning networks is hindered by various factors such as workload of doctors, different devices, and ethical privacy policies. These factors can limit the full potential of deep learning network models. In recent years, generative models, particularly diffusion models, have made significant progress in synthesizing realistic images in various domains. Nevertheless, there is currently limited research on the application of Latent Diffusion Models (LDMs) in medical image generation, especially when generating different types of medical images using a unified process. In our research, we explore the use of LDMs to generate synthetic images from various medical image datasets. We train the LDMs on the BreastMRI, Hand, HeadCT, and CXR images extracted from the MedNIST dataset. By employing a consistent LDMs architecture, we aim to generate images from different medical device sources. Additionally, we assess the similarity between the generated images and the real images to evaluate the performance of our approach.",Deep learning;Training;Performance evaluation;Image segmentation;Technological innovation;Privacy;Image synthesis;Medical Image;Latent Diffusion Models;Image Generation,"{'month': 'Sep.', 'issn': '', 'doi': '10.1109/ICAII59460.2023.10497435', 'keywords': 'Deep learning;Training;Performance evaluation;Image segmentation;Technological innovation;Privacy;Image synthesis;Medical Image;Latent Diffusion Models;Image Generation', 'abstract': 'The application of deep learning networks in med-ical image analysis has become increasingly mature. However, the availability of a large amount of medical image data for training deep learning networks is hindered by various factors such as workload of doctors, different devices, and ethical privacy policies. These factors can limit the full potential of deep learning network models. In recent years, generative models, particularly diffusion models, have made significant progress in synthesizing realistic images in various domains. Nevertheless, there is currently limited research on the application of Latent Diffusion Models (LDMs) in medical image generation, especially when generating different types of medical images using a unified process. In our research, we explore the use of LDMs to generate synthetic images from various medical image datasets. We train the LDMs on the BreastMRI, Hand, HeadCT, and CXR images extracted from the MedNIST dataset. By employing a consistent LDMs architecture, we aim to generate images from different medical device sources. Additionally, we assess the similarity between the generated images and the real images to evaluate the performance of our approach.', 'pages': '89-93', 'number': '', 'volume': '', 'year': '2023', 'title': 'Medical Image Generation based on Latent Diffusion Models', 'booktitle': '2023 International Conference on Artificial Intelligence Innovation (ICAII)', 'author': 'Song, Wenbo and Jiang, Yan and Fang, Yin and Cao, Xinyu and Wu, Peiyan and Xing, Hanshuo and Wu, Xinglong', 'ENTRYTYPE': 'inproceedings', 'ID': '10497435'}"
10378949,Vision Intelligence Assisted Lung Function Estimation Based on Transformer Encoder–Decoder Network With Invertible Modeling,"Chen, Liuyin and Lu, Di and Zhai, Jianxue and Cai, Kaican and Wang, Long and Zhang, Zijun",Chen,10.1109/TAI.2023.3348428,2024,IEEE Transactions on Artificial Intelligence,"Lung function evaluation is important to many medical applications, but conducting pulmonary function tests is constrained by different conditions. This article presents a pioneer study of an integrated invertible deep learning method for lung function estimation via using computed tomography (CT) images. First, the projection method is proposed to flatten the three-dimensional (3-D) image onto a two-dimensional (2-D) plane, with preserving location information in 3-D. Next, the MBConv transformer-based encoder–decoder structure is developed to extract latent features. Finally, we develop an invertible normalizing flow (NF) model to infer lung function based on the extracted features and design two loss functions for two directions. The method enables both estimating the lung function based on CT images and metadata as well as generating the corresponding simulated CT image according to the lung function. Computational studies show that the proposed regression model outperforms all state-of-the-art image regression models. A comprehensive comparative analysis also demonstrates the effectiveness of using generated images and confirms the superiority of the proposed method. To the best of our knowledge, this work is the first of its kind in combining encoder–decoder network with NFs to ensure the effectiveness of the fully invertible framework, especially in lung CT image analysis.",Lung;Computed tomography;Computational modeling;Estimation;Metadata;Three-dimensional displays;Transformers;Generative model;image regression;lung function;normalizing flows (NFs);ViT,"{'month': 'July', 'issn': '2691-4581', 'doi': '10.1109/TAI.2023.3348428', 'keywords': 'Lung;Computed tomography;Computational modeling;Estimation;Metadata;Three-dimensional displays;Transformers;Generative model;image regression;lung function;normalizing flows (NFs);ViT', 'abstract': 'Lung function evaluation is important to many medical applications, but conducting pulmonary function tests is constrained by different conditions. This article presents a pioneer study of an integrated invertible deep learning method for lung function estimation via using computed tomography (CT) images. First, the projection method is proposed to flatten the three-dimensional (3-D) image onto a two-dimensional (2-D) plane, with preserving location information in 3-D. Next, the MBConv transformer-based encoder–decoder structure is developed to extract latent features. Finally, we develop an invertible normalizing flow (NF) model to infer lung function based on the extracted features and design two loss functions for two directions. The method enables both estimating the lung function based on CT images and metadata as well as generating the corresponding simulated CT image according to the lung function. Computational studies show that the proposed regression model outperforms all state-of-the-art image regression models. A comprehensive comparative analysis also demonstrates the effectiveness of using generated images and confirms the superiority of the proposed method. To the best of our knowledge, this work is the first of its kind in combining encoder–decoder network with NFs to ensure the effectiveness of the fully invertible framework, especially in lung CT image analysis.', 'pages': '3336-3349', 'number': '7', 'volume': '5', 'year': '2024', 'title': 'Vision Intelligence Assisted Lung Function Estimation Based on Transformer Encoder–Decoder Network With Invertible Modeling', 'journal': 'IEEE Transactions on Artificial Intelligence', 'author': 'Chen, Liuyin and Lu, Di and Zhai, Jianxue and Cai, Kaican and Wang, Long and Zhang, Zijun', 'ENTRYTYPE': 'article', 'ID': '10378949'}"
10003937,Identification of abnormal tissue from CT images using improved ResNet34,"Honda, Naoya and Kamiya, Tohru and Kido, Shoji",Honda,10.23919/ICCAS55662.2022.10003937,2022,"2022 22nd International Conference on Control, Automation and Systems (ICCAS)","In recent years, CT examinations have been widely used as a screening method to detect lung cancer. However, reading enormous CT images become a heavy burden to the physician. To avoid this problem, computer-aided diagnosis systems have been introduced on CT screening. In general, physicians consider patient information in addition to image information when they make a diagnosis, new efforts are being made to improve the accuracy of diagnosis by mimicking this information with a machine. In this paper, we propose a method for identifying pulmonary nodules by adding medical record information to images to improve the accuracy of diagnosis. We classify nodules from unknown data by assigning branching information of vascular opacities, straight vascular shadows, and nodular shadows as labeled image, which are a cause of misrecognition based on image features in machine learning. In the experiment, the classification accuracy of the nodule class was improved by adding clinical information to 644 images including 161 nodal images.",Computed tomography;Lung cancer;Lung;Machine learning;Generative adversarial networks;Control systems;Computer aided diagnosis;CT;computer-aided diagnosis;clinical information;multimodal;deep learning,"{'month': 'Nov', 'issn': '2642-3901', 'doi': '10.23919/ICCAS55662.2022.10003937', 'keywords': 'Computed tomography;Lung cancer;Lung;Machine learning;Generative adversarial networks;Control systems;Computer aided diagnosis;CT;computer-aided diagnosis;clinical information;multimodal;deep learning', 'abstract': 'In recent years, CT examinations have been widely used as a screening method to detect lung cancer. However, reading enormous CT images become a heavy burden to the physician. To avoid this problem, computer-aided diagnosis systems have been introduced on CT screening. In general, physicians consider patient information in addition to image information when they make a diagnosis, new efforts are being made to improve the accuracy of diagnosis by mimicking this information with a machine. In this paper, we propose a method for identifying pulmonary nodules by adding medical record information to images to improve the accuracy of diagnosis. We classify nodules from unknown data by assigning branching information of vascular opacities, straight vascular shadows, and nodular shadows as labeled image, which are a cause of misrecognition based on image features in machine learning. In the experiment, the classification accuracy of the nodule class was improved by adding clinical information to 644 images including 161 nodal images.', 'pages': '532-536', 'number': '', 'volume': '', 'year': '2022', 'title': 'Identification of abnormal tissue from CT images using improved ResNet34', 'booktitle': '2022 22nd International Conference on Control, Automation and Systems (ICCAS)', 'author': 'Honda, Naoya and Kamiya, Tohru and Kido, Shoji', 'ENTRYTYPE': 'inproceedings', 'ID': '10003937'}"
10854312,Network traffic classification method based on attention-CNN-BiLSTM,"Zhang, Qichen",Zhang,10.1049/icp.2024.4308,2024,6th International Conference on Artificial Intelligence and Advanced Manufacturing (AIAM 2024),"With the wide application of artificial intelligence, cloud computing and Internet of things, we have entered a new era of intelligent interconnection. However, network security threats and risks are also increasing. At present, it is urgent to solve how to effectively combat network attacks and ensure network security. Network traffic analysis plays a key role in intrusion detection. By accurately classifying network traffic, network intrusion can be more targeted. However, due to the imbalance of categories in the data samples, the recognition accuracy of minority classes is low. In order to cope with this challenge, an improved network traffic analysis method based on Attention-CNN-Bi LSTM is proposed. This method first balances the data set, then uses the feature selection strategy to eliminate unnecessary features, and uses the combined neural network model for training. The experimental simulation of this method shows that the AUC value of the minority class exceeds 0.85 on both data sets, which shows that the method has good generalization performance for the minority class and solves the problem of low accuracy of the minority class in the unbalanced data set.",,"{'month': 'Oct', 'issn': '', 'doi': '10.1049/icp.2024.4308', 'keywords': '', 'abstract': 'With the wide application of artificial intelligence, cloud computing and Internet of things, we have entered a new era of intelligent interconnection. However, network security threats and risks are also increasing. At present, it is urgent to solve how to effectively combat network attacks and ensure network security. Network traffic analysis plays a key role in intrusion detection. By accurately classifying network traffic, network intrusion can be more targeted. However, due to the imbalance of categories in the data samples, the recognition accuracy of minority classes is low. In order to cope with this challenge, an improved network traffic analysis method based on Attention-CNN-Bi LSTM is proposed. This method first balances the data set, then uses the feature selection strategy to eliminate unnecessary features, and uses the combined neural network model for training. The experimental simulation of this method shows that the AUC value of the minority class exceeds 0.85 on both data sets, which shows that the method has good generalization performance for the minority class and solves the problem of low accuracy of the minority class in the unbalanced data set.', 'pages': '735-742', 'number': '', 'volume': '2024', 'year': '2024', 'title': 'Network traffic classification method based on attention-CNN-BiLSTM', 'booktitle': '6th International Conference on Artificial Intelligence and Advanced Manufacturing (AIAM 2024)', 'author': 'Zhang, Qichen', 'ENTRYTYPE': 'inproceedings', 'ID': '10854312'}"
