{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adb9bffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Módulo cargado. Define rutas y ejecuta las funciones de prueba con un subconjunto de registros.\n"
     ]
    }
   ],
   "source": [
    "# Pipeline inicial para Requerimiento 5\n",
    "# - instalar dependencias si faltan (solo si lo deseas)\n",
    "# - parsear archivo .bib grande a un DataFrame\n",
    "# - extraer primer autor y campos clave\n",
    "# - enriquecer por DOI usando Crossref (con cache) para obtener afiliación/pais\n",
    "# NOTA: Este bloque está diseñado para ejecutarse por partes; por defecto procesa un subconjunto de registros para pruebas.\n",
    "import importlib, subprocess, sys, os, time, json, re\n",
    "from pathlib import Path\n",
    "\n",
    "def ensure_packages(packages):\n",
    "    \"\"\"Instala paquetes pip que no estén presentes.\n",
    "    No instala si ya están disponibles.\n",
    "    \"\"\"\n",
    "    for pkg in packages:\n",
    "        try:\n",
    "            importlib.import_module(pkg)\n",
    "        except Exception:\n",
    "            print(f\"Instalando {pkg}...\")\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg])\n",
    "\n",
    "# Paquetes recomendados\n",
    "REQUIRED = ['bibtexparser','pandas','requests','pycountry','tqdm','rapidfuzz']\n",
    "# Descomenta la siguiente línea si quieres que el notebook instale dependencias automáticamente\n",
    "# ensure_packages(REQUIRED)\n",
    "\n",
    "# Imports principales (después de instalar si es necesario)\n",
    "import bibtexparser\n",
    "import pandas as pd\n",
    "import requests\n",
    "import pycountry\n",
    "from tqdm import tqdm\n",
    "from rapidfuzz import fuzz, process\n",
    "\n",
    "# ---------- Utilidades de parseo y normalización ----------\n",
    "def parse_bib_to_df(bib_path, max_entries=None):\n",
    "    \"\"\"Parsea un archivo .bib a un DataFrame con campos clave.\n",
    "    Devuelve pandas.DataFrame con columnas: id, title, authors, first_author, doi, year, venue, abstract, keywords, raw_entry\n",
    "    \"\"\"\n",
    "    bib_text = Path(bib_path).read_text(encoding='utf-8', errors='ignore')\n",
    "    bib_db = bibtexparser.loads(bib_text)\n",
    "    rows = []\n",
    "    for i, entry in enumerate(bib_db.entries):\n",
    "        if max_entries is not None and i >= max_entries:\n",
    "            break\n",
    "        eid = entry.get('ID') or entry.get('key') or f'row{i}'\n",
    "        title = entry.get('title','').strip()\n",
    "        authors = entry.get('author','').strip()\n",
    "        doi = entry.get('doi','').strip()\n",
    "        year = entry.get('year','').strip()\n",
    "        venue = entry.get('journal', entry.get('booktitle','')).strip()\n",
    "        abstract = entry.get('abstract','').strip()\n",
    "        keywords = entry.get('keywords', entry.get('keyword','')).strip()\n",
    "        raw = str(entry)\n",
    "        first_author = extract_first_author(authors)\n",
    "        rows.append({\n",
    "            'id': eid,\n",
    "            'title': title,\n",
    "            'authors': authors,\n",
    "            'first_author': first_author,\n",
    "            'doi': doi,\n",
    "            'year': year,\n",
    "            'venue': venue,\n",
    "            'abstract': abstract,\n",
    "            'keywords': keywords,\n",
    "            'raw_entry': raw,\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_first_author(authors_str):\n",
    "    if not authors_str:\n",
    "        return ''\n",
    "    # BibTeX authors are usually separated by ' and '\n",
    "    parts = [p.strip() for p in authors_str.split(' and ')]\n",
    "    first = parts[0] if parts else ''\n",
    "    # Normalize formats like 'Last, First' -> 'Last' or 'First Last' -> 'Last'\n",
    "    if ',' in first:\n",
    "        last = first.split(',')[0].strip()\n",
    "    else:\n",
    "        toks = first.split()\n",
    "        last = toks[-1] if toks else first\n",
    "    return last\n",
    "\n",
    "# ---------- Enriquecimiento: Crossref + heurísticas ----------\n",
    "COUNTRIES = [c.name.lower() for c in pycountry.countries]\n",
    "# Add some common aliases\n",
    "ALIASES = {'usa':'united states','us':'united states','u.s.a.':'united states','uk':'united kingdom','england':'united kingdom'}\n",
    "\n",
    "def find_country_in_text(text):\n",
    "    if not text:\n",
    "        return None\n",
    "    t = text.lower()\n",
    "    # direct match country names\n",
    "    for cname in COUNTRIES:\n",
    "        if cname in t:\n",
    "            return cname.title()\n",
    "    # aliases\n",
    "    for a,v in ALIASES.items():\n",
    "        if a in t:\n",
    "            return v.title()\n",
    "    return None\n",
    "\n",
    "\n",
    "def enrich_by_doi(doi, email=None, sleep=1.0):\n",
    "    \"\"\"Consulta Crossref por DOI y trata de extraer afiliación/pais del primer autor.\n",
    "    Devuelve dict con keys: affiliation_raw, country, country_iso2, source, confidence\n",
    "    \"\"\"\n",
    "    if not doi:\n",
    "        return {'affiliation_raw':'','country':'','country_iso2':'','source':'','confidence':0.0}\n",
    "    # normalize doi for URL (remove leading DOI: if present)\n",
    "    doi_clean = doi.strip()\n",
    "    doi_clean = doi_clean.replace('doi:','').replace('DOI:','').strip()\n",
    "    url = f'https://api.crossref.org/works/{requests.utils.requote_uri(doi_clean)}'\n",
    "    headers = {'User-Agent': f'proyecto-analisis-algoritmos (mailto:{email})' if email else 'proyecto-analisis-algoritmos'}\n",
    "    try:\n",
    "        r = requests.get(url, headers=headers, timeout=20)\n",
    "        if r.status_code != 200:\n",
    "            return {'affiliation_raw':'','country':'','country_iso2':'','source':'crossref','confidence':0.0}\n",
    "        data = r.json()\n",
    "        msg = data.get('message', {})\n",
    "        authors = msg.get('author', [])\n",
    "        if not authors:\n",
    "            return {'affiliation_raw':'','country':'','country_iso2':'','source':'crossref','confidence':0.0}\n",
    "        first = authors[0]\n",
    "        affs = first.get('affiliation', [])\n",
    "        aff_text = ''\n",
    "        if affs:\n",
    "            # affiliation is often list of dicts with 'name'\n",
    "            if isinstance(affs, list):\n",
    "                aff_text = ' '.join([a.get('name','') for a in affs if isinstance(a, dict)])\n",
    "            else:\n",
    "                aff_text = str(affs)\n",
    "        # try to find country in affiliation text\n",
    "        country = find_country_in_text(aff_text)\n",
    "        country_iso = ''\n",
    "        if country:\n",
    "            try:\n",
    "                c = pycountry.countries.get(name=country) or pycountry.countries.get(common_name=country)\n",
    "                if c:\n",
    "                    country_iso = c.alpha_2\n",
    "            except Exception:\n",
    "                country_iso = ''\n",
    "        # sleep to respect rate limits\n",
    "        time.sleep(sleep)\n",
    "        return {'affiliation_raw':aff_text, 'country': country.title() if country else '', 'country_iso2': country_iso, 'source':'crossref', 'confidence': 0.9 if country else 0.5}\n",
    "    except Exception as e:\n",
    "        # error contacting crossref\n",
    "        return {'affiliation_raw':'','country':'','country_iso2':'','source':'crossref_error','confidence':0.0, 'error': str(e)}\n",
    "\n",
    "\n",
    "def load_cache(path):\n",
    "    if not os.path.exists(path):\n",
    "        return {}\n",
    "    try:\n",
    "        return pd.read_csv(path, dtype=str).set_index('id').to_dict(orient='index')\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "\n",
    "def save_cache(dct, path):\n",
    "    df = pd.DataFrame.from_dict(dct, orient='index')\n",
    "    df.index.name = 'id'\n",
    "    df.reset_index(inplace=True)\n",
    "    df.to_csv(path, index=False, encoding='utf-8')\n",
    "\n",
    "\n",
    "def batch_enrich(df, cache_path='country_lookup.csv', email=None, sleep=1.0, max_rows=None):\n",
    "    cache = load_cache(cache_path)\n",
    "    updated = False\n",
    "    total = len(df) if max_rows is None else min(len(df), max_rows)\n",
    "    for idx in tqdm(range(total)):\n",
    "        row = df.iloc[idx]\n",
    "        rid = row['id']\n",
    "        if rid in cache:\n",
    "            continue\n",
    "        doi = row.get('doi','')\n",
    "        if not doi:\n",
    "            # try heuristics on raw_entry\n",
    "            aff = find_country_in_text(row.get('raw_entry',''))\n",
    "            cache[rid] = {'doi': doi, 'affiliation_raw': row.get('raw_entry',''), 'country': aff.title() if aff else '', 'country_iso2': '', 'source':'heuristic' if aff else '', 'confidence': 0.3 if aff else 0.0}\n",
    "            updated = True\n",
    "            continue\n",
    "        res = enrich_by_doi(doi, email=email, sleep=sleep)\n",
    "        rowd = {'doi': doi, 'affiliation_raw': res.get('affiliation_raw',''), 'country': res.get('country',''), 'country_iso2': res.get('country_iso2',''), 'source': res.get('source',''), 'confidence': res.get('confidence',0.0)}\n",
    "        cache[rid] = rowd\n",
    "        updated = True\n",
    "    if updated:\n",
    "        save_cache(cache, cache_path)\n",
    "    return cache\n",
    "\n",
    "# ---------- Guardar registros y preparar estado para la nube ----------\n",
    "def prepare_state_files(df, out_dir='proyecto/requerimiento5/data'):\n",
    "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "    records_path = Path(out_dir)/'records.csv'\n",
    "    df.to_csv(records_path, index=False, encoding='utf-8')\n",
    "    # frequencies.json placeholder\n",
    "    freq_path = Path(out_dir)/'frequencies.json'\n",
    "    if not freq_path.exists():\n",
    "        with open(freq_path,'w',encoding='utf-8') as f:\n",
    "            json.dump({'total_terms':0,'terms':{}}, f, ensure_ascii=False, indent=2)\n",
    "    return records_path, Path(out_dir)/'country_lookup.csv', freq_path\n",
    "\n",
    "print('Módulo cargado. Define rutas y ejecuta las funciones de prueba con un subconjunto de registros.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74b3c69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing completo del archivo .bib — esto puede tardar dependiendo del tamaño (~10k registros).\n",
      "Parsed 500 records (total)\n",
      "Records guardados en data\\records.csv\n",
      "Parsed 500 records (total)\n",
      "Records guardados en data\\records.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [10:37<00:00,  1.27s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriquecimiento finalizado. Caché guardada en data\\country_lookup.csv\n",
      "Paises asignados: 377 / 500 = 75.40%\n",
      "Fichero de frecuencias (placeholder): data\\frequencies.json\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de uso: procesar TODO el .bib (sin límite)\n",
    "# Ajusta rutas según estructura del repositorio\n",
    "BIB_PATH = '../primeros_' \\\n",
    "'500.bib'  # ruta relativa al notebook (ajusta si es necesario)\n",
    "OUT_DIR = 'data'\n",
    "\n",
    "# Parsear TODO el .bib (sin max_entries)\n",
    "print('Parsing completo del archivo .bib — esto puede tardar dependiendo del tamaño (~10k registros).')\n",
    "df = parse_bib_to_df(BIB_PATH, max_entries=None)\n",
    "print(f'Parsed {len(df)} records (total)')\n",
    "\n",
    "# Guardar registros y preparar archivos de estado\n",
    "records_csv, country_cache_path, freq_path = prepare_state_files(df, out_dir=OUT_DIR)\n",
    "print('Records guardados en', records_csv)\n",
    "\n",
    "# Enriquecer por DOI (ejecútalo si quieres probar crossref; suministra tu correo en email)\n",
    "# Para evitar bloqueos, procesa con sleep>=1.0 y considera ejecutar por la noche para muchos registros.\n",
    "email = ''  # opcional: tu correo para User-Agent en Crossref\n",
    "# Nota: batch_enrich ahora procesará todos los registros pendientes en cache (sin max_rows)\n",
    "cache = batch_enrich(df, cache_path=str(country_cache_path), email=email, sleep=1.0, max_rows=None)\n",
    "print('Enriquecimiento finalizado. Caché guardada en', country_cache_path)\n",
    "\n",
    "# Mostrar estadísticas de cobertura\n",
    "covered = sum(1 for v in cache.values() if v.get('country'))\n",
    "print(f'Paises asignados: {covered} / {len(df)} = {covered/len(df):.2%}')\n",
    "\n",
    "# Frecuencias (placeholder)\n",
    "print('Fichero de frecuencias (placeholder):', freq_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4dbd4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No se encontró proyecto\\requerimiento5\\data\\records.csv. Asegúrate de haber ejecutado la celda de parseo anteriormente.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m cache_path = Path(\u001b[33m'\u001b[39m\u001b[33mproyecto/requerimiento5/data/country_lookup.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m records_path.exists():\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo se encontró \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecords_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Asegúrate de haber ejecutado la celda de parseo anteriormente.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m records = pd.read_csv(records_path, dtype=\u001b[38;5;28mstr\u001b[39m).fillna(\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     19\u001b[39m cache = pd.read_csv(cache_path, dtype=\u001b[38;5;28mstr\u001b[39m).fillna(\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m cache_path.exists() \u001b[38;5;28;01melse\u001b[39;00m pd.DataFrame(columns=[\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mcountry\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mcountry_iso2\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[31mFileNotFoundError\u001b[39m: No se encontró proyecto\\requerimiento5\\data\\records.csv. Asegúrate de haber ejecutado la celda de parseo anteriormente."
     ]
    }
   ],
   "source": [
    "# Mapa de calor por país (choropleth) — versión con guardado robusto\n",
    "# Carga los registros y la caché de países, calcula conteos y dibuja un choropleth.\n",
    "import pandas as pd\n",
    "import pycountry\n",
    "import plotly.express as px\n",
    "from pathlib import Path\n",
    "\n",
    "# Salidas centralizadas dentro del subdirectorio del requerimiento\n",
    "OUT_DIR = Path('outputs')\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "records_path = Path('data/records.csv')\n",
    "cache_path = Path('data/country_lookup.csv')\n",
    "\n",
    "if not records_path.exists():\n",
    "    raise FileNotFoundError(f\"No se encontró {records_path}. Asegúrate de haber ejecutado la celda de parseo anteriormente.\")\n",
    "\n",
    "records = pd.read_csv(records_path, dtype=str).fillna('')\n",
    "cache = pd.read_csv(cache_path, dtype=str).fillna('') if cache_path.exists() else pd.DataFrame(columns=['id','country','country_iso2'])\n",
    "\n",
    "# Asegurar que 'id' exista en records\n",
    "if 'id' not in records.columns:\n",
    "    raise ValueError('La tabla de records no contiene la columna `id`.')\n",
    "\n",
    "if cache.empty:\n",
    "    print('Advertencia: cache de países vacía. No hay datos geográficos para plotear.')\n",
    "\n",
    "# Normalizar cache: asegurar columna country_iso2 en mayúsculas\n",
    "if 'country_iso2' in cache.columns:\n",
    "    cache['country_iso2'] = cache['country_iso2'].str.upper().replace({'NAN':''})\n",
    "\n",
    "# Merge\n",
    "merged = records.merge(cache[['id','country','country_iso2']], on='id', how='left')\n",
    "\n",
    "# Contar por país (usar country_iso2 preferiblemente)\n",
    "agg = merged.groupby(['country','country_iso2'], dropna=False).size().reset_index(name='count')\n",
    "# Filtrar filas con country not empty\n",
    "agg = agg[ (agg['country'].notna()) & (agg['country']!='') ]\n",
    "\n",
    "# Función para convertir ISO2 -> ISO3\n",
    "def iso2_to_iso3(a2):\n",
    "    try:\n",
    "        if not a2 or pd.isna(a2):\n",
    "            return None\n",
    "        c = pycountry.countries.get(alpha_2=str(a2).upper())\n",
    "        if c:\n",
    "            return c.alpha_3\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "agg['iso3'] = agg['country_iso2'].apply(iso2_to_iso3)\n",
    "\n",
    "# Si algunas filas no obtuvieron iso3, intentar mapear por nombre de país (menos fiable)\n",
    "import numpy as np\n",
    "if agg['iso3'].isna().any():\n",
    "    def name_to_iso3(name):\n",
    "        try:\n",
    "            if not name or pd.isna(name):\n",
    "                return None\n",
    "            c = pycountry.countries.lookup(name)\n",
    "            return c.alpha_3\n",
    "        except Exception:\n",
    "            return None\n",
    "    agg['iso3'] = agg.apply(lambda r: r['iso3'] if pd.notna(r['iso3']) else name_to_iso3(r['country']), axis=1)\n",
    "\n",
    "# Elige filas válidas\n",
    "df_plot = agg.dropna(subset=['iso3'])\n",
    "\n",
    "if df_plot.empty:\n",
    "    print('No hay países con ISO3 válido para plotear. Revisa `proyecto/requerimiento5/data/country_lookup.csv`.')\n",
    "    display(agg.sort_values('count', ascending=False).head(20))\n",
    "else:\n",
    "    fig = px.choropleth(df_plot, locations='iso3', color='count', hover_name='country',\n",
    "                        color_continuous_scale='Viridis', projection='natural earth',\n",
    "                        title='Publicaciones por país (primer autor)')\n",
    "    fig.update_layout(coloraxis_colorbar=dict(title='Número de publicaciones'))\n",
    "    fig.show()\n",
    "\n",
    "    # Intentar guardar como PDF con kaleido; si falla, fallback a PNG y conversión a PDF con Pillow\n",
    "    out_pdf = OUT_DIR / 'mapa_paises.pdf'\n",
    "    out_png = OUT_DIR / 'mapa_paises.png'\n",
    "    saved = False\n",
    "    try:\n",
    "        # Preferir engine kaleido\n",
    "        fig.write_image(str(out_pdf), format='pdf', engine='kaleido')\n",
    "        print('Mapa guardado en', out_pdf)\n",
    "        saved = True\n",
    "    except Exception as e:\n",
    "        print('No se pudo guardar como PDF directamente con kaleido. Error:', e)\n",
    "        try:\n",
    "            fig.write_image(str(out_png), format='png', engine='kaleido')\n",
    "            print('Mapa guardado en PNG en', out_png)\n",
    "            # Convertir PNG -> PDF usando Pillow\n",
    "            try:\n",
    "                from PIL import Image\n",
    "                im = Image.open(out_png).convert('RGB')\n",
    "                im.save(out_pdf, 'PDF', resolution=300)\n",
    "                print('PNG convertido a PDF en', out_pdf)\n",
    "                saved = True\n",
    "            except Exception as e2:\n",
    "                print('Error al convertir PNG a PDF con Pillow:', e2)\n",
    "        except Exception as e3:\n",
    "            print('No se pudo exportar imagen con `kaleido`. Intenta `pip install -U kaleido pillow`. Error:', e3)\n",
    "    if not saved:\n",
    "        print('Fallo guardado: revisa la instalación de `kaleido` y `Pillow` y reinicia el kernel si acabas de instalarlas.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
