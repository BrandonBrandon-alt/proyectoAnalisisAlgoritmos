# Proyecto de An√°lisis de Algoritmos

## Descripci√≥n General

Este proyecto implementa un sistema completo de **web scraping, procesamiento y an√°lisis de art√≠culos cient√≠ficos** sobre Inteligencia Artificial Generativa. El sistema recopila art√≠culos de m√∫ltiples bases de datos acad√©micas (IEEE, ScienceDirect, Taylor & Francis), los procesa y realiza diversos an√°lisis bibliom√©tricos.

## Estructura del Proyecto

```
proyecto/
‚îú‚îÄ‚îÄ data/                    # Scripts de web scraping
‚îÇ   ‚îú‚îÄ‚îÄ IEEE.ipynb          # Scraping de IEEE Xplore
‚îÇ   ‚îú‚îÄ‚îÄ science.ipynb       # Scraping de ScienceDirect
‚îÇ   ‚îî‚îÄ‚îÄ taylor.ipynb        # Scraping de Taylor & Francis
‚îú‚îÄ‚îÄ requerimiento1/         # Consolidaci√≥n y limpieza de datos
‚îú‚îÄ‚îÄ requerimiento2/         # An√°lisis de similitud entre art√≠culos
‚îú‚îÄ‚îÄ requerimiento3/         # An√°lisis de frecuencia de palabras
‚îú‚îÄ‚îÄ requerimiento4/         # Clustering jer√°rquico
‚îú‚îÄ‚îÄ requerimiento5/         # An√°lisis geogr√°fico de autores
‚îú‚îÄ‚îÄ main.ipynb              # Orquestador principal
‚îî‚îÄ‚îÄ consolidado.bib         # Base de datos consolidada
```

## Requisitos Previos

### 1. Dependencias
Instalar todas las dependencias desde la ra√≠z del proyecto:
```bash
pip install -r requirements.txt
```

### 2. Variables de Entorno
Crear un archivo `.env` en la ra√≠z del proyecto con:
```env
EMAIL=tu_email@ejemplo.com
PASSWORD=tu_contrase√±a
DOWNLOAD_PATH=/ruta/absoluta/a/carpeta/descargas
```

### 3. Configuraci√≥n de Selenium
- Tener Google Chrome instalado
- El driver de Chrome se gestiona autom√°ticamente con Selenium 4+

## M√≥dulos del Proyecto

### üì• Data Collection (carpeta `data/`)

Scripts de web scraping para recopilar art√≠culos cient√≠ficos:

#### **IEEE.ipynb**
- **Fuente**: IEEE Xplore
- **B√∫squeda**: "generative artificial intelligence"
- **Formato**: BibTeX con abstract
- **Paginaci√≥n**: Autom√°tica (100 resultados por p√°gina)
- **Salida**: `descargas/ieee/ieee_generative_ai_page_*.bib`

#### **science.ipynb**
- **Fuente**: ScienceDirect
- **B√∫squeda**: "generative artificial intelligence"
- **Formato**: BibTeX con abstract
- **Salida**: `descargas/science/science_generative_ai_page_*.bib`

#### **taylor.ipynb**
- **Fuente**: Taylor & Francis
- **B√∫squeda**: "generative artificial intelligence"
- **Formato**: BibTeX con abstract
- **Salida**: `descargas/taylor/taylor_generative_ai_page_*.bib`

**‚ö†Ô∏è Importante**: 
- Los scripts requieren autenticaci√≥n institucional
- El proceso puede tardar varios minutos
- Se recomienda ejecutar en horarios de baja carga

### üìä An√°lisis y Procesamiento

Ver READMEs individuales en cada carpeta de requerimiento:
- [Requerimiento 1](./requerimiento1/README.md) - Consolidaci√≥n y limpieza
- [Requerimiento 2](./requerimiento2/README.md) - An√°lisis de similitud
- [Requerimiento 3](./requerimiento3/README.md) - Frecuencia de palabras
- [Requerimiento 4](./requerimiento4/README.md) - Clustering jer√°rquico
- [Requerimiento 5](./requerimiento5/README.md) - An√°lisis geogr√°fico

## Flujo de Trabajo

### Opci√≥n 1: Ejecuci√≥n Manual
Ejecutar cada notebook en orden:
1. `data/IEEE.ipynb` ‚Üí Descargar art√≠culos de IEEE
2. `data/science.ipynb` ‚Üí Descargar art√≠culos de ScienceDirect
3. `data/taylor.ipynb` ‚Üí Descargar art√≠culos de Taylor & Francis
4. `requerimiento1/Requerimiento1.ipynb` ‚Üí Consolidar y limpiar
5. `requerimiento2/Requeriemito2.ipynb` ‚Üí An√°lisis de similitud
6. `requerimiento3/Requerimiento3.ipynb` ‚Üí An√°lisis de frecuencias
7. `requerimiento4/Requerimiento4.ipynb` ‚Üí Clustering
8. `requerimiento5/Requerimiento5.ipynb` ‚Üí An√°lisis geogr√°fico

### Opci√≥n 2: Ejecuci√≥n Autom√°tica
Usar el orquestador principal:
```bash
jupyter notebook main.ipynb
```

## Datos Generados

### Archivos de Entrada
- Archivos `.bib` descargados de cada fuente
- Total aproximado: 10,000+ art√≠culos

### Archivos de Salida
- `consolidado.bib` - Base de datos unificada
- `outputs/` - Gr√°ficos, tablas y an√°lisis
- Archivos JSON con metadatos procesados

## Caracter√≠sticas Principales

‚úÖ **Web Scraping Automatizado**
- Manejo de paginaci√≥n
- Gesti√≥n de cookies y autenticaci√≥n
- Reintentos autom√°ticos

‚úÖ **Procesamiento Robusto**
- Detecci√≥n y eliminaci√≥n de duplicados
- Normalizaci√≥n de campos
- Validaci√≥n de datos

‚úÖ **An√°lisis Avanzados**
- Similitud sem√°ntica con SBERT
- Clustering jer√°rquico
- An√°lisis geogr√°fico con APIs

‚úÖ **Visualizaciones**
- Dendrogramas
- Mapas de calor
- Gr√°ficos de frecuencia

## Soluci√≥n de Problemas

### Error de autenticaci√≥n
- Verificar credenciales en `.env`
- Confirmar acceso institucional activo

### Selenium no encuentra elementos
- Actualizar selectores XPath/CSS
- Verificar que Chrome est√© actualizado
- Aumentar tiempos de espera

### Memoria insuficiente
- Procesar en lotes m√°s peque√±os
- Cerrar aplicaciones innecesarias
- Usar subconjuntos de datos para pruebas

## Notas T√©cnicas

- **Tiempo de ejecuci√≥n**: 2-4 horas (completo)
- **Espacio en disco**: ~500 MB para datos completos
- **RAM recomendada**: 8 GB m√≠nimo
- **Python**: 3.8+

## Autores y Licencia

Proyecto acad√©mico - Universidad del Norte
An√°lisis de Algoritmos - 2025
