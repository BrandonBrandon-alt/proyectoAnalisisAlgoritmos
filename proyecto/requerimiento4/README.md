# Requerimiento 4: Clustering JerÃ¡rquico de ArtÃ­culos

## Objetivo

Agrupar artÃ­culos cientÃ­ficos en clusters temÃ¡ticos utilizando **clustering jerÃ¡rquico aglomerativo** basado en la similitud de sus abstracts, y visualizar los resultados mediante dendrogramas.

## DescripciÃ³n General

Este mÃ³dulo aplica tÃ©cnicas de clustering no supervisado para descubrir grupos naturales de artÃ­culos con contenido similar, permitiendo identificar subtemas y Ã¡reas de investigaciÃ³n dentro del dominio de Inteligencia Artificial Generativa.

## Fundamentos TeÃ³ricos

### Clustering JerÃ¡rquico Aglomerativo

**Algoritmo**:
1. Iniciar con cada artÃ­culo como un cluster individual
2. Calcular distancias entre todos los pares de clusters
3. Fusionar los dos clusters mÃ¡s cercanos
4. Repetir hasta tener un Ãºnico cluster (o alcanzar criterio de parada)

**Ventajas**:
- âœ… No requiere especificar nÃºmero de clusters a priori
- âœ… Genera estructura jerÃ¡rquica completa
- âœ… Visualizable mediante dendrogramas
- âœ… DeterminÃ­stico (mismo resultado en cada ejecuciÃ³n)

**Desventajas**:
- âŒ Complejidad O(nÂ³) - lento para grandes datasets
- âŒ No puede deshacer fusiones incorrectas
- âŒ Sensible a outliers

## Proceso de Clustering

### 1. PreparaciÃ³n de Datos

```python
import bibtexparser
import nltk
from nltk.corpus import stopwords

# Cargar abstracts
with open("consolidado.bib") as f:
    bib_database = bibtexparser.load(f)

abstracts = [entry.get('abstract', '') for entry in bib_database.entries]
```

**Preprocesamiento**:
- Limpieza de texto
- EliminaciÃ³n de stopwords
- NormalizaciÃ³n (minÃºsculas, sin puntuaciÃ³n)

### 2. VectorizaciÃ³n TF-IDF

```python
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(
    max_features=5000,        # Top 5000 tÃ©rminos mÃ¡s importantes
    stop_words='english',     # Eliminar stopwords
    ngram_range=(1, 2),       # Unigramas y bigramas
    min_df=2,                 # MÃ­nimo 2 documentos
    max_df=0.8                # MÃ¡ximo 80% de documentos
)

tfidf_matrix = vectorizer.fit_transform(abstracts)
```

**ParÃ¡metros TF-IDF**:

| ParÃ¡metro | Valor | DescripciÃ³n |
|-----------|-------|-------------|
| `max_features` | 5000 | NÃºmero mÃ¡ximo de tÃ©rminos |
| `stop_words` | 'english' | Eliminar palabras comunes |
| `ngram_range` | (1, 2) | Palabras individuales y pares |
| `min_df` | 2 | Frecuencia mÃ­nima en documentos |
| `max_df` | 0.8 | Frecuencia mÃ¡xima (80%) |

**Resultado**: Matriz dispersa de dimensiones `(n_artÃ­culos, 5000)`

### 3. CÃ¡lculo de Distancias

```python
from scipy.spatial.distance import pdist

# Calcular distancias entre todos los pares
distances = pdist(tfidf_matrix.toarray(), metric='cosine')
```

**MÃ©tricas de distancia disponibles**:

| MÃ©trica | DescripciÃ³n | Uso Recomendado |
|---------|-------------|-----------------|
| `cosine` | 1 - similitud coseno | **Textos (recomendado)** |
| `euclidean` | Distancia euclidiana | Datos numÃ©ricos |
| `manhattan` | Distancia Manhattan | Datos dispersos |
| `jaccard` | Similitud de Jaccard | Datos binarios |

**SelecciÃ³n**: `cosine` es Ã³ptima para vectores TF-IDF

### 4. Clustering JerÃ¡rquico

```python
from scipy.cluster.hierarchy import linkage, dendrogram

# Realizar clustering
Z = linkage(distances, method='ward')
```

**MÃ©todos de enlace (linkage)**:

| MÃ©todo | DescripciÃ³n | CaracterÃ­sticas |
|--------|-------------|-----------------|
| `ward` | Minimiza varianza intra-cluster | **Recomendado** - clusters compactos |
| `complete` | MÃ¡xima distancia entre clusters | Clusters esfÃ©ricos |
| `average` | Promedio de distancias | Balanceado |
| `single` | MÃ­nima distancia | Sensible a outliers |

**SelecciÃ³n**: `ward` produce los clusters mÃ¡s coherentes

### 5. VisualizaciÃ³n - Dendrograma

```python
import matplotlib.pyplot as plt

plt.figure(figsize=(15, 8))
dendrogram(
    Z,
    labels=titulos,           # Etiquetas de artÃ­culos
    leaf_rotation=90,         # Rotar etiquetas
    leaf_font_size=8,         # TamaÃ±o de fuente
    truncate_mode='lastp',    # Mostrar Ãºltimos p clusters
    p=30                      # NÃºmero de clusters a mostrar
)
plt.title('Dendrograma de Clustering JerÃ¡rquico')
plt.xlabel('ArtÃ­culos')
plt.ylabel('Distancia')
plt.tight_layout()
plt.savefig('outputs/dendrograma.png', dpi=300)
plt.show()
```

## Estructura del CÃ³digo

```python
# 1. Importar librerÃ­as
import bibtexparser
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.spatial.distance import pdist
from scipy.cluster.hierarchy import linkage, dendrogram
import matplotlib.pyplot as plt
import numpy as np

# 2. Cargar y preprocesar datos
abstracts = cargar_abstracts()
abstracts_limpios = preprocesar_textos(abstracts)

# 3. Vectorizar con TF-IDF
vectorizer = TfidfVectorizer(...)
tfidf_matrix = vectorizer.fit_transform(abstracts_limpios)

# 4. Calcular distancias
distances = pdist(tfidf_matrix.toarray(), metric='cosine')

# 5. Clustering jerÃ¡rquico
Z = linkage(distances, method='ward')

# 6. Visualizar dendrograma
plot_dendrogram(Z, labels=titulos)

# 7. Extraer clusters
clusters = extraer_clusters(Z, n_clusters=5)

# 8. Analizar clusters
analizar_clusters(clusters, abstracts)
```

## Uso

### EjecuciÃ³n del Notebook

```bash
jupyter notebook Requerimiento4.ipynb
```

### ConfiguraciÃ³n de ParÃ¡metros

```python
# VectorizaciÃ³n
MAX_FEATURES = 5000
NGRAM_RANGE = (1, 2)
MIN_DF = 2
MAX_DF = 0.8

# Clustering
LINKAGE_METHOD = 'ward'
DISTANCE_METRIC = 'cosine'

# VisualizaciÃ³n
N_CLUSTERS_DISPLAY = 30
FIGURE_SIZE = (15, 8)
DPI = 300

# Corte del dendrograma (para extraer clusters)
DISTANCE_THRESHOLD = 0.7  # Ajustar segÃºn dendrograma
```

## InterpretaciÃ³n del Dendrograma

### Elementos del Dendrograma

```
                    â”‚
                    â”œâ”€â”€â”€â”€â”€â”
                    â”‚     â”‚
              â”Œâ”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”
              â”‚     â”‚     â”‚     â”‚
         â”Œâ”€â”€â”€â”€â”¤     â”‚     â”‚     â”œâ”€â”€â”€â”€â”
         â”‚    â”‚     â”‚     â”‚     â”‚    â”‚
        Art1 Art2  Art3  Art4  Art5 Art6
```

**Componentes**:
- **Hojas**: ArtÃ­culos individuales (abajo)
- **Nodos**: Fusiones de clusters (arriba)
- **Altura**: Distancia a la que se fusionan
- **Ramas**: Grupos de artÃ­culos similares

### CÃ³mo Leer el Dendrograma

1. **Altura de fusiÃ³n**: Mayor altura = menor similitud
2. **Clusters naturales**: Ramas que se separan tarde
3. **Outliers**: ArtÃ­culos que se fusionan muy arriba
4. **Subclusters**: Divisiones dentro de ramas principales

### Determinar NÃºmero Ã“ptimo de Clusters

```python
from scipy.cluster.hierarchy import fcluster

# MÃ©todo 1: Corte a distancia especÃ­fica
clusters = fcluster(Z, t=0.7, criterion='distance')

# MÃ©todo 2: NÃºmero fijo de clusters
clusters = fcluster(Z, t=5, criterion='maxclust')

# MÃ©todo 3: MÃ©todo del codo (elbow method)
last_merges = Z[-10:, 2]
plt.plot(range(1, 11), last_merges[::-1])
plt.xlabel('NÃºmero de clusters')
plt.ylabel('Distancia de fusiÃ³n')
plt.show()
```

## Resultados Esperados

### Ejemplo de Clusters Identificados

```
ğŸ“Š Resumen de Clustering:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Total de artÃ­culos:              10,226
NÃºmero de clusters:                   5
MÃ©todo de enlace:                  ward
MÃ©trica de distancia:            cosine
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Cluster 1 (2,345 artÃ­culos):
  Tema principal: GeneraciÃ³n de imÃ¡genes con GANs
  Palabras clave: image, generation, GAN, adversarial
  
Cluster 2 (1,987 artÃ­culos):
  Tema principal: Modelos de lenguaje (LLMs)
  Palabras clave: language, model, GPT, transformer
  
Cluster 3 (2,156 artÃ­culos):
  Tema principal: Aplicaciones mÃ©dicas
  Palabras clave: medical, diagnosis, healthcare, clinical
  
Cluster 4 (1,876 artÃ­culos):
  Tema principal: Computer Vision
  Palabras clave: vision, detection, recognition, CNN
  
Cluster 5 (1,862 artÃ­culos):
  Tema principal: Procesamiento de audio
  Palabras clave: audio, speech, voice, synthesis
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

### AnÃ¡lisis de Clusters

Para cada cluster, se calcula:

```python
# Palabras mÃ¡s representativas (TF-IDF promedio)
def palabras_cluster(cluster_docs, vectorizer):
    cluster_tfidf = tfidf_matrix[cluster_docs].mean(axis=0)
    top_indices = cluster_tfidf.argsort()[0, -10:]
    return vectorizer.get_feature_names_out()[top_indices]

# Coherencia intra-cluster
def coherencia_cluster(cluster_docs, distances):
    cluster_distances = distances[cluster_docs][:, cluster_docs]
    return cluster_distances.mean()
```

## Dependencias

```python
bibtexparser==1.4.3
nltk==3.9.2
scikit-learn==1.7.2
scipy==1.16.1
numpy==2.2.6
matplotlib==3.10.5
```

## MÃ©tricas de EvaluaciÃ³n

### 1. Coeficiente de Silueta

```python
from sklearn.metrics import silhouette_score

score = silhouette_score(tfidf_matrix, cluster_labels, metric='cosine')
```

**InterpretaciÃ³n**:
- `0.7 - 1.0`: Excelente separaciÃ³n
- `0.5 - 0.7`: Buena separaciÃ³n
- `0.25 - 0.5`: SeparaciÃ³n dÃ©bil
- `< 0.25`: Clusters mal definidos

### 2. Ãndice de Davies-Bouldin

```python
from sklearn.metrics import davies_bouldin_score

score = davies_bouldin_score(tfidf_matrix.toarray(), cluster_labels)
```

**InterpretaciÃ³n**: Valores mÃ¡s bajos = mejor clustering

### 3. Ãndice de Calinski-Harabasz

```python
from sklearn.metrics import calinski_harabasz_score

score = calinski_harabasz_score(tfidf_matrix.toarray(), cluster_labels)
```

**InterpretaciÃ³n**: Valores mÃ¡s altos = mejor clustering

## OptimizaciÃ³n

### Para Datasets Grandes (> 10,000 artÃ­culos)

```python
# OpciÃ³n 1: Usar muestra representativa
sample_size = 5000
sample_indices = np.random.choice(len(abstracts), sample_size, replace=False)
abstracts_sample = [abstracts[i] for i in sample_indices]

# OpciÃ³n 2: Clustering en dos etapas
# 1. K-means para pre-agrupar
# 2. JerÃ¡rquico dentro de cada grupo

# OpciÃ³n 3: Mini-batch TF-IDF
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(max_features=1000)  # Reducir features
```

## Problemas Comunes

### Error: "Memory Error"

```python
# Reducir dimensionalidad
from sklearn.decomposition import TruncatedSVD

svd = TruncatedSVD(n_components=100)
tfidf_reduced = svd.fit_transform(tfidf_matrix)
```

### Warning: "Dendrograma muy denso"

```python
# Usar truncamiento
dendrogram(Z, truncate_mode='lastp', p=30)
```

### Clusters poco coherentes

```python
# Ajustar parÃ¡metros TF-IDF
vectorizer = TfidfVectorizer(
    min_df=5,      # Aumentar frecuencia mÃ­nima
    max_df=0.5,    # Reducir frecuencia mÃ¡xima
    max_features=3000  # Reducir features
)
```

## Tiempo de EjecuciÃ³n

| OperaciÃ³n | 1,000 docs | 5,000 docs | 10,000 docs |
|-----------|------------|------------|-------------|
| VectorizaciÃ³n TF-IDF | 2s | 10s | 25s |
| CÃ¡lculo de distancias | 5s | 2min | 10min |
| Clustering jerÃ¡rquico | 3s | 45s | 5min |
| GeneraciÃ³n dendrograma | 1s | 3s | 8s |
| **Total** | **11s** | **3min** | **15min** |

## Salidas Generadas

### Archivos
- `outputs/dendrograma.png` - VisualizaciÃ³n del dendrograma
- `outputs/clusters.json` - AsignaciÃ³n de clusters
- `outputs/cluster_analysis.txt` - AnÃ¡lisis de cada cluster
- `outputs/top_terms_per_cluster.csv` - TÃ©rminos representativos

### Datos
```python
# Guardar resultados
import json

resultados = {
    'n_clusters': n_clusters,
    'cluster_labels': cluster_labels.tolist(),
    'cluster_sizes': cluster_sizes,
    'top_terms': top_terms_per_cluster
}

with open('outputs/resultados_clustering.json', 'w') as f:
    json.dump(resultados, f, indent=2)
```

## Aplicaciones

âœ… **OrganizaciÃ³n de literatura**: Agrupar artÃ­culos por tema
âœ… **Descubrimiento de subtemas**: Identificar Ã¡reas emergentes
âœ… **Sistemas de recomendaciÃ³n**: Sugerir artÃ­culos similares
âœ… **AnÃ¡lisis de tendencias**: EvoluciÃ³n de clusters en el tiempo

## PrÃ³ximos Pasos

Los clusters pueden usarse para:
- AnÃ¡lisis temporal de temas
- IdentificaciÃ³n de gaps de investigaciÃ³n
- GeneraciÃ³n de taxonomÃ­as
- Sistemas de navegaciÃ³n temÃ¡tica
