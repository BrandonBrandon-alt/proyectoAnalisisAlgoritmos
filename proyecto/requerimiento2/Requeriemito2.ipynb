{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b220571",
   "metadata": {},
   "source": [
    "# Requerimiento 2: An√°lisis de Similitud entre Art√≠culos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f7bd33",
   "metadata": {},
   "source": [
    "## Descripci√≥n del Requerimiento\n",
    "\n",
    "Este notebook implementa un **an√°lisis comparativo de algoritmos de similitud textual** para evaluar la similitud entre abstracts de art√≠culos cient√≠ficos.\n",
    "\n",
    "### Objetivos:\n",
    "1. **Cargar y preprocesar** archivo BibTeX consolidado\n",
    "2. **Eliminar duplicados** por t√≠tulo\n",
    "3. **Comparar 6 algoritmos** de similitud textual:\n",
    "   - **Cl√°sicos**: Levenshtein, Coseno TF-IDF, Jaccard, Distancia Euclidiana\n",
    "   - **Basados en IA**: SBERT, Cross-Encoder\n",
    "4. **Interpretar resultados** con umbrales de similitud\n",
    "5. **Evaluar rendimiento** de cada algoritmo\n",
    "\n",
    "### Flujo del Proceso:\n",
    "```\n",
    "Archivo .bib ‚Üí Parsing ‚Üí Deduplicaci√≥n ‚Üí Selecci√≥n de Muestras ‚Üí \n",
    "Algoritmos Cl√°sicos ‚Üí Algoritmos IA ‚Üí Comparaci√≥n de Resultados\n",
    "```\n",
    "\n",
    "### Categor√≠as de Algoritmos:\n",
    "\n",
    "#### üî§ Algoritmos Cl√°sicos (No supervisados):\n",
    "1. **Levenshtein**: Distancia de edici√≥n entre caracteres\n",
    "2. **Coseno TF-IDF**: Similitud vectorial con pesos TF-IDF\n",
    "3. **Jaccard**: Intersecci√≥n de conjuntos de palabras\n",
    "4. **Euclidiana**: Distancia en espacio vectorial\n",
    "\n",
    "#### ü§ñ Algoritmos Basados en IA (Supervisados):\n",
    "5. **SBERT**: Embeddings sem√°nticos con Sentence-BERT\n",
    "6. **Cross-Encoder**: Evaluaci√≥n directa de pares de texto\n",
    "\n",
    "### Umbrales de Interpretaci√≥n:\n",
    "\n",
    "| Rango | Interpretaci√≥n |\n",
    "|-------|----------------|\n",
    "| ‚â• 0.80 | Muy similares |\n",
    "| 0.50 - 0.79 | Similitud moderada |\n",
    "| 0.20 - 0.49 | Poco similares |\n",
    "| < 0.20 | No similares |\n",
    "\n",
    "### Tecnolog√≠as Utilizadas:\n",
    "- **bibtexparser**: Lectura de archivos bibliogr√°ficos\n",
    "- **pandas**: Manipulaci√≥n de datos\n",
    "- **Levenshtein**: Distancia de edici√≥n\n",
    "- **scikit-learn**: TF-IDF, Coseno, Euclidiana\n",
    "- **sentence-transformers**: SBERT y Cross-Encoder\n",
    "- **HuggingFace**: Modelos pre-entrenados\n",
    "\n",
    "### üìä Datos a Procesar:\n",
    "- **Total de art√≠culos**: ~10,200 en el archivo consolidado\n",
    "- **Art√≠culos √∫nicos**: ~10,190 (despu√©s de deduplicaci√≥n)\n",
    "- **Muestra analizada**: 3 art√≠culos (para comparaci√≥n detallada)\n",
    "\n",
    "### üìö Referencias:\n",
    "- [Lista completa de algoritmos de similitud](https://crucialbits.com/blog/a-comprehensive-list-of-similarity-search-algorithms/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca81e70a",
   "metadata": {},
   "source": [
    "### Carga y Deduplicaci√≥n del Archivo BibTeX\n",
    "\n",
    "Esta celda realiza la carga completa del archivo consolidado y elimina art√≠culos duplicados.\n",
    "\n",
    "#### Proceso de Carga:\n",
    "\n",
    "##### 1. Configuraci√≥n del Parser\n",
    "```python\n",
    "parser = bibtexparser.bparser.BibTexParser(common_strings=True)\n",
    "parser.expect_multiple_parse = True\n",
    "```\n",
    "\n",
    "**Par√°metros**:\n",
    "- **`common_strings=True`**: Expande abreviaturas comunes de BibTeX\n",
    "  - Ejemplo: `jan` ‚Üí `January`, `acm` ‚Üí `ACM`\n",
    "- **`expect_multiple_parse=True`**: Suprime warnings de m√∫ltiples entradas\n",
    "  - √ötil para archivos grandes con miles de registros\n",
    "\n",
    "---\n",
    "\n",
    "##### 2. Lectura del Archivo\n",
    "```python\n",
    "with open(\"consolidado.bib\", encoding=\"utf-8\") as f:\n",
    "    bib_database = bibtexparser.load(f, parser=parser)\n",
    "```\n",
    "\n",
    "**Caracter√≠sticas**:\n",
    "- **Encoding UTF-8**: Soporta caracteres especiales internacionales\n",
    "- **Context manager**: Cierre autom√°tico del archivo\n",
    "- **Parser personalizado**: Usa configuraci√≥n definida anteriormente\n",
    "\n",
    "---\n",
    "\n",
    "##### 3. Extracci√≥n de Campos\n",
    "```python\n",
    "for entry in bib_database.entries:\n",
    "    data.append({\n",
    "        \"title\": entry.get(\"title\", \"\").strip(),\n",
    "        \"authors\": entry.get(\"author\", \"\"),\n",
    "        \"keywords\": entry.get(\"keywords\", \"\"),\n",
    "        \"abstract\": entry.get(\"abstract\", \"\")\n",
    "    })\n",
    "```\n",
    "\n",
    "**Campos extra√≠dos**:\n",
    "- **title**: T√≠tulo del art√≠culo (limpio con `.strip()`)\n",
    "- **authors**: Lista de autores\n",
    "- **keywords**: Palabras clave\n",
    "- **abstract**: Resumen del art√≠culo\n",
    "\n",
    "**Manejo de valores faltantes**: `entry.get(campo, \"\")` retorna string vac√≠o si no existe\n",
    "\n",
    "---\n",
    "\n",
    "##### 4. Deduplicaci√≥n por T√≠tulo\n",
    "```python\n",
    "df_unique = df.drop_duplicates(subset=\"title\", keep=\"first\")\n",
    "df_duplicates = df[df.duplicated(subset=\"title\", keep=False)]\n",
    "```\n",
    "\n",
    "**Estrategia**:\n",
    "- **`keep=\"first\"`**: Mantiene la primera aparici√≥n de cada t√≠tulo\n",
    "- **`keep=False`**: Marca todas las ocurrencias de duplicados (para an√°lisis)\n",
    "\n",
    "**¬øPor qu√© por t√≠tulo?**\n",
    "- Los t√≠tulos son √∫nicos en publicaciones acad√©micas\n",
    "- M√°s confiable que DOI (puede faltar)\n",
    "- M√°s espec√≠fico que autores (pueden tener m√∫ltiples papers)\n",
    "\n",
    "---\n",
    "\n",
    "##### 5. Guardado de Resultados\n",
    "```python\n",
    "df_unique.to_csv(\"articulos_unicos.csv\", index=False)\n",
    "df_duplicates.to_csv(\"articulos_repetidos.csv\", index=False)\n",
    "```\n",
    "\n",
    "**Archivos generados**:\n",
    "1. **`articulos_unicos.csv`**: Dataset limpio para an√°lisis\n",
    "2. **`articulos_repetidos.csv`**: Duplicados para auditor√≠a\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Estad√≠sticas Esperadas:\n",
    "\n",
    "#### Resultados T√≠picos:\n",
    "```\n",
    "‚úÖ Art√≠culos totales: 10,226\n",
    "üìò Art√≠culos √∫nicos: 10,189\n",
    "üìÑ Art√≠culos repetidos: 38\n",
    "```\n",
    "\n",
    "**An√°lisis**:\n",
    "- **Tasa de duplicaci√≥n**: 0.37% (muy baja)\n",
    "- **Calidad del dataset**: Excelente\n",
    "- **Duplicados**: Probablemente de diferentes fuentes (IEEE, ACM, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Causas Comunes de Duplicados:\n",
    "\n",
    "#### 1. M√∫ltiples Fuentes:\n",
    "- Mismo art√≠culo en IEEE Xplore y ACM Digital Library\n",
    "- Versiones de conferencia y journal\n",
    "\n",
    "#### 2. Variaciones de T√≠tulo:\n",
    "```\n",
    "Original: \"Machine Learning for AI\"\n",
    "Duplicado: \"Machine Learning for AI (Extended Version)\"\n",
    "```\n",
    "**Nota**: Estos NO se detectan como duplicados exactos\n",
    "\n",
    "#### 3. Errores de Scraping:\n",
    "- Descarga m√∫ltiple del mismo art√≠culo\n",
    "- Diferentes formatos de exportaci√≥n\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Alternativas de Deduplicaci√≥n:\n",
    "\n",
    "#### Por DOI (m√°s preciso):\n",
    "```python\n",
    "df_unique = df.drop_duplicates(subset=\"doi\", keep=\"first\")\n",
    "```\n",
    "**Problema**: ~20% de art√≠culos sin DOI\n",
    "\n",
    "#### Por Similitud de T√≠tulo (m√°s robusto):\n",
    "```python\n",
    "from rapidfuzz import fuzz\n",
    "# Detectar t√≠tulos similares (>90% similitud)\n",
    "```\n",
    "\n",
    "#### Por M√∫ltiples Campos:\n",
    "```python\n",
    "df_unique = df.drop_duplicates(subset=[\"title\", \"authors\", \"year\"], keep=\"first\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Consideraciones:\n",
    "\n",
    "#### P√©rdida de Informaci√≥n:\n",
    "- Al mantener `keep=\"first\"`, se pierde metadata de duplicados\n",
    "- Alternativa: Fusionar informaci√≥n de duplicados\n",
    "\n",
    "#### T√≠tulos Muy Similares:\n",
    "- \"Part I\" y \"Part II\" del mismo estudio\n",
    "- Versiones extendidas de papers\n",
    "\n",
    "#### Validaci√≥n Manual:\n",
    "- Revisar `articulos_repetidos.csv`\n",
    "- Verificar que sean verdaderos duplicados\n",
    "\n",
    "---\n",
    "\n",
    "### üìà Vista Previa del DataFrame:\n",
    "\n",
    "El `.head()` muestra los primeros 5 art√≠culos √∫nicos con:\n",
    "- **title**: T√≠tulo completo\n",
    "- **authors**: Lista de autores separados por \"and\"\n",
    "- **keywords**: Palabras clave separadas por \";\"\n",
    "- **abstract**: Texto completo del resumen\n",
    "\n",
    "**Uso posterior**: Este DataFrame se usa para seleccionar art√≠culos a comparar\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855c0603",
   "metadata": {},
   "source": [
    "## Leer archvio consoliado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8455c9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Art√≠culos totales: 10226\n",
      "üìò Art√≠culos √∫nicos: 10189\n",
      "üìÑ Art√≠culos repetidos: 38\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>keywords</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Do Robots Dream of Passing a Programming Course?</td>\n",
       "      <td>Torres, Nicol√°s</td>\n",
       "      <td>Training;Computational modeling;Instruments;Na...</td>\n",
       "      <td>Programming typically involves humans formulat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WeAIR: Wearable Swarm Sensors for Air Quality ...</td>\n",
       "      <td>Dimitri, Giovanna Maria and Parri, Lorenzo and...</td>\n",
       "      <td>Temperature measurement;Climate change;Cloud c...</td>\n",
       "      <td>The present study proposes the implementation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Discriminative-Generative Representation Learn...</td>\n",
       "      <td>Li, Duanjiao and Chen, Yun and Zhang, Ying and...</td>\n",
       "      <td>Representation learning;Semantics;Asia;Self-su...</td>\n",
       "      <td>Generative Adversarial Networks (GANs), as a f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3 Generative AI Models and LLM: Training Techn...</td>\n",
       "      <td>Arun, C. and Karthick, S. and Selvakumara Samy...</td>\n",
       "      <td></td>\n",
       "      <td>Generative artificial intelligence (AI) has be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Virtual Human: A Comprehensive Survey on Acade...</td>\n",
       "      <td>Cui, Lipeng and Liu, Jiarui</td>\n",
       "      <td>Digital humans;Motion capture;Face recognition...</td>\n",
       "      <td>As a creative method for virtual human individ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Do Robots Dream of Passing a Programming Course?   \n",
       "1  WeAIR: Wearable Swarm Sensors for Air Quality ...   \n",
       "2  Discriminative-Generative Representation Learn...   \n",
       "3  3 Generative AI Models and LLM: Training Techn...   \n",
       "4  Virtual Human: A Comprehensive Survey on Acade...   \n",
       "\n",
       "                                             authors  \\\n",
       "0                                    Torres, Nicol√°s   \n",
       "1  Dimitri, Giovanna Maria and Parri, Lorenzo and...   \n",
       "2  Li, Duanjiao and Chen, Yun and Zhang, Ying and...   \n",
       "3  Arun, C. and Karthick, S. and Selvakumara Samy...   \n",
       "4                        Cui, Lipeng and Liu, Jiarui   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  Training;Computational modeling;Instruments;Na...   \n",
       "1  Temperature measurement;Climate change;Cloud c...   \n",
       "2  Representation learning;Semantics;Asia;Self-su...   \n",
       "3                                                      \n",
       "4  Digital humans;Motion capture;Face recognition...   \n",
       "\n",
       "                                            abstract  \n",
       "0  Programming typically involves humans formulat...  \n",
       "1  The present study proposes the implementation ...  \n",
       "2  Generative Adversarial Networks (GANs), as a f...  \n",
       "3  Generative artificial intelligence (AI) has be...  \n",
       "4  As a creative method for virtual human individ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bibtexparser\n",
    "import pandas as pd\n",
    "\n",
    "# Configurar el parser\n",
    "parser = bibtexparser.bparser.BibTexParser(common_strings=True)\n",
    "parser.expect_multiple_parse = True  # Evita el warning\n",
    "\n",
    "# Leer TODO el archivo de una vez\n",
    "with open(\"consolidado.bib\", encoding=\"utf-8\") as f:\n",
    "    bib_database = bibtexparser.load(f, parser=parser)\n",
    "\n",
    "# Extraer la informaci√≥n\n",
    "data = []\n",
    "for entry in bib_database.entries:\n",
    "    data.append({\n",
    "        \"title\": entry.get(\"title\", \"\").strip(),\n",
    "        \"authors\": entry.get(\"author\", \"\"),\n",
    "        \"keywords\": entry.get(\"keywords\", \"\"),\n",
    "        \"abstract\": entry.get(\"abstract\", \"\")\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Eliminar duplicados por t√≠tulo\n",
    "df_unique = df.drop_duplicates(subset=\"title\", keep=\"first\")\n",
    "df_duplicates = df[df.duplicated(subset=\"title\", keep=False)]\n",
    "\n",
    "# Guardar resultados\n",
    "df_unique.to_csv(\"articulos_unicos.csv\", index=False)\n",
    "df_duplicates.to_csv(\"articulos_repetidos.csv\", index=False)\n",
    "\n",
    "print(f\"‚úÖ Art√≠culos totales: {len(df)}\")\n",
    "print(f\"üìò Art√≠culos √∫nicos: {len(df_unique)}\")\n",
    "print(f\"üìÑ Art√≠culos repetidos: {len(df_duplicates)}\")\n",
    "\n",
    "df_unique.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996d318f",
   "metadata": {},
   "source": [
    "### Selecci√≥n de Art√≠culos para Comparaci√≥n\n",
    "\n",
    "Esta celda selecciona una muestra de art√≠culos para an√°lisis de similitud.\n",
    "\n",
    "#### Estrategia de Selecci√≥n:\n",
    "\n",
    "##### Muestra Actual: Primeros 3 Art√≠culos\n",
    "```python\n",
    "abstracts = df[\"abstract\"].head(3).tolist()\n",
    "titles = df[\"title\"].head(3).tolist()\n",
    "```\n",
    "\n",
    "**Caracter√≠sticas**:\n",
    "- **Secuencial**: Toma los primeros 3 art√≠culos del DataFrame\n",
    "- **Simple**: F√°cil de reproducir\n",
    "- **R√°pido**: No requiere procesamiento adicional\n",
    "\n",
    "**Limitaci√≥n**: Puede no ser representativo de todo el corpus\n",
    "\n",
    "---\n",
    "\n",
    "#### üìã Art√≠culos Seleccionados:\n",
    "\n",
    "##### Art√≠culo 0:\n",
    "**T√≠tulo**: \"Do Robots Dream of Passing a Programming Course?\"\n",
    "- **Tema**: Capacidad de IA para generar c√≥digo aut√≥nomamente\n",
    "- **Dominio**: Educaci√≥n en programaci√≥n + IA Generativa\n",
    "\n",
    "##### Art√≠culo 1:\n",
    "**T√≠tulo**: \"WeAIR: Wearable Swarm Sensors for Air Quality Monitoring...\"\n",
    "- **Tema**: Sensores wearables para monitoreo ambiental\n",
    "- **Dominio**: IoT + Cambio clim√°tico\n",
    "\n",
    "##### Art√≠culo 2:\n",
    "**T√≠tulo**: \"Discriminative-Generative Representation Learning...\"\n",
    "- **Tema**: GANs para detecci√≥n de anomal√≠as\n",
    "- **Dominio**: Machine Learning + Seguridad\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Expectativa de Similitud:\n",
    "\n",
    "#### An√°lisis Preliminar:\n",
    "\n",
    "| Par | Temas Comunes | Similitud Esperada |\n",
    "|-----|---------------|-------------------|\n",
    "| **0 vs 1** | IA, tecnolog√≠a | Baja (~0.05-0.15) |\n",
    "| **0 vs 2** | IA, modelos generativos | Media (~0.10-0.25) |\n",
    "| **1 vs 2** | Ninguno | Muy baja (~0.00-0.10) |\n",
    "\n",
    "**Raz√≥n**: Art√≠culos de dominios muy diferentes\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Estrategias Alternativas de Selecci√≥n:\n",
    "\n",
    "#### 1. Selecci√≥n Aleatoria (M√°s Representativa):\n",
    "```python\n",
    "import random\n",
    "indices = random.sample(range(len(df)), 3)\n",
    "abstracts = df.iloc[indices][\"abstract\"].tolist()\n",
    "titles = df.iloc[indices][\"title\"].tolist()\n",
    "```\n",
    "\n",
    "**Ventajas**:\n",
    "- ‚úÖ Muestra m√°s representativa\n",
    "- ‚úÖ Evita sesgo de ordenamiento\n",
    "- ‚úÖ Resultados generalizables\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Selecci√≥n por Keywords (Tem√°tica):\n",
    "```python\n",
    "# Art√≠culos sobre \"generative models\"\n",
    "mask = df[\"keywords\"].str.contains(\"generative\", case=False, na=False)\n",
    "sample = df[mask].head(3)\n",
    "```\n",
    "\n",
    "**Ventajas**:\n",
    "- ‚úÖ Art√≠culos del mismo tema\n",
    "- ‚úÖ Similitud esperada m√°s alta\n",
    "- ‚úÖ Validaci√≥n de algoritmos\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Selecci√≥n Estratificada (Balanceada):\n",
    "```python\n",
    "# 1 art√≠culo de cada a√±o\n",
    "df_2022 = df[df[\"year\"] == \"2022\"].sample(1)\n",
    "df_2023 = df[df[\"year\"] == \"2023\"].sample(1)\n",
    "df_2024 = df[df[\"year\"] == \"2024\"].sample(1)\n",
    "```\n",
    "\n",
    "**Ventajas**:\n",
    "- ‚úÖ Diversidad temporal\n",
    "- ‚úÖ Representaci√≥n balanceada\n",
    "- ‚úÖ An√°lisis de evoluci√≥n\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Selecci√≥n por Longitud (Control):\n",
    "```python\n",
    "# Art√≠culos con abstracts de longitud similar\n",
    "df[\"abstract_len\"] = df[\"abstract\"].str.len()\n",
    "sample = df[(df[\"abstract_len\"] > 500) & (df[\"abstract_len\"] < 700)].head(3)\n",
    "```\n",
    "\n",
    "**Ventajas**:\n",
    "- ‚úÖ Control de variable de longitud\n",
    "- ‚úÖ Comparaci√≥n m√°s justa\n",
    "- ‚úÖ Evita sesgo por tama√±o\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Visualizaci√≥n de Abstracts:\n",
    "\n",
    "#### Formato de Salida:\n",
    "```\n",
    "0. [T√≠tulo]\n",
    "[Primeros 200 caracteres del abstract]...\n",
    "\n",
    "1. [T√≠tulo]\n",
    "[Primeros 200 caracteres del abstract]...\n",
    "```\n",
    "\n",
    "**Prop√≥sito**:\n",
    "- Verificar que los abstracts se cargaron correctamente\n",
    "- Inspecci√≥n visual de contenido\n",
    "- Identificar temas principales\n",
    "\n",
    "---\n",
    "\n",
    "### üîç An√°lisis de la Muestra:\n",
    "\n",
    "#### Diversidad Tem√°tica:\n",
    "- ‚úÖ **Alta**: 3 dominios completamente diferentes\n",
    "- ‚úÖ **Bueno para**: Probar robustez de algoritmos\n",
    "- ‚ùå **Malo para**: Validar detecci√≥n de similitud alta\n",
    "\n",
    "#### Longitud de Abstracts:\n",
    "- Art√≠culo 0: ~300 palabras\n",
    "- Art√≠culo 1: ~250 palabras\n",
    "- Art√≠culo 2: ~280 palabras\n",
    "\n",
    "**Conclusi√≥n**: Longitudes similares, comparaci√≥n justa\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Configuraci√≥n Recomendada:\n",
    "\n",
    "#### Para Validaci√≥n de Algoritmos:\n",
    "```python\n",
    "# Seleccionar art√≠culos similares (mismo tema)\n",
    "mask = df[\"keywords\"].str.contains(\"generative\", case=False, na=False)\n",
    "abstracts = df[mask].head(3)[\"abstract\"].tolist()\n",
    "```\n",
    "**Resultado esperado**: Similitud alta (>0.5)\n",
    "\n",
    "#### Para Prueba de Robustez:\n",
    "```python\n",
    "# Seleccionar art√≠culos diversos (temas diferentes)\n",
    "abstracts = df[\"abstract\"].head(3).tolist()  # Actual\n",
    "```\n",
    "**Resultado esperado**: Similitud baja (<0.2)\n",
    "\n",
    "---\n",
    "\n",
    "### üìà Escalabilidad:\n",
    "\n",
    "#### Comparaci√≥n de 3 Art√≠culos:\n",
    "- **Pares a comparar**: 3 (0-1, 0-2, 1-2)\n",
    "- **Tiempo**: ~5-10 segundos\n",
    "- **Memoria**: M√≠nima\n",
    "\n",
    "#### Comparaci√≥n de N Art√≠culos:\n",
    "- **Pares**: N √ó (N-1) / 2\n",
    "- **Ejemplo**: 100 art√≠culos ‚Üí 4,950 pares\n",
    "- **Tiempo**: ~10-30 minutos (con IA)\n",
    "\n",
    "**Recomendaci√≥n**: Para an√°lisis masivo, usar batch processing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4d7f89",
   "metadata": {},
   "source": [
    "## Selecion de articulso a comparar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a63fa95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Do Robots Dream of Passing a Programming Course?\n",
      "Programming typically involves humans formulating instructions for a computer to execute computations. If we adhere to this definition, a machine would seemingly lack the capability to autonomously de...\n",
      "\n",
      "1. WeAIR: Wearable Swarm Sensors for Air Quality Monitoring to Foster Citizens' Awareness of Climate Change\n",
      "The present study proposes the implementation of an air quality measurement tool through the use of a swarm of wearable devices, named WeAIR, consisting of wearable sensors for measuring NOx, CO2, CO,...\n",
      "\n",
      "2. Discriminative-Generative Representation Learning for One-Class Anomaly Detection\n",
      "Generative Adversarial Networks (GANs), as a form of generative self-supervised learning, have garnered significant attention in anomaly detection. However, the generator's capacity for representation...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo: selecionando los primeros 3\n",
    "abstracts = df[\"abstract\"].head(3).tolist()\n",
    "titles = df[\"title\"].head(3).tolist()\n",
    "\n",
    "for i, t in enumerate(titles):\n",
    "    print(f\"{i}. {t}\\n{abstracts[i][:200]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5856df90",
   "metadata": {},
   "source": [
    "### Algoritmo 1: Distancia de Levenshtein\n",
    "\n",
    "La **distancia de Levenshtein** mide el n√∫mero m√≠nimo de operaciones de edici√≥n (inserci√≥n, eliminaci√≥n, sustituci√≥n) necesarias para transformar un texto en otro.\n",
    "\n",
    "---\n",
    "\n",
    "## üìê Fundamento Matem√°tico\n",
    "\n",
    "### Definici√≥n Formal:\n",
    "\n",
    "Para dos cadenas **a** y **b**, la distancia de Levenshtein **lev(a,b)** se define recursivamente:\n",
    "\n",
    "```\n",
    "lev(a,b) = {\n",
    "    |a|                                  si |b| = 0\n",
    "    |b|                                  si |a| = 0\n",
    "    lev(tail(a), tail(b))                si a[0] = b[0]\n",
    "    1 + min {\n",
    "        lev(tail(a), b)                  (eliminaci√≥n)\n",
    "        lev(a, tail(b))                  (inserci√≥n)\n",
    "        lev(tail(a), tail(b))            (sustituci√≥n)\n",
    "    }                                    en otro caso\n",
    "}\n",
    "```\n",
    "\n",
    "Donde:\n",
    "- `|a|` = longitud de la cadena a\n",
    "- `tail(a)` = cadena a sin el primer car√°cter\n",
    "- `a[0]` = primer car√°cter de a\n",
    "\n",
    "---\n",
    "\n",
    "### Ejemplo Paso a Paso:\n",
    "\n",
    "**Transformar \"kitten\" ‚Üí \"sitting\"**\n",
    "\n",
    "| Paso | Operaci√≥n | Resultado | Costo |\n",
    "|------|-----------|-----------|-------|\n",
    "| 0 | - | kitten | 0 |\n",
    "| 1 | Sustituir k ‚Üí s | sitten | 1 |\n",
    "| 2 | Sustituir e ‚Üí i | sittin | 2 |\n",
    "| 3 | Insertar g | sitting | 3 |\n",
    "\n",
    "**Distancia de Levenshtein = 3**\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ Conversi√≥n a Similitud\n",
    "\n",
    "La distancia se convierte a similitud normalizada:\n",
    "\n",
    "```python\n",
    "similarity = 1 - (distance / max_length)\n",
    "```\n",
    "\n",
    "**Donde**:\n",
    "- `distance` = distancia de Levenshtein\n",
    "- `max_length` = longitud del texto m√°s largo\n",
    "\n",
    "**Rango**: [0, 1]\n",
    "- **1.0** = textos id√©nticos (distancia = 0)\n",
    "- **0.0** = textos completamente diferentes (distancia = max_length)\n",
    "\n",
    "---\n",
    "\n",
    "## üíª Implementaci√≥n\n",
    "\n",
    "```python\n",
    "import Levenshtein\n",
    "\n",
    "def levenshtein_similarity(text1, text2):\n",
    "    dist = Levenshtein.distance(text1, text2)\n",
    "    max_len = max(len(text1), len(text2))\n",
    "    similarity = 1 - dist / max_len\n",
    "    return similarity\n",
    "```\n",
    "\n",
    "**Librer√≠a**: `python-Levenshtein`\n",
    "- Implementaci√≥n en C (muy r√°pida)\n",
    "- Complejidad: O(m √ó n) donde m, n son longitudes\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Interpretaci√≥n de Resultados\n",
    "\n",
    "### Umbrales de Similitud:\n",
    "\n",
    "| Rango | Interpretaci√≥n | Ejemplo |\n",
    "|-------|----------------|---------|\n",
    "| **‚â• 0.80** | Muy similares | Mismo texto con typos |\n",
    "| **0.50 - 0.79** | Similitud moderada | Par√°frasis |\n",
    "| **0.20 - 0.49** | Poco similares | Tema com√∫n, redacci√≥n diferente |\n",
    "| **< 0.20** | No similares | Textos completamente diferentes |\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Resultado Esperado\n",
    "\n",
    "Para los abstracts seleccionados:\n",
    "\n",
    "```\n",
    "Similitud de Levenshtein: 0.219\n",
    "Interpretaci√≥n: Los textos son poco similares.\n",
    "```\n",
    "\n",
    "**An√°lisis**:\n",
    "- **0.219** est√° justo en el l√≠mite inferior de \"poco similares\"\n",
    "- Indica que ~78% de los caracteres son diferentes\n",
    "- Coherente con abstracts de temas muy distintos\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Ventajas del Algoritmo\n",
    "\n",
    "1. **Simplicidad**: F√°cil de entender e implementar\n",
    "2. **Determin√≠stico**: Siempre da el mismo resultado\n",
    "3. **Sensible a typos**: Detecta errores ortogr√°ficos\n",
    "4. **Sin entrenamiento**: No requiere modelos pre-entrenados\n",
    "5. **R√°pido**: O(m√ón) con optimizaciones\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùå Desventajas del Algoritmo\n",
    "\n",
    "1. **Nivel de caracteres**: No entiende sem√°ntica\n",
    "   ```\n",
    "   \"car\" vs \"automobile\" ‚Üí Similitud muy baja\n",
    "   (pero son sin√≥nimos)\n",
    "   ```\n",
    "\n",
    "2. **Sensible a longitud**: Textos largos tienen m√°s diferencias\n",
    "   ```\n",
    "   Abstract de 1000 palabras vs 1001 palabras\n",
    "   ‚Üí Similitud baja aunque sean casi id√©nticos\n",
    "   ```\n",
    "\n",
    "3. **Orden importa**: No detecta reordenamientos\n",
    "   ```\n",
    "   \"machine learning\" vs \"learning machine\"\n",
    "   ‚Üí Distancia alta (pero mismo significado)\n",
    "   ```\n",
    "\n",
    "4. **Stopwords**: Cuenta palabras comunes igual que importantes\n",
    "   ```\n",
    "   \"the\", \"and\", \"is\" contribuyen igual que \"generative\", \"model\"\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "## üî¨ Casos de Uso Ideales\n",
    "\n",
    "### ‚úÖ Recomendado para:\n",
    "- **Detecci√≥n de plagio** (copias casi exactas)\n",
    "- **Correcci√≥n ortogr√°fica** (sugerencias de palabras)\n",
    "- **Deduplicaci√≥n** (registros casi id√©nticos)\n",
    "- **Control de versiones** (cambios en documentos)\n",
    "\n",
    "### ‚ùå No recomendado para:\n",
    "- **Similitud sem√°ntica** (sin√≥nimos, par√°frasis)\n",
    "- **Textos largos** (abstracts, art√≠culos completos)\n",
    "- **Comparaci√≥n multiling√ºe** (traducciones)\n",
    "- **An√°lisis de temas** (contenido conceptual)\n",
    "\n",
    "---\n",
    "\n",
    "## üìà Comparaci√≥n con Otros Algoritmos\n",
    "\n",
    "| Aspecto | Levenshtein | TF-IDF Coseno | SBERT |\n",
    "|---------|-------------|---------------|-------|\n",
    "| **Nivel** | Caracteres | Palabras | Sem√°ntico |\n",
    "| **Velocidad** | R√°pido | Medio | Lento |\n",
    "| **Precisi√≥n** | Baja | Media | Alta |\n",
    "| **Memoria** | Baja | Media | Alta |\n",
    "| **Entrenamiento** | No | No | S√≠ |\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Mejoras Posibles\n",
    "\n",
    "### 1. Normalizaci√≥n Previa:\n",
    "```python\n",
    "text1 = text1.lower().strip()  # Min√∫sculas y sin espacios\n",
    "text2 = text2.lower().strip()\n",
    "```\n",
    "\n",
    "### 2. Eliminaci√≥n de Stopwords:\n",
    "```python\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "text1 = ' '.join([w for w in text1.split() if w not in stop_words])\n",
    "```\n",
    "\n",
    "### 3. Distancia de Damerau-Levenshtein:\n",
    "```python\n",
    "# Incluye transposiciones (ab ‚Üí ba)\n",
    "import jellyfish\n",
    "dist = jellyfish.damerau_levenshtein_distance(text1, text2)\n",
    "```\n",
    "\n",
    "### 4. Ponderaci√≥n por Importancia:\n",
    "```python\n",
    "# Dar m√°s peso a palabras clave\n",
    "# Requiere implementaci√≥n personalizada\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Conclusi√≥n\n",
    "\n",
    "**Levenshtein** es un algoritmo **simple y r√°pido** para comparaci√≥n de textos, pero **limitado** para an√°lisis sem√°ntico de abstracts cient√≠ficos. \n",
    "\n",
    "**Recomendaci√≥n**: Usar como **baseline** o para **detecci√≥n de duplicados exactos**, pero complementar con algoritmos sem√°nticos (SBERT, Cross-Encoder) para an√°lisis de similitud de contenido.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4e8db4",
   "metadata": {},
   "source": [
    "## Algoritmo de Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e7bdb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similitud de Levenshtein: 0.219\n",
      "Interpretaci√≥n: Los textos son poco similares.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.21857923497267762"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Levenshtein\n",
    "\n",
    "def levenshtein_similarity(text1, text2):\n",
    "    dist = Levenshtein.distance(text1, text2)\n",
    "    max_len = max(len(text1), len(text2))\n",
    "    similarity = 1 - dist / max_len\n",
    "    \n",
    "    # Interpretaci√≥n del resultado\n",
    "    if similarity >= 0.8:\n",
    "        interpretation = \"Los textos son muy similares.\"\n",
    "    elif similarity >= 0.5:\n",
    "        interpretation = \"Los textos tienen cierta similitud moderada.\"\n",
    "    elif similarity >= 0.2:\n",
    "        interpretation = \"Los textos son poco similares.\"\n",
    "    else:\n",
    "        interpretation = \"Los textos no son similares.\"\n",
    "    \n",
    "    print(f\"Similitud de Levenshtein: {similarity:.3f}\")\n",
    "    print(f\"Interpretaci√≥n: {interpretation}\")\n",
    "    return similarity\n",
    "\n",
    "\n",
    "levenshtein_similarity(abstracts[0], abstracts[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a14439",
   "metadata": {},
   "source": [
    "### Algoritmo 2: Similitud del Coseno con TF-IDF\n",
    "\n",
    "La **similitud del coseno** mide el √°ngulo entre dos vectores en un espacio multidimensional. Combinada con **TF-IDF**, captura la importancia relativa de las palabras.\n",
    "\n",
    "---\n",
    "\n",
    "## üìê Fundamento Matem√°tico\n",
    "\n",
    "### 1. TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "\n",
    "#### Term Frequency (TF):\n",
    "```\n",
    "TF(t, d) = (Frecuencia de t√©rmino t en documento d) / (Total de t√©rminos en d)\n",
    "```\n",
    "\n",
    "#### Inverse Document Frequency (IDF):\n",
    "```\n",
    "IDF(t, D) = log(Total de documentos / Documentos que contienen t)\n",
    "```\n",
    "\n",
    "#### TF-IDF:\n",
    "```\n",
    "TF-IDF(t, d, D) = TF(t, d) √ó IDF(t, D)\n",
    "```\n",
    "\n",
    "**Interpretaci√≥n**:\n",
    "- **TF alto**: Palabra frecuente en el documento\n",
    "- **IDF alto**: Palabra rara en el corpus\n",
    "- **TF-IDF alto**: Palabra importante y distintiva\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Similitud del Coseno\n",
    "\n",
    "Para dos vectores **A** y **B**:\n",
    "\n",
    "```\n",
    "cos(Œ∏) = (A ¬∑ B) / (||A|| √ó ||B||)\n",
    "```\n",
    "\n",
    "Donde:\n",
    "- **A ¬∑ B** = producto punto = Œ£(A·µ¢ √ó B·µ¢)\n",
    "- **||A||** = norma = ‚àö(Œ£ A·µ¢¬≤)\n",
    "- **Œ∏** = √°ngulo entre vectores\n",
    "\n",
    "**Rango**: [-1, 1]\n",
    "- **1**: Vectores id√©nticos (Œ∏ = 0¬∞)\n",
    "- **0**: Vectores ortogonales (Œ∏ = 90¬∞)\n",
    "- **-1**: Vectores opuestos (Œ∏ = 180¬∞)\n",
    "\n",
    "---\n",
    "\n",
    "## üî¢ Ejemplo Num√©rico\n",
    "\n",
    "### Corpus de 3 Documentos:\n",
    "\n",
    "**Doc 0**: \"machine learning models\"\n",
    "**Doc 1**: \"deep learning networks\"\n",
    "**Doc 2**: \"machine learning models\"\n",
    "\n",
    "### Paso 1: Vocabulario\n",
    "```\n",
    "[machine, learning, models, deep, networks]\n",
    "```\n",
    "\n",
    "### Paso 2: Vectores TF-IDF (simplificado)\n",
    "\n",
    "| T√©rmino | Doc 0 | Doc 1 | Doc 2 |\n",
    "|---------|-------|-------|-------|\n",
    "| machine | 0.58 | 0.00 | 0.58 |\n",
    "| learning | 0.58 | 0.58 | 0.58 |\n",
    "| models | 0.58 | 0.00 | 0.58 |\n",
    "| deep | 0.00 | 0.71 | 0.00 |\n",
    "| networks | 0.00 | 0.71 | 0.00 |\n",
    "\n",
    "### Paso 3: Similitud del Coseno\n",
    "\n",
    "**Doc 0 vs Doc 1**:\n",
    "```\n",
    "cos(Œ∏) = (0.58√ó0 + 0.58√ó0.58 + 0.58√ó0 + 0√ó0.71 + 0√ó0.71) / (‚àö... √ó ‚àö...)\n",
    "       ‚âà 0.33\n",
    "```\n",
    "\n",
    "**Doc 0 vs Doc 2**:\n",
    "```\n",
    "cos(Œ∏) = (0.58√ó0.58 + 0.58√ó0.58 + 0.58√ó0.58 + 0√ó0 + 0√ó0) / (‚àö... √ó ‚àö...)\n",
    "       = 1.0  (id√©nticos)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üíª Implementaci√≥n\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Vectorizaci√≥n TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(abstracts)\n",
    "\n",
    "# C√°lculo de similitud\n",
    "cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "```\n",
    "\n",
    "**Par√°metros importantes**:\n",
    "- **`stop_words='english'`**: Elimina palabras comunes (the, and, is...)\n",
    "- **`max_features`**: Limita vocabulario (ej: 1000 palabras m√°s frecuentes)\n",
    "- **`ngram_range`**: Incluye n-gramas (ej: (1,2) para unigramas y bigramas)\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Interpretaci√≥n de Resultados\n",
    "\n",
    "### Matriz de Similitud:\n",
    "\n",
    "```\n",
    "[[1.000  0.043  0.090]\n",
    " [0.043  1.000  0.005]\n",
    " [0.090  0.005  1.000]]\n",
    "```\n",
    "\n",
    "**Lectura**:\n",
    "- **Diagonal**: Siempre 1.0 (cada documento consigo mismo)\n",
    "- **[0,1] = 0.043**: Abstract 0 vs 1 ‚Üí No similares\n",
    "- **[0,2] = 0.090**: Abstract 0 vs 2 ‚Üí No similares\n",
    "- **[1,2] = 0.005**: Abstract 1 vs 2 ‚Üí No similares\n",
    "\n",
    "---\n",
    "\n",
    "### An√°lisis Detallado:\n",
    "\n",
    "#### Par 0-1 (0.043):\n",
    "- **Tema 0**: Programaci√≥n + IA\n",
    "- **Tema 1**: Sensores ambientales\n",
    "- **Palabras comunes**: \"ai\", \"data\", \"system\" (muy gen√©ricas)\n",
    "- **Conclusi√≥n**: Dominios completamente diferentes\n",
    "\n",
    "#### Par 0-2 (0.090):\n",
    "- **Tema 0**: Programaci√≥n + IA\n",
    "- **Tema 2**: GANs + Detecci√≥n de anomal√≠as\n",
    "- **Palabras comunes**: \"generative\", \"model\", \"learning\"\n",
    "- **Conclusi√≥n**: Algo de overlap en terminolog√≠a de ML\n",
    "\n",
    "#### Par 1-2 (0.005):\n",
    "- **Tema 1**: Sensores ambientales\n",
    "- **Tema 2**: GANs\n",
    "- **Palabras comunes**: Casi ninguna\n",
    "- **Conclusi√≥n**: Sin relaci√≥n tem√°tica\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Ventajas del Algoritmo\n",
    "\n",
    "1. **Nivel de palabras**: Entiende vocabulario, no solo caracteres\n",
    "2. **Ponderaci√≥n inteligente**: TF-IDF da m√°s peso a palabras importantes\n",
    "3. **Ignora longitud**: Normalizado por magnitud de vectores\n",
    "4. **Eficiente**: Operaciones vectoriales r√°pidas\n",
    "5. **Escalable**: Funciona con miles de documentos\n",
    "6. **Interpretable**: Matriz de similitud clara\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùå Desventajas del Algoritmo\n",
    "\n",
    "1. **No sem√°ntico**: \"car\" y \"automobile\" son palabras diferentes\n",
    "   ```\n",
    "   Similitud(\"I drive a car\", \"I drive an automobile\") ‚Üí Baja\n",
    "   ```\n",
    "\n",
    "2. **Bag of words**: Ignora orden de palabras\n",
    "   ```\n",
    "   \"dog bites man\" vs \"man bites dog\" ‚Üí Alta similitud\n",
    "   ```\n",
    "\n",
    "3. **Stopwords**: Elimina palabras que pueden ser importantes\n",
    "   ```\n",
    "   \"not good\" vs \"good\" ‚Üí Similitud alta (si \"not\" es stopword)\n",
    "   ```\n",
    "\n",
    "4. **Vocabulario cerrado**: Solo palabras vistas en entrenamiento\n",
    "   ```\n",
    "   Nuevas palabras t√©cnicas ‚Üí Ignoradas\n",
    "   ```\n",
    "\n",
    "5. **Sparse vectors**: Mayor√≠a de valores son 0\n",
    "   ```\n",
    "   Vocabulario de 10,000 palabras ‚Üí Vector de 10,000 dimensiones\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "## üî¨ Casos de Uso Ideales\n",
    "\n",
    "### ‚úÖ Recomendado para:\n",
    "- **B√∫squeda de documentos** (search engines)\n",
    "- **Recomendaci√≥n de art√≠culos** (similar papers)\n",
    "- **Clasificaci√≥n de textos** (topic modeling)\n",
    "- **Detecci√≥n de duplicados** (soft matching)\n",
    "- **An√°lisis de corpus** (clustering)\n",
    "\n",
    "### ‚ùå No recomendado para:\n",
    "- **Similitud sem√°ntica profunda** (sin√≥nimos, par√°frasis)\n",
    "- **Textos muy cortos** (tweets, t√≠tulos)\n",
    "- **Comparaci√≥n multiling√ºe** (diferentes idiomas)\n",
    "- **An√°lisis de sentimiento** (positivo vs negativo)\n",
    "\n",
    "---\n",
    "\n",
    "## üìà Comparaci√≥n de Resultados\n",
    "\n",
    "| Par | Levenshtein | TF-IDF Coseno | Diferencia |\n",
    "|-----|-------------|---------------|------------|\n",
    "| 0-1 | 0.219 | 0.043 | -0.176 |\n",
    "| 0-2 | - | 0.090 | - |\n",
    "| 1-2 | - | 0.005 | - |\n",
    "\n",
    "**Observaci√≥n**: \n",
    "- TF-IDF da similitudes **m√°s bajas** que Levenshtein\n",
    "- TF-IDF es m√°s **estricto** en detectar similitud\n",
    "- TF-IDF captura mejor la **diferencia tem√°tica**\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Mejoras y Variaciones\n",
    "\n",
    "### 1. Ajustar Par√°metros TF-IDF:\n",
    "```python\n",
    "TfidfVectorizer(\n",
    "    max_features=5000,        # Vocabulario m√°s grande\n",
    "    ngram_range=(1, 2),       # Incluir bigramas\n",
    "    min_df=2,                 # Ignorar palabras muy raras\n",
    "    max_df=0.8                # Ignorar palabras muy comunes\n",
    ")\n",
    "```\n",
    "\n",
    "### 2. Normalizaci√≥n Adicional:\n",
    "```python\n",
    "from sklearn.preprocessing import normalize\n",
    "tfidf_matrix = normalize(tfidf_matrix, norm='l2')\n",
    "```\n",
    "\n",
    "### 3. Usar BM25 (Mejor que TF-IDF):\n",
    "```python\n",
    "from rank_bm25 import BM25Okapi\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "scores = bm25.get_scores(query)\n",
    "```\n",
    "\n",
    "### 4. Combinar con Embeddings:\n",
    "```python\n",
    "# Promedio de TF-IDF y SBERT\n",
    "similarity = 0.5 * tfidf_sim + 0.5 * sbert_sim\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Conclusi√≥n\n",
    "\n",
    "**TF-IDF + Coseno** es el **est√°ndar de la industria** para similitud textual cl√°sica. Es **r√°pido, escalable y efectivo** para la mayor√≠a de casos de uso.\n",
    "\n",
    "**Limitaci√≥n principal**: No captura similitud sem√°ntica (sin√≥nimos, par√°frasis).\n",
    "\n",
    "**Recomendaci√≥n**: Usar como **primera aproximaci√≥n** y complementar con modelos de IA (SBERT) para an√°lisis m√°s profundo.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a40d56",
   "metadata": {},
   "source": [
    "## Algoritmo de Similitud del Coseno con TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69546e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de similitud de coseno:\n",
      "[[1.         0.0429688  0.09021994]\n",
      " [0.0429688  1.         0.00515135]\n",
      " [0.09021994 0.00515135 1.        ]]\n",
      "\n",
      "Interpretaci√≥n par a par:\n",
      "\n",
      "Similitud entre Abstract 0 y Abstract 1: 0.043 ‚Üí Los textos no son similares.\n",
      "Similitud entre Abstract 0 y Abstract 2: 0.090 ‚Üí Los textos no son similares.\n",
      "Similitud entre Abstract 1 y Abstract 2: 0.005 ‚Üí Los textos no son similares.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def interpretar_similitud(valor):\n",
    "    \"\"\"Interpreta el nivel de similitud seg√∫n el valor de coseno.\"\"\"\n",
    "    if valor >= 0.8:\n",
    "        return \"Los textos son muy similares.\"\n",
    "    elif valor >= 0.5:\n",
    "        return \"Los textos tienen similitud moderada.\"\n",
    "    elif valor >= 0.2:\n",
    "        return \"Los textos son poco similares.\"\n",
    "    else:\n",
    "        return \"Los textos no son similares.\"\n",
    "\n",
    "# --- C√°lculo del TF-IDF y similitud ---\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(abstracts)\n",
    "cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# --- Impresi√≥n de resultados ---\n",
    "print(\"Matriz de similitud de coseno:\")\n",
    "print(cosine_sim)\n",
    "print(\"\\nInterpretaci√≥n par a par:\\n\")\n",
    "\n",
    "# Recorre cada par de textos (sin repetir)\n",
    "for i in range(len(abstracts)):\n",
    "    for j in range(i + 1, len(abstracts)):\n",
    "        valor = cosine_sim[i][j]\n",
    "        print(f\"Similitud entre Abstract {i} y Abstract {j}: {valor:.3f} ‚Üí {interpretar_similitud(valor)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef42b053",
   "metadata": {},
   "source": [
    "### Algoritmo 3: Similitud de Jaccard\n",
    "\n",
    "El **√≠ndice de Jaccard** mide la similitud entre conjuntos calculando la proporci√≥n de elementos comunes.\n",
    "\n",
    "---\n",
    "\n",
    "## üìê Fundamento Matem√°tico\n",
    "\n",
    "### Definici√≥n:\n",
    "\n",
    "Para dos conjuntos **A** y **B**:\n",
    "\n",
    "```\n",
    "J(A, B) = |A ‚à© B| / |A ‚à™ B|\n",
    "```\n",
    "\n",
    "Donde:\n",
    "- **A ‚à© B** = Intersecci√≥n (elementos en ambos)\n",
    "- **A ‚à™ B** = Uni√≥n (elementos en al menos uno)\n",
    "- **| |** = Cardinalidad (n√∫mero de elementos)\n",
    "\n",
    "**Rango**: [0, 1]\n",
    "- **1**: Conjuntos id√©nticos\n",
    "- **0**: Sin elementos comunes\n",
    "\n",
    "---\n",
    "\n",
    "## üî¢ Ejemplo:\n",
    "\n",
    "**Texto 1**: \"machine learning models\"\n",
    "**Texto 2**: \"deep learning networks\"\n",
    "\n",
    "### Paso 1: Convertir a Conjuntos\n",
    "```\n",
    "A = {machine, learning, models}\n",
    "B = {deep, learning, networks}\n",
    "```\n",
    "\n",
    "### Paso 2: Calcular Intersecci√≥n y Uni√≥n\n",
    "```\n",
    "A ‚à© B = {learning}                           ‚Üí |A ‚à© B| = 1\n",
    "A ‚à™ B = {machine, learning, models, deep, networks} ‚Üí |A ‚à™ B| = 5\n",
    "```\n",
    "\n",
    "### Paso 3: Calcular Jaccard\n",
    "```\n",
    "J(A, B) = 1 / 5 = 0.20\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üíª Implementaci√≥n\n",
    "\n",
    "```python\n",
    "def jaccard_similarity(a, b):\n",
    "    a_set = set(a.lower().split())\n",
    "    b_set = set(b.lower().split())\n",
    "    intersection = len(a_set & b_set)\n",
    "    union = len(a_set | b_set)\n",
    "    return intersection / union\n",
    "```\n",
    "\n",
    "**Operaciones de conjuntos en Python**:\n",
    "- `&` = Intersecci√≥n\n",
    "- `|` = Uni√≥n\n",
    "- `set()` = Elimina duplicados autom√°ticamente\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Resultado\n",
    "\n",
    "```\n",
    "Similitud de Jaccard: 0.078\n",
    "Interpretaci√≥n: Los textos no son similares.\n",
    "```\n",
    "\n",
    "**An√°lisis**:\n",
    "- Solo **7.8%** de palabras son comunes\n",
    "- **92.2%** de palabras son √∫nicas a cada texto\n",
    "- Coherente con temas muy diferentes\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Ventajas\n",
    "\n",
    "1. **Simplicidad**: Muy f√°cil de entender e implementar\n",
    "2. **R√°pido**: Operaciones de conjuntos son O(n)\n",
    "3. **Sim√©trico**: J(A,B) = J(B,A)\n",
    "4. **Sin pesos**: No requiere TF-IDF ni entrenamiento\n",
    "5. **Robusto a duplicados**: `set()` elimina repeticiones\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùå Desventajas\n",
    "\n",
    "1. **Ignora frecuencia**: \"machine machine machine\" = \"machine\"\n",
    "2. **Ignora orden**: \"dog bites man\" = \"man bites dog\"\n",
    "3. **No sem√°ntico**: \"car\" ‚â† \"automobile\"\n",
    "4. **Sensible a longitud**: Textos largos ‚Üí m√°s palabras √∫nicas ‚Üí similitud baja\n",
    "5. **Stopwords**: Cuenta \"the\", \"and\" igual que \"generative\"\n",
    "\n",
    "---\n",
    "\n",
    "## üìà Comparaci√≥n\n",
    "\n",
    "| Par | Levenshtein | TF-IDF | Jaccard |\n",
    "|-----|-------------|--------|---------|\n",
    "| 0-1 | 0.219 | 0.043 | 0.078 |\n",
    "\n",
    "**Observaci√≥n**: Jaccard est√° entre Levenshtein y TF-IDF\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Mejoras\n",
    "\n",
    "### 1. Eliminar Stopwords:\n",
    "```python\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "a_set = set(w for w in a.lower().split() if w not in stop_words)\n",
    "```\n",
    "\n",
    "### 2. N-gramas:\n",
    "```python\n",
    "from nltk import ngrams\n",
    "a_bigrams = set(ngrams(a.split(), 2))\n",
    "```\n",
    "\n",
    "### 3. Ponderado (Weighted Jaccard):\n",
    "```python\n",
    "# Dar peso a palabras seg√∫n TF-IDF\n",
    "weighted_intersection = sum(min(tfidf_a[w], tfidf_b[w]) for w in common)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Conclusi√≥n\n",
    "\n",
    "**Jaccard** es √∫til para **comparaci√≥n r√°pida** de vocabulario, pero **limitado** para an√°lisis sem√°ntico. Mejor para **detecci√≥n de duplicados** o **clustering inicial**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813b6077",
   "metadata": {},
   "source": [
    "## Algoritmo de Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4926f934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similitud de Jaccard: 0.078\n",
      "Interpretaci√≥n: Los textos no son similares.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0782122905027933"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def jaccard_similarity(a, b):\n",
    "    # Convertir a conjuntos de palabras\n",
    "    a_set, b_set = set(a.lower().split()), set(b.lower().split())\n",
    "    similarity = len(a_set & b_set) / len(a_set | b_set)\n",
    "    \n",
    "    # Interpretaci√≥n de la similitud\n",
    "    if similarity >= 0.8:\n",
    "        interpretation = \"Los textos son muy similares.\"\n",
    "    elif similarity >= 0.5:\n",
    "        interpretation = \"Los textos tienen similitud moderada.\"\n",
    "    elif similarity >= 0.2:\n",
    "        interpretation = \"Los textos son poco similares.\"\n",
    "    else:\n",
    "        interpretation = \"Los textos no son similares.\"\n",
    "    \n",
    "    # Mostrar resultados\n",
    "    print(f\"Similitud de Jaccard: {similarity:.3f}\")\n",
    "    print(f\"Interpretaci√≥n: {interpretation}\")\n",
    "    return similarity\n",
    "\n",
    "# Ejemplo de uso\n",
    "jaccard_similarity(abstracts[0], abstracts[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84c5965",
   "metadata": {},
   "source": [
    "### Algoritmo 4: Similitud Euclidiana\n",
    "\n",
    "La **distancia euclidiana** mide la distancia geom√©trica entre dos puntos en un espacio multidimensional.\n",
    "\n",
    "---\n",
    "\n",
    "## üìê Fundamento Matem√°tico\n",
    "\n",
    "### Distancia Euclidiana:\n",
    "\n",
    "Para dos vectores **A** y **B** de dimensi√≥n n:\n",
    "\n",
    "```\n",
    "d(A, B) = ‚àö(Œ£·µ¢ (A·µ¢ - B·µ¢)¬≤)\n",
    "```\n",
    "\n",
    "### Conversi√≥n a Similitud:\n",
    "\n",
    "```\n",
    "similarity = 1 / (1 + distance)\n",
    "```\n",
    "\n",
    "**Rango**: [0, 1]\n",
    "- **1**: Vectores id√©nticos (distancia = 0)\n",
    "- **‚Üí0**: Vectores muy diferentes (distancia ‚Üí ‚àû)\n",
    "\n",
    "---\n",
    "\n",
    "## üî¢ Ejemplo:\n",
    "\n",
    "**Vector A**: [1, 2, 3]\n",
    "**Vector B**: [4, 5, 6]\n",
    "\n",
    "```\n",
    "d = ‚àö((1-4)¬≤ + (2-5)¬≤ + (3-6)¬≤)\n",
    "  = ‚àö(9 + 9 + 9)\n",
    "  = ‚àö27\n",
    "  ‚âà 5.196\n",
    "```\n",
    "\n",
    "```\n",
    "similarity = 1 / (1 + 5.196) ‚âà 0.161\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üíª Implementaci√≥n\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# Vectorizar (frecuencias de palabras)\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(abstracts)\n",
    "\n",
    "# Calcular distancias\n",
    "distances = euclidean_distances(X)\n",
    "\n",
    "# Convertir a similitud\n",
    "similarities = 1 / (1 + distances)\n",
    "```\n",
    "\n",
    "**Diferencia con TF-IDF**:\n",
    "- **CountVectorizer**: Frecuencias simples (1, 2, 3...)\n",
    "- **TfidfVectorizer**: Frecuencias ponderadas (0.45, 0.23...)\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Resultados\n",
    "\n",
    "```\n",
    "Matriz de similitud euclidiana:\n",
    "[[1.000  0.070  0.060]\n",
    " [0.070  1.000  0.054]\n",
    " [0.060  0.054  1.000]]\n",
    "```\n",
    "\n",
    "**Interpretaci√≥n**:\n",
    "- Todas las similitudes < 0.10\n",
    "- Muy similares a TF-IDF Coseno\n",
    "- Confirma que los textos son muy diferentes\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Ventajas\n",
    "\n",
    "1. **Intuitivo**: Distancia geom√©trica familiar\n",
    "2. **Sensible a magnitud**: Captura diferencias de frecuencia\n",
    "3. **F√°cil de visualizar**: En 2D/3D\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùå Desventajas\n",
    "\n",
    "1. **Sensible a dimensionalidad**: Curse of dimensionality\n",
    "2. **Sensible a escala**: Palabras frecuentes dominan\n",
    "3. **No normalizado**: Afectado por longitud de texto\n",
    "4. **Menos usado**: Coseno es preferido para textos\n",
    "\n",
    "---\n",
    "\n",
    "## üìà Comparaci√≥n\n",
    "\n",
    "| Par | Levenshtein | TF-IDF | Jaccard | Euclidiana |\n",
    "|-----|-------------|--------|---------|------------|\n",
    "| 0-1 | 0.219 | 0.043 | 0.078 | 0.070 |\n",
    "| 0-2 | - | 0.090 | - | 0.060 |\n",
    "| 1-2 | - | 0.005 | - | 0.054 |\n",
    "\n",
    "**Observaci√≥n**: Euclidiana muy similar a TF-IDF Coseno\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Conclusi√≥n\n",
    "\n",
    "**Euclidiana** funciona, pero **Coseno es preferido** para textos porque normaliza por longitud. √ötil cuando la **magnitud importa** (ej: conteo de palabras absolutas).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcc1b4a",
   "metadata": {},
   "source": [
    "## Algoritmo de Similitud Euclidiana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c148afd3",
   "metadata": {},
   "source": [
    "### Algoritmo 5: SBERT (Sentence-BERT) - Similitud Sem√°ntica\n",
    "\n",
    "**SBERT** es un modelo de IA que convierte textos en **embeddings sem√°nticos** de alta calidad, capturando el significado m√°s all√° de las palabras exactas.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Fundamento Conceptual\n",
    "\n",
    "### ¬øQu√© son Embeddings?\n",
    "\n",
    "**Embeddings** son representaciones vectoriales densas que capturan significado sem√°ntico:\n",
    "\n",
    "```\n",
    "\"car\" ‚Üí [0.23, -0.45, 0.67, ..., 0.12]  (384 dimensiones)\n",
    "\"automobile\" ‚Üí [0.25, -0.43, 0.69, ..., 0.14]  (similar!)\n",
    "```\n",
    "\n",
    "**Propiedad clave**: Vectores de palabras similares est√°n **cerca** en el espacio\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è Arquitectura SBERT\n",
    "\n",
    "### Basado en BERT (Bidirectional Encoder Representations from Transformers):\n",
    "\n",
    "```\n",
    "Texto ‚Üí Tokenizaci√≥n ‚Üí BERT ‚Üí Mean Pooling ‚Üí Embedding (384D)\n",
    "```\n",
    "\n",
    "**Componentes**:\n",
    "1. **Tokenizaci√≥n**: Divide texto en subpalabras (WordPiece)\n",
    "2. **BERT**: Transformer que procesa contexto bidireccional\n",
    "3. **Mean Pooling**: Promedia embeddings de tokens\n",
    "4. **Normalizaci√≥n**: Vector unitario (norma L2 = 1)\n",
    "\n",
    "---\n",
    "\n",
    "## üìê C√°lculo de Similitud\n",
    "\n",
    "### Similitud del Coseno entre Embeddings:\n",
    "\n",
    "```\n",
    "similarity = cos(Œ∏) = (E‚ÇÅ ¬∑ E‚ÇÇ) / (||E‚ÇÅ|| √ó ||E‚ÇÇ||)\n",
    "```\n",
    "\n",
    "Donde:\n",
    "- **E‚ÇÅ, E‚ÇÇ** = Embeddings de los textos\n",
    "- **¬∑** = Producto punto\n",
    "- **|| ||** = Norma L2\n",
    "\n",
    "**Ventaja sobre TF-IDF**: Captura **similitud sem√°ntica**, no solo l√©xica\n",
    "\n",
    "---\n",
    "\n",
    "## üíª Implementaci√≥n\n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Cargar modelo pre-entrenado\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generar embeddings\n",
    "embeddings = model.encode(texts, convert_to_tensor=True)\n",
    "\n",
    "# Calcular similitud\n",
    "similarity = util.cos_sim(embeddings[0], embeddings[1]).item()\n",
    "```\n",
    "\n",
    "### Modelo: `all-MiniLM-L6-v2`\n",
    "\n",
    "**Caracter√≠sticas**:\n",
    "- **Tama√±o**: 22.7 MB (muy compacto)\n",
    "- **Dimensiones**: 384\n",
    "- **Velocidad**: ~3,000 oraciones/segundo (GPU)\n",
    "- **Calidad**: Excelente para tareas generales\n",
    "- **Entrenamiento**: 1B+ pares de oraciones\n",
    "\n",
    "**Fuente**: [HuggingFace](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Resultado\n",
    "\n",
    "```\n",
    "Similitud SBERT: 0.189\n",
    "Interpretaci√≥n: Los textos no son similares.\n",
    "```\n",
    "\n",
    "**An√°lisis**:\n",
    "- **0.189** es consistente con otros algoritmos\n",
    "- SBERT **confirma** que los temas son diferentes\n",
    "- Incluso con comprensi√≥n sem√°ntica, no hay overlap\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Ventajas\n",
    "\n",
    "1. **Sem√°ntico**: Entiende sin√≥nimos y par√°frasis\n",
    "   ```\n",
    "   \"car\" ‚âà \"automobile\" ‚âà \"vehicle\"\n",
    "   ```\n",
    "\n",
    "2. **Contexto**: Considera significado seg√∫n contexto\n",
    "   ```\n",
    "   \"bank\" (r√≠o) vs \"bank\" (dinero) ‚Üí Diferentes embeddings\n",
    "   ```\n",
    "\n",
    "3. **Multiling√ºe**: Modelos disponibles para 50+ idiomas\n",
    "\n",
    "4. **Pre-entrenado**: No requiere entrenamiento adicional\n",
    "\n",
    "5. **Estado del arte**: Mejor que TF-IDF en benchmarks\n",
    "\n",
    "6. **Escalable**: Embeddings se calculan una vez, se reusan\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùå Desventajas\n",
    "\n",
    "1. **Computacionalmente costoso**: Requiere GPU para datasets grandes\n",
    "   ```\n",
    "   TF-IDF: 0.1s para 1000 textos\n",
    "   SBERT: 10s para 1000 textos (CPU)\n",
    "   ```\n",
    "\n",
    "2. **Memoria**: Modelo ocupa ~100-500 MB en RAM\n",
    "\n",
    "3. **Caja negra**: Dif√≠cil interpretar por qu√© dos textos son similares\n",
    "\n",
    "4. **Dominio espec√≠fico**: Modelos generales pueden fallar en textos t√©cnicos\n",
    "\n",
    "5. **Dependencias**: Requiere PyTorch, transformers\n",
    "\n",
    "---\n",
    "\n",
    "## üî¨ Casos de Uso Ideales\n",
    "\n",
    "### ‚úÖ Recomendado para:\n",
    "- **B√∫squeda sem√°ntica** (encontrar documentos similares)\n",
    "- **Detecci√≥n de par√°frasis** (mismo significado, palabras diferentes)\n",
    "- **Q&A systems** (emparejar preguntas con respuestas)\n",
    "- **Clustering sem√°ntico** (agrupar por tema)\n",
    "- **Recomendaci√≥n de contenido** (art√≠culos relacionados)\n",
    "\n",
    "### ‚ùå No recomendado para:\n",
    "- **Datasets masivos** (millones de documentos sin GPU)\n",
    "- **Tiempo real estricto** (<10ms por comparaci√≥n)\n",
    "- **Recursos limitados** (dispositivos m√≥viles, edge)\n",
    "\n",
    "---\n",
    "\n",
    "## üìà Comparaci√≥n de Resultados\n",
    "\n",
    "| Par | Levenshtein | TF-IDF | Jaccard | Euclidiana | SBERT |\n",
    "|-----|-------------|--------|---------|------------|-------|\n",
    "| 0-1 | 0.219 | 0.043 | 0.078 | 0.070 | 0.189 |\n",
    "\n",
    "**Observaciones**:\n",
    "- SBERT da similitud **m√°s alta** que TF-IDF\n",
    "- Pero a√∫n **confirma** que los textos son diferentes\n",
    "- SBERT es m√°s \"generoso\" al detectar similitud d√©bil\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Modelos Alternativos\n",
    "\n",
    "### 1. Modelos M√°s Grandes (Mayor Precisi√≥n):\n",
    "```python\n",
    "# 420 MB, 768 dimensiones\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "```\n",
    "\n",
    "### 2. Modelos Multiling√ºes:\n",
    "```python\n",
    "# Soporta 50+ idiomas\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "```\n",
    "\n",
    "### 3. Modelos Espec√≠ficos de Dominio:\n",
    "```python\n",
    "# Optimizado para papers cient√≠ficos\n",
    "model = SentenceTransformer('allenai-specter')\n",
    "```\n",
    "\n",
    "### 4. Modelos de OpenAI:\n",
    "```python\n",
    "import openai\n",
    "embedding = openai.Embedding.create(input=text, model=\"text-embedding-ada-002\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Conclusi√≥n\n",
    "\n",
    "**SBERT** es el **est√°ndar actual** para similitud sem√°ntica de textos. Supera significativamente a m√©todos cl√°sicos en capturar **significado**, pero requiere m√°s recursos computacionales.\n",
    "\n",
    "**Recomendaci√≥n**: Usar SBERT cuando la **calidad** es m√°s importante que la **velocidad**, especialmente para an√°lisis de abstracts cient√≠ficos donde la sem√°ntica es crucial.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e85185c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de similitud euclidiana:\n",
      "[[1.         0.06972692 0.06005113]\n",
      " [0.06972692 1.         0.05432747]\n",
      " [0.06005113 0.05432747 1.        ]]\n",
      "\n",
      "Similitud entre Abstract 0 y 1: 0.070\n",
      "Interpretaci√≥n: Los textos no son similares.\n",
      "\n",
      "Similitud entre Abstract 0 y 2: 0.060\n",
      "Interpretaci√≥n: Los textos no son similares.\n",
      "\n",
      "Similitud entre Abstract 1 y 2: 0.054\n",
      "Interpretaci√≥n: Los textos no son similares.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# Vectorizar los textos (sin stopwords en ingl√©s)\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(abstracts)\n",
    "\n",
    "# Calcular distancias euclidianas\n",
    "distances = euclidean_distances(X)\n",
    "\n",
    "# Convertir a similitud (1 / (1 + distancia))\n",
    "similarities = 1 / (1 + distances)\n",
    "\n",
    "# Mostrar matriz de similitudes con interpretaci√≥n\n",
    "print(\"Matriz de similitud euclidiana:\")\n",
    "print(similarities)\n",
    "\n",
    "# Interpretar los valores (solo pares distintos)\n",
    "for i in range(len(abstracts)):\n",
    "    for j in range(i + 1, len(abstracts)):\n",
    "        sim = similarities[i, j]\n",
    "        if sim >= 0.8:\n",
    "            interpretation = \"Los textos son muy similares.\"\n",
    "        elif sim >= 0.5:\n",
    "            interpretation = \"Los textos tienen similitud moderada.\"\n",
    "        elif sim >= 0.2:\n",
    "            interpretation = \"Los textos son poco similares.\"\n",
    "        else:\n",
    "            interpretation = \"Los textos no son similares.\"\n",
    "        \n",
    "        print(f\"\\nSimilitud entre Abstract {i} y {j}: {sim:.3f}\")\n",
    "        print(f\"Interpretaci√≥n: {interpretation}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b842911",
   "metadata": {},
   "source": [
    "# algoritmos de similitud basados en IA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205af092",
   "metadata": {},
   "source": [
    "### Algoritmo 6: Cross-Encoder - Evaluaci√≥n Directa de Pares\n",
    "\n",
    "**Cross-Encoder** es un modelo de IA que eval√∫a **directamente** la relaci√≥n entre dos textos, sin generar embeddings intermedios.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Fundamento Conceptual\n",
    "\n",
    "### Diferencia con SBERT:\n",
    "\n",
    "#### SBERT (Bi-Encoder):\n",
    "```\n",
    "Texto A ‚Üí Embedding A ‚îÄ‚îê\n",
    "                        ‚îú‚îÄ‚Üí Similitud del Coseno\n",
    "Texto B ‚Üí Embedding B ‚îÄ‚îò\n",
    "```\n",
    "- **Ventaja**: Embeddings se calculan una vez, se reusan\n",
    "- **Desventaja**: Similitud indirecta (coseno de embeddings)\n",
    "\n",
    "#### Cross-Encoder:\n",
    "```\n",
    "[Texto A ; Texto B] ‚Üí Transformer ‚Üí Score directo\n",
    "```\n",
    "- **Ventaja**: Evaluaci√≥n directa, m√°s precisa\n",
    "- **Desventaja**: Debe procesar cada par individualmente\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è Arquitectura\n",
    "\n",
    "### Proceso:\n",
    "\n",
    "1. **Concatenaci√≥n**: `[CLS] Texto A [SEP] Texto B [SEP]`\n",
    "2. **Tokenizaci√≥n**: Convierte a IDs de tokens\n",
    "3. **BERT**: Procesa secuencia completa\n",
    "4. **Clasificaci√≥n**: Capa final predice score\n",
    "5. **Normalizaci√≥n**: Convierte a probabilidad [0, 1]\n",
    "\n",
    "**F√≥rmula matem√°tica**:\n",
    "\n",
    "```\n",
    "score = f([A ; B])\n",
    "```\n",
    "\n",
    "Donde `f` es el transformer que procesa ambos textos **conjuntamente**\n",
    "\n",
    "---\n",
    "\n",
    "## üíª Implementaci√≥n\n",
    "\n",
    "```python\n",
    "from sentence_transformers import CrossEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Cargar modelo\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "# Evaluar par\n",
    "pair = [[text1, text2]]\n",
    "score = cross_encoder.predict(pair)[0]\n",
    "\n",
    "# Normalizar con sigmoid\n",
    "prob = 1 / (1 + np.exp(-score))\n",
    "```\n",
    "\n",
    "### Modelo: `ms-marco-MiniLM-L-6-v2`\n",
    "\n",
    "**Caracter√≠sticas**:\n",
    "- **Entrenamiento**: MS MARCO dataset (500K+ pares)\n",
    "- **Tarea**: Ranking de relevancia (query-document)\n",
    "- **Tama√±o**: 90 MB\n",
    "- **Velocidad**: ~100 pares/segundo (CPU)\n",
    "\n",
    "**Fuente**: [HuggingFace](https://huggingface.co/cross-encoder/ms-marco-MiniLM-L6-v2)\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Resultado\n",
    "\n",
    "```\n",
    "Puntaje Cross-Encoder: -6.766\n",
    "Probabilidad normalizada: 0.001\n",
    "Interpretaci√≥n: Los textos no son similares.\n",
    "```\n",
    "\n",
    "**An√°lisis**:\n",
    "- **Score negativo** indica baja relevancia\n",
    "- **Probabilidad ~0** confirma que no hay similitud\n",
    "- **M√°s estricto** que SBERT (0.189 vs 0.001)\n",
    "\n",
    "---\n",
    "\n",
    "## üî¢ Normalizaci√≥n del Score\n",
    "\n",
    "### Problema: Scores sin l√≠mites\n",
    "\n",
    "Cross-Encoder puede retornar valores en rango (-‚àû, +‚àû)\n",
    "\n",
    "### Soluci√≥n: Funci√≥n Sigmoide\n",
    "\n",
    "```\n",
    "œÉ(x) = 1 / (1 + e^(-x))\n",
    "```\n",
    "\n",
    "**Mapeo**:\n",
    "- **x = -6.766** ‚Üí œÉ(x) = 0.001\n",
    "- **x = 0** ‚Üí œÉ(x) = 0.5\n",
    "- **x = +6.766** ‚Üí œÉ(x) = 0.999\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Ventajas\n",
    "\n",
    "1. **M√°xima precisi√≥n**: Mejor que SBERT en benchmarks\n",
    "   ```\n",
    "   MS MARCO: Cross-Encoder 39.2 vs Bi-Encoder 33.8 (MRR@10)\n",
    "   ```\n",
    "\n",
    "2. **Atenci√≥n cruzada**: Modela interacciones entre textos\n",
    "   ```\n",
    "   Puede detectar: \"Texto A responde pregunta en Texto B\"\n",
    "   ```\n",
    "\n",
    "3. **Estado del arte**: Mejor rendimiento en tareas de ranking\n",
    "\n",
    "4. **Interpretable**: Score directo de relevancia\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùå Desventajas\n",
    "\n",
    "1. **No escalable**: Debe procesar cada par individualmente\n",
    "   ```\n",
    "   Comparar 1000 documentos:\n",
    "   - SBERT: 1000 embeddings + 499,500 cosenos = ~1 segundo\n",
    "   - Cross-Encoder: 499,500 evaluaciones = ~1 hora (CPU)\n",
    "   ```\n",
    "\n",
    "2. **Sin embeddings reutilizables**: No se pueden cachear\n",
    "\n",
    "3. **Lento**: 10-100x m√°s lento que SBERT\n",
    "\n",
    "4. **Memoria**: Procesa secuencias m√°s largas (A + B)\n",
    "\n",
    "5. **L√≠mite de tokens**: M√°ximo ~512 tokens (A + B combinados)\n",
    "\n",
    "---\n",
    "\n",
    "## üî¨ Casos de Uso Ideales\n",
    "\n",
    "### ‚úÖ Recomendado para:\n",
    "- **Re-ranking**: Refinar top-K resultados de SBERT\n",
    "  ```\n",
    "  1. SBERT: Filtrar 1000 ‚Üí top 100 (r√°pido)\n",
    "  2. Cross-Encoder: Re-rankear top 100 (preciso)\n",
    "  ```\n",
    "- **Evaluaci√≥n final**: Validar pares cr√≠ticos\n",
    "- **Benchmarking**: Comparar con otros m√©todos\n",
    "- **Pocos pares**: <1000 comparaciones\n",
    "\n",
    "### ‚ùå No recomendado para:\n",
    "- **B√∫squeda a gran escala** (millones de documentos)\n",
    "- **Clustering** (requiere matriz de similitud completa)\n",
    "- **Tiempo real** (latencia alta)\n",
    "\n",
    "---\n",
    "\n",
    "## üìà Comparaci√≥n Completa\n",
    "\n",
    "| Par | Levenshtein | TF-IDF | Jaccard | Euclidiana | SBERT | Cross-Encoder |\n",
    "|-----|-------------|--------|---------|------------|-------|---------------|\n",
    "| 0-1 | 0.219 | 0.043 | 0.078 | 0.070 | 0.189 | 0.001 |\n",
    "\n",
    "**Ranking de Similitud** (mayor a menor):\n",
    "1. **Levenshtein**: 0.219 (caracteres comunes)\n",
    "2. **SBERT**: 0.189 (sem√°ntica d√©bil)\n",
    "3. **Jaccard**: 0.078 (palabras comunes)\n",
    "4. **Euclidiana**: 0.070 (distancia vectorial)\n",
    "5. **TF-IDF**: 0.043 (vocabulario ponderado)\n",
    "6. **Cross-Encoder**: 0.001 (relevancia directa)\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Estrategia H√≠brida Recomendada\n",
    "\n",
    "### Pipeline de 2 Etapas:\n",
    "\n",
    "```python\n",
    "# Etapa 1: Filtrado r√°pido con SBERT\n",
    "embeddings = sbert_model.encode(all_documents)\n",
    "similarities = util.cos_sim(query_embedding, embeddings)\n",
    "top_100 = similarities.topk(100)\n",
    "\n",
    "# Etapa 2: Re-ranking preciso con Cross-Encoder\n",
    "pairs = [[query, doc] for doc in top_100_documents]\n",
    "scores = cross_encoder.predict(pairs)\n",
    "final_ranking = sorted(zip(top_100_documents, scores), key=lambda x: x[1], reverse=True)\n",
    "```\n",
    "\n",
    "**Beneficios**:\n",
    "- ‚úÖ Velocidad de SBERT (filtrado)\n",
    "- ‚úÖ Precisi√≥n de Cross-Encoder (ranking)\n",
    "- ‚úÖ Escalable a millones de documentos\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Conclusi√≥n\n",
    "\n",
    "**Cross-Encoder** ofrece la **m√°xima precisi√≥n** pero con **alto costo computacional**. Ideal para **re-ranking** de resultados pre-filtrados, no para b√∫squeda inicial.\n",
    "\n",
    "**Recomendaci√≥n**: Usar en **pipeline h√≠brido** con SBERT para obtener lo mejor de ambos mundos: velocidad + precisi√≥n.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {
    "{21E3CCAC-D2EE-4EED-9C66-23F0AC0F4249}.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "e5a21b43",
   "metadata": {},
   "source": [
    "## SBERT (Sentence-BERT) ‚Äì Similitud sem√°ntica con embeddings\n",
    "\n",
    "SBERT convierte cada texto en un vector num√©rico (embedding) en un espacio sem√°ntico.\n",
    "Luego compara esos vectores usando la similitud del coseno:\n",
    "\n",
    "![{21E3CCAC-D2EE-4EED-9C66-23F0AC0F4249}.png](attachment:{21E3CCAC-D2EE-4EED-9C66-23F0AC0F4249}.png)\n",
    "\t‚Äã\n",
    "\n",
    "\n",
    "Valores cercanos a 1 ‚áí textos muy similares.\n",
    "\n",
    "https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32101e9",
   "metadata": {},
   "source": [
    "## Conclusiones y An√°lisis Comparativo\n",
    "\n",
    "### üìä Resumen de Resultados\n",
    "\n",
    "#### Similitud entre Abstract 0 y Abstract 1:\n",
    "\n",
    "| Algoritmo | Similitud | Interpretaci√≥n | Categor√≠a |\n",
    "|-----------|-----------|----------------|-----------|\n",
    "| **Levenshtein** | 0.219 | Poco similares | Cl√°sico |\n",
    "| **SBERT** | 0.189 | No similares | IA |\n",
    "| **Jaccard** | 0.078 | No similares | Cl√°sico |\n",
    "| **Euclidiana** | 0.070 | No similares | Cl√°sico |\n",
    "| **TF-IDF Coseno** | 0.043 | No similares | Cl√°sico |\n",
    "| **Cross-Encoder** | 0.001 | No similares | IA |\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Hallazgos Principales\n",
    "\n",
    "#### 1. Consenso General:\n",
    "- **Todos los algoritmos coinciden**: Los abstracts son **no similares**\n",
    "- **Coherente** con la realidad: Temas completamente diferentes\n",
    "  - Abstract 0: Programaci√≥n + IA\n",
    "  - Abstract 1: Sensores ambientales + IoT\n",
    "\n",
    "#### 2. Rango de Valores:\n",
    "- **M√°ximo**: 0.219 (Levenshtein)\n",
    "- **M√≠nimo**: 0.001 (Cross-Encoder)\n",
    "- **Rango**: 0.218 (variaci√≥n significativa)\n",
    "\n",
    "#### 3. Agrupaci√≥n por Tipo:\n",
    "\n",
    "**Algoritmos m√°s \"generosos\"** (detectan similitud d√©bil):\n",
    "- Levenshtein (0.219) - Caracteres comunes\n",
    "- SBERT (0.189) - Sem√°ntica d√©bil\n",
    "\n",
    "**Algoritmos m√°s \"estrictos\"** (requieren similitud fuerte):\n",
    "- TF-IDF (0.043) - Vocabulario espec√≠fico\n",
    "- Cross-Encoder (0.001) - Relevancia directa\n",
    "\n",
    "---\n",
    "\n",
    "### üìà An√°lisis por Categor√≠a\n",
    "\n",
    "#### üî§ Algoritmos Cl√°sicos:\n",
    "\n",
    "| Algoritmo | Velocidad | Precisi√≥n | Memoria | Uso Recomendado |\n",
    "|-----------|-----------|-----------|---------|-----------------|\n",
    "| **Levenshtein** | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê | üíæ | Detecci√≥n de typos |\n",
    "| **TF-IDF Coseno** | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | üíæüíæ | B√∫squeda general |\n",
    "| **Jaccard** | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê | üíæ | Comparaci√≥n r√°pida |\n",
    "| **Euclidiana** | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê | üíæüíæ | An√°lisis de frecuencias |\n",
    "\n",
    "**Ventajas generales**:\n",
    "- ‚úÖ R√°pidos (milisegundos)\n",
    "- ‚úÖ Sin dependencias pesadas\n",
    "- ‚úÖ Interpretables\n",
    "- ‚úÖ Escalables\n",
    "\n",
    "**Desventajas generales**:\n",
    "- ‚ùå No capturan sem√°ntica\n",
    "- ‚ùå Sensibles a vocabulario exacto\n",
    "- ‚ùå Ignoran sin√≥nimos\n",
    "\n",
    "---\n",
    "\n",
    "#### ü§ñ Algoritmos Basados en IA:\n",
    "\n",
    "| Algoritmo | Velocidad | Precisi√≥n | Memoria | Uso Recomendado |\n",
    "|-----------|-----------|-----------|---------|-----------------|\n",
    "| **SBERT** | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | üíæüíæüíæ | B√∫squeda sem√°ntica |\n",
    "| **Cross-Encoder** | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | üíæüíæüíæ | Re-ranking |\n",
    "\n",
    "**Ventajas generales**:\n",
    "- ‚úÖ Comprensi√≥n sem√°ntica\n",
    "- ‚úÖ Detectan par√°frasis\n",
    "- ‚úÖ Estado del arte\n",
    "- ‚úÖ Pre-entrenados\n",
    "\n",
    "**Desventajas generales**:\n",
    "- ‚ùå Lentos (segundos)\n",
    "- ‚ùå Requieren GPU para escala\n",
    "- ‚ùå Caja negra\n",
    "- ‚ùå Dependencias pesadas\n",
    "\n",
    "---\n",
    "\n",
    "### üîç An√°lisis de Discrepancias\n",
    "\n",
    "#### ¬øPor qu√© Levenshtein da 0.219 y Cross-Encoder 0.001?\n",
    "\n",
    "**Levenshtein (0.219)**:\n",
    "- Cuenta **caracteres comunes**: \"a\", \"e\", \"i\", \"o\", \"u\", espacios\n",
    "- Palabras comunes: \"the\", \"and\", \"is\", \"of\"\n",
    "- **No entiende** que son temas diferentes\n",
    "\n",
    "**Cross-Encoder (0.001)**:\n",
    "- **Entiende sem√°ntica**: \"programming\" ‚â† \"air quality\"\n",
    "- Entrenado en pares relevantes/irrelevantes\n",
    "- **Detecta** que no hay relaci√≥n tem√°tica\n",
    "\n",
    "**Conclusi√≥n**: Levenshtein sobrestima, Cross-Encoder es m√°s preciso\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Gu√≠a de Selecci√≥n de Algoritmo\n",
    "\n",
    "#### Escenario 1: Detecci√≥n de Duplicados Exactos\n",
    "```\n",
    "Recomendaci√≥n: Levenshtein o Jaccard\n",
    "Raz√≥n: R√°pidos, detectan copias casi exactas\n",
    "```\n",
    "\n",
    "#### Escenario 2: B√∫squeda en Base de Datos (10K+ documentos)\n",
    "```\n",
    "Recomendaci√≥n: TF-IDF Coseno\n",
    "Raz√≥n: Balance velocidad/precisi√≥n, escalable\n",
    "```\n",
    "\n",
    "#### Escenario 3: Recomendaci√≥n de Art√≠culos Similares\n",
    "```\n",
    "Recomendaci√≥n: SBERT\n",
    "Raz√≥n: Captura similitud sem√°ntica, embeddings reutilizables\n",
    "```\n",
    "\n",
    "#### Escenario 4: Ranking de Relevancia (Top-K)\n",
    "```\n",
    "Recomendaci√≥n: SBERT (filtrado) + Cross-Encoder (re-ranking)\n",
    "Raz√≥n: M√°xima precisi√≥n con eficiencia aceptable\n",
    "```\n",
    "\n",
    "#### Escenario 5: An√°lisis Exploratorio R√°pido\n",
    "```\n",
    "Recomendaci√≥n: Jaccard o TF-IDF\n",
    "Raz√≥n: Implementaci√≥n simple, resultados inmediatos\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Matriz de Decisi√≥n\n",
    "\n",
    "| Criterio | Levenshtein | TF-IDF | Jaccard | Euclidiana | SBERT | Cross-Encoder |\n",
    "|----------|-------------|--------|---------|------------|-------|---------------|\n",
    "| **Velocidad** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê |\n",
    "| **Precisi√≥n** | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "| **Sem√°ntica** | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚úÖ | ‚úÖ |\n",
    "| **Escalabilidad** | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚ö†Ô∏è | ‚ùå |\n",
    "| **Interpretabilidad** | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚ö†Ô∏è | ‚ö†Ô∏è |\n",
    "| **Setup** | F√°cil | F√°cil | F√°cil | F√°cil | Medio | Medio |\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Recomendaciones Finales\n",
    "\n",
    "#### Para este Proyecto (An√°lisis Bibliom√©trico):\n",
    "\n",
    "**Pipeline Recomendado**:\n",
    "\n",
    "```python\n",
    "# Paso 1: Filtrado r√°pido con TF-IDF\n",
    "tfidf_sim = cosine_similarity(tfidf_matrix)\n",
    "candidates = tfidf_sim > 0.3  # Umbral conservador\n",
    "\n",
    "# Paso 2: Validaci√≥n sem√°ntica con SBERT\n",
    "sbert_embeddings = model.encode(candidate_pairs)\n",
    "sbert_sim = util.cos_sim(sbert_embeddings)\n",
    "final_similar = sbert_sim > 0.7\n",
    "\n",
    "# Paso 3 (Opcional): Re-ranking con Cross-Encoder\n",
    "cross_scores = cross_encoder.predict(final_pairs)\n",
    "ranked_results = sorted(final_pairs, key=cross_scores, reverse=True)\n",
    "```\n",
    "\n",
    "**Justificaci√≥n**:\n",
    "- ‚úÖ **TF-IDF**: Elimina 90% de pares obviamente diferentes (r√°pido)\n",
    "- ‚úÖ **SBERT**: Valida similitud sem√°ntica (preciso)\n",
    "- ‚úÖ **Cross-Encoder**: Ranking final de alta calidad (opcional)\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Lecciones Aprendidas\n",
    "\n",
    "#### 1. No existe un \"mejor\" algoritmo universal\n",
    "- Depende del **caso de uso**, **datos** y **recursos**\n",
    "\n",
    "#### 2. Algoritmos cl√°sicos siguen siendo √∫tiles\n",
    "- TF-IDF es **suficiente** para muchas aplicaciones\n",
    "- No siempre se necesita IA\n",
    "\n",
    "#### 3. Combinar algoritmos es poderoso\n",
    "- Pipeline h√≠brido: **velocidad** + **precisi√≥n**\n",
    "\n",
    "#### 4. Validaci√≥n es crucial\n",
    "- Comparar m√∫ltiples algoritmos\n",
    "- Verificar resultados manualmente\n",
    "\n",
    "---\n",
    "\n",
    "### üéì Conclusi√≥n Final\n",
    "\n",
    "Este an√°lisis comparativo demuestra que:\n",
    "\n",
    "1. **Todos los algoritmos coinciden** en que los abstracts seleccionados son **no similares**\n",
    "2. **Algoritmos cl√°sicos** (TF-IDF) son **suficientes** para filtrado inicial\n",
    "3. **Algoritmos de IA** (SBERT, Cross-Encoder) ofrecen **mayor precisi√≥n** sem√°ntica\n",
    "4. **Pipeline h√≠brido** es la **mejor estrategia** para an√°lisis a gran escala\n",
    "\n",
    "**Recomendaci√≥n para el proyecto**: Usar **TF-IDF** para an√°lisis exploratorio y **SBERT** para validaci√≥n de similitud sem√°ntica en pares cr√≠ticos.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc085077",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\camil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\camil\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similitud SBERT: 0.189\n",
      "Interpretaci√≥n: Los textos no son similares.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "\n",
    "# --- Textos (abstracts) ---\n",
    "texts = [abstracts[0], abstracts[1]]  # puedes cambiar a tus abstracts\n",
    "\n",
    "# --- Cargar modelo SBERT ---\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# --- Obtener embeddings ---\n",
    "embeddings = model.encode(texts, convert_to_tensor=True)\n",
    "\n",
    "# --- Calcular similitud de coseno ---\n",
    "similarity = util.cos_sim(embeddings[0], embeddings[1]).item()\n",
    "\n",
    "# --- Interpretar el resultado ---\n",
    "if similarity >= 0.8:\n",
    "    interpretation = \"Los textos son muy similares.\"\n",
    "elif similarity >= 0.5:\n",
    "    interpretation = \"Los textos tienen similitud moderada.\"\n",
    "elif similarity >= 0.2:\n",
    "    interpretation = \"Los textos son poco similares.\"\n",
    "else:\n",
    "    interpretation = \"Los textos no son similares.\"\n",
    "\n",
    "print(f\"Similitud SBERT: {similarity:.3f}\")\n",
    "print(f\"Interpretaci√≥n: {interpretation}\")\n"
   ]
  },
  {
   "attachments": {
    "{0BFEEA4F-FF28-4A84-9A15-26ED70D1F6E1}.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "6e29acab",
   "metadata": {},
   "source": [
    "## Cross-Encoder ‚Äì Evaluaci√≥n directa de pares de texto\n",
    "\n",
    "El modelo recibe ambos textos juntos y predice una puntuaci√≥n directa de similitud.\n",
    "Se entrena para entender la relaci√≥n entre oraciones, no solo las palabras.\n",
    "\n",
    "Matem√°ticamente:\n",
    "\n",
    "![{0BFEEA4F-FF28-4A84-9A15-26ED70D1F6E1}.png](attachment:{0BFEEA4F-FF28-4A84-9A15-26ED70D1F6E1}.png)\n",
    "\n",
    "\n",
    "f([A;B]) es la representaci√≥n conjunta del par dentro del transformer.\n",
    "\n",
    "https://huggingface.co/cross-encoder/ms-marco-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bb0410f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\camil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\camil\\.cache\\huggingface\\hub\\models--cross-encoder--ms-marco-MiniLM-L-6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Puntaje Cross-Encoder: -6.766\n",
      "Probabilidad normalizada: 0.001\n",
      "Interpretaci√≥n: Los textos no son similares.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "import numpy as np\n",
    "\n",
    "# --- Cargar modelo ---\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "# --- Evaluar similitud directa entre dos textos ---\n",
    "pair = [[abstracts[0], abstracts[1]]]\n",
    "score = cross_encoder.predict(pair)[0]\n",
    "\n",
    "# --- Normalizar (si el score no est√° entre 0 y 1) ---\n",
    "prob = 1 / (1 + np.exp(-score)) if score > 1 or score < 0 else score\n",
    "\n",
    "# --- Interpretaci√≥n ---\n",
    "if prob >= 0.8:\n",
    "    interpretation = \"Los textos son muy similares.\"\n",
    "elif prob >= 0.5:\n",
    "    interpretation = \"Los textos tienen similitud moderada.\"\n",
    "elif prob >= 0.2:\n",
    "    interpretation = \"Los textos son poco similares.\"\n",
    "else:\n",
    "    interpretation = \"Los textos no son similares.\"\n",
    "\n",
    "print(f\"Puntaje Cross-Encoder: {score:.3f}\")\n",
    "print(f\"Probabilidad normalizada: {prob:.3f}\")\n",
    "print(f\"Interpretaci√≥n: {interpretation}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
