{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b220571",
   "metadata": {},
   "source": [
    "# Requerimiento 2: Análisis de Similitud entre Artículos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f7bd33",
   "metadata": {},
   "source": [
    "## Descripción del Requerimiento\n",
    "\n",
    "Este notebook implementa un **análisis comparativo de algoritmos de similitud textual** para evaluar la similitud entre abstracts de artículos científicos.\n",
    "\n",
    "### Objetivos:\n",
    "1. Cargar y preprocesar archivo BibTeX consolidado\n",
    "2. Eliminar duplicados por título\n",
    "3. Comparar 6 algoritmos de similitud textual\n",
    "4. Interpretar resultados con umbrales de similitud\n",
    "\n",
    "### Categorías de Algoritmos:\n",
    "\n",
    "#### Algoritmos Clásicos (No supervisados):\n",
    "1. **Levenshtein**: Distancia de edición entre caracteres\n",
    "2. **Coseno TF-IDF**: Similitud vectorial con pesos TF-IDF\n",
    "3. **Jaccard**: Intersección de conjuntos de palabras\n",
    "4. **Euclidiana**: Distancia en espacio vectorial\n",
    "\n",
    "#### Algoritmos Basados en IA (Supervisados):\n",
    "5. **SBERT**: Embeddings semánticos con Sentence-BERT\n",
    "6. **Cross-Encoder**: Evaluación directa de pares de texto\n",
    "\n",
    "### Umbrales de Interpretación:\n",
    "\n",
    "| Rango | Interpretación |\n",
    "|-------|----------------|\n",
    "| ≥ 0.80 | Muy similares |\n",
    "| 0.50 - 0.79 | Similitud moderada |\n",
    "| 0.20 - 0.49 | Poco similares |\n",
    "| < 0.20 | No similares |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca81e70a",
   "metadata": {},
   "source": [
    "### Carga y Deduplicación del Archivo BibTeX\n",
    "\n",
    "Esta celda carga el archivo consolidado y elimina artículos duplicados.\n",
    "\n",
    "#### Proceso:\n",
    "\n",
    "##### 1. Configuración del Parser\n",
    "```python\n",
    "parser = bibtexparser.bparser.BibTexParser(common_strings=True)\n",
    "parser.expect_multiple_parse = True\n",
    "```\n",
    "- **`common_strings=True`**: Expande abreviaturas comunes de BibTeX\n",
    "- **`expect_multiple_parse=True`**: Suprime warnings de múltiples entradas\n",
    "\n",
    "##### 2. Lectura del Archivo\n",
    "```python\n",
    "with open(CONSOLIDADO_PATH, encoding=\"utf-8\") as f:\n",
    "    bib_database = bibtexparser.load(f, parser=parser)\n",
    "```\n",
    "- **Encoding UTF-8**: Soporta caracteres especiales\n",
    "- **Context manager**: Cierre automático del archivo\n",
    "\n",
    "##### 3. Extracción de Campos\n",
    "```python\n",
    "for entry in bib_database.entries:\n",
    "    data.append({\n",
    "        \"title\": entry.get(\"title\", \"\").strip(),\n",
    "        \"authors\": entry.get(\"author\", \"\"),\n",
    "        \"keywords\": entry.get(\"keywords\", \"\"),\n",
    "        \"abstract\": entry.get(\"abstract\", \"\")\n",
    "    })\n",
    "```\n",
    "- Extrae título, autores, keywords y abstract\n",
    "- Manejo de valores faltantes con `entry.get(campo, \"\")`\n",
    "\n",
    "##### 4. Deduplicación por Título\n",
    "```python\n",
    "df_unique = df.drop_duplicates(subset=\"title\", keep=\"first\")\n",
    "```\n",
    "- **`keep=\"first\"`**: Mantiene la primera aparición de cada título\n",
    "- Los títulos son únicos en publicaciones académicas\n",
    "\n",
    "##### 5. Guardado de Resultados\n",
    "```python\n",
    "df_unique.to_csv(\"articulos_unicos.csv\", index=False)\n",
    "df_duplicates.to_csv(\"articulos_repetidos.csv\", index=False)\n",
    "```\n",
    "- **`articulos_unicos.csv`**: Dataset limpio para análisis\n",
    "- **`articulos_repetidos.csv`**: Duplicados para auditoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8455c9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo archivo: /home/yep/Documentos/proyectoAnalisisAlgoritmos/proyecto/consolidado.bib\n",
      "Artículos totales: 10226\n",
      "Artículos únicos: 10189\n",
      "Artículos repetidos: 38\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>keywords</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Do Robots Dream of Passing a Programming Course?</td>\n",
       "      <td>Torres, Nicolás</td>\n",
       "      <td>Training;Computational modeling;Instruments;Na...</td>\n",
       "      <td>Programming typically involves humans formulat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WeAIR: Wearable Swarm Sensors for Air Quality ...</td>\n",
       "      <td>Dimitri, Giovanna Maria and Parri, Lorenzo and...</td>\n",
       "      <td>Temperature measurement;Climate change;Cloud c...</td>\n",
       "      <td>The present study proposes the implementation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Discriminative-Generative Representation Learn...</td>\n",
       "      <td>Li, Duanjiao and Chen, Yun and Zhang, Ying and...</td>\n",
       "      <td>Representation learning;Semantics;Asia;Self-su...</td>\n",
       "      <td>Generative Adversarial Networks (GANs), as a f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3 Generative AI Models and LLM: Training Techn...</td>\n",
       "      <td>Arun, C. and Karthick, S. and Selvakumara Samy...</td>\n",
       "      <td></td>\n",
       "      <td>Generative artificial intelligence (AI) has be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Virtual Human: A Comprehensive Survey on Acade...</td>\n",
       "      <td>Cui, Lipeng and Liu, Jiarui</td>\n",
       "      <td>Digital humans;Motion capture;Face recognition...</td>\n",
       "      <td>As a creative method for virtual human individ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Do Robots Dream of Passing a Programming Course?   \n",
       "1  WeAIR: Wearable Swarm Sensors for Air Quality ...   \n",
       "2  Discriminative-Generative Representation Learn...   \n",
       "3  3 Generative AI Models and LLM: Training Techn...   \n",
       "4  Virtual Human: A Comprehensive Survey on Acade...   \n",
       "\n",
       "                                             authors  \\\n",
       "0                                    Torres, Nicolás   \n",
       "1  Dimitri, Giovanna Maria and Parri, Lorenzo and...   \n",
       "2  Li, Duanjiao and Chen, Yun and Zhang, Ying and...   \n",
       "3  Arun, C. and Karthick, S. and Selvakumara Samy...   \n",
       "4                        Cui, Lipeng and Liu, Jiarui   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  Training;Computational modeling;Instruments;Na...   \n",
       "1  Temperature measurement;Climate change;Cloud c...   \n",
       "2  Representation learning;Semantics;Asia;Self-su...   \n",
       "3                                                      \n",
       "4  Digital humans;Motion capture;Face recognition...   \n",
       "\n",
       "                                            abstract  \n",
       "0  Programming typically involves humans formulat...  \n",
       "1  The present study proposes the implementation ...  \n",
       "2  Generative Adversarial Networks (GANs), as a f...  \n",
       "3  Generative artificial intelligence (AI) has be...  \n",
       "4  As a creative method for virtual human individ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bibtexparser\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Cargar variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "# Obtener ruta del archivo consolidado\n",
    "CONSOLIDADO_PATH = os.getenv(\"CONSOLIDADO_PATH\", \"../salidas/consolidado.bib\")\n",
    "\n",
    "# Verificar que el archivo existe\n",
    "if not os.path.exists(CONSOLIDADO_PATH):\n",
    "    print(f\"Error: No se encuentra el archivo en {CONSOLIDADO_PATH}\")\n",
    "    print(f\"Directorio actual: {os.getcwd()}\")\n",
    "    print(f\"Archivos disponibles:\")\n",
    "    # Buscar archivos .bib en directorios comunes\n",
    "    for root, dirs, files in os.walk(\"..\"):\n",
    "        for file in files:\n",
    "            if file.endswith(\"consolidado.bib\"):\n",
    "                print(f\"Encontrado: {os.path.join(root, file)}\")\n",
    "    raise FileNotFoundError(f\"No se encuentra {CONSOLIDADO_PATH}\")\n",
    "\n",
    "print(f\"Leyendo archivo: {CONSOLIDADO_PATH}\")\n",
    "\n",
    "# Configurar el parser\n",
    "parser = bibtexparser.bparser.BibTexParser(common_strings=True)\n",
    "parser.expect_multiple_parse = True  # Evita el warning\n",
    "\n",
    "# Leer TODO el archivo de una vez\n",
    "with open(CONSOLIDADO_PATH, encoding=\"utf-8\") as f:\n",
    "    bib_database = bibtexparser.load(f, parser=parser)\n",
    "\n",
    "# Extraer la información\n",
    "data = []\n",
    "for entry in bib_database.entries:\n",
    "    data.append({\n",
    "        \"title\": entry.get(\"title\", \"\").strip(),\n",
    "        \"authors\": entry.get(\"author\", \"\"),\n",
    "        \"keywords\": entry.get(\"keywords\", \"\"),\n",
    "        \"abstract\": entry.get(\"abstract\", \"\")\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Eliminar duplicados por título\n",
    "df_unique = df.drop_duplicates(subset=\"title\", keep=\"first\")\n",
    "df_duplicates = df[df.duplicated(subset=\"title\", keep=False)]\n",
    "\n",
    "# Guardar resultados\n",
    "df_unique.to_csv(\"articulos_unicos.csv\", index=False)\n",
    "df_duplicates.to_csv(\"articulos_repetidos.csv\", index=False)\n",
    "\n",
    "print(f\"Artículos totales: {len(df)}\")\n",
    "print(f\"Artículos únicos: {len(df_unique)}\")\n",
    "print(f\"Artículos repetidos: {len(df_duplicates)}\")\n",
    "\n",
    "df_unique.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996d318f",
   "metadata": {},
   "source": [
    "### Selección de Artículos para Comparación\n",
    "\n",
    "Esta celda selecciona una muestra de artículos para análisis de similitud.\n",
    "\n",
    "#### Estrategia de Selección:\n",
    "\n",
    "```python\n",
    "abstracts = df[\"abstract\"].head(3).tolist()\n",
    "titles = df[\"title\"].head(3).tolist()\n",
    "```\n",
    "\n",
    "**Características**:\n",
    "- Toma los primeros N artículos del DataFrame\n",
    "- Simple y reproducible\n",
    "- Puede ajustarse según necesidad (`.head(N)` o `.sample(N)`)\n",
    "\n",
    "#### Alternativas de Selección:\n",
    "\n",
    "**Selección Aleatoria**:\n",
    "```python\n",
    "indices = random.sample(range(len(df)), 3)\n",
    "abstracts = df.iloc[indices][\"abstract\"].tolist()\n",
    "```\n",
    "\n",
    "**Selección por Keywords**:\n",
    "```python\n",
    "mask = df[\"keywords\"].str.contains(\"generative\", case=False, na=False)\n",
    "sample = df[mask].head(3)\n",
    "```\n",
    "\n",
    "**Selección por Longitud**:\n",
    "```python\n",
    "df[\"abstract_len\"] = df[\"abstract\"].str.len()\n",
    "sample = df[(df[\"abstract_len\"] > 500) & (df[\"abstract_len\"] < 700)].head(3)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a63fa95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Do Robots Dream of Passing a Programming Course?\n",
      "Programming typically involves humans formulating instructions for a computer to execute computations. If we adhere to this definition, a machine would seemingly lack the capability to autonomously de...\n",
      "\n",
      "1. WeAIR: Wearable Swarm Sensors for Air Quality Monitoring to Foster Citizens' Awareness of Climate Change\n",
      "The present study proposes the implementation of an air quality measurement tool through the use of a swarm of wearable devices, named WeAIR, consisting of wearable sensors for measuring NOx, CO2, CO,...\n",
      "\n",
      "2. Discriminative-Generative Representation Learning for One-Class Anomaly Detection\n",
      "Generative Adversarial Networks (GANs), as a form of generative self-supervised learning, have garnered significant attention in anomaly detection. However, the generator's capacity for representation...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo: selecionando los primeros 3\n",
    "abstracts = df[\"abstract\"].head(3).tolist()\n",
    "titles = df[\"title\"].head(3).tolist()\n",
    "\n",
    "for i, t in enumerate(titles):\n",
    "    print(f\"{i}. {t}\\n{abstracts[i][:200]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5856df90",
   "metadata": {},
   "source": [
    "### Algoritmo 1: Distancia de Levenshtein\n",
    "\n",
    "La **distancia de Levenshtein** mide el número mínimo de operaciones de edición (inserción, eliminación, sustitución) necesarias para transformar un texto en otro.\n",
    "\n",
    "#### Fundamento Matemático\n",
    "\n",
    "Para dos cadenas **a** y **b**, la distancia de Levenshtein se define recursivamente:\n",
    "\n",
    "```\n",
    "lev(a,b) = {\n",
    "    |a|                                  si |b| = 0\n",
    "    |b|                                  si |a| = 0\n",
    "    lev(tail(a), tail(b))                si a[0] = b[0]\n",
    "    1 + min {\n",
    "        lev(tail(a), b)                  (eliminación)\n",
    "        lev(a, tail(b))                  (inserción)\n",
    "        lev(tail(a), tail(b))            (sustitución)\n",
    "    }                                    en otro caso\n",
    "}\n",
    "```\n",
    "\n",
    "#### Conversión a Similitud\n",
    "\n",
    "```python\n",
    "similarity = 1 - (distance / max_length)\n",
    "```\n",
    "\n",
    "**Rango**: [0, 1]\n",
    "- **1.0** = textos idénticos\n",
    "- **0.0** = textos completamente diferentes\n",
    "\n",
    "#### Implementación\n",
    "\n",
    "```python\n",
    "import Levenshtein\n",
    "\n",
    "def levenshtein_similarity(text1, text2):\n",
    "    dist = Levenshtein.distance(text1, text2)\n",
    "    max_len = max(len(text1), len(text2))\n",
    "    similarity = 1 - dist / max_len\n",
    "    return similarity\n",
    "```\n",
    "\n",
    "**Complejidad**: O(m × n) donde m, n son longitudes de los textos\n",
    "\n",
    "#### Ventajas\n",
    "- Simplicidad y facilidad de implementación\n",
    "- Determinístico (siempre da el mismo resultado)\n",
    "- Sensible a errores ortográficos\n",
    "- No requiere entrenamiento\n",
    "\n",
    "#### Desventajas\n",
    "- Opera a nivel de caracteres, no entiende semántica\n",
    "- Sensible a longitud de textos\n",
    "- No detecta sinónimos o paráfrasis\n",
    "- El orden de palabras afecta significativamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e7bdb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similitud de Levenshtein: 0.219\n",
      "Interpretación: Los textos son poco similares.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.21857923497267762"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Levenshtein\n",
    "\n",
    "def levenshtein_similarity(text1, text2):\n",
    "    dist = Levenshtein.distance(text1, text2)\n",
    "    max_len = max(len(text1), len(text2))\n",
    "    similarity = 1 - dist / max_len\n",
    "    \n",
    "    # Interpretación del resultado\n",
    "    if similarity >= 0.8:\n",
    "        interpretation = \"Los textos son muy similares.\"\n",
    "    elif similarity >= 0.5:\n",
    "        interpretation = \"Los textos tienen cierta similitud moderada.\"\n",
    "    elif similarity >= 0.2:\n",
    "        interpretation = \"Los textos son poco similares.\"\n",
    "    else:\n",
    "        interpretation = \"Los textos no son similares.\"\n",
    "    \n",
    "    print(f\"Similitud de Levenshtein: {similarity:.3f}\")\n",
    "    print(f\"Interpretación: {interpretation}\")\n",
    "    return similarity\n",
    "\n",
    "\n",
    "levenshtein_similarity(abstracts[0], abstracts[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a14439",
   "metadata": {},
   "source": [
    "### Algoritmo 2: Similitud del Coseno con TF-IDF\n",
    "\n",
    "La **similitud del coseno** mide el ángulo entre dos vectores en un espacio multidimensional. Combinada con **TF-IDF**, captura la importancia relativa de las palabras.\n",
    "\n",
    "#### Fundamento Matemático\n",
    "\n",
    "**TF-IDF (Term Frequency - Inverse Document Frequency)**\n",
    "\n",
    "```\n",
    "TF(t, d) = (Frecuencia de término t en documento d) / (Total de términos en d)\n",
    "IDF(t, D) = log(Total de documentos / Documentos que contienen t)\n",
    "TF-IDF(t, d, D) = TF(t, d) × IDF(t, D)\n",
    "```\n",
    "\n",
    "**Similitud del Coseno**\n",
    "\n",
    "Para dos vectores **A** y **B**:\n",
    "\n",
    "```\n",
    "cos(θ) = (A · B) / (||A|| × ||B||)\n",
    "```\n",
    "\n",
    "Donde:\n",
    "- **A · B** = producto punto = Σ(Aᵢ × Bᵢ)\n",
    "- **||A||** = norma = √(Σ Aᵢ²)\n",
    "- **θ** = ángulo entre vectores\n",
    "\n",
    "**Rango**: [-1, 1]\n",
    "- **1**: Vectores idénticos (θ = 0°)\n",
    "- **0**: Vectores ortogonales (θ = 90°)\n",
    "- **-1**: Vectores opuestos (θ = 180°)\n",
    "\n",
    "#### Implementación\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(abstracts)\n",
    "cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "```\n",
    "\n",
    "**Parámetros importantes**:\n",
    "- **`stop_words='english'`**: Elimina palabras comunes (the, and, is...)\n",
    "- **`max_features`**: Limita vocabulario\n",
    "- **`ngram_range`**: Incluye n-gramas\n",
    "\n",
    "#### Interpretación de Resultados\n",
    "\n",
    "**Matriz de Similitud**:\n",
    "- **Diagonal**: Siempre 1.0 (cada documento consigo mismo)\n",
    "- **Valores fuera de diagonal**: Similitud entre pares de documentos\n",
    "\n",
    "#### Ventajas\n",
    "- Opera a nivel de palabras, no caracteres\n",
    "- TF-IDF pondera palabras importantes\n",
    "- Normalizado por magnitud de vectores\n",
    "- Eficiente y escalable\n",
    "- Interpretable\n",
    "\n",
    "#### Desventajas\n",
    "- No captura similitud semántica (sinónimos)\n",
    "- Bag of words: ignora orden de palabras\n",
    "- Vocabulario cerrado (solo palabras vistas)\n",
    "- Vectores dispersos (sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69546e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de similitud de coseno:\n",
      "[[1.         0.0429688  0.09021994]\n",
      " [0.0429688  1.         0.00515135]\n",
      " [0.09021994 0.00515135 1.        ]]\n",
      "\n",
      "Interpretación par a par:\n",
      "\n",
      "Similitud entre Abstract 0 y Abstract 1: 0.043 → Los textos no son similares.\n",
      "Similitud entre Abstract 0 y Abstract 2: 0.090 → Los textos no son similares.\n",
      "Similitud entre Abstract 1 y Abstract 2: 0.005 → Los textos no son similares.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def interpretar_similitud(valor):\n",
    "    \"\"\"Interpreta el nivel de similitud según el valor de coseno.\"\"\"\n",
    "    if valor >= 0.8:\n",
    "        return \"Los textos son muy similares.\"\n",
    "    elif valor >= 0.5:\n",
    "        return \"Los textos tienen similitud moderada.\"\n",
    "    elif valor >= 0.2:\n",
    "        return \"Los textos son poco similares.\"\n",
    "    else:\n",
    "        return \"Los textos no son similares.\"\n",
    "\n",
    "# --- Cálculo del TF-IDF y similitud ---\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(abstracts)\n",
    "cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# --- Impresión de resultados ---\n",
    "print(\"Matriz de similitud de coseno:\")\n",
    "print(cosine_sim)\n",
    "print(\"\\nInterpretación par a par:\\n\")\n",
    "\n",
    "# Recorre cada par de textos (sin repetir)\n",
    "for i in range(len(abstracts)):\n",
    "    for j in range(i + 1, len(abstracts)):\n",
    "        valor = cosine_sim[i][j]\n",
    "        print(f\"Similitud entre Abstract {i} y Abstract {j}: {valor:.3f} → {interpretar_similitud(valor)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef42b053",
   "metadata": {},
   "source": [
    "### Algoritmo 3: Similitud de Jaccard\n",
    "\n",
    "El **índice de Jaccard** mide la similitud entre conjuntos calculando la proporción de elementos comunes.\n",
    "\n",
    "#### Fundamento Matemático\n",
    "\n",
    "Para dos conjuntos **A** y **B**:\n",
    "\n",
    "```\n",
    "J(A, B) = |A ∩ B| / |A ∪ B|\n",
    "```\n",
    "\n",
    "Donde:\n",
    "- **A ∩ B** = Intersección (elementos en ambos)\n",
    "- **A ∪ B** = Unión (elementos en al menos uno)\n",
    "- **| |** = Cardinalidad (número de elementos)\n",
    "\n",
    "**Rango**: [0, 1]\n",
    "- **1**: Conjuntos idénticos\n",
    "- **0**: Sin elementos comunes\n",
    "\n",
    "#### Ejemplo\n",
    "\n",
    "**Texto 1**: \"machine learning models\"\n",
    "**Texto 2**: \"deep learning networks\"\n",
    "\n",
    "```\n",
    "A = {machine, learning, models}\n",
    "B = {deep, learning, networks}\n",
    "A ∩ B = {learning}                           → |A ∩ B| = 1\n",
    "A ∪ B = {machine, learning, models, deep, networks} → |A ∪ B| = 5\n",
    "J(A, B) = 1 / 5 = 0.20\n",
    "```\n",
    "\n",
    "#### Implementación\n",
    "\n",
    "```python\n",
    "def jaccard_similarity(a, b):\n",
    "    a_set = set(a.lower().split())\n",
    "    b_set = set(b.lower().split())\n",
    "    intersection = len(a_set & b_set)\n",
    "    union = len(a_set | b_set)\n",
    "    return intersection / union\n",
    "```\n",
    "\n",
    "**Operaciones de conjuntos en Python**:\n",
    "- `&` = Intersección\n",
    "- `|` = Unión\n",
    "- `set()` = Elimina duplicados automáticamente\n",
    "\n",
    "#### Ventajas\n",
    "- Muy simple de entender e implementar\n",
    "- Rápido: operaciones de conjuntos son O(n)\n",
    "- Simétrico: J(A,B) = J(B,A)\n",
    "- No requiere pesos ni entrenamiento\n",
    "- Robusto a duplicados\n",
    "\n",
    "#### Desventajas\n",
    "- Ignora frecuencia de palabras\n",
    "- Ignora orden de palabras\n",
    "- No captura similitud semántica\n",
    "- Sensible a longitud de textos\n",
    "- Cuenta stopwords igual que palabras importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4926f934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similitud de Jaccard: 0.078\n",
      "Interpretación: Los textos no son similares.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0782122905027933"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def jaccard_similarity(a, b):\n",
    "    # Convertir a conjuntos de palabras\n",
    "    a_set, b_set = set(a.lower().split()), set(b.lower().split())\n",
    "    similarity = len(a_set & b_set) / len(a_set | b_set)\n",
    "    \n",
    "    # Interpretación de la similitud\n",
    "    if similarity >= 0.8:\n",
    "        interpretation = \"Los textos son muy similares.\"\n",
    "    elif similarity >= 0.5:\n",
    "        interpretation = \"Los textos tienen similitud moderada.\"\n",
    "    elif similarity >= 0.2:\n",
    "        interpretation = \"Los textos son poco similares.\"\n",
    "    else:\n",
    "        interpretation = \"Los textos no son similares.\"\n",
    "    \n",
    "    # Mostrar resultados\n",
    "    print(f\"Similitud de Jaccard: {similarity:.3f}\")\n",
    "    print(f\"Interpretación: {interpretation}\")\n",
    "    return similarity\n",
    "\n",
    "# Ejemplo de uso\n",
    "jaccard_similarity(abstracts[0], abstracts[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84c5965",
   "metadata": {},
   "source": [
    "### Algoritmo 4: Similitud Euclidiana\n",
    "\n",
    "La **distancia euclidiana** mide la distancia geométrica entre dos puntos en un espacio multidimensional.\n",
    "\n",
    "#### Fundamento Matemático\n",
    "\n",
    "**Distancia Euclidiana**:\n",
    "\n",
    "Para dos vectores **A** y **B** de dimensión n:\n",
    "\n",
    "```\n",
    "d(A, B) = √(Σᵢ (Aᵢ - Bᵢ)²)\n",
    "```\n",
    "\n",
    "**Conversión a Similitud**:\n",
    "\n",
    "```\n",
    "similarity = 1 / (1 + distance)\n",
    "```\n",
    "\n",
    "**Rango**: [0, 1]\n",
    "- **1**: Vectores idénticos (distancia = 0)\n",
    "- **→0**: Vectores muy diferentes (distancia → ∞)\n",
    "\n",
    "#### Implementación\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(abstracts)\n",
    "distances = euclidean_distances(X)\n",
    "similarities = 1 / (1 + distances)\n",
    "```\n",
    "\n",
    "**Diferencia con TF-IDF**:\n",
    "- **CountVectorizer**: Frecuencias simples (1, 2, 3...)\n",
    "- **TfidfVectorizer**: Frecuencias ponderadas\n",
    "\n",
    "#### Ventajas\n",
    "- Intuitivo: distancia geométrica familiar\n",
    "- Sensible a magnitud (captura diferencias de frecuencia)\n",
    "- Fácil de visualizar en 2D/3D\n",
    "\n",
    "#### Desventajas\n",
    "- Sensible a dimensionalidad alta\n",
    "- Sensible a escala (palabras frecuentes dominan)\n",
    "- No normalizado por longitud de texto\n",
    "- Menos usado que coseno para textos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcc1b4a",
   "metadata": {},
   "source": [
    "## Algoritmo de Similitud Euclidiana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c148afd3",
   "metadata": {},
   "source": [
    "### Algoritmo 5: SBERT (Sentence-BERT)\n",
    "\n",
    "**SBERT** es un modelo de IA que convierte textos en **embeddings semánticos** de alta calidad, capturando el significado más allá de las palabras exactas.\n",
    "\n",
    "#### Fundamento Conceptual\n",
    "\n",
    "**Embeddings** son representaciones vectoriales densas que capturan significado semántico:\n",
    "\n",
    "```\n",
    "\"car\" → [0.23, -0.45, 0.67, ..., 0.12]  (384 dimensiones)\n",
    "\"automobile\" → [0.25, -0.43, 0.69, ..., 0.14]  (similar)\n",
    "```\n",
    "\n",
    "**Propiedad clave**: Vectores de palabras similares están cerca en el espacio\n",
    "\n",
    "#### Arquitectura SBERT\n",
    "\n",
    "Basado en BERT (Bidirectional Encoder Representations from Transformers):\n",
    "\n",
    "```\n",
    "Texto → Tokenización → BERT → Mean Pooling → Embedding (384D)\n",
    "```\n",
    "\n",
    "**Componentes**:\n",
    "1. **Tokenización**: Divide texto en subpalabras (WordPiece)\n",
    "2. **BERT**: Transformer que procesa contexto bidireccional\n",
    "3. **Mean Pooling**: Promedia embeddings de tokens\n",
    "4. **Normalización**: Vector unitario (norma L2 = 1)\n",
    "\n",
    "#### Cálculo de Similitud\n",
    "\n",
    "Similitud del Coseno entre Embeddings:\n",
    "\n",
    "```\n",
    "similarity = cos(θ) = (E₁ · E₂) / (||E₁|| × ||E₂||)\n",
    "```\n",
    "\n",
    "**Ventaja sobre TF-IDF**: Captura similitud semántica, no solo léxica\n",
    "\n",
    "#### Implementación\n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(texts, convert_to_tensor=True)\n",
    "similarity = util.cos_sim(embeddings[0], embeddings[1]).item()\n",
    "```\n",
    "\n",
    "**Modelo**: `all-MiniLM-L6-v2`\n",
    "- **Tamaño**: 22.7 MB\n",
    "- **Dimensiones**: 384\n",
    "- **Velocidad**: ~3,000 oraciones/segundo (GPU)\n",
    "\n",
    "#### Ventajas\n",
    "- Comprende semántica: entiende sinónimos y paráfrasis\n",
    "- Considera contexto de las palabras\n",
    "- Modelos multilingües disponibles\n",
    "- Pre-entrenado, no requiere entrenamiento adicional\n",
    "- Embeddings se calculan una vez y se reusan\n",
    "\n",
    "#### Desventajas\n",
    "- Computacionalmente costoso (requiere GPU para datasets grandes)\n",
    "- Modelo ocupa ~100-500 MB en RAM\n",
    "- Difícil interpretar por qué dos textos son similares\n",
    "- Requiere dependencias (PyTorch, transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e85185c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de similitud euclidiana:\n",
      "[[1.         0.06972692 0.06005113]\n",
      " [0.06972692 1.         0.05432747]\n",
      " [0.06005113 0.05432747 1.        ]]\n",
      "\n",
      "Similitud entre Abstract 0 y 1: 0.070\n",
      "Interpretación: Los textos no son similares.\n",
      "\n",
      "Similitud entre Abstract 0 y 2: 0.060\n",
      "Interpretación: Los textos no son similares.\n",
      "\n",
      "Similitud entre Abstract 1 y 2: 0.054\n",
      "Interpretación: Los textos no son similares.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# Vectorizar los textos (sin stopwords en inglés)\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(abstracts)\n",
    "\n",
    "# Calcular distancias euclidianas\n",
    "distances = euclidean_distances(X)\n",
    "\n",
    "# Convertir a similitud (1 / (1 + distancia))\n",
    "similarities = 1 / (1 + distances)\n",
    "\n",
    "# Mostrar matriz de similitudes con interpretación\n",
    "print(\"Matriz de similitud euclidiana:\")\n",
    "print(similarities)\n",
    "\n",
    "# Interpretar los valores (solo pares distintos)\n",
    "for i in range(len(abstracts)):\n",
    "    for j in range(i + 1, len(abstracts)):\n",
    "        sim = similarities[i, j]\n",
    "        if sim >= 0.8:\n",
    "            interpretation = \"Los textos son muy similares.\"\n",
    "        elif sim >= 0.5:\n",
    "            interpretation = \"Los textos tienen similitud moderada.\"\n",
    "        elif sim >= 0.2:\n",
    "            interpretation = \"Los textos son poco similares.\"\n",
    "        else:\n",
    "            interpretation = \"Los textos no son similares.\"\n",
    "        \n",
    "        print(f\"\\nSimilitud entre Abstract {i} y {j}: {sim:.3f}\")\n",
    "        print(f\"Interpretación: {interpretation}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b842911",
   "metadata": {},
   "source": [
    "# algoritmos de similitud basados en IA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205af092",
   "metadata": {},
   "source": [
    "### Algoritmo 6: Cross-Encoder\n",
    "\n",
    "**Cross-Encoder** es un modelo de IA que evalúa **directamente** la relación entre dos textos, sin generar embeddings intermedios.\n",
    "\n",
    "#### Diferencia con SBERT\n",
    "\n",
    "**SBERT (Bi-Encoder)**:\n",
    "```\n",
    "Texto A → Embedding A ─┐\n",
    "                        ├─→ Similitud del Coseno\n",
    "Texto B → Embedding B ─┘\n",
    "```\n",
    "- Embeddings se calculan una vez, se reusan\n",
    "- Similitud indirecta (coseno de embeddings)\n",
    "\n",
    "**Cross-Encoder**:\n",
    "```\n",
    "[Texto A ; Texto B] → Transformer → Score directo\n",
    "```\n",
    "- Evaluación directa, más precisa\n",
    "- Debe procesar cada par individualmente\n",
    "\n",
    "#### Arquitectura\n",
    "\n",
    "**Proceso**:\n",
    "\n",
    "1. **Concatenación**: `[CLS] Texto A [SEP] Texto B [SEP]`\n",
    "2. **Tokenización**: Convierte a IDs de tokens\n",
    "3. **BERT**: Procesa secuencia completa\n",
    "4. **Clasificación**: Capa final predice score\n",
    "5. **Normalización**: Convierte a probabilidad [0, 1]\n",
    "\n",
    "**Fórmula**: `score = f([A ; B])` donde `f` es el transformer que procesa ambos textos conjuntamente\n",
    "\n",
    "#### Implementación\n",
    "\n",
    "```python\n",
    "from sentence_transformers import CrossEncoder\n",
    "import numpy as np\n",
    "\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "pair = [[text1, text2]]\n",
    "score = cross_encoder.predict(pair)[0]\n",
    "prob = 1 / (1 + np.exp(-score))  # Normalización con sigmoid\n",
    "```\n",
    "\n",
    "**Modelo**: `ms-marco-MiniLM-L-6-v2`\n",
    "- **Entrenamiento**: MS MARCO dataset (500K+ pares)\n",
    "- **Tamaño**: 90 MB\n",
    "- **Velocidad**: ~100 pares/segundo (CPU)\n",
    "\n",
    "#### Normalización del Score\n",
    "\n",
    "Cross-Encoder retorna valores en rango (-∞, +∞). Se normaliza con función sigmoide:\n",
    "\n",
    "```\n",
    "σ(x) = 1 / (1 + e^(-x))\n",
    "```\n",
    "\n",
    "#### Ventajas\n",
    "- Máxima precisión en benchmarks\n",
    "- Atención cruzada: modela interacciones entre textos\n",
    "- Score directo de relevancia\n",
    "- Estado del arte en tareas de ranking\n",
    "\n",
    "#### Desventajas\n",
    "- No escalable: debe procesar cada par individualmente\n",
    "- Sin embeddings reutilizables\n",
    "- 10-100x más lento que SBERT\n",
    "- Límite de tokens: máximo ~512 tokens (A + B combinados)\n",
    "\n",
    "#### Estrategia Híbrida Recomendada\n",
    "\n",
    "Pipeline de 2 etapas:\n",
    "\n",
    "```python\n",
    "# Etapa 1: Filtrado rápido con SBERT\n",
    "embeddings = sbert_model.encode(all_documents)\n",
    "top_100 = similarities.topk(100)\n",
    "\n",
    "# Etapa 2: Re-ranking preciso con Cross-Encoder\n",
    "pairs = [[query, doc] for doc in top_100_documents]\n",
    "scores = cross_encoder.predict(pairs)\n",
    "```\n",
    "\n",
    "**Beneficios**: Velocidad de SBERT + Precisión de Cross-Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a21b43",
   "metadata": {},
   "source": [
    "### SBERT (Sentence-BERT) - Similitud Semántica con Embeddings\n",
    "\n",
    "SBERT convierte cada texto en un vector numérico (embedding) en un espacio semántico. Luego compara esos vectores usando la similitud del coseno.\n",
    "\n",
    "Valores cercanos a 1 indican textos muy similares.\n",
    "\n",
    "**Referencia**: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32101e9",
   "metadata": {},
   "source": [
    "### Análisis Comparativo de Algoritmos\n",
    "\n",
    "Este análisis compara el rendimiento de 6 algoritmos de similitud textual.\n",
    "\n",
    "#### Categorías de Algoritmos\n",
    "\n",
    "**Algoritmos Clásicos (No supervisados)**:\n",
    "- **Levenshtein**: Distancia de edición a nivel de caracteres\n",
    "- **TF-IDF Coseno**: Similitud vectorial con ponderación de términos\n",
    "- **Jaccard**: Intersección de conjuntos de palabras\n",
    "- **Euclidiana**: Distancia geométrica en espacio vectorial\n",
    "\n",
    "**Algoritmos Basados en IA (Supervisados)**:\n",
    "- **SBERT**: Embeddings semánticos con Sentence-BERT\n",
    "- **Cross-Encoder**: Evaluación directa de pares de texto\n",
    "\n",
    "#### Comparación de Características\n",
    "\n",
    "| Algoritmo | Velocidad | Precisión | Semántica | Escalabilidad |\n",
    "|-----------|-----------|-----------|-----------|---------------|\n",
    "| **Levenshtein** | Muy alta | Baja | No | Alta |\n",
    "| **TF-IDF Coseno** | Alta | Media | No | Alta |\n",
    "| **Jaccard** | Muy alta | Baja | No | Alta |\n",
    "| **Euclidiana** | Alta | Baja | No | Alta |\n",
    "| **SBERT** | Media | Alta | Sí | Media |\n",
    "| **Cross-Encoder** | Baja | Muy alta | Sí | Baja |\n",
    "\n",
    "#### Guía de Selección\n",
    "\n",
    "**Detección de duplicados exactos**: Levenshtein o Jaccard\n",
    "- Rápidos, detectan copias casi exactas\n",
    "\n",
    "**Búsqueda en base de datos (10K+ documentos)**: TF-IDF Coseno\n",
    "- Balance velocidad/precisión, escalable\n",
    "\n",
    "**Recomendación de artículos similares**: SBERT\n",
    "- Captura similitud semántica, embeddings reutilizables\n",
    "\n",
    "**Ranking de relevancia (Top-K)**: SBERT + Cross-Encoder\n",
    "- SBERT para filtrado, Cross-Encoder para re-ranking\n",
    "\n",
    "**Análisis exploratorio rápido**: Jaccard o TF-IDF\n",
    "- Implementación simple, resultados inmediatos\n",
    "\n",
    "#### Pipeline Híbrido Recomendado\n",
    "\n",
    "```python\n",
    "# Paso 1: Filtrado rápido con TF-IDF\n",
    "tfidf_sim = cosine_similarity(tfidf_matrix)\n",
    "candidates = tfidf_sim > 0.3\n",
    "\n",
    "# Paso 2: Validación semántica con SBERT\n",
    "sbert_embeddings = model.encode(candidate_pairs)\n",
    "sbert_sim = util.cos_sim(sbert_embeddings)\n",
    "final_similar = sbert_sim > 0.7\n",
    "\n",
    "# Paso 3 (Opcional): Re-ranking con Cross-Encoder\n",
    "cross_scores = cross_encoder.predict(final_pairs)\n",
    "```\n",
    "\n",
    "**Justificación**:\n",
    "- TF-IDF elimina pares obviamente diferentes (rápido)\n",
    "- SBERT valida similitud semántica (preciso)\n",
    "- Cross-Encoder proporciona ranking final de alta calidad (opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc085077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similitud SBERT: 0.189\n",
      "Interpretación: Los textos no son similares.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "\n",
    "# --- Textos (abstracts) ---\n",
    "texts = [abstracts[0], abstracts[1]]  # puedes cambiar a tus abstracts\n",
    "\n",
    "# --- Cargar modelo SBERT ---\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# --- Obtener embeddings ---\n",
    "embeddings = model.encode(texts, convert_to_tensor=True)\n",
    "\n",
    "# --- Calcular similitud de coseno ---\n",
    "similarity = util.cos_sim(embeddings[0], embeddings[1]).item()\n",
    "\n",
    "# --- Interpretar el resultado ---\n",
    "if similarity >= 0.8:\n",
    "    interpretation = \"Los textos son muy similares.\"\n",
    "elif similarity >= 0.5:\n",
    "    interpretation = \"Los textos tienen similitud moderada.\"\n",
    "elif similarity >= 0.2:\n",
    "    interpretation = \"Los textos son poco similares.\"\n",
    "else:\n",
    "    interpretation = \"Los textos no son similares.\"\n",
    "\n",
    "print(f\"Similitud SBERT: {similarity:.3f}\")\n",
    "print(f\"Interpretación: {interpretation}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e29acab",
   "metadata": {},
   "source": [
    "### Cross-Encoder - Evaluación Directa de Pares de Texto\n",
    "\n",
    "El modelo recibe ambos textos juntos y predice una puntuación directa de similitud. Se entrena para entender la relación entre oraciones, no solo las palabras.\n",
    "\n",
    "**Fórmula**: f([A;B]) es la representación conjunta del par dentro del transformer.\n",
    "\n",
    "**Referencia**: https://huggingface.co/cross-encoder/ms-marco-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bb0410f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Puntaje Cross-Encoder: -6.766\n",
      "Probabilidad normalizada: 0.001\n",
      "Interpretación: Los textos no son similares.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "import numpy as np\n",
    "\n",
    "# --- Cargar modelo ---\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "# --- Evaluar similitud directa entre dos textos ---\n",
    "pair = [[abstracts[0], abstracts[1]]]\n",
    "score = cross_encoder.predict(pair)[0]\n",
    "\n",
    "# --- Normalizar (si el score no está entre 0 y 1) ---\n",
    "prob = 1 / (1 + np.exp(-score)) if score > 1 or score < 0 else score\n",
    "\n",
    "# --- Interpretación ---\n",
    "if prob >= 0.8:\n",
    "    interpretation = \"Los textos son muy similares.\"\n",
    "elif prob >= 0.5:\n",
    "    interpretation = \"Los textos tienen similitud moderada.\"\n",
    "elif prob >= 0.2:\n",
    "    interpretation = \"Los textos son poco similares.\"\n",
    "else:\n",
    "    interpretation = \"Los textos no son similares.\"\n",
    "\n",
    "print(f\"Puntaje Cross-Encoder: {score:.3f}\")\n",
    "print(f\"Probabilidad normalizada: {prob:.3f}\")\n",
    "print(f\"Interpretación: {interpretation}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
