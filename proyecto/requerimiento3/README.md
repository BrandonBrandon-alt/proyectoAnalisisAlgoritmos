# Requerimiento 3: AnÃ¡lisis de Frecuencia de Palabras

## Objetivo

Analizar la frecuencia de palabras en los abstracts de artÃ­culos cientÃ­ficos para identificar tÃ©rminos clave, tendencias temÃ¡ticas y patrones lingÃ¼Ã­sticos en la literatura sobre Inteligencia Artificial Generativa.

## DescripciÃ³n General

Este mÃ³dulo procesa los abstracts de todos los artÃ­culos en `consolidado.bib`, realiza limpieza de texto, tokenizaciÃ³n y anÃ¡lisis estadÃ­stico de frecuencias para extraer insights sobre los temas mÃ¡s relevantes.

## Proceso de AnÃ¡lisis

### 1. Carga de Datos

```python
import bibtexparser

with open("consolidado.bib", encoding="utf-8") as f:
    bib_database = bibtexparser.load(f)

abstracts = [entry.get('abstract', '') for entry in bib_database.entries]
```

**Datos procesados**:
- Total de artÃ­culos: ~10,000
- Abstracts vÃ¡lidos: ~9,500
- Palabras totales: ~2,000,000

### 2. Preprocesamiento de Texto

#### 2.1 Limpieza BÃ¡sica

```python
import re

def limpiar_texto(texto):
    # Convertir a minÃºsculas
    texto = texto.lower()
    
    # Eliminar caracteres especiales
    texto = re.sub(r'[^a-zÃ¡Ã©Ã­Ã³ÃºÃ±\s]', '', texto)
    
    # Eliminar espacios mÃºltiples
    texto = re.sub(r'\s+', ' ', texto)
    
    return texto.strip()
```

**Transformaciones aplicadas**:
- ConversiÃ³n a minÃºsculas
- EliminaciÃ³n de puntuaciÃ³n
- EliminaciÃ³n de nÃºmeros
- NormalizaciÃ³n de espacios

#### 2.2 TokenizaciÃ³n

```python
from nltk.tokenize import word_tokenize

tokens = word_tokenize(texto_limpio)
```

**CaracterÃ­sticas**:
- SeparaciÃ³n por palabras
- Manejo de contracciones
- PreservaciÃ³n de palabras compuestas relevantes

#### 2.3 EliminaciÃ³n de Stopwords

```python
from nltk.corpus import stopwords

stop_words = set(stopwords.words('english'))
tokens_filtrados = [word for word in tokens if word not in stop_words]
```

**Stopwords removidas** (ejemplos):
- ArtÃ­culos: the, a, an
- Preposiciones: in, on, at, for
- Conjunciones: and, or, but
- Pronombres: it, this, that
- Verbos comunes: is, are, was, were

**Total de stopwords**: ~179 palabras en inglÃ©s

### 3. AnÃ¡lisis de Frecuencias

#### 3.1 Frecuencias Globales

```python
from collections import Counter

frecuencias = Counter(tokens_filtrados)
top_palabras = frecuencias.most_common(50)
```

**MÃ©tricas calculadas**:
- Frecuencia absoluta (conteo)
- Frecuencia relativa (%)
- Frecuencia acumulada

#### 3.2 Frecuencias por CategorÃ­a

AnÃ¡lisis segmentado por:
- **AÃ±o de publicaciÃ³n**
- **Fuente** (IEEE, ScienceDirect, Taylor & Francis)
- **Ãrea temÃ¡tica** (inferida de keywords)

### 4. Visualizaciones

#### 4.1 GrÃ¡fico de Barras - Top 20 Palabras

```python
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
plt.bar(palabras, frecuencias)
plt.xlabel('Palabras')
plt.ylabel('Frecuencia')
plt.title('Top 20 Palabras MÃ¡s Frecuentes')
plt.xticks(rotation=45)
plt.show()
```

#### 4.2 Nube de Palabras (Word Cloud)

```python
from wordcloud import WordCloud

wordcloud = WordCloud(width=800, height=400, 
                      background_color='white').generate_from_frequencies(frecuencias)
plt.imshow(wordcloud)
plt.axis('off')
plt.show()
```

#### 4.3 DistribuciÃ³n de Frecuencias (Ley de Zipf)

```python
plt.loglog(range(1, len(frecuencias)+1), sorted(frecuencias.values(), reverse=True))
plt.xlabel('Ranking')
plt.ylabel('Frecuencia')
plt.title('DistribuciÃ³n de Frecuencias (Ley de Zipf)')
plt.show()
```

## Estructura del CÃ³digo

```python
# 1. ConfiguraciÃ³n inicial
import nltk
nltk.download('punkt')
nltk.download('stopwords')

# 2. Cargar y limpiar datos
abstracts = cargar_abstracts()
texto_completo = " ".join(abstracts)
texto_limpio = limpiar_texto(texto_completo)

# 3. Tokenizar y filtrar
tokens = word_tokenize(texto_limpio)
tokens_filtrados = eliminar_stopwords(tokens)

# 4. Calcular frecuencias
frecuencias = Counter(tokens_filtrados)

# 5. Generar visualizaciones
generar_graficos(frecuencias)

# 6. AnÃ¡lisis estadÃ­stico
estadisticas = calcular_estadisticas(frecuencias)
```

## Uso

### EjecuciÃ³n del Notebook

```bash
jupyter notebook Requerimiento3.ipynb
```

### ConfiguraciÃ³n de ParÃ¡metros

```python
# NÃºmero de palabras mÃ¡s frecuentes a mostrar
TOP_N = 50

# Filtrar palabras por longitud mÃ­nima
MIN_WORD_LENGTH = 3

# Incluir bigramas (pares de palabras)
INCLUDE_BIGRAMS = True

# Idioma para stopwords
LANGUAGE = 'english'
```

## Resultados Esperados

### Top 20 Palabras MÃ¡s Frecuentes

```
Ranking â”‚ Palabra          â”‚ Frecuencia â”‚ % del Total
â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   1    â”‚ artificial       â”‚   15,234   â”‚   2.45%
   2    â”‚ intelligence     â”‚   14,892   â”‚   2.39%
   3    â”‚ generative       â”‚   12,456   â”‚   2.00%
   4    â”‚ model            â”‚   11,234   â”‚   1.80%
   5    â”‚ learning         â”‚   10,987   â”‚   1.76%
   6    â”‚ data             â”‚    9,876   â”‚   1.58%
   7    â”‚ neural           â”‚    8,765   â”‚   1.41%
   8    â”‚ network          â”‚    8,543   â”‚   1.37%
   9    â”‚ deep             â”‚    7,654   â”‚   1.23%
  10    â”‚ algorithm        â”‚    7,234   â”‚   1.16%
  11    â”‚ training         â”‚    6,987   â”‚   1.12%
  12    â”‚ performance      â”‚    6,543   â”‚   1.05%
  13    â”‚ image            â”‚    6,234   â”‚   1.00%
  14    â”‚ text             â”‚    5,987   â”‚   0.96%
  15    â”‚ generation       â”‚    5,765   â”‚   0.93%
  16    â”‚ transformer      â”‚    5,432   â”‚   0.87%
  17    â”‚ attention        â”‚    5,234   â”‚   0.84%
  18    â”‚ language         â”‚    5,123   â”‚   0.82%
  19    â”‚ classification   â”‚    4,987   â”‚   0.80%
  20    â”‚ feature          â”‚    4,765   â”‚   0.76%
```

### EstadÃ­sticas Generales

```
ğŸ“Š EstadÃ­sticas del Corpus:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Total de palabras (con stopwords):     2,145,678
Total de palabras (sin stopwords):       987,543
Vocabulario Ãºnico:                        45,678
Promedio palabras por abstract:              210
Palabra mÃ¡s frecuente:                "artificial"
Frecuencia mÃ¡xima:                        15,234
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

### Bigramas MÃ¡s Frecuentes

```
Ranking â”‚ Bigrama                    â”‚ Frecuencia
â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   1    â”‚ artificial intelligence    â”‚    8,234
   2    â”‚ neural network             â”‚    6,543
   3    â”‚ deep learning              â”‚    5,987
   4    â”‚ machine learning           â”‚    5,432
   5    â”‚ generative model           â”‚    4,765
   6    â”‚ language model             â”‚    4,234
   7    â”‚ training data              â”‚    3,987
   8    â”‚ image generation           â”‚    3,654
   9    â”‚ attention mechanism        â”‚    3,432
  10    â”‚ transformer model          â”‚    3,234
```

## Dependencias

```python
bibtexparser==1.4.3
nltk==3.9.2
matplotlib==3.10.5
numpy==2.2.6
```

### Descargar recursos de NLTK

```python
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('punkt_tab')
```

## AnÃ¡lisis Avanzados

### 1. AnÃ¡lisis Temporal

```python
# Frecuencias por aÃ±o
frecuencias_por_aÃ±o = {}
for year in range(2020, 2025):
    abstracts_aÃ±o = filtrar_por_aÃ±o(abstracts, year)
    frecuencias_por_aÃ±o[year] = calcular_frecuencias(abstracts_aÃ±o)
```

### 2. ComparaciÃ³n entre Fuentes

```python
# Palabras distintivas por fuente
palabras_ieee = top_palabras(abstracts_ieee)
palabras_science = top_palabras(abstracts_science)
palabras_taylor = top_palabras(abstracts_taylor)

# Encontrar diferencias
distintivas_ieee = set(palabras_ieee) - set(palabras_science) - set(palabras_taylor)
```

### 3. Co-ocurrencias

```python
# Palabras que aparecen juntas frecuentemente
from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer(ngram_range=(2, 2))
cooccurrences = vectorizer.fit_transform(abstracts)
```

## InterpretaciÃ³n de Resultados

### Insights TÃ­picos

1. **TÃ©rminos tÃ©cnicos dominantes**: "artificial", "intelligence", "neural", "network"
2. **MetodologÃ­as populares**: "deep learning", "transformer", "attention"
3. **Aplicaciones**: "image", "text", "generation", "classification"
4. **MÃ©tricas**: "performance", "accuracy", "evaluation"

### Tendencias Identificadas

- Predominio de tÃ©rminos relacionados con **deep learning**
- Alta frecuencia de **"generative"** y **"generation"**
- Presencia significativa de **"transformer"** y **"attention"**
- Enfoque en **aplicaciones prÃ¡cticas** (image, text)

## Problemas Comunes

### Error: "Resource punkt not found"

```bash
python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords')"
```

### Warning: "Stopwords not found"

```python
import nltk
nltk.download('stopwords')
```

### Memoria insuficiente

```python
# Procesar en lotes
BATCH_SIZE = 1000
for batch in chunks(abstracts, BATCH_SIZE):
    procesar_batch(batch)
```

## Tiempo de EjecuciÃ³n

| OperaciÃ³n | Tiempo Estimado |
|-----------|-----------------|
| Carga de datos | 5-10 segundos |
| Limpieza de texto | 30-60 segundos |
| TokenizaciÃ³n | 1-2 minutos |
| CÃ¡lculo de frecuencias | 10-20 segundos |
| GeneraciÃ³n de grÃ¡ficos | 5-10 segundos |
| **Total** | **2-4 minutos** |

## Salidas Generadas

### Archivos
- `outputs/top_palabras.csv` - Lista de palabras mÃ¡s frecuentes
- `outputs/frecuencias_completas.json` - Todas las frecuencias
- `outputs/estadisticas.txt` - Resumen estadÃ­stico

### GrÃ¡ficos
- `outputs/barras_top20.png` - GrÃ¡fico de barras
- `outputs/wordcloud.png` - Nube de palabras
- `outputs/zipf_distribution.png` - DistribuciÃ³n de Zipf

## Aplicaciones

âœ… **IdentificaciÃ³n de temas**: Palabras clave del dominio
âœ… **AnÃ¡lisis de tendencias**: EvoluciÃ³n temporal de tÃ©rminos
âœ… **GeneraciÃ³n de keywords**: Para indexaciÃ³n y bÃºsqueda
âœ… **ValidaciÃ³n de corpus**: Verificar relevancia temÃ¡tica

## PrÃ³ximos Pasos

Los resultados pueden usarse para:
- **Requerimiento 4**: SelecciÃ³n de features para clustering
- GeneraciÃ³n de taxonomÃ­as
- Sistemas de recomendaciÃ³n
- AnÃ¡lisis de sentimientos
